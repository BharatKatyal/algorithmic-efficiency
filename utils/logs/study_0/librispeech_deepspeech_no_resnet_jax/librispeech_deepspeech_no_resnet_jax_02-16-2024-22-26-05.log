python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech_no_resnet --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/imagenet_resnet/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --num_tuning_trials=1 --rng_seed=3377931912 --max_global_steps=48000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_no_resnet_jax_02-16-2024-22-26-05.log
I0216 22:26:26.431108 139627123971904 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_no_resnet_jax.
I0216 22:26:27.417730 139627123971904 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0216 22:26:27.418415 139627123971904 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0216 22:26:27.418560 139627123971904 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0216 22:26:27.425135 139627123971904 submission_runner.py:542] Using RNG seed 3377931912
I0216 22:26:28.509205 139627123971904 submission_runner.py:551] --- Tuning run 1/1 ---
I0216 22:26:28.509395 139627123971904 submission_runner.py:556] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_no_resnet_jax/trial_1.
I0216 22:26:28.509772 139627123971904 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_no_resnet_jax/trial_1/hparams.json.
I0216 22:26:28.690985 139627123971904 submission_runner.py:206] Initializing dataset.
I0216 22:26:28.691196 139627123971904 submission_runner.py:213] Initializing model.
I0216 22:26:31.029294 139627123971904 submission_runner.py:255] Initializing optimizer.
I0216 22:26:31.695800 139627123971904 submission_runner.py:262] Initializing metrics bundle.
I0216 22:26:31.695994 139627123971904 submission_runner.py:280] Initializing checkpoint and logger.
I0216 22:26:31.696919 139627123971904 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_no_resnet_jax/trial_1 with prefix checkpoint_
I0216 22:26:31.697063 139627123971904 submission_runner.py:300] Saving meta data to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_no_resnet_jax/trial_1/meta_data_0.json.
I0216 22:26:31.697270 139627123971904 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0216 22:26:31.697330 139627123971904 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0216 22:26:31.966911 139627123971904 logger_utils.py:220] Unable to record git information. Continuing without it.
I0216 22:26:32.212667 139627123971904 submission_runner.py:304] Saving flags to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_no_resnet_jax/trial_1/flags_0.json.
I0216 22:26:32.225636 139627123971904 submission_runner.py:314] Starting training loop.
I0216 22:26:32.522961 139627123971904 input_pipeline.py:20] Loading split = train-clean-100
I0216 22:26:32.575170 139627123971904 input_pipeline.py:20] Loading split = train-clean-360
I0216 22:26:32.732936 139627123971904 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0216 22:27:14.119730 139462117947136 logging_writer.py:48] [0] global_step=0, grad_norm=16.433698654174805, loss=34.96839141845703
I0216 22:27:14.152553 139627123971904 spec.py:321] Evaluating on the training split.
I0216 22:27:14.401656 139627123971904 input_pipeline.py:20] Loading split = train-clean-100
I0216 22:27:14.436519 139627123971904 input_pipeline.py:20] Loading split = train-clean-360
I0216 22:27:14.789831 139627123971904 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0216 22:28:04.429669 139627123971904 spec.py:333] Evaluating on the validation split.
I0216 22:28:04.621939 139627123971904 input_pipeline.py:20] Loading split = dev-clean
I0216 22:28:04.627369 139627123971904 input_pipeline.py:20] Loading split = dev-other
I0216 22:28:56.796293 139627123971904 spec.py:349] Evaluating on the test split.
I0216 22:28:56.991991 139627123971904 input_pipeline.py:20] Loading split = test-clean
I0216 22:29:26.597599 139627123971904 submission_runner.py:408] Time since start: 174.37s, 	Step: 1, 	{'train/ctc_loss': Array(31.423384, dtype=float32), 'train/wer': 0.9438654394769173, 'validation/ctc_loss': Array(30.389412, dtype=float32), 'validation/wer': 0.8970427797677091, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.538706, dtype=float32), 'test/wer': 0.8997826660979424, 'test/num_examples': 2472, 'score': 41.92684984207153, 'total_duration': 174.3698627948761, 'accumulated_submission_time': 41.92684984207153, 'accumulated_eval_time': 132.44295573234558, 'accumulated_logging_time': 0}
I0216 22:29:26.623303 139456615020288 logging_writer.py:48] [1] accumulated_eval_time=132.442956, accumulated_logging_time=0, accumulated_submission_time=41.926850, global_step=1, preemption_count=0, score=41.926850, test/ctc_loss=30.538705825805664, test/num_examples=2472, test/wer=0.899783, total_duration=174.369863, train/ctc_loss=31.423383712768555, train/wer=0.943865, validation/ctc_loss=30.38941192626953, validation/num_examples=5348, validation/wer=0.897043
I0216 22:29:35.482416 139469192075008 logging_writer.py:48] [1] global_step=1, grad_norm=16.42439079284668, loss=34.59178924560547
I0216 22:29:36.307138 139469200467712 logging_writer.py:48] [2] global_step=2, grad_norm=25.83055305480957, loss=31.842092514038086
I0216 22:29:37.106308 139469192075008 logging_writer.py:48] [3] global_step=3, grad_norm=21.411251068115234, loss=19.11958122253418
I0216 22:29:37.911408 139469200467712 logging_writer.py:48] [4] global_step=4, grad_norm=10.993600845336914, loss=10.27441120147705
I0216 22:29:38.782307 139469192075008 logging_writer.py:48] [5] global_step=5, grad_norm=18.91133689880371, loss=11.446022033691406
I0216 22:29:39.680058 139469200467712 logging_writer.py:48] [6] global_step=6, grad_norm=9.147826194763184, loss=8.56468391418457
I0216 22:29:40.569370 139469192075008 logging_writer.py:48] [7] global_step=7, grad_norm=9.032204627990723, loss=10.270830154418945
I0216 22:29:41.452682 139469200467712 logging_writer.py:48] [8] global_step=8, grad_norm=18.77117156982422, loss=9.564854621887207
I0216 22:29:42.340051 139469192075008 logging_writer.py:48] [9] global_step=9, grad_norm=17.66107177734375, loss=9.328760147094727
I0216 22:29:43.227827 139469200467712 logging_writer.py:48] [10] global_step=10, grad_norm=6.475497722625732, loss=9.26017951965332
I0216 22:29:44.084413 139469192075008 logging_writer.py:48] [11] global_step=11, grad_norm=5.068875312805176, loss=8.581064224243164
I0216 22:29:44.963340 139469200467712 logging_writer.py:48] [12] global_step=12, grad_norm=3.3532772064208984, loss=8.102139472961426
I0216 22:29:45.826578 139469192075008 logging_writer.py:48] [13] global_step=13, grad_norm=8.480804443359375, loss=7.802709579467773
I0216 22:29:46.701626 139469200467712 logging_writer.py:48] [14] global_step=14, grad_norm=9.612896919250488, loss=8.182215690612793
I0216 22:29:47.552705 139469192075008 logging_writer.py:48] [15] global_step=15, grad_norm=3.4136438369750977, loss=7.587283134460449
I0216 22:29:48.442278 139469200467712 logging_writer.py:48] [16] global_step=16, grad_norm=2.3302206993103027, loss=7.563858985900879
I0216 22:29:49.302537 139469192075008 logging_writer.py:48] [17] global_step=17, grad_norm=6.798856735229492, loss=7.455060958862305
I0216 22:29:50.168290 139469200467712 logging_writer.py:48] [18] global_step=18, grad_norm=1.904589295387268, loss=7.001056671142578
I0216 22:29:51.032317 139469192075008 logging_writer.py:48] [19] global_step=19, grad_norm=3.160351276397705, loss=6.99805212020874
I0216 22:29:51.911279 139469200467712 logging_writer.py:48] [20] global_step=20, grad_norm=2.4831550121307373, loss=7.296655178070068
I0216 22:29:52.783607 139469192075008 logging_writer.py:48] [21] global_step=21, grad_norm=2.3390793800354004, loss=6.769186496734619
I0216 22:29:53.651319 139469200467712 logging_writer.py:48] [22] global_step=22, grad_norm=3.4274537563323975, loss=6.654068946838379
I0216 22:29:54.512881 139469192075008 logging_writer.py:48] [23] global_step=23, grad_norm=6.06984281539917, loss=6.782182216644287
I0216 22:29:55.393221 139469200467712 logging_writer.py:48] [24] global_step=24, grad_norm=9.745697021484375, loss=7.103240966796875
I0216 22:29:56.262529 139469192075008 logging_writer.py:48] [25] global_step=25, grad_norm=7.199565410614014, loss=6.992171287536621
I0216 22:29:57.135654 139469200467712 logging_writer.py:48] [26] global_step=26, grad_norm=1.0347682237625122, loss=6.453489303588867
I0216 22:29:58.000073 139469192075008 logging_writer.py:48] [27] global_step=27, grad_norm=1.272550106048584, loss=6.306225776672363
I0216 22:29:58.868058 139469200467712 logging_writer.py:48] [28] global_step=28, grad_norm=1.9803054332733154, loss=6.291670799255371
I0216 22:29:59.731061 139469192075008 logging_writer.py:48] [29] global_step=29, grad_norm=1.4557743072509766, loss=6.2132697105407715
I0216 22:30:00.606705 139469200467712 logging_writer.py:48] [30] global_step=30, grad_norm=1.3517134189605713, loss=6.166342258453369
I0216 22:30:01.461013 139469192075008 logging_writer.py:48] [31] global_step=31, grad_norm=1.773324728012085, loss=6.138496398925781
I0216 22:30:02.348143 139469200467712 logging_writer.py:48] [32] global_step=32, grad_norm=3.5305254459381104, loss=6.193609237670898
I0216 22:30:03.218207 139469192075008 logging_writer.py:48] [33] global_step=33, grad_norm=4.852303504943848, loss=6.261102676391602
I0216 22:30:04.083781 139469200467712 logging_writer.py:48] [34] global_step=34, grad_norm=10.540189743041992, loss=6.417848110198975
I0216 22:30:04.944768 139469192075008 logging_writer.py:48] [35] global_step=35, grad_norm=9.038674354553223, loss=7.029842376708984
I0216 22:30:05.811814 139469200467712 logging_writer.py:48] [36] global_step=36, grad_norm=3.9262702465057373, loss=6.387768745422363
I0216 22:30:06.672595 139469192075008 logging_writer.py:48] [37] global_step=37, grad_norm=3.5045289993286133, loss=6.244156360626221
I0216 22:30:07.532139 139469200467712 logging_writer.py:48] [38] global_step=38, grad_norm=4.831857681274414, loss=6.26997184753418
I0216 22:30:08.392167 139469192075008 logging_writer.py:48] [39] global_step=39, grad_norm=3.2934658527374268, loss=6.208630561828613
I0216 22:30:09.278707 139469200467712 logging_writer.py:48] [40] global_step=40, grad_norm=1.379768967628479, loss=6.1109161376953125
I0216 22:30:10.136990 139469192075008 logging_writer.py:48] [41] global_step=41, grad_norm=0.8005102276802063, loss=6.089946269989014
I0216 22:30:10.998428 139469200467712 logging_writer.py:48] [42] global_step=42, grad_norm=0.73615962266922, loss=6.095766067504883
I0216 22:30:11.856023 139469192075008 logging_writer.py:48] [43] global_step=43, grad_norm=1.1885511875152588, loss=6.061703681945801
I0216 22:30:12.718990 139469200467712 logging_writer.py:48] [44] global_step=44, grad_norm=3.2115910053253174, loss=6.0861735343933105
I0216 22:30:13.589590 139469192075008 logging_writer.py:48] [45] global_step=45, grad_norm=4.887994289398193, loss=6.189417839050293
I0216 22:30:14.457543 139469200467712 logging_writer.py:48] [46] global_step=46, grad_norm=7.4143290519714355, loss=6.295450687408447
I0216 22:30:15.320836 139469192075008 logging_writer.py:48] [47] global_step=47, grad_norm=5.772371292114258, loss=6.372877597808838
I0216 22:30:16.199234 139469200467712 logging_writer.py:48] [48] global_step=48, grad_norm=5.408802032470703, loss=8.122586250305176
I0216 22:30:17.083611 139469192075008 logging_writer.py:48] [49] global_step=49, grad_norm=2.5403754711151123, loss=6.557003498077393
I0216 22:30:17.952293 139469200467712 logging_writer.py:48] [50] global_step=50, grad_norm=6.075437068939209, loss=6.4058356285095215
I0216 22:30:18.817079 139469192075008 logging_writer.py:48] [51] global_step=51, grad_norm=7.731095790863037, loss=6.818264484405518
I0216 22:30:19.682157 139469200467712 logging_writer.py:48] [52] global_step=52, grad_norm=11.77502155303955, loss=6.945730209350586
I0216 22:30:20.541897 139469192075008 logging_writer.py:48] [53] global_step=53, grad_norm=6.672048568725586, loss=7.116059303283691
I0216 22:30:21.410437 139469200467712 logging_writer.py:48] [54] global_step=54, grad_norm=1.6711348295211792, loss=6.422834873199463
I0216 22:30:22.302011 139469192075008 logging_writer.py:48] [55] global_step=55, grad_norm=2.31050968170166, loss=6.401773929595947
I0216 22:30:23.198699 139469200467712 logging_writer.py:48] [56] global_step=56, grad_norm=1.6142877340316772, loss=6.275791168212891
I0216 22:30:24.061109 139469192075008 logging_writer.py:48] [57] global_step=57, grad_norm=2.0007503032684326, loss=6.225064277648926
I0216 22:30:24.928060 139469200467712 logging_writer.py:48] [58] global_step=58, grad_norm=2.454458713531494, loss=6.207278728485107
I0216 22:30:25.788024 139469192075008 logging_writer.py:48] [59] global_step=59, grad_norm=4.877885341644287, loss=6.364703178405762
I0216 22:30:26.665505 139469200467712 logging_writer.py:48] [60] global_step=60, grad_norm=5.535561561584473, loss=6.553879737854004
I0216 22:30:27.537042 139469192075008 logging_writer.py:48] [61] global_step=61, grad_norm=7.184309005737305, loss=6.600337505340576
I0216 22:30:28.422667 139469200467712 logging_writer.py:48] [62] global_step=62, grad_norm=6.203229904174805, loss=6.777904033660889
I0216 22:30:29.325411 139469192075008 logging_writer.py:48] [63] global_step=63, grad_norm=3.509042739868164, loss=6.426598072052002
I0216 22:30:30.193046 139469200467712 logging_writer.py:48] [64] global_step=64, grad_norm=2.1132993698120117, loss=6.306970596313477
I0216 22:30:31.063768 139469192075008 logging_writer.py:48] [65] global_step=65, grad_norm=2.2018637657165527, loss=6.2114691734313965
I0216 22:30:31.923149 139469200467712 logging_writer.py:48] [66] global_step=66, grad_norm=1.4012881517410278, loss=6.112683296203613
I0216 22:30:32.786895 139469192075008 logging_writer.py:48] [67] global_step=67, grad_norm=0.8188465237617493, loss=6.1061835289001465
I0216 22:30:33.654530 139469200467712 logging_writer.py:48] [68] global_step=68, grad_norm=0.774962306022644, loss=6.039463996887207
I0216 22:30:34.539078 139469192075008 logging_writer.py:48] [69] global_step=69, grad_norm=0.5702271461486816, loss=6.023599624633789
I0216 22:30:35.426992 139469200467712 logging_writer.py:48] [70] global_step=70, grad_norm=1.0124077796936035, loss=6.013998031616211
I0216 22:30:36.301776 139469192075008 logging_writer.py:48] [71] global_step=71, grad_norm=0.6867088675498962, loss=5.993775367736816
I0216 22:30:37.170670 139469200467712 logging_writer.py:48] [72] global_step=72, grad_norm=1.0346453189849854, loss=5.968673229217529
I0216 22:30:38.035984 139469192075008 logging_writer.py:48] [73] global_step=73, grad_norm=1.3992645740509033, loss=5.964745044708252
I0216 22:30:38.913301 139469200467712 logging_writer.py:48] [74] global_step=74, grad_norm=1.4953336715698242, loss=5.95772647857666
I0216 22:30:39.775437 139469192075008 logging_writer.py:48] [75] global_step=75, grad_norm=1.991573691368103, loss=5.998626708984375
I0216 22:30:40.659600 139469200467712 logging_writer.py:48] [76] global_step=76, grad_norm=2.8669683933258057, loss=6.039524555206299
I0216 22:30:41.519350 139469192075008 logging_writer.py:48] [77] global_step=77, grad_norm=3.867077112197876, loss=6.109192848205566
I0216 22:30:42.392498 139469200467712 logging_writer.py:48] [78] global_step=78, grad_norm=4.292879581451416, loss=6.2407073974609375
I0216 22:30:43.260987 139469192075008 logging_writer.py:48] [79] global_step=79, grad_norm=3.8907597064971924, loss=6.148905277252197
I0216 22:30:44.137525 139469200467712 logging_writer.py:48] [80] global_step=80, grad_norm=2.564263105392456, loss=6.04304313659668
I0216 22:30:44.998525 139469192075008 logging_writer.py:48] [81] global_step=81, grad_norm=1.2564185857772827, loss=5.96529483795166
I0216 22:30:45.887143 139469200467712 logging_writer.py:48] [82] global_step=82, grad_norm=0.5331295132637024, loss=5.920290470123291
I0216 22:30:46.769873 139469192075008 logging_writer.py:48] [83] global_step=83, grad_norm=0.29938197135925293, loss=5.914454936981201
I0216 22:30:47.635758 139469200467712 logging_writer.py:48] [84] global_step=84, grad_norm=0.24683061242103577, loss=5.87599515914917
I0216 22:30:48.498078 139469192075008 logging_writer.py:48] [85] global_step=85, grad_norm=0.4184277057647705, loss=5.886830806732178
I0216 22:30:49.371471 139469200467712 logging_writer.py:48] [86] global_step=86, grad_norm=0.22191859781742096, loss=5.885615825653076
I0216 22:30:50.238356 139469192075008 logging_writer.py:48] [87] global_step=87, grad_norm=0.23179170489311218, loss=5.8870744705200195
I0216 22:30:51.101196 139469200467712 logging_writer.py:48] [88] global_step=88, grad_norm=0.3707219362258911, loss=5.893309593200684
I0216 22:30:51.977529 139469192075008 logging_writer.py:48] [89] global_step=89, grad_norm=0.46534469723701477, loss=5.891932010650635
I0216 22:30:52.862581 139469200467712 logging_writer.py:48] [90] global_step=90, grad_norm=0.5173495411872864, loss=5.885505199432373
I0216 22:30:53.727869 139469192075008 logging_writer.py:48] [91] global_step=91, grad_norm=0.5339656472206116, loss=5.881113529205322
I0216 22:30:54.600887 139469200467712 logging_writer.py:48] [92] global_step=92, grad_norm=0.7730312943458557, loss=5.894143581390381
I0216 22:30:55.478084 139469192075008 logging_writer.py:48] [93] global_step=93, grad_norm=0.9696874618530273, loss=5.879056453704834
I0216 22:30:56.353978 139469200467712 logging_writer.py:48] [94] global_step=94, grad_norm=1.2812367677688599, loss=5.897899150848389
I0216 22:30:57.225612 139469192075008 logging_writer.py:48] [95] global_step=95, grad_norm=1.4464445114135742, loss=5.94431734085083
I0216 22:30:58.095746 139469200467712 logging_writer.py:48] [96] global_step=96, grad_norm=1.925577998161316, loss=5.930089950561523
I0216 22:30:58.987384 139469192075008 logging_writer.py:48] [97] global_step=97, grad_norm=2.425184488296509, loss=5.967389106750488
I0216 22:30:59.857537 139469200467712 logging_writer.py:48] [98] global_step=98, grad_norm=3.021998167037964, loss=6.00407075881958
I0216 22:31:00.724447 139469192075008 logging_writer.py:48] [99] global_step=99, grad_norm=3.0590980052948, loss=6.094973087310791
I0216 22:31:01.596279 139469200467712 logging_writer.py:48] [100] global_step=100, grad_norm=2.317629814147949, loss=5.978664398193359
I0216 22:35:59.842746 139469192075008 logging_writer.py:48] [500] global_step=500, grad_norm=65.52980041503906, loss=402.01568603515625
I0216 22:42:12.440962 139469200467712 logging_writer.py:48] [1000] global_step=1000, grad_norm=12.082469940185547, loss=1706.422119140625
I0216 22:48:27.664138 139471200118528 logging_writer.py:48] [1500] global_step=1500, grad_norm=6.932973519724328e-07, loss=1775.1766357421875
I0216 22:53:26.874549 139627123971904 spec.py:321] Evaluating on the training split.
I0216 22:54:02.195465 139627123971904 spec.py:333] Evaluating on the validation split.
I0216 22:54:45.490019 139627123971904 spec.py:349] Evaluating on the test split.
I0216 22:55:07.179815 139627123971904 submission_runner.py:408] Time since start: 1714.95s, 	Step: 1903, 	{'train/ctc_loss': Array(1767.6216, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(3356.9526, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.3506, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1482.096818447113, 'total_duration': 1714.94912648201, 'accumulated_submission_time': 1482.096818447113, 'accumulated_eval_time': 232.74323987960815, 'accumulated_logging_time': 0.04260540008544922}
I0216 22:55:07.214133 139471200118528 logging_writer.py:48] [1903] accumulated_eval_time=232.743240, accumulated_logging_time=0.042605, accumulated_submission_time=1482.096818, global_step=1903, preemption_count=0, score=1482.096818, test/ctc_loss=3189.3505859375, test/num_examples=2472, test/wer=0.899580, total_duration=1714.949126, train/ctc_loss=1767.62158203125, train/wer=0.944636, validation/ctc_loss=3356.95263671875, validation/num_examples=5348, validation/wer=0.896618
I0216 22:56:20.465202 139471191725824 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0, loss=1928.423583984375
I0216 23:02:35.826669 139470544758528 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.09196023643016815, loss=1781.74072265625
I0216 23:08:50.976401 139470536365824 logging_writer.py:48] [3000] global_step=3000, grad_norm=2.2494641595141486e-21, loss=1762.1021728515625
I0216 23:15:09.301288 139470544758528 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0, loss=1809.558349609375
I0216 23:19:07.756248 139627123971904 spec.py:321] Evaluating on the training split.
I0216 23:19:42.767117 139627123971904 spec.py:333] Evaluating on the validation split.
I0216 23:20:25.765826 139627123971904 spec.py:349] Evaluating on the test split.
I0216 23:20:47.602238 139627123971904 submission_runner.py:408] Time since start: 3255.37s, 	Step: 3822, 	{'train/ctc_loss': Array(1761.5707, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2922.555701494217, 'total_duration': 3255.370903491974, 'accumulated_submission_time': 2922.555701494217, 'accumulated_eval_time': 332.5835964679718, 'accumulated_logging_time': 0.0936136245727539}
I0216 23:20:47.636967 139469961078528 logging_writer.py:48] [3822] accumulated_eval_time=332.583596, accumulated_logging_time=0.093614, accumulated_submission_time=2922.555701, global_step=3822, preemption_count=0, score=2922.555701, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=3255.370903, train/ctc_loss=1761.5706787109375, train/wer=0.942722, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 23:23:00.854130 139469952685824 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0, loss=1800.0155029296875
I0216 23:29:16.187514 139469961078528 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0, loss=1780.9810791015625
I0216 23:35:30.953543 139469952685824 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1820.6259765625
I0216 23:41:53.783651 139469961078528 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.0158782550490784e-22, loss=1829.212158203125
I0216 23:44:47.634530 139627123971904 spec.py:321] Evaluating on the training split.
I0216 23:45:24.037945 139627123971904 spec.py:333] Evaluating on the validation split.
I0216 23:46:07.303896 139627123971904 spec.py:349] Evaluating on the test split.
I0216 23:46:29.182370 139627123971904 submission_runner.py:408] Time since start: 4796.95s, 	Step: 5734, 	{'train/ctc_loss': Array(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4362.468790531158, 'total_duration': 4796.950233697891, 'accumulated_submission_time': 4362.468790531158, 'accumulated_eval_time': 434.1249997615814, 'accumulated_logging_time': 0.14481449127197266}
I0216 23:46:29.215681 139470329718528 logging_writer.py:48] [5734] accumulated_eval_time=434.125000, accumulated_logging_time=0.144814, accumulated_submission_time=4362.468791, global_step=5734, preemption_count=0, score=4362.468791, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=4796.950234, train/ctc_loss=1741.2979736328125, train/wer=0.943324, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 23:49:48.050836 139470321325824 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1876.5445556640625
I0216 23:56:03.507268 139471200118528 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1726.6842041015625
I0217 00:02:16.315587 139471191725824 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.6193917196669645e-08, loss=1816.620849609375
I0217 00:08:41.023602 139470329718528 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1783.5869140625
I0217 00:10:29.218518 139627123971904 spec.py:321] Evaluating on the training split.
I0217 00:11:05.786269 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 00:11:48.892958 139627123971904 spec.py:349] Evaluating on the test split.
I0217 00:12:10.653216 139627123971904 submission_runner.py:408] Time since start: 6338.42s, 	Step: 7646, 	{'train/ctc_loss': Array(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 5802.390080213547, 'total_duration': 6338.421497344971, 'accumulated_submission_time': 5802.390080213547, 'accumulated_eval_time': 535.5537095069885, 'accumulated_logging_time': 0.19235849380493164}
I0217 00:12:10.694516 139471200118528 logging_writer.py:48] [7646] accumulated_eval_time=535.553710, accumulated_logging_time=0.192358, accumulated_submission_time=5802.390080, global_step=7646, preemption_count=0, score=5802.390080, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=6338.421497, train/ctc_loss=1724.861328125, train/wer=0.943700, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 00:16:34.604068 139471191725824 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.0, loss=1849.965576171875
I0217 00:22:52.185794 139471200118528 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0, loss=1850.2327880859375
I0217 00:29:06.402892 139471191725824 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0, loss=1814.3035888671875
I0217 00:35:36.170640 139471200118528 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.0, loss=1856.808837890625
I0217 00:36:11.010929 139627123971904 spec.py:321] Evaluating on the training split.
I0217 00:36:47.148402 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 00:37:30.507867 139627123971904 spec.py:349] Evaluating on the test split.
I0217 00:37:52.294461 139627123971904 submission_runner.py:408] Time since start: 7880.06s, 	Step: 9548, 	{'train/ctc_loss': Array(1832.9288, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7242.625440597534, 'total_duration': 7880.063672065735, 'accumulated_submission_time': 7242.625440597534, 'accumulated_eval_time': 636.8322005271912, 'accumulated_logging_time': 0.2474377155303955}
I0217 00:37:52.329955 139470401398528 logging_writer.py:48] [9548] accumulated_eval_time=636.832201, accumulated_logging_time=0.247438, accumulated_submission_time=7242.625441, global_step=9548, preemption_count=0, score=7242.625441, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=7880.063672, train/ctc_loss=1832.9288330078125, train/wer=0.941551, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 00:43:27.831819 139470393005824 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.0, loss=1789.8218994140625
I0217 00:49:55.854959 139470401398528 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.0, loss=1873.9346923828125
I0217 00:56:12.506654 139470393005824 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.0, loss=1872.837890625
I0217 01:01:52.908135 139627123971904 spec.py:321] Evaluating on the training split.
I0217 01:02:29.819931 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 01:03:12.956084 139627123971904 spec.py:349] Evaluating on the test split.
I0217 01:03:34.859181 139627123971904 submission_runner.py:408] Time since start: 9422.63s, 	Step: 11427, 	{'train/ctc_loss': Array(1752.8004, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 8683.113170146942, 'total_duration': 9422.627868413925, 'accumulated_submission_time': 8683.113170146942, 'accumulated_eval_time': 738.7778081893921, 'accumulated_logging_time': 0.30493879318237305}
I0217 01:03:34.897783 139469592430336 logging_writer.py:48] [11427] accumulated_eval_time=738.777808, accumulated_logging_time=0.304939, accumulated_submission_time=8683.113170, global_step=11427, preemption_count=0, score=8683.113170, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=9422.627868, train/ctc_loss=1752.8004150390625, train/wer=0.942641, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 01:04:29.894784 139469584037632 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.0, loss=1787.4473876953125
I0217 01:10:41.611714 139469592430336 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.0, loss=1817.523681640625
I0217 01:17:12.990195 139469592430336 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.0, loss=1884.8345947265625
I0217 01:23:28.137953 139469584037632 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.0, loss=1810.1981201171875
I0217 01:27:35.643102 139627123971904 spec.py:321] Evaluating on the training split.
I0217 01:28:12.233701 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 01:28:55.613410 139627123971904 spec.py:349] Evaluating on the test split.
I0217 01:29:17.841556 139627123971904 submission_runner.py:408] Time since start: 10965.61s, 	Step: 13305, 	{'train/ctc_loss': Array(1746.111, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 10123.776574373245, 'total_duration': 10965.610308885574, 'accumulated_submission_time': 10123.776574373245, 'accumulated_eval_time': 840.9707329273224, 'accumulated_logging_time': 0.3580210208892822}
I0217 01:29:17.881922 139469592430336 logging_writer.py:48] [13305] accumulated_eval_time=840.970733, accumulated_logging_time=0.358021, accumulated_submission_time=10123.776574, global_step=13305, preemption_count=0, score=10123.776574, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=10965.610309, train/ctc_loss=1746.1109619140625, train/wer=0.942824, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 01:31:46.665463 139468609390336 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.0, loss=1928.1331787109375
I0217 01:38:00.001366 139468600997632 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.0, loss=1816.87890625
I0217 01:44:40.983157 139468609390336 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.0, loss=1813.6607666015625
I0217 01:50:53.054300 139468600997632 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.0, loss=1863.702880859375
I0217 01:53:18.573108 139627123971904 spec.py:321] Evaluating on the training split.
I0217 01:53:55.299542 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 01:54:38.969249 139627123971904 spec.py:349] Evaluating on the test split.
I0217 01:55:01.245599 139627123971904 submission_runner.py:408] Time since start: 12509.01s, 	Step: 15179, 	{'train/ctc_loss': Array(1733.7393, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 11564.386147737503, 'total_duration': 12509.013834953308, 'accumulated_submission_time': 11564.386147737503, 'accumulated_eval_time': 943.6371881961823, 'accumulated_logging_time': 0.41151976585388184}
I0217 01:55:01.279496 139471200118528 logging_writer.py:48] [15179] accumulated_eval_time=943.637188, accumulated_logging_time=0.411520, accumulated_submission_time=11564.386148, global_step=15179, preemption_count=0, score=11564.386148, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=12509.013835, train/ctc_loss=1733.7392578125, train/wer=0.944086, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 01:59:03.243865 139471200118528 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.0, loss=1773.946533203125
I0217 02:05:13.946158 139471191725824 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.0, loss=1905.883056640625
I0217 02:11:58.357771 139471200118528 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.0, loss=1750.2955322265625
I0217 02:18:10.149411 139471191725824 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.0, loss=1787.072998046875
I0217 02:19:01.430677 139627123971904 spec.py:321] Evaluating on the training split.
I0217 02:19:38.157667 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 02:20:21.668353 139627123971904 spec.py:349] Evaluating on the test split.
I0217 02:20:43.648193 139627123971904 submission_runner.py:408] Time since start: 14051.42s, 	Step: 17070, 	{'train/ctc_loss': Array(1786.8647, dtype=float32), 'train/wer': 0.9427990785714666, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 13004.454290151596, 'total_duration': 14051.416600704193, 'accumulated_submission_time': 13004.454290151596, 'accumulated_eval_time': 1045.8488268852234, 'accumulated_logging_time': 0.45844268798828125}
I0217 02:20:43.685461 139470693238528 logging_writer.py:48] [17070] accumulated_eval_time=1045.848827, accumulated_logging_time=0.458443, accumulated_submission_time=13004.454290, global_step=17070, preemption_count=0, score=13004.454290, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=14051.416601, train/ctc_loss=1786.86474609375, train/wer=0.942799, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 02:26:05.172194 139470684845824 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.0, loss=1772.7181396484375
I0217 02:32:18.641901 139470693238528 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.0, loss=1787.94677734375
I0217 02:38:57.658269 139470684845824 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.0, loss=1813.9178466796875
I0217 02:44:43.783904 139627123971904 spec.py:321] Evaluating on the training split.
I0217 02:45:20.419574 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 02:46:03.516639 139627123971904 spec.py:349] Evaluating on the test split.
I0217 02:46:25.609715 139627123971904 submission_runner.py:408] Time since start: 15593.38s, 	Step: 18961, 	{'train/ctc_loss': Array(1755.9379, dtype=float32), 'train/wer': 0.9423383225986367, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14444.469913721085, 'total_duration': 15593.378989458084, 'accumulated_submission_time': 14444.469913721085, 'accumulated_eval_time': 1147.6698172092438, 'accumulated_logging_time': 0.5090396404266357}
I0217 02:46:25.648559 139470037878528 logging_writer.py:48] [18961] accumulated_eval_time=1147.669817, accumulated_logging_time=0.509040, accumulated_submission_time=14444.469914, global_step=18961, preemption_count=0, score=14444.469914, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=15593.378989, train/ctc_loss=1755.9378662109375, train/wer=0.942338, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 02:46:55.448426 139470029485824 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.0, loss=1871.6055908203125
I0217 02:53:11.214899 139470037878528 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.0, loss=1841.31640625
I0217 02:59:29.708644 139470693238528 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.0, loss=1803.820556640625
I0217 03:06:05.918451 139470684845824 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.0, loss=1872.427001953125
I0217 03:10:26.105163 139627123971904 spec.py:321] Evaluating on the training split.
I0217 03:11:02.989606 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 03:11:46.672736 139627123971904 spec.py:349] Evaluating on the test split.
I0217 03:12:08.824655 139627123971904 submission_runner.py:408] Time since start: 17136.59s, 	Step: 20839, 	{'train/ctc_loss': Array(1731.2493, dtype=float32), 'train/wer': 0.9431396916893625, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15884.840661287308, 'total_duration': 17136.593011379242, 'accumulated_submission_time': 15884.840661287308, 'accumulated_eval_time': 1250.383582353592, 'accumulated_logging_time': 0.5642483234405518}
I0217 03:12:08.861738 139469449074432 logging_writer.py:48] [20839] accumulated_eval_time=1250.383582, accumulated_logging_time=0.564248, accumulated_submission_time=15884.840661, global_step=20839, preemption_count=0, score=15884.840661, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=17136.593011, train/ctc_loss=1731.249267578125, train/wer=0.943140, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 03:14:09.050655 139469440681728 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.0, loss=1833.798095703125
I0217 03:20:31.271046 139469449074432 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.0, loss=1862.8890380859375
I0217 03:26:53.501281 139469449074432 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.0, loss=1821.5328369140625
I0217 03:33:21.655498 139469440681728 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.0, loss=1781.600830078125
I0217 03:36:08.983368 139627123971904 spec.py:321] Evaluating on the training split.
I0217 03:36:46.040058 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 03:37:29.545234 139627123971904 spec.py:349] Evaluating on the test split.
I0217 03:37:51.677111 139627123971904 submission_runner.py:408] Time since start: 18679.45s, 	Step: 22707, 	{'train/ctc_loss': Array(1763.6166, dtype=float32), 'train/wer': 0.9432716912443612, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 17324.87673664093, 'total_duration': 18679.44549226761, 'accumulated_submission_time': 17324.87673664093, 'accumulated_eval_time': 1353.0715968608856, 'accumulated_logging_time': 0.6181318759918213}
I0217 03:37:51.713537 139470693238528 logging_writer.py:48] [22707] accumulated_eval_time=1353.071597, accumulated_logging_time=0.618132, accumulated_submission_time=17324.876737, global_step=22707, preemption_count=0, score=17324.876737, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=18679.445492, train/ctc_loss=1763.6165771484375, train/wer=0.943272, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 03:41:30.513268 139470684845824 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.0, loss=1880.680419921875
I0217 03:47:48.111636 139470693238528 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.0, loss=1822.4405517578125
I0217 03:54:14.714904 139470693238528 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.0, loss=1808.7911376953125
I0217 04:00:40.363875 139470684845824 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.0, loss=1843.836181640625
I0217 04:01:51.895492 139627123971904 spec.py:321] Evaluating on the training split.
I0217 04:02:29.228427 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 04:03:12.896500 139627123971904 spec.py:349] Evaluating on the test split.
I0217 04:03:35.018903 139627123971904 submission_runner.py:408] Time since start: 20222.79s, 	Step: 24590, 	{'train/ctc_loss': Array(1739.3486, dtype=float32), 'train/wer': 0.944685667249717, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 18764.974794387817, 'total_duration': 20222.787495851517, 'accumulated_submission_time': 18764.974794387817, 'accumulated_eval_time': 1456.1893367767334, 'accumulated_logging_time': 0.6684412956237793}
I0217 04:03:35.052789 139470908278528 logging_writer.py:48] [24590] accumulated_eval_time=1456.189337, accumulated_logging_time=0.668441, accumulated_submission_time=18764.974794, global_step=24590, preemption_count=0, score=18764.974794, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=20222.787496, train/ctc_loss=1739.3486328125, train/wer=0.944686, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 04:08:42.968498 139470908278528 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.0, loss=1856.6741943359375
I0217 04:15:10.155198 139470899885824 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.0, loss=1837.7474365234375
I0217 04:21:43.322606 139470908278528 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.0, loss=1792.0771484375
I0217 04:27:35.448775 139627123971904 spec.py:321] Evaluating on the training split.
I0217 04:28:12.644224 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 04:28:56.397773 139627123971904 spec.py:349] Evaluating on the test split.
I0217 04:29:18.489349 139627123971904 submission_runner.py:408] Time since start: 21766.26s, 	Step: 26461, 	{'train/ctc_loss': Array(1769.4735, dtype=float32), 'train/wer': 0.9432456399645285, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 20205.28769826889, 'total_duration': 21766.257615804672, 'accumulated_submission_time': 20205.28769826889, 'accumulated_eval_time': 1559.2238938808441, 'accumulated_logging_time': 0.7156109809875488}
I0217 04:29:18.526185 139471200118528 logging_writer.py:48] [26461] accumulated_eval_time=1559.223894, accumulated_logging_time=0.715611, accumulated_submission_time=20205.287698, global_step=26461, preemption_count=0, score=20205.287698, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=21766.257616, train/ctc_loss=1769.4735107421875, train/wer=0.943246, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 04:29:48.405299 139471191725824 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.0, loss=1837.7474365234375
I0217 04:36:04.324879 139471200118528 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.0, loss=1792.328125
I0217 04:42:28.843197 139471191725824 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.0, loss=1789.9468994140625
I0217 04:49:07.179847 139469449074432 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.0, loss=1805.474365234375
I0217 04:53:18.828097 139627123971904 spec.py:321] Evaluating on the training split.
I0217 04:53:56.413739 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 04:54:40.166636 139627123971904 spec.py:349] Evaluating on the test split.
I0217 04:55:02.452198 139627123971904 submission_runner.py:408] Time since start: 23310.22s, 	Step: 28341, 	{'train/ctc_loss': Array(1736.9211, dtype=float32), 'train/wer': 0.9439109001278072, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 21645.502215862274, 'total_duration': 23310.221333026886, 'accumulated_submission_time': 21645.502215862274, 'accumulated_eval_time': 1662.8428773880005, 'accumulated_logging_time': 0.7697498798370361}
I0217 04:55:02.494834 139469152110336 logging_writer.py:48] [28341] accumulated_eval_time=1662.842877, accumulated_logging_time=0.769750, accumulated_submission_time=21645.502216, global_step=28341, preemption_count=0, score=21645.502216, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=23310.221333, train/ctc_loss=1736.921142578125, train/wer=0.943911, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 04:57:00.993371 139469143717632 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.0, loss=1841.31640625
I0217 05:03:22.875056 139469152110336 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.0, loss=1738.2916259765625
I0217 05:09:39.564104 139469143717632 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.0, loss=1800.648681640625
I0217 05:16:20.697551 139468609390336 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.0, loss=1926.1021728515625
I0217 05:19:02.944808 139627123971904 spec.py:321] Evaluating on the training split.
I0217 05:19:39.957972 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 05:20:23.636571 139627123971904 spec.py:349] Evaluating on the test split.
I0217 05:20:45.991680 139627123971904 submission_runner.py:408] Time since start: 24853.76s, 	Step: 30220, 	{'train/ctc_loss': Array(1715.2471, dtype=float32), 'train/wer': 0.9450143703143059, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 23085.864436626434, 'total_duration': 24853.760822057724, 'accumulated_submission_time': 23085.864436626434, 'accumulated_eval_time': 1765.8846318721771, 'accumulated_logging_time': 0.8292407989501953}
I0217 05:20:46.028381 139470150518528 logging_writer.py:48] [30220] accumulated_eval_time=1765.884632, accumulated_logging_time=0.829241, accumulated_submission_time=23085.864437, global_step=30220, preemption_count=0, score=23085.864437, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=24853.760822, train/ctc_loss=1715.2470703125, train/wer=0.945014, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 05:24:14.920461 139470142125824 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.0, loss=1823.60888671875
I0217 05:30:43.792202 139471200118528 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.0, loss=1723.08154296875
I0217 05:36:57.349418 139471191725824 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.0, loss=1867.7822265625
I0217 05:43:42.459714 139471200118528 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.0, loss=1839.9930419921875
I0217 05:44:46.144963 139627123971904 spec.py:321] Evaluating on the training split.
I0217 05:45:22.869357 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 05:46:06.625758 139627123971904 spec.py:349] Evaluating on the test split.
I0217 05:46:28.670606 139627123971904 submission_runner.py:408] Time since start: 26396.44s, 	Step: 32087, 	{'train/ctc_loss': Array(1783.4977, dtype=float32), 'train/wer': 0.9417576703068122, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 24525.896454811096, 'total_duration': 26396.438911676407, 'accumulated_submission_time': 24525.896454811096, 'accumulated_eval_time': 1868.4043147563934, 'accumulated_logging_time': 0.8797473907470703}
I0217 05:46:28.704775 139470616438528 logging_writer.py:48] [32087] accumulated_eval_time=1868.404315, accumulated_logging_time=0.879747, accumulated_submission_time=24525.896455, global_step=32087, preemption_count=0, score=24525.896455, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=26396.438912, train/ctc_loss=1783.4976806640625, train/wer=0.941758, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 05:51:36.299214 139470608045824 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.0, loss=1798.3717041015625
I0217 05:58:11.143299 139470288758528 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.0, loss=1804.96533203125
I0217 06:04:22.543883 139470280365824 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.0, loss=1813.0185546875
I0217 06:10:29.195430 139627123971904 spec.py:321] Evaluating on the training split.
I0217 06:11:06.736406 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 06:11:51.084711 139627123971904 spec.py:349] Evaluating on the test split.
I0217 06:12:13.636556 139627123971904 submission_runner.py:408] Time since start: 27941.41s, 	Step: 33955, 	{'train/ctc_loss': Array(1824.6918, dtype=float32), 'train/wer': 0.9416600198590335, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 25966.301414489746, 'total_duration': 27941.405210018158, 'accumulated_submission_time': 25966.301414489746, 'accumulated_eval_time': 1972.8398277759552, 'accumulated_logging_time': 0.9289171695709229}
I0217 06:12:13.673603 139469991794432 logging_writer.py:48] [33955] accumulated_eval_time=1972.839828, accumulated_logging_time=0.928917, accumulated_submission_time=25966.301414, global_step=33955, preemption_count=0, score=25966.301414, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=27941.405210, train/ctc_loss=1824.6917724609375, train/wer=0.941660, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 06:12:51.327970 139470616438528 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.0, loss=1813.1468505859375
I0217 06:19:01.799782 139470608045824 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.0, loss=1872.837890625
I0217 06:25:47.213429 139470616438528 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.0, loss=1810.582275390625
I0217 06:32:02.494521 139469664114432 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.0, loss=1860.046630859375
I0217 06:36:14.153033 139627123971904 spec.py:321] Evaluating on the training split.
I0217 06:36:52.362179 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 06:37:36.689237 139627123971904 spec.py:349] Evaluating on the test split.
I0217 06:37:59.429899 139627123971904 submission_runner.py:408] Time since start: 29487.20s, 	Step: 35821, 	{'train/ctc_loss': Array(1692.7738, dtype=float32), 'train/wer': 0.9447677853176417, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 27406.69121861458, 'total_duration': 29487.197845697403, 'accumulated_submission_time': 27406.69121861458, 'accumulated_eval_time': 2078.1103806495667, 'accumulated_logging_time': 0.9847147464752197}
I0217 06:37:59.468932 139470616438528 logging_writer.py:48] [35821] accumulated_eval_time=2078.110381, accumulated_logging_time=0.984715, accumulated_submission_time=27406.691219, global_step=35821, preemption_count=0, score=27406.691219, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=29487.197846, train/ctc_loss=1692.7738037109375, train/wer=0.944768, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 06:40:13.144116 139470608045824 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.0, loss=1839.9930419921875
I0217 06:46:27.283495 139470073718528 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.0, loss=1803.058349609375
I0217 06:53:14.821957 139470065325824 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.0, loss=1856.9434814453125
I0217 06:59:35.239905 139470616438528 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.0, loss=1891.5194091796875
I0217 07:01:59.500044 139627123971904 spec.py:321] Evaluating on the training split.
I0217 07:02:37.001611 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 07:03:21.391884 139627123971904 spec.py:349] Evaluating on the test split.
I0217 07:03:43.877466 139627123971904 submission_runner.py:408] Time since start: 31031.65s, 	Step: 37689, 	{'train/ctc_loss': Array(1787.1797, dtype=float32), 'train/wer': 0.9427091658940503, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 28846.636228322983, 'total_duration': 31031.64558815956, 'accumulated_submission_time': 28846.636228322983, 'accumulated_eval_time': 2182.4816431999207, 'accumulated_logging_time': 1.0395748615264893}
I0217 07:03:43.914965 139470770038528 logging_writer.py:48] [37689] accumulated_eval_time=2182.481643, accumulated_logging_time=1.039575, accumulated_submission_time=28846.636228, global_step=37689, preemption_count=0, score=28846.636228, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=31031.645588, train/ctc_loss=1787.1796875, train/wer=0.942709, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 07:07:36.240460 139470761645824 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.0, loss=1856.808837890625
I0217 07:13:53.404213 139470770038528 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.0, loss=1832.2232666015625
I0217 07:20:30.838762 139470761645824 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.0, loss=1826.9927978515625
I0217 07:26:55.308348 139470114678528 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.0, loss=1880.818603515625
I0217 07:27:44.331165 139627123971904 spec.py:321] Evaluating on the training split.
I0217 07:28:21.986693 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 07:29:07.130520 139627123971904 spec.py:349] Evaluating on the test split.
I0217 07:29:30.131731 139627123971904 submission_runner.py:408] Time since start: 32577.90s, 	Step: 39567, 	{'train/ctc_loss': Array(1714.3282, dtype=float32), 'train/wer': 0.9448971433842748, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 30286.965601682663, 'total_duration': 32577.90078020096, 'accumulated_submission_time': 30286.965601682663, 'accumulated_eval_time': 2288.276973247528, 'accumulated_logging_time': 1.0922460556030273}
I0217 07:29:30.172601 139471200118528 logging_writer.py:48] [39567] accumulated_eval_time=2288.276973, accumulated_logging_time=1.092246, accumulated_submission_time=30286.965602, global_step=39567, preemption_count=0, score=30286.965602, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=32577.900780, train/ctc_loss=1714.3282470703125, train/wer=0.944897, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 07:34:54.679661 139471191725824 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.0, loss=1952.8431396484375
I0217 07:41:20.940171 139471200118528 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.0, loss=1862.8890380859375
I0217 07:47:54.510445 139471191725824 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.0, loss=1849.2972412109375
I0217 07:53:30.318191 139627123971904 spec.py:321] Evaluating on the training split.
I0217 07:54:08.034961 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 07:54:52.536001 139627123971904 spec.py:349] Evaluating on the test split.
I0217 07:55:15.618876 139627123971904 submission_runner.py:408] Time since start: 34123.39s, 	Step: 41431, 	{'train/ctc_loss': Array(1760.688, dtype=float32), 'train/wer': 0.9432324554919642, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 31727.023243904114, 'total_duration': 34123.387323856354, 'accumulated_submission_time': 31727.023243904114, 'accumulated_eval_time': 2393.5720071792603, 'accumulated_logging_time': 1.1517698764801025}
I0217 07:55:15.657410 139471200118528 logging_writer.py:48] [41431] accumulated_eval_time=2393.572007, accumulated_logging_time=1.151770, accumulated_submission_time=31727.023244, global_step=41431, preemption_count=0, score=31727.023244, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=34123.387324, train/ctc_loss=1760.68798828125, train/wer=0.943232, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 07:56:07.693612 139471191725824 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.0, loss=1819.7200927734375
I0217 08:02:25.119161 139471200118528 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.0, loss=1795.3448486328125
I0217 08:08:55.749016 139471200118528 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.0, loss=1856.2701416015625
I0217 08:15:22.477091 139471191725824 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.0, loss=1872.56396484375
I0217 08:19:16.294595 139627123971904 spec.py:321] Evaluating on the training split.
I0217 08:19:53.324205 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 08:20:37.562118 139627123971904 spec.py:349] Evaluating on the test split.
I0217 08:20:59.643787 139627123971904 submission_runner.py:408] Time since start: 35667.41s, 	Step: 43285, 	{'train/ctc_loss': Array(1852.6235, dtype=float32), 'train/wer': 0.941680272071945, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 33167.57418704033, 'total_duration': 35667.413115262985, 'accumulated_submission_time': 33167.57418704033, 'accumulated_eval_time': 2496.9164566993713, 'accumulated_logging_time': 1.2062418460845947}
I0217 08:20:59.677564 139470985078528 logging_writer.py:48] [43285] accumulated_eval_time=2496.916457, accumulated_logging_time=1.206242, accumulated_submission_time=33167.574187, global_step=43285, preemption_count=0, score=33167.574187, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=35667.413115, train/ctc_loss=1852.62353515625, train/wer=0.941680, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 08:23:40.078433 139470976685824 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.0, loss=1858.6962890625
I0217 08:29:55.242981 139470985078528 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.0, loss=1811.6072998046875
I0217 08:36:31.182183 139470985078528 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.0, loss=1830.78173828125
I0217 08:42:56.434311 139470976685824 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.0, loss=1855.3284912109375
I0217 08:45:00.173364 139627123971904 spec.py:321] Evaluating on the training split.
I0217 08:45:37.557184 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 08:46:21.771152 139627123971904 spec.py:349] Evaluating on the test split.
I0217 08:46:44.421420 139627123971904 submission_runner.py:408] Time since start: 37212.19s, 	Step: 45154, 	{'train/ctc_loss': Array(1901.3732, dtype=float32), 'train/wer': 0.940312095793757, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 34607.98229575157, 'total_duration': 37212.19028520584, 'accumulated_submission_time': 34607.98229575157, 'accumulated_eval_time': 2601.159108400345, 'accumulated_logging_time': 1.255842924118042}
I0217 08:46:44.459014 139471200118528 logging_writer.py:48] [45154] accumulated_eval_time=2601.159108, accumulated_logging_time=1.255843, accumulated_submission_time=34607.982296, global_step=45154, preemption_count=0, score=34607.982296, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=37212.190285, train/ctc_loss=1901.3731689453125, train/wer=0.940312, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 08:51:05.591248 139470329718528 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.0, loss=1828.820068359375
I0217 08:57:28.633229 139470321325824 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.0, loss=1798.8773193359375
I0217 09:04:11.817108 139470329718528 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.0, loss=1844.899169921875
I0217 09:10:33.476013 139470321325824 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.0, loss=1794.3382568359375
I0217 09:10:44.426686 139627123971904 spec.py:321] Evaluating on the training split.
I0217 09:11:21.715165 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 09:12:06.147636 139627123971904 spec.py:349] Evaluating on the test split.
I0217 09:12:28.829059 139627123971904 submission_runner.py:408] Time since start: 38756.60s, 	Step: 47015, 	{'train/ctc_loss': Array(1959.8696, dtype=float32), 'train/wer': 0.9371047844119075, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 36047.8643784523, 'total_duration': 38756.597259521484, 'accumulated_submission_time': 36047.8643784523, 'accumulated_eval_time': 2705.5553891658783, 'accumulated_logging_time': 1.3077659606933594}
I0217 09:12:28.864854 139470693238528 logging_writer.py:48] [47015] accumulated_eval_time=2705.555389, accumulated_logging_time=1.307766, accumulated_submission_time=36047.864378, global_step=47015, preemption_count=0, score=36047.864378, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=38756.597260, train/ctc_loss=1959.86962890625, train/wer=0.937105, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 09:18:32.976878 139470693238528 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.0, loss=1838.935546875
I0217 09:24:51.779320 139627123971904 spec.py:321] Evaluating on the training split.
I0217 09:25:28.460334 139627123971904 spec.py:333] Evaluating on the validation split.
I0217 09:26:12.119297 139627123971904 spec.py:349] Evaluating on the test split.
I0217 09:26:34.685759 139627123971904 submission_runner.py:408] Time since start: 39602.46s, 	Step: 48000, 	{'train/ctc_loss': Array(2009.2771, dtype=float32), 'train/wer': 0.9366967129626904, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 36790.7276763916, 'total_duration': 39602.457629442215, 'accumulated_submission_time': 36790.7276763916, 'accumulated_eval_time': 2808.459444999695, 'accumulated_logging_time': 1.3564789295196533}
I0217 09:26:34.727939 139470693238528 logging_writer.py:48] [48000] accumulated_eval_time=2808.459445, accumulated_logging_time=1.356479, accumulated_submission_time=36790.727676, global_step=48000, preemption_count=0, score=36790.727676, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=39602.457629, train/ctc_loss=2009.277099609375, train/wer=0.936697, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0217 09:26:34.753637 139470684845824 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=36790.727676
I0217 09:26:34.968475 139627123971904 checkpoints.py:490] Saving checkpoint at step: 48000
I0217 09:26:35.969569 139627123971904 checkpoints.py:422] Saved checkpoint at /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_no_resnet_jax/trial_1/checkpoint_48000
I0217 09:26:35.991901 139627123971904 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_no_resnet_jax/trial_1/checkpoint_48000.
I0217 09:26:37.266845 139627123971904 submission_runner.py:583] Tuning trial 1/1
I0217 09:26:37.267102 139627123971904 submission_runner.py:584] Hyperparameters: Hyperparameters(learning_rate=4.131896390902391, beta1=0.9274758113254791, beta2=0.9978504782314613, warmup_steps=6999, decay_steps_factor=0.9007765761611038, end_factor=0.001, weight_decay=5.6687777311501786e-06, label_smoothing=0.2)
I0217 09:26:37.278319 139627123971904 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.423384, dtype=float32), 'train/wer': 0.9438654394769173, 'validation/ctc_loss': Array(30.389412, dtype=float32), 'validation/wer': 0.8970427797677091, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.538706, dtype=float32), 'test/wer': 0.8997826660979424, 'test/num_examples': 2472, 'score': 41.92684984207153, 'total_duration': 174.3698627948761, 'accumulated_submission_time': 41.92684984207153, 'accumulated_eval_time': 132.44295573234558, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1903, {'train/ctc_loss': Array(1767.6216, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(3356.9526, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.3506, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1482.096818447113, 'total_duration': 1714.94912648201, 'accumulated_submission_time': 1482.096818447113, 'accumulated_eval_time': 232.74323987960815, 'accumulated_logging_time': 0.04260540008544922, 'global_step': 1903, 'preemption_count': 0}), (3822, {'train/ctc_loss': Array(1761.5707, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2922.555701494217, 'total_duration': 3255.370903491974, 'accumulated_submission_time': 2922.555701494217, 'accumulated_eval_time': 332.5835964679718, 'accumulated_logging_time': 0.0936136245727539, 'global_step': 3822, 'preemption_count': 0}), (5734, {'train/ctc_loss': Array(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4362.468790531158, 'total_duration': 4796.950233697891, 'accumulated_submission_time': 4362.468790531158, 'accumulated_eval_time': 434.1249997615814, 'accumulated_logging_time': 0.14481449127197266, 'global_step': 5734, 'preemption_count': 0}), (7646, {'train/ctc_loss': Array(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 5802.390080213547, 'total_duration': 6338.421497344971, 'accumulated_submission_time': 5802.390080213547, 'accumulated_eval_time': 535.5537095069885, 'accumulated_logging_time': 0.19235849380493164, 'global_step': 7646, 'preemption_count': 0}), (9548, {'train/ctc_loss': Array(1832.9288, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7242.625440597534, 'total_duration': 7880.063672065735, 'accumulated_submission_time': 7242.625440597534, 'accumulated_eval_time': 636.8322005271912, 'accumulated_logging_time': 0.2474377155303955, 'global_step': 9548, 'preemption_count': 0}), (11427, {'train/ctc_loss': Array(1752.8004, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 8683.113170146942, 'total_duration': 9422.627868413925, 'accumulated_submission_time': 8683.113170146942, 'accumulated_eval_time': 738.7778081893921, 'accumulated_logging_time': 0.30493879318237305, 'global_step': 11427, 'preemption_count': 0}), (13305, {'train/ctc_loss': Array(1746.111, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 10123.776574373245, 'total_duration': 10965.610308885574, 'accumulated_submission_time': 10123.776574373245, 'accumulated_eval_time': 840.9707329273224, 'accumulated_logging_time': 0.3580210208892822, 'global_step': 13305, 'preemption_count': 0}), (15179, {'train/ctc_loss': Array(1733.7393, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 11564.386147737503, 'total_duration': 12509.013834953308, 'accumulated_submission_time': 11564.386147737503, 'accumulated_eval_time': 943.6371881961823, 'accumulated_logging_time': 0.41151976585388184, 'global_step': 15179, 'preemption_count': 0}), (17070, {'train/ctc_loss': Array(1786.8647, dtype=float32), 'train/wer': 0.9427990785714666, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 13004.454290151596, 'total_duration': 14051.416600704193, 'accumulated_submission_time': 13004.454290151596, 'accumulated_eval_time': 1045.8488268852234, 'accumulated_logging_time': 0.45844268798828125, 'global_step': 17070, 'preemption_count': 0}), (18961, {'train/ctc_loss': Array(1755.9379, dtype=float32), 'train/wer': 0.9423383225986367, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14444.469913721085, 'total_duration': 15593.378989458084, 'accumulated_submission_time': 14444.469913721085, 'accumulated_eval_time': 1147.6698172092438, 'accumulated_logging_time': 0.5090396404266357, 'global_step': 18961, 'preemption_count': 0}), (20839, {'train/ctc_loss': Array(1731.2493, dtype=float32), 'train/wer': 0.9431396916893625, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15884.840661287308, 'total_duration': 17136.593011379242, 'accumulated_submission_time': 15884.840661287308, 'accumulated_eval_time': 1250.383582353592, 'accumulated_logging_time': 0.5642483234405518, 'global_step': 20839, 'preemption_count': 0}), (22707, {'train/ctc_loss': Array(1763.6166, dtype=float32), 'train/wer': 0.9432716912443612, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 17324.87673664093, 'total_duration': 18679.44549226761, 'accumulated_submission_time': 17324.87673664093, 'accumulated_eval_time': 1353.0715968608856, 'accumulated_logging_time': 0.6181318759918213, 'global_step': 22707, 'preemption_count': 0}), (24590, {'train/ctc_loss': Array(1739.3486, dtype=float32), 'train/wer': 0.944685667249717, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 18764.974794387817, 'total_duration': 20222.787495851517, 'accumulated_submission_time': 18764.974794387817, 'accumulated_eval_time': 1456.1893367767334, 'accumulated_logging_time': 0.6684412956237793, 'global_step': 24590, 'preemption_count': 0}), (26461, {'train/ctc_loss': Array(1769.4735, dtype=float32), 'train/wer': 0.9432456399645285, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 20205.28769826889, 'total_duration': 21766.257615804672, 'accumulated_submission_time': 20205.28769826889, 'accumulated_eval_time': 1559.2238938808441, 'accumulated_logging_time': 0.7156109809875488, 'global_step': 26461, 'preemption_count': 0}), (28341, {'train/ctc_loss': Array(1736.9211, dtype=float32), 'train/wer': 0.9439109001278072, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 21645.502215862274, 'total_duration': 23310.221333026886, 'accumulated_submission_time': 21645.502215862274, 'accumulated_eval_time': 1662.8428773880005, 'accumulated_logging_time': 0.7697498798370361, 'global_step': 28341, 'preemption_count': 0}), (30220, {'train/ctc_loss': Array(1715.2471, dtype=float32), 'train/wer': 0.9450143703143059, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 23085.864436626434, 'total_duration': 24853.760822057724, 'accumulated_submission_time': 23085.864436626434, 'accumulated_eval_time': 1765.8846318721771, 'accumulated_logging_time': 0.8292407989501953, 'global_step': 30220, 'preemption_count': 0}), (32087, {'train/ctc_loss': Array(1783.4977, dtype=float32), 'train/wer': 0.9417576703068122, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 24525.896454811096, 'total_duration': 26396.438911676407, 'accumulated_submission_time': 24525.896454811096, 'accumulated_eval_time': 1868.4043147563934, 'accumulated_logging_time': 0.8797473907470703, 'global_step': 32087, 'preemption_count': 0}), (33955, {'train/ctc_loss': Array(1824.6918, dtype=float32), 'train/wer': 0.9416600198590335, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 25966.301414489746, 'total_duration': 27941.405210018158, 'accumulated_submission_time': 25966.301414489746, 'accumulated_eval_time': 1972.8398277759552, 'accumulated_logging_time': 0.9289171695709229, 'global_step': 33955, 'preemption_count': 0}), (35821, {'train/ctc_loss': Array(1692.7738, dtype=float32), 'train/wer': 0.9447677853176417, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 27406.69121861458, 'total_duration': 29487.197845697403, 'accumulated_submission_time': 27406.69121861458, 'accumulated_eval_time': 2078.1103806495667, 'accumulated_logging_time': 0.9847147464752197, 'global_step': 35821, 'preemption_count': 0}), (37689, {'train/ctc_loss': Array(1787.1797, dtype=float32), 'train/wer': 0.9427091658940503, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 28846.636228322983, 'total_duration': 31031.64558815956, 'accumulated_submission_time': 28846.636228322983, 'accumulated_eval_time': 2182.4816431999207, 'accumulated_logging_time': 1.0395748615264893, 'global_step': 37689, 'preemption_count': 0}), (39567, {'train/ctc_loss': Array(1714.3282, dtype=float32), 'train/wer': 0.9448971433842748, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 30286.965601682663, 'total_duration': 32577.90078020096, 'accumulated_submission_time': 30286.965601682663, 'accumulated_eval_time': 2288.276973247528, 'accumulated_logging_time': 1.0922460556030273, 'global_step': 39567, 'preemption_count': 0}), (41431, {'train/ctc_loss': Array(1760.688, dtype=float32), 'train/wer': 0.9432324554919642, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 31727.023243904114, 'total_duration': 34123.387323856354, 'accumulated_submission_time': 31727.023243904114, 'accumulated_eval_time': 2393.5720071792603, 'accumulated_logging_time': 1.1517698764801025, 'global_step': 41431, 'preemption_count': 0}), (43285, {'train/ctc_loss': Array(1852.6235, dtype=float32), 'train/wer': 0.941680272071945, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 33167.57418704033, 'total_duration': 35667.413115262985, 'accumulated_submission_time': 33167.57418704033, 'accumulated_eval_time': 2496.9164566993713, 'accumulated_logging_time': 1.2062418460845947, 'global_step': 43285, 'preemption_count': 0}), (45154, {'train/ctc_loss': Array(1901.3732, dtype=float32), 'train/wer': 0.940312095793757, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 34607.98229575157, 'total_duration': 37212.19028520584, 'accumulated_submission_time': 34607.98229575157, 'accumulated_eval_time': 2601.159108400345, 'accumulated_logging_time': 1.255842924118042, 'global_step': 45154, 'preemption_count': 0}), (47015, {'train/ctc_loss': Array(1959.8696, dtype=float32), 'train/wer': 0.9371047844119075, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 36047.8643784523, 'total_duration': 38756.597259521484, 'accumulated_submission_time': 36047.8643784523, 'accumulated_eval_time': 2705.5553891658783, 'accumulated_logging_time': 1.3077659606933594, 'global_step': 47015, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(2009.2771, dtype=float32), 'train/wer': 0.9366967129626904, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 36790.7276763916, 'total_duration': 39602.457629442215, 'accumulated_submission_time': 36790.7276763916, 'accumulated_eval_time': 2808.459444999695, 'accumulated_logging_time': 1.3564789295196533, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0217 09:26:37.278508 139627123971904 submission_runner.py:586] Timing: 36790.7276763916
I0217 09:26:37.278577 139627123971904 submission_runner.py:588] Total number of evals: 27
I0217 09:26:37.278658 139627123971904 submission_runner.py:589] ====================
I0217 09:26:37.287367 139627123971904 submission_runner.py:673] Final librispeech_deepspeech_no_resnet score: 36790.7276763916
