python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech_tanh --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/imagenet_resnet/tuning_search_space.json --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --num_tuning_trials=1 --rng_seed=1884662704 --max_global_steps=48000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab 2>&1 | tee -a /logs/librispeech_deepspeech_tanh_jax_02-15-2024-20-07-23.log
I0215 20:07:48.995307 140014697482048 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_tanh_jax.
I0215 20:07:50.038466 140014697482048 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0215 20:07:50.039229 140014697482048 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0215 20:07:50.039353 140014697482048 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0215 20:07:50.046743 140014697482048 submission_runner.py:542] Using RNG seed 1884662704
I0215 20:07:51.467324 140014697482048 submission_runner.py:551] --- Tuning run 1/1 ---
I0215 20:07:51.467512 140014697482048 submission_runner.py:556] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_tanh_jax/trial_1.
I0215 20:07:51.467689 140014697482048 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_tanh_jax/trial_1/hparams.json.
I0215 20:07:51.650909 140014697482048 submission_runner.py:206] Initializing dataset.
I0215 20:07:51.651115 140014697482048 submission_runner.py:213] Initializing model.
I0215 20:07:54.100570 140014697482048 submission_runner.py:255] Initializing optimizer.
I0215 20:07:54.782682 140014697482048 submission_runner.py:262] Initializing metrics bundle.
I0215 20:07:54.782863 140014697482048 submission_runner.py:280] Initializing checkpoint and logger.
I0215 20:07:54.783963 140014697482048 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_tanh_jax/trial_1 with prefix checkpoint_
I0215 20:07:54.784128 140014697482048 submission_runner.py:300] Saving meta data to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_tanh_jax/trial_1/meta_data_0.json.
I0215 20:07:54.784370 140014697482048 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0215 20:07:54.784440 140014697482048 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0215 20:07:55.065387 140014697482048 logger_utils.py:220] Unable to record git information. Continuing without it.
I0215 20:07:55.305685 140014697482048 submission_runner.py:304] Saving flags to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_tanh_jax/trial_1/flags_0.json.
I0215 20:07:55.323613 140014697482048 submission_runner.py:314] Starting training loop.
I0215 20:07:55.611503 140014697482048 input_pipeline.py:20] Loading split = train-clean-100
I0215 20:07:55.675249 140014697482048 input_pipeline.py:20] Loading split = train-clean-360
I0215 20:07:55.831162 140014697482048 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0215 20:08:39.736164 139850321823488 logging_writer.py:48] [0] global_step=0, grad_norm=21.74662971496582, loss=33.64865493774414
I0215 20:08:39.773564 140014697482048 spec.py:321] Evaluating on the training split.
I0215 20:08:40.036977 140014697482048 input_pipeline.py:20] Loading split = train-clean-100
I0215 20:08:40.073156 140014697482048 input_pipeline.py:20] Loading split = train-clean-360
I0215 20:08:40.447369 140014697482048 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0215 20:11:02.161831 140014697482048 spec.py:333] Evaluating on the validation split.
I0215 20:11:02.356187 140014697482048 input_pipeline.py:20] Loading split = dev-clean
I0215 20:11:02.366939 140014697482048 input_pipeline.py:20] Loading split = dev-other
I0215 20:12:26.405798 140014697482048 spec.py:349] Evaluating on the test split.
I0215 20:12:26.605651 140014697482048 input_pipeline.py:20] Loading split = test-clean
I0215 20:13:12.974149 140014697482048 submission_runner.py:408] Time since start: 317.65s, 	Step: 1, 	{'train/ctc_loss': Array(32.390755, dtype=float32), 'train/wer': 4.916930429169357, 'validation/ctc_loss': Array(30.971638, dtype=float32), 'validation/wer': 4.484287052144781, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.25611, dtype=float32), 'test/wer': 4.7169581378343794, 'test/num_examples': 2472, 'score': 44.44989728927612, 'total_duration': 317.6483597755432, 'accumulated_submission_time': 44.44989728927612, 'accumulated_eval_time': 273.1984131336212, 'accumulated_logging_time': 0}
I0215 20:13:13.002316 139842054780672 logging_writer.py:48] [1] accumulated_eval_time=273.198413, accumulated_logging_time=0, accumulated_submission_time=44.449897, global_step=1, preemption_count=0, score=44.449897, test/ctc_loss=31.2561092376709, test/num_examples=2472, test/wer=4.716958, total_duration=317.648360, train/ctc_loss=32.39075469970703, train/wer=4.916930, validation/ctc_loss=30.971637725830078, validation/num_examples=5348, validation/wer=4.484287
I0215 20:13:22.109446 139856863164160 logging_writer.py:48] [1] global_step=1, grad_norm=20.942848205566406, loss=33.24131774902344
I0215 20:13:22.962142 139856871556864 logging_writer.py:48] [2] global_step=2, grad_norm=72.49436950683594, loss=27.41762924194336
I0215 20:13:23.809676 139856863164160 logging_writer.py:48] [3] global_step=3, grad_norm=11.211845397949219, loss=14.360149383544922
I0215 20:13:24.662706 139856871556864 logging_writer.py:48] [4] global_step=4, grad_norm=6.132278919219971, loss=10.881719589233398
I0215 20:13:25.576755 139856863164160 logging_writer.py:48] [5] global_step=5, grad_norm=4.644908905029297, loss=9.533370971679688
I0215 20:13:26.484992 139856871556864 logging_writer.py:48] [6] global_step=6, grad_norm=5.098289966583252, loss=8.724730491638184
I0215 20:13:27.386607 139856863164160 logging_writer.py:48] [7] global_step=7, grad_norm=3.4220433235168457, loss=8.107142448425293
I0215 20:13:28.299011 139856871556864 logging_writer.py:48] [8] global_step=8, grad_norm=15.815953254699707, loss=8.19753646850586
I0215 20:13:29.216002 139856863164160 logging_writer.py:48] [9] global_step=9, grad_norm=3.7261719703674316, loss=8.020630836486816
I0215 20:13:30.114968 139856871556864 logging_writer.py:48] [10] global_step=10, grad_norm=2.653953790664673, loss=7.954573154449463
I0215 20:13:31.016569 139856863164160 logging_writer.py:48] [11] global_step=11, grad_norm=2.813307762145996, loss=7.8296308517456055
I0215 20:13:31.924566 139856871556864 logging_writer.py:48] [12] global_step=12, grad_norm=2.8958706855773926, loss=7.800713539123535
I0215 20:13:32.835352 139856863164160 logging_writer.py:48] [13] global_step=13, grad_norm=2.116013288497925, loss=7.641742706298828
I0215 20:13:33.741961 139856871556864 logging_writer.py:48] [14] global_step=14, grad_norm=2.2681543827056885, loss=7.6361846923828125
I0215 20:13:34.642133 139856863164160 logging_writer.py:48] [15] global_step=15, grad_norm=1.6333885192871094, loss=7.534472942352295
I0215 20:13:35.564822 139856871556864 logging_writer.py:48] [16] global_step=16, grad_norm=2.2315585613250732, loss=7.184521198272705
I0215 20:13:36.468054 139856863164160 logging_writer.py:48] [17] global_step=17, grad_norm=1.3333938121795654, loss=7.0420002937316895
I0215 20:13:37.376377 139856871556864 logging_writer.py:48] [18] global_step=18, grad_norm=2.128659248352051, loss=6.938880920410156
I0215 20:13:38.287670 139856863164160 logging_writer.py:48] [19] global_step=19, grad_norm=9.36979866027832, loss=6.8095479011535645
I0215 20:13:39.191407 139856871556864 logging_writer.py:48] [20] global_step=20, grad_norm=61.403419494628906, loss=11.626927375793457
I0215 20:13:40.112939 139856863164160 logging_writer.py:48] [21] global_step=21, grad_norm=21.8165283203125, loss=14.434653282165527
I0215 20:13:41.022888 139856871556864 logging_writer.py:48] [22] global_step=22, grad_norm=22.0427188873291, loss=14.1624116897583
I0215 20:13:41.932557 139856863164160 logging_writer.py:48] [23] global_step=23, grad_norm=17.786300659179688, loss=11.172016143798828
I0215 20:13:42.840391 139856871556864 logging_writer.py:48] [24] global_step=24, grad_norm=4.959066867828369, loss=8.674665451049805
I0215 20:13:43.758528 139856863164160 logging_writer.py:48] [25] global_step=25, grad_norm=3.8646581172943115, loss=8.376208305358887
I0215 20:13:44.657498 139856871556864 logging_writer.py:48] [26] global_step=26, grad_norm=3.946075916290283, loss=8.305336952209473
I0215 20:13:45.567759 139856863164160 logging_writer.py:48] [27] global_step=27, grad_norm=2.899184465408325, loss=8.130538940429688
I0215 20:13:46.495270 139856871556864 logging_writer.py:48] [28] global_step=28, grad_norm=2.844059467315674, loss=7.992788791656494
I0215 20:13:47.409498 139856863164160 logging_writer.py:48] [29] global_step=29, grad_norm=2.6239359378814697, loss=7.542159080505371
I0215 20:13:48.308022 139856871556864 logging_writer.py:48] [30] global_step=30, grad_norm=2.521193504333496, loss=7.290677070617676
I0215 20:13:49.221484 139856863164160 logging_writer.py:48] [31] global_step=31, grad_norm=2.4617886543273926, loss=6.855393886566162
I0215 20:13:50.135320 139856871556864 logging_writer.py:48] [32] global_step=32, grad_norm=1.610819697380066, loss=6.580259799957275
I0215 20:13:51.045611 139856863164160 logging_writer.py:48] [33] global_step=33, grad_norm=1.964820384979248, loss=6.497470378875732
I0215 20:13:51.953305 139856871556864 logging_writer.py:48] [34] global_step=34, grad_norm=1.4613434076309204, loss=6.30388069152832
I0215 20:13:52.865984 139856863164160 logging_writer.py:48] [35] global_step=35, grad_norm=1.9130016565322876, loss=6.23137903213501
I0215 20:13:53.774027 139856871556864 logging_writer.py:48] [36] global_step=36, grad_norm=4.988738536834717, loss=6.271942138671875
I0215 20:13:54.709169 139856863164160 logging_writer.py:48] [37] global_step=37, grad_norm=3.1329846382141113, loss=6.176373481750488
I0215 20:13:55.627835 139856871556864 logging_writer.py:48] [38] global_step=38, grad_norm=5.288836479187012, loss=6.22802209854126
I0215 20:13:56.537693 139856863164160 logging_writer.py:48] [39] global_step=39, grad_norm=6.597017765045166, loss=6.226100444793701
I0215 20:13:57.441217 139856871556864 logging_writer.py:48] [40] global_step=40, grad_norm=6.0812273025512695, loss=6.26609992980957
I0215 20:13:58.341127 139856863164160 logging_writer.py:48] [41] global_step=41, grad_norm=6.101195335388184, loss=6.248629570007324
I0215 20:13:59.256463 139856871556864 logging_writer.py:48] [42] global_step=42, grad_norm=7.850325584411621, loss=6.28080415725708
I0215 20:14:00.170518 139856863164160 logging_writer.py:48] [43] global_step=43, grad_norm=8.886136054992676, loss=6.36121940612793
I0215 20:14:01.084324 139856871556864 logging_writer.py:48] [44] global_step=44, grad_norm=9.287437438964844, loss=6.264377117156982
I0215 20:14:02.030689 139856863164160 logging_writer.py:48] [45] global_step=45, grad_norm=10.24184799194336, loss=6.44449520111084
I0215 20:14:02.947753 139856871556864 logging_writer.py:48] [46] global_step=46, grad_norm=14.668485641479492, loss=6.508526802062988
I0215 20:14:03.854946 139856863164160 logging_writer.py:48] [47] global_step=47, grad_norm=14.778376579284668, loss=7.136563777923584
I0215 20:14:04.769251 139856871556864 logging_writer.py:48] [48] global_step=48, grad_norm=13.263915061950684, loss=6.636202812194824
I0215 20:14:05.681742 139856863164160 logging_writer.py:48] [49] global_step=49, grad_norm=13.754362106323242, loss=7.0695600509643555
I0215 20:14:06.603221 139856871556864 logging_writer.py:48] [50] global_step=50, grad_norm=22.45648956298828, loss=7.288034439086914
I0215 20:14:07.509878 139856863164160 logging_writer.py:48] [51] global_step=51, grad_norm=17.42194938659668, loss=8.737430572509766
I0215 20:14:08.424713 139856871556864 logging_writer.py:48] [52] global_step=52, grad_norm=2.1927077770233154, loss=7.3468403816223145
I0215 20:14:09.336884 139856863164160 logging_writer.py:48] [53] global_step=53, grad_norm=1.6650933027267456, loss=7.105961322784424
I0215 20:14:10.251034 139856871556864 logging_writer.py:48] [54] global_step=54, grad_norm=4.204876899719238, loss=7.0074591636657715
I0215 20:14:11.170368 139856863164160 logging_writer.py:48] [55] global_step=55, grad_norm=5.356278419494629, loss=6.8727288246154785
I0215 20:14:12.085440 139856871556864 logging_writer.py:48] [56] global_step=56, grad_norm=9.127756118774414, loss=6.856813907623291
I0215 20:14:12.992897 139856863164160 logging_writer.py:48] [57] global_step=57, grad_norm=11.250798225402832, loss=7.079637050628662
I0215 20:14:13.903131 139856871556864 logging_writer.py:48] [58] global_step=58, grad_norm=18.4514217376709, loss=7.328426837921143
I0215 20:14:14.808432 139856863164160 logging_writer.py:48] [59] global_step=59, grad_norm=15.128544807434082, loss=8.110361099243164
I0215 20:14:15.712415 139856871556864 logging_writer.py:48] [60] global_step=60, grad_norm=5.144347190856934, loss=7.100549697875977
I0215 20:14:16.618983 139856863164160 logging_writer.py:48] [61] global_step=61, grad_norm=4.356258869171143, loss=6.973248481750488
I0215 20:14:17.532343 139856871556864 logging_writer.py:48] [62] global_step=62, grad_norm=8.370999336242676, loss=7.089537143707275
I0215 20:14:18.446267 139856863164160 logging_writer.py:48] [63] global_step=63, grad_norm=9.538333892822266, loss=7.101722717285156
I0215 20:14:19.354649 139856871556864 logging_writer.py:48] [64] global_step=64, grad_norm=12.012216567993164, loss=7.12622594833374
I0215 20:14:20.271838 139856863164160 logging_writer.py:48] [65] global_step=65, grad_norm=11.395746231079102, loss=7.398576736450195
I0215 20:14:21.199308 139856871556864 logging_writer.py:48] [66] global_step=66, grad_norm=8.49020004272461, loss=7.042622089385986
I0215 20:14:22.112143 139856863164160 logging_writer.py:48] [67] global_step=67, grad_norm=7.8027544021606445, loss=6.918091297149658
I0215 20:14:23.021470 139856871556864 logging_writer.py:48] [68] global_step=68, grad_norm=8.098661422729492, loss=6.8397321701049805
I0215 20:14:23.934481 139856863164160 logging_writer.py:48] [69] global_step=69, grad_norm=7.210006237030029, loss=6.846519947052002
I0215 20:14:24.858419 139856871556864 logging_writer.py:48] [70] global_step=70, grad_norm=5.266287803649902, loss=6.672010898590088
I0215 20:14:25.770483 139856863164160 logging_writer.py:48] [71] global_step=71, grad_norm=4.347029685974121, loss=6.54432487487793
I0215 20:14:26.678651 139856871556864 logging_writer.py:48] [72] global_step=72, grad_norm=3.6964173316955566, loss=6.412912368774414
I0215 20:14:27.584144 139856863164160 logging_writer.py:48] [73] global_step=73, grad_norm=2.456799030303955, loss=6.2858686447143555
I0215 20:14:28.507335 139856871556864 logging_writer.py:48] [74] global_step=74, grad_norm=2.2796905040740967, loss=6.255912780761719
I0215 20:14:29.397958 139856863164160 logging_writer.py:48] [75] global_step=75, grad_norm=1.2670369148254395, loss=6.213968753814697
I0215 20:14:30.314764 139856871556864 logging_writer.py:48] [76] global_step=76, grad_norm=1.2266772985458374, loss=6.161749362945557
I0215 20:14:31.223160 139856863164160 logging_writer.py:48] [77] global_step=77, grad_norm=0.9263891577720642, loss=6.128347873687744
I0215 20:14:32.125923 139856871556864 logging_writer.py:48] [78] global_step=78, grad_norm=0.5827030539512634, loss=6.117870807647705
I0215 20:14:33.025842 139856863164160 logging_writer.py:48] [79] global_step=79, grad_norm=0.2766987979412079, loss=6.07235860824585
I0215 20:14:33.938498 139856871556864 logging_writer.py:48] [80] global_step=80, grad_norm=0.5764909386634827, loss=6.055828094482422
I0215 20:14:34.858381 139856863164160 logging_writer.py:48] [81] global_step=81, grad_norm=0.40368765592575073, loss=6.034899711608887
I0215 20:14:35.781062 139856871556864 logging_writer.py:48] [82] global_step=82, grad_norm=0.23957620561122894, loss=6.031623840332031
I0215 20:14:36.702086 139856863164160 logging_writer.py:48] [83] global_step=83, grad_norm=0.23414191603660583, loss=6.0014214515686035
I0215 20:14:37.610239 139856871556864 logging_writer.py:48] [84] global_step=84, grad_norm=0.29727044701576233, loss=6.00495719909668
I0215 20:14:38.536816 139856863164160 logging_writer.py:48] [85] global_step=85, grad_norm=0.25993475317955017, loss=6.017772674560547
I0215 20:14:39.478676 139856871556864 logging_writer.py:48] [86] global_step=86, grad_norm=1.0585534572601318, loss=5.985064506530762
I0215 20:14:40.396016 139856863164160 logging_writer.py:48] [87] global_step=87, grad_norm=1.1127310991287231, loss=5.978471755981445
I0215 20:14:41.303633 139856871556864 logging_writer.py:48] [88] global_step=88, grad_norm=0.30318620800971985, loss=5.9698805809021
I0215 20:14:42.225682 139856863164160 logging_writer.py:48] [89] global_step=89, grad_norm=0.4164052903652191, loss=5.946008205413818
I0215 20:14:43.139069 139856871556864 logging_writer.py:48] [90] global_step=90, grad_norm=0.7210682034492493, loss=5.92936372756958
I0215 20:14:44.056005 139856863164160 logging_writer.py:48] [91] global_step=91, grad_norm=0.6931551098823547, loss=5.943708896636963
I0215 20:14:44.961081 139856871556864 logging_writer.py:48] [92] global_step=92, grad_norm=1.1019773483276367, loss=5.960409164428711
I0215 20:14:45.875893 139856863164160 logging_writer.py:48] [93] global_step=93, grad_norm=1.1725574731826782, loss=5.9349799156188965
I0215 20:14:46.785485 139856871556864 logging_writer.py:48] [94] global_step=94, grad_norm=0.7390155792236328, loss=5.925119876861572
I0215 20:14:47.688450 139856863164160 logging_writer.py:48] [95] global_step=95, grad_norm=0.4083738625049591, loss=5.945093154907227
I0215 20:14:48.596404 139856871556864 logging_writer.py:48] [96] global_step=96, grad_norm=0.44882461428642273, loss=5.941102981567383
I0215 20:14:49.523607 139856863164160 logging_writer.py:48] [97] global_step=97, grad_norm=0.7602614164352417, loss=5.92342472076416
I0215 20:14:50.444713 139856871556864 logging_writer.py:48] [98] global_step=98, grad_norm=1.3033205270767212, loss=5.940658092498779
I0215 20:14:51.346921 139856863164160 logging_writer.py:48] [99] global_step=99, grad_norm=1.4599006175994873, loss=5.939321994781494
I0215 20:14:52.249730 139856871556864 logging_writer.py:48] [100] global_step=100, grad_norm=1.42056143283844, loss=5.947716236114502
I0215 20:20:49.657293 139856863164160 logging_writer.py:48] [500] global_step=500, grad_norm=152.22581481933594, loss=636.4047241210938
I0215 20:28:57.929905 139856871556864 logging_writer.py:48] [1000] global_step=1000, grad_norm=177.0475616455078, loss=630.284423828125
I0215 20:36:07.429305 139858871207680 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.0720386505126953, loss=1858.31005859375
I0215 20:37:12.995311 140014697482048 spec.py:321] Evaluating on the training split.
I0215 20:37:50.100673 140014697482048 spec.py:333] Evaluating on the validation split.
I0215 20:38:39.203317 140014697482048 spec.py:349] Evaluating on the test split.
I0215 20:39:02.837718 140014697482048 submission_runner.py:408] Time since start: 1867.51s, 	Step: 1568, 	{'train/ctc_loss': Array(1743.7164, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(3002.7078, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2862.2117, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1484.367047548294, 'total_duration': 1867.507966041565, 'accumulated_submission_time': 1484.367047548294, 'accumulated_eval_time': 383.03476428985596, 'accumulated_logging_time': 0.04356122016906738}
I0215 20:39:02.873672 139858871207680 logging_writer.py:48] [1568] accumulated_eval_time=383.034764, accumulated_logging_time=0.043561, accumulated_submission_time=1484.367048, global_step=1568, preemption_count=0, score=1484.367048, test/ctc_loss=2862.211669921875, test/num_examples=2472, test/wer=0.899580, total_duration=1867.507966, train/ctc_loss=1743.7164306640625, train/wer=0.944636, validation/ctc_loss=3002.707763671875, validation/num_examples=5348, validation/wer=0.896618
I0215 20:45:51.028854 139858862814976 logging_writer.py:48] [2000] global_step=2000, grad_norm=5.562728404998779, loss=1803.58935546875
I0215 20:53:06.197783 139858871207680 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0, loss=1783.0899658203125
I0215 21:01:21.274928 139858862814976 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.09103388339281082, loss=1778.807861328125
I0215 21:03:03.429106 140014697482048 spec.py:321] Evaluating on the training split.
I0215 21:03:39.733869 140014697482048 spec.py:333] Evaluating on the validation split.
I0215 21:04:27.763540 140014697482048 spec.py:349] Evaluating on the test split.
I0215 21:04:52.121555 140014697482048 submission_runner.py:408] Time since start: 3416.80s, 	Step: 3105, 	{'train/ctc_loss': Array(1761.4772, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': Array(3355.5432, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3188.455, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2924.839037179947, 'total_duration': 3416.7955255508423, 'accumulated_submission_time': 2924.839037179947, 'accumulated_eval_time': 491.7250773906708, 'accumulated_logging_time': 0.10099291801452637}
I0215 21:04:52.150486 139858871207680 logging_writer.py:48] [3105] accumulated_eval_time=491.725077, accumulated_logging_time=0.100993, accumulated_submission_time=2924.839037, global_step=3105, preemption_count=0, score=2924.839037, test/ctc_loss=3188.455078125, test/num_examples=2472, test/wer=0.899580, total_duration=3416.795526, train/ctc_loss=1761.4771728515625, train/wer=0.942722, validation/ctc_loss=3355.543212890625, validation/num_examples=5348, validation/wer=0.896618
I0215 21:10:11.569097 139858862814976 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0495276115834713, loss=1819.980224609375
I0215 21:18:31.779260 139858871207680 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0, loss=1852.7772216796875
I0215 21:25:47.717841 139858871207680 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0, loss=1866.1483154296875
I0215 21:28:53.329497 140014697482048 spec.py:321] Evaluating on the training split.
I0215 21:29:31.612757 140014697482048 spec.py:333] Evaluating on the validation split.
I0215 21:30:18.025494 140014697482048 spec.py:349] Evaluating on the test split.
I0215 21:30:41.308766 140014697482048 submission_runner.py:408] Time since start: 4965.98s, 	Step: 4693, 	{'train/ctc_loss': Array(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4365.939663410187, 'total_duration': 4965.9764885902405, 'accumulated_submission_time': 4365.939663410187, 'accumulated_eval_time': 599.6957597732544, 'accumulated_logging_time': 0.14436078071594238}
I0215 21:30:41.344310 139858871207680 logging_writer.py:48] [4693] accumulated_eval_time=599.695760, accumulated_logging_time=0.144361, accumulated_submission_time=4365.939663, global_step=4693, preemption_count=0, score=4365.939663, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=4965.976489, train/ctc_loss=1741.2979736328125, train/wer=0.943324, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0215 21:35:08.291439 139858862814976 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1884.002197265625
I0215 21:42:28.682596 139858871207680 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0, loss=1775.5458984375
I0215 21:50:33.571926 139858862814976 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1932.06201171875
I0215 21:54:41.802603 140014697482048 spec.py:321] Evaluating on the training split.
I0215 21:55:19.094402 140014697482048 spec.py:333] Evaluating on the validation split.
I0215 21:56:06.803318 140014697482048 spec.py:349] Evaluating on the test split.
I0215 21:56:30.715335 140014697482048 submission_runner.py:408] Time since start: 6515.39s, 	Step: 6259, 	{'train/ctc_loss': Array(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 5806.31437253952, 'total_duration': 6515.386138916016, 'accumulated_submission_time': 5806.31437253952, 'accumulated_eval_time': 708.6031692028046, 'accumulated_logging_time': 0.20008563995361328}
I0215 21:56:30.747461 139858871207680 logging_writer.py:48] [6259] accumulated_eval_time=708.603169, accumulated_logging_time=0.200086, accumulated_submission_time=5806.314373, global_step=6259, preemption_count=0, score=5806.314373, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=6515.386139, train/ctc_loss=1724.861328125, train/wer=0.943700, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0215 21:59:37.960520 139858862814976 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1832.3543701171875
I0215 22:07:51.260746 139858871207680 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0, loss=1802.1697998046875
I0215 22:15:22.373932 139858871207680 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1826.9927978515625
I0215 22:20:31.739232 140014697482048 spec.py:321] Evaluating on the training split.
I0215 22:21:10.208916 140014697482048 spec.py:333] Evaluating on the validation split.
I0215 22:21:57.847903 140014697482048 spec.py:349] Evaluating on the test split.
I0215 22:22:21.205392 140014697482048 submission_runner.py:408] Time since start: 8065.87s, 	Step: 7828, 	{'train/ctc_loss': Array(1832.9288, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7247.225501060486, 'total_duration': 8065.873816013336, 'accumulated_submission_time': 7247.225501060486, 'accumulated_eval_time': 818.0614292621613, 'accumulated_logging_time': 0.2494363784790039}
I0215 22:22:21.243676 139858871207680 logging_writer.py:48] [7828] accumulated_eval_time=818.061429, accumulated_logging_time=0.249436, accumulated_submission_time=7247.225501, global_step=7828, preemption_count=0, score=7247.225501, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=8065.873816, train/ctc_loss=1832.9288330078125, train/wer=0.941551, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0215 22:24:40.931792 139858862814976 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.0, loss=1846.4959716796875
I0215 22:32:22.532431 139858871207680 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0, loss=1844.766357421875
I0215 22:40:26.628934 139858862814976 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0, loss=1848.229248046875
I0215 22:46:21.881505 140014697482048 spec.py:321] Evaluating on the training split.
I0215 22:46:59.144793 140014697482048 spec.py:333] Evaluating on the validation split.
I0215 22:47:46.947503 140014697482048 spec.py:349] Evaluating on the test split.
I0215 22:48:10.451100 140014697482048 submission_runner.py:408] Time since start: 9615.12s, 	Step: 9366, 	{'train/ctc_loss': Array(1752.8004, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 8687.782214164734, 'total_duration': 9615.12493777275, 'accumulated_submission_time': 8687.782214164734, 'accumulated_eval_time': 926.628751039505, 'accumulated_logging_time': 0.3049924373626709}
I0215 22:48:10.483329 139858287527680 logging_writer.py:48] [9366] accumulated_eval_time=926.628751, accumulated_logging_time=0.304992, accumulated_submission_time=8687.782214, global_step=9366, preemption_count=0, score=8687.782214, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=9615.124938, train/ctc_loss=1752.8004150390625, train/wer=0.942641, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0215 22:49:54.088235 139858279134976 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.0, loss=1889.9832763671875
I0215 22:57:43.898637 139858287527680 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.0, loss=1769.0430908203125
I0215 23:05:30.883521 139857632167680 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.0, loss=1839.33203125
I0215 23:12:10.997875 140014697482048 spec.py:321] Evaluating on the training split.
I0215 23:12:52.895255 140014697482048 spec.py:333] Evaluating on the validation split.
I0215 23:13:41.233631 140014697482048 spec.py:349] Evaluating on the test split.
I0215 23:14:05.343339 140014697482048 submission_runner.py:408] Time since start: 11170.01s, 	Step: 10927, 	{'train/ctc_loss': Array(1746.111, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 10128.221009969711, 'total_duration': 11170.00968503952, 'accumulated_submission_time': 10128.221009969711, 'accumulated_eval_time': 1040.9642670154572, 'accumulated_logging_time': 0.34908127784729004}
I0215 23:14:05.392227 139858287527680 logging_writer.py:48] [10927] accumulated_eval_time=1040.964267, accumulated_logging_time=0.349081, accumulated_submission_time=10128.221010, global_step=10927, preemption_count=0, score=10128.221010, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=11170.009685, train/ctc_loss=1746.1109619140625, train/wer=0.942824, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0215 23:15:01.910104 139858279134976 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.0, loss=1753.4124755859375
I0215 23:22:32.765036 139858287527680 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.0, loss=1791.1993408203125
I0215 23:30:26.936588 139858279134976 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.0, loss=1818.9443359375
I0215 23:38:05.800126 140014697482048 spec.py:321] Evaluating on the training split.
I0215 23:38:43.824464 140014697482048 spec.py:333] Evaluating on the validation split.
I0215 23:39:31.216261 140014697482048 spec.py:349] Evaluating on the test split.
I0215 23:39:55.268766 140014697482048 submission_runner.py:408] Time since start: 12719.94s, 	Step: 12476, 	{'train/ctc_loss': Array(1733.7393, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 11568.53969836235, 'total_duration': 12719.939601182938, 'accumulated_submission_time': 11568.53969836235, 'accumulated_eval_time': 1150.4276537895203, 'accumulated_logging_time': 0.4226975440979004}
I0215 23:39:55.308687 139858287527680 logging_writer.py:48] [12476] accumulated_eval_time=1150.427654, accumulated_logging_time=0.422698, accumulated_submission_time=11568.539698, global_step=12476, preemption_count=0, score=11568.539698, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=12719.939601, train/ctc_loss=1733.7392578125, train/wer=0.944086, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0215 23:40:14.528333 139858279134976 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.0, loss=1818.5567626953125
I0215 23:47:43.797640 139858287527680 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.0, loss=1763.5589599609375
I0215 23:55:49.077906 139858287527680 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.0, loss=1754.734619140625
I0216 00:03:24.021443 139858279134976 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.0, loss=1817.65283203125
I0216 00:03:55.867240 140014697482048 spec.py:321] Evaluating on the training split.
I0216 00:04:34.238728 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 00:05:22.467790 140014697482048 spec.py:349] Evaluating on the test split.
I0216 00:05:46.643633 140014697482048 submission_runner.py:408] Time since start: 14271.31s, 	Step: 14032, 	{'train/ctc_loss': Array(1786.8647, dtype=float32), 'train/wer': 0.9427990785714666, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 13009.017201900482, 'total_duration': 14271.31238245964, 'accumulated_submission_time': 13009.017201900482, 'accumulated_eval_time': 1261.1964766979218, 'accumulated_logging_time': 0.4794437885284424}
I0216 00:05:46.678931 139857263519488 logging_writer.py:48] [14032] accumulated_eval_time=1261.196477, accumulated_logging_time=0.479444, accumulated_submission_time=13009.017202, global_step=14032, preemption_count=0, score=13009.017202, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=14271.312382, train/ctc_loss=1786.86474609375, train/wer=0.942799, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 00:12:46.732497 139856935839488 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.0, loss=1802.2967529296875
I0216 00:20:01.763585 139856927446784 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.0, loss=1835.63916015625
I0216 00:28:02.520188 139857263519488 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.0, loss=1806.876220703125
I0216 00:29:46.709402 140014697482048 spec.py:321] Evaluating on the training split.
I0216 00:30:24.052660 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 00:31:10.024160 140014697482048 spec.py:349] Evaluating on the test split.
I0216 00:31:33.294578 140014697482048 submission_runner.py:408] Time since start: 15817.97s, 	Step: 15638, 	{'train/ctc_loss': Array(1755.9379, dtype=float32), 'train/wer': 0.9423383225986367, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14448.97100520134, 'total_duration': 15817.968080759048, 'accumulated_submission_time': 14448.97100520134, 'accumulated_eval_time': 1367.7788231372833, 'accumulated_logging_time': 0.5296204090118408}
I0216 00:31:33.320437 139858287527680 logging_writer.py:48] [15638] accumulated_eval_time=1367.778823, accumulated_logging_time=0.529620, accumulated_submission_time=14448.971005, global_step=15638, preemption_count=0, score=14448.971005, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=15817.968081, train/ctc_loss=1755.9378662109375, train/wer=0.942338, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 00:36:29.304286 139858279134976 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.0, loss=1782.59326171875
I0216 00:44:40.691781 139857202087680 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.0, loss=1896.5640869140625
I0216 00:51:39.166569 139857193694976 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.0, loss=1790.5728759765625
I0216 00:55:33.638691 140014697482048 spec.py:321] Evaluating on the training split.
I0216 00:56:12.156691 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 00:56:57.819818 140014697482048 spec.py:349] Evaluating on the test split.
I0216 00:57:21.394316 140014697482048 submission_runner.py:408] Time since start: 17366.07s, 	Step: 17247, 	{'train/ctc_loss': Array(1731.2493, dtype=float32), 'train/wer': 0.9431396916893625, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15889.21781039238, 'total_duration': 17366.065151691437, 'accumulated_submission_time': 15889.21781039238, 'accumulated_eval_time': 1475.5289466381073, 'accumulated_logging_time': 0.5665364265441895}
I0216 00:57:21.431306 139857990563584 logging_writer.py:48] [17247] accumulated_eval_time=1475.528947, accumulated_logging_time=0.566536, accumulated_submission_time=15889.217810, global_step=17247, preemption_count=0, score=15889.217810, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=17366.065152, train/ctc_loss=1731.249267578125, train/wer=0.943140, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 01:00:52.602255 139857982170880 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.0, loss=1804.710693359375
I0216 01:07:57.090896 139857335203584 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.0, loss=1816.749755859375
I0216 01:16:04.588459 139857326810880 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.0, loss=1801.53564453125
I0216 01:21:21.790148 140014697482048 spec.py:321] Evaluating on the training split.
I0216 01:21:59.894908 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 01:22:45.771795 140014697482048 spec.py:349] Evaluating on the test split.
I0216 01:23:09.168697 140014697482048 submission_runner.py:408] Time since start: 18913.84s, 	Step: 18886, 	{'train/ctc_loss': Array(1763.6166, dtype=float32), 'train/wer': 0.9432716912443612, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 17329.50151705742, 'total_duration': 18913.84265613556, 'accumulated_submission_time': 17329.50151705742, 'accumulated_eval_time': 1582.9052891731262, 'accumulated_logging_time': 0.6175887584686279}
I0216 01:23:09.197891 139857007523584 logging_writer.py:48] [18886] accumulated_eval_time=1582.905289, accumulated_logging_time=0.617589, accumulated_submission_time=17329.501517, global_step=18886, preemption_count=0, score=17329.501517, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=18913.842656, train/ctc_loss=1763.6165771484375, train/wer=0.943272, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 01:24:36.971508 139856999130880 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.0, loss=1810.326171875
I0216 01:32:19.686702 139857007523584 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.0, loss=1884.973388671875
I0216 01:39:14.462842 139856352163584 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.0, loss=1867.5096435546875
I0216 01:47:09.193902 140014697482048 spec.py:321] Evaluating on the training split.
I0216 01:47:48.254545 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 01:48:33.873809 140014697482048 spec.py:349] Evaluating on the test split.
I0216 01:48:57.370144 140014697482048 submission_runner.py:408] Time since start: 20462.04s, 	Step: 20497, 	{'train/ctc_loss': Array(1739.3486, dtype=float32), 'train/wer': 0.944685667249717, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 18769.421981573105, 'total_duration': 20462.040531396866, 'accumulated_submission_time': 18769.421981573105, 'accumulated_eval_time': 1691.0756063461304, 'accumulated_logging_time': 0.6611666679382324}
I0216 01:48:57.411472 139856464803584 logging_writer.py:48] [20497] accumulated_eval_time=1691.075606, accumulated_logging_time=0.661167, accumulated_submission_time=18769.421982, global_step=20497, preemption_count=0, score=18769.421982, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=20462.040531, train/ctc_loss=1739.3486328125, train/wer=0.944686, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 01:49:00.594639 139856456410880 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.0, loss=1858.156494140625
I0216 01:55:42.683021 139857990563584 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.0, loss=1792.8302001953125
I0216 02:03:35.602406 139857982170880 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.0, loss=1801.408935546875
I0216 02:10:42.259795 139857335203584 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.0, loss=1852.64306640625
I0216 02:12:57.421981 140014697482048 spec.py:321] Evaluating on the training split.
I0216 02:13:35.654646 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 02:14:21.549337 140014697482048 spec.py:349] Evaluating on the test split.
I0216 02:14:44.624881 140014697482048 submission_runner.py:408] Time since start: 22009.30s, 	Step: 22147, 	{'train/ctc_loss': Array(1769.4735, dtype=float32), 'train/wer': 0.9432456399645285, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 20209.35424208641, 'total_duration': 22009.295882701874, 'accumulated_submission_time': 20209.35424208641, 'accumulated_eval_time': 1798.2731993198395, 'accumulated_logging_time': 0.7187881469726562}
I0216 02:14:44.658173 139858871207680 logging_writer.py:48] [22147] accumulated_eval_time=1798.273199, accumulated_logging_time=0.718788, accumulated_submission_time=20209.354242, global_step=22147, preemption_count=0, score=20209.354242, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=22009.295883, train/ctc_loss=1769.4735107421875, train/wer=0.943246, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 02:19:52.424728 139858862814976 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.0, loss=1812.7618408203125
I0216 02:27:03.201919 139858871207680 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.0, loss=1779.4954833984375
I0216 02:34:55.088079 139858862814976 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.0, loss=1867.9183349609375
I0216 02:38:45.216437 140014697482048 spec.py:321] Evaluating on the training split.
I0216 02:39:22.526475 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 02:40:08.318201 140014697482048 spec.py:349] Evaluating on the test split.
I0216 02:40:31.232810 140014697482048 submission_runner.py:408] Time since start: 23555.91s, 	Step: 23747, 	{'train/ctc_loss': Array(1736.9211, dtype=float32), 'train/wer': 0.9439109001278072, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 21649.83579683304, 'total_duration': 23555.906695127487, 'accumulated_submission_time': 21649.83579683304, 'accumulated_eval_time': 1904.2872927188873, 'accumulated_logging_time': 0.7687721252441406}
I0216 02:40:31.259882 139857038239488 logging_writer.py:48] [23747] accumulated_eval_time=1904.287293, accumulated_logging_time=0.768772, accumulated_submission_time=21649.835797, global_step=23747, preemption_count=0, score=21649.835797, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=23555.906695, train/ctc_loss=1736.921142578125, train/wer=0.943911, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 02:43:44.265792 139857029846784 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.0, loss=1843.968994140625
I0216 02:51:15.533432 139857038239488 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.0, loss=1802.6773681640625
I0216 02:58:24.842907 139857038239488 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.0, loss=1855.597412109375
I0216 03:04:31.320525 140014697482048 spec.py:321] Evaluating on the training split.
I0216 03:05:10.431921 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 03:05:56.739599 140014697482048 spec.py:349] Evaluating on the test split.
I0216 03:06:20.183489 140014697482048 submission_runner.py:408] Time since start: 25104.85s, 	Step: 25402, 	{'train/ctc_loss': Array(1715.2471, dtype=float32), 'train/wer': 0.9450143703143059, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 23089.824274778366, 'total_duration': 25104.85315823555, 'accumulated_submission_time': 23089.824274778366, 'accumulated_eval_time': 2013.1436035633087, 'accumulated_logging_time': 0.8072834014892578}
I0216 03:06:20.219721 139858871207680 logging_writer.py:48] [25402] accumulated_eval_time=2013.143604, accumulated_logging_time=0.807283, accumulated_submission_time=23089.824275, global_step=25402, preemption_count=0, score=23089.824275, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=25104.853158, train/ctc_loss=1715.2470703125, train/wer=0.945014, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 03:07:35.654386 139858862814976 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.0, loss=1810.710205078125
I0216 03:14:46.966153 139858871207680 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.0, loss=1815.847900390625
I0216 03:22:27.686131 139858862814976 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.0, loss=1822.0513916015625
I0216 03:29:52.844972 139858871207680 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.0, loss=1828.5587158203125
I0216 03:30:20.458292 140014697482048 spec.py:321] Evaluating on the training split.
I0216 03:30:58.196602 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 03:31:44.791510 140014697482048 spec.py:349] Evaluating on the test split.
I0216 03:32:08.363233 140014697482048 submission_runner.py:408] Time since start: 26653.04s, 	Step: 27035, 	{'train/ctc_loss': Array(1783.4977, dtype=float32), 'train/wer': 0.9417576703068122, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 24529.986037015915, 'total_duration': 26653.03680229187, 'accumulated_submission_time': 24529.986037015915, 'accumulated_eval_time': 2121.0457701683044, 'accumulated_logging_time': 0.8580546379089355}
I0216 03:32:08.389373 139857990563584 logging_writer.py:48] [27035] accumulated_eval_time=2121.045770, accumulated_logging_time=0.858055, accumulated_submission_time=24529.986037, global_step=27035, preemption_count=0, score=24529.986037, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=26653.036802, train/ctc_loss=1783.4976806640625, train/wer=0.941758, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 03:38:55.247334 139857982170880 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.0, loss=1820.75537109375
I0216 03:46:23.378010 139857335203584 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.0, loss=1872.56396484375
I0216 03:53:54.014236 139857326810880 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.0, loss=1821.5328369140625
I0216 03:56:09.821931 140014697482048 spec.py:321] Evaluating on the training split.
I0216 03:56:48.230353 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 03:57:34.499995 140014697482048 spec.py:349] Evaluating on the test split.
I0216 03:57:57.750605 140014697482048 submission_runner.py:408] Time since start: 28202.42s, 	Step: 28643, 	{'train/ctc_loss': Array(1824.6918, dtype=float32), 'train/wer': 0.9416600198590335, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 25971.34206390381, 'total_duration': 28202.42156791687, 'accumulated_submission_time': 25971.34206390381, 'accumulated_eval_time': 2228.9691021442413, 'accumulated_logging_time': 0.8990771770477295}
I0216 03:57:57.783843 139857335203584 logging_writer.py:48] [28643] accumulated_eval_time=2228.969102, accumulated_logging_time=0.899077, accumulated_submission_time=25971.342064, global_step=28643, preemption_count=0, score=25971.342064, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=28202.421568, train/ctc_loss=1824.6917724609375, train/wer=0.941660, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 04:02:37.497487 139857990563584 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.0, loss=1927.5523681640625
I0216 04:10:09.708575 139857982170880 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.0, loss=1810.83837890625
I0216 04:17:52.597173 139857990563584 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.0, loss=1867.2371826171875
I0216 04:21:58.461198 140014697482048 spec.py:321] Evaluating on the training split.
I0216 04:22:36.878308 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 04:23:22.872047 140014697482048 spec.py:349] Evaluating on the test split.
I0216 04:23:46.408272 140014697482048 submission_runner.py:408] Time since start: 29751.08s, 	Step: 30295, 	{'train/ctc_loss': Array(1692.7738, dtype=float32), 'train/wer': 0.9447677853176417, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 27411.942588567734, 'total_duration': 29751.078408956528, 'accumulated_submission_time': 27411.942588567734, 'accumulated_eval_time': 2336.9099802970886, 'accumulated_logging_time': 0.9452264308929443}
I0216 04:23:46.441021 139857990563584 logging_writer.py:48] [30295] accumulated_eval_time=2336.909980, accumulated_logging_time=0.945226, accumulated_submission_time=27411.942589, global_step=30295, preemption_count=0, score=27411.942589, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=29751.078409, train/ctc_loss=1692.7738037109375, train/wer=0.944768, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 04:26:31.137284 139857982170880 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.0, loss=1829.081298828125
I0216 04:34:12.219370 139857335203584 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.0, loss=1823.8687744140625
I0216 04:41:29.875373 139857326810880 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.0, loss=1821.2735595703125
I0216 04:47:47.082095 140014697482048 spec.py:321] Evaluating on the training split.
I0216 04:48:25.431091 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 04:49:11.282341 140014697482048 spec.py:349] Evaluating on the test split.
I0216 04:49:35.019495 140014697482048 submission_runner.py:408] Time since start: 31299.69s, 	Step: 31897, 	{'train/ctc_loss': Array(1787.1797, dtype=float32), 'train/wer': 0.9427091658940503, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 28852.50607395172, 'total_duration': 31299.69019818306, 'accumulated_submission_time': 28852.50607395172, 'accumulated_eval_time': 2444.841779232025, 'accumulated_logging_time': 0.9952611923217773}
I0216 04:49:35.058076 139857335203584 logging_writer.py:48] [31897] accumulated_eval_time=2444.841779, accumulated_logging_time=0.995261, accumulated_submission_time=28852.506074, global_step=31897, preemption_count=0, score=28852.506074, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=31299.690198, train/ctc_loss=1787.1796875, train/wer=0.942709, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 04:50:59.867847 139857335203584 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.0, loss=1779.371826171875
I0216 04:58:13.429830 139857326810880 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.0, loss=1820.6259765625
I0216 05:06:14.194529 139857990563584 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.0, loss=1838.4073486328125
I0216 05:13:22.123109 139857982170880 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.0, loss=1836.5609130859375
I0216 05:13:35.607211 140014697482048 spec.py:321] Evaluating on the training split.
I0216 05:14:14.224128 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 05:15:00.518123 140014697482048 spec.py:349] Evaluating on the test split.
I0216 05:15:24.105893 140014697482048 submission_runner.py:408] Time since start: 32848.78s, 	Step: 33515, 	{'train/ctc_loss': Array(1714.3282, dtype=float32), 'train/wer': 0.9448971433842748, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 30292.97639656067, 'total_duration': 32848.77649331093, 'accumulated_submission_time': 30292.97639656067, 'accumulated_eval_time': 2553.334731578827, 'accumulated_logging_time': 1.0508801937103271}
I0216 05:15:24.140895 139857990563584 logging_writer.py:48] [33515] accumulated_eval_time=2553.334732, accumulated_logging_time=1.050880, accumulated_submission_time=30292.976397, global_step=33515, preemption_count=0, score=30292.976397, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=32848.776493, train/ctc_loss=1714.3282470703125, train/wer=0.944897, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 05:22:34.587425 139857335203584 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.0, loss=1896.84521484375
I0216 05:29:28.966690 139857326810880 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.0, loss=1825.820068359375
I0216 05:37:32.994172 139857335203584 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.0, loss=1819.3321533203125
I0216 05:39:24.153693 140014697482048 spec.py:321] Evaluating on the training split.
I0216 05:40:01.303456 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 05:40:47.524690 140014697482048 spec.py:349] Evaluating on the test split.
I0216 05:41:11.445589 140014697482048 submission_runner.py:408] Time since start: 34396.12s, 	Step: 35138, 	{'train/ctc_loss': Array(1760.688, dtype=float32), 'train/wer': 0.9432324554919642, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 31732.911568164825, 'total_duration': 34396.11934876442, 'accumulated_submission_time': 31732.911568164825, 'accumulated_eval_time': 2660.624226331711, 'accumulated_logging_time': 1.1010878086090088}
I0216 05:41:11.474218 139858287527680 logging_writer.py:48] [35138] accumulated_eval_time=2660.624226, accumulated_logging_time=1.101088, accumulated_submission_time=31732.911568, global_step=35138, preemption_count=0, score=31732.911568, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=34396.119349, train/ctc_loss=1760.68798828125, train/wer=0.943232, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 05:46:00.126291 139858279134976 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.0, loss=1780.9810791015625
I0216 05:53:54.566103 139858287527680 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.0, loss=1836.033935546875
I0216 06:00:49.849314 139857417127680 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.0, loss=1840.257568359375
I0216 06:05:12.122116 140014697482048 spec.py:321] Evaluating on the training split.
I0216 06:05:50.932233 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 06:06:37.781444 140014697482048 spec.py:349] Evaluating on the test split.
I0216 06:07:01.834075 140014697482048 submission_runner.py:408] Time since start: 35946.50s, 	Step: 36770, 	{'train/ctc_loss': Array(1852.6235, dtype=float32), 'train/wer': 0.941680272071945, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 33173.48407006264, 'total_duration': 35946.50369143486, 'accumulated_submission_time': 33173.48407006264, 'accumulated_eval_time': 2770.329491376877, 'accumulated_logging_time': 1.1431169509887695}
I0216 06:07:01.875268 139857478559488 logging_writer.py:48] [36770] accumulated_eval_time=2770.329491, accumulated_logging_time=1.143117, accumulated_submission_time=33173.484070, global_step=36770, preemption_count=0, score=33173.484070, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=35946.503691, train/ctc_loss=1852.62353515625, train/wer=0.941680, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 06:10:13.171553 139857470166784 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.0, loss=1825.29931640625
I0216 06:17:21.240131 139856823199488 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.0, loss=1787.1978759765625
I0216 06:25:26.383341 139856814806784 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.0, loss=1827.5145263671875
I0216 06:31:02.472225 140014697482048 spec.py:321] Evaluating on the training split.
I0216 06:31:40.161188 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 06:32:26.914092 140014697482048 spec.py:349] Evaluating on the test split.
I0216 06:32:50.247754 140014697482048 submission_runner.py:408] Time since start: 37494.92s, 	Step: 38394, 	{'train/ctc_loss': Array(1901.3732, dtype=float32), 'train/wer': 0.940312095793757, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 34614.00135946274, 'total_duration': 37494.92159795761, 'accumulated_submission_time': 34614.00135946274, 'accumulated_eval_time': 2878.1027014255524, 'accumulated_logging_time': 1.2007122039794922}
I0216 06:32:50.274629 139858441127680 logging_writer.py:48] [38394] accumulated_eval_time=2878.102701, accumulated_logging_time=1.200712, accumulated_submission_time=34614.001359, global_step=38394, preemption_count=0, score=34614.001359, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=37494.921598, train/ctc_loss=1901.3731689453125, train/wer=0.940312, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 06:34:12.213384 139858432734976 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.0, loss=1856.6741943359375
I0216 06:41:58.839921 139858441127680 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.0, loss=1811.3509521484375
I0216 06:49:08.544088 139858441127680 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.0, loss=1806.748779296875
I0216 06:56:50.553402 140014697482048 spec.py:321] Evaluating on the training split.
I0216 06:57:29.145313 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 06:58:15.633507 140014697482048 spec.py:349] Evaluating on the test split.
I0216 06:58:39.217553 140014697482048 submission_runner.py:408] Time since start: 39043.89s, 	Step: 39983, 	{'train/ctc_loss': Array(1959.8696, dtype=float32), 'train/wer': 0.9371047844119075, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 36054.20668363571, 'total_duration': 39043.88767552376, 'accumulated_submission_time': 36054.20668363571, 'accumulated_eval_time': 2986.7606551647186, 'accumulated_logging_time': 1.2392642498016357}
I0216 06:58:39.255721 139858871207680 logging_writer.py:48] [39983] accumulated_eval_time=2986.760655, accumulated_logging_time=1.239264, accumulated_submission_time=36054.206684, global_step=39983, preemption_count=0, score=36054.206684, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=39043.887676, train/ctc_loss=1959.86962890625, train/wer=0.937105, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 06:58:53.081862 139858862814976 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.0, loss=1787.1978759765625
I0216 07:05:43.487373 139858543527680 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.0, loss=1836.1656494140625
I0216 07:13:52.607019 139858535134976 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.0, loss=1894.0384521484375
I0216 07:21:13.321930 139858543527680 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.0, loss=1846.2296142578125
I0216 07:22:39.379935 140014697482048 spec.py:321] Evaluating on the training split.
I0216 07:23:17.321328 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 07:24:04.261329 140014697482048 spec.py:349] Evaluating on the test split.
I0216 07:24:28.328858 140014697482048 submission_runner.py:408] Time since start: 40593.00s, 	Step: 41600, 	{'train/ctc_loss': Array(2009.2771, dtype=float32), 'train/wer': 0.9366967129626904, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 37494.25252366066, 'total_duration': 40592.99976372719, 'accumulated_submission_time': 37494.25252366066, 'accumulated_eval_time': 3095.704182624817, 'accumulated_logging_time': 1.292348861694336}
I0216 07:24:28.367697 139858543527680 logging_writer.py:48] [41600] accumulated_eval_time=3095.704183, accumulated_logging_time=1.292349, accumulated_submission_time=37494.252524, global_step=41600, preemption_count=0, score=37494.252524, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=40592.999764, train/ctc_loss=2009.277099609375, train/wer=0.936697, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 07:30:33.173156 139858535134976 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.0, loss=1891.7989501953125
I0216 07:37:57.362654 139858871207680 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.0, loss=1864.38134765625
I0216 07:45:54.875492 139858862814976 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.0, loss=1798.498046875
I0216 07:48:29.824311 140014697482048 spec.py:321] Evaluating on the training split.
I0216 07:49:08.069314 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 07:49:55.581462 140014697482048 spec.py:349] Evaluating on the test split.
I0216 07:50:19.827722 140014697482048 submission_runner.py:408] Time since start: 42144.50s, 	Step: 43157, 	{'train/ctc_loss': Array(1873.3467, dtype=float32), 'train/wer': 0.9401422956587576, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 38935.62834739685, 'total_duration': 42144.497827768326, 'accumulated_submission_time': 38935.62834739685, 'accumulated_eval_time': 3205.701399087906, 'accumulated_logging_time': 1.350768804550171}
I0216 07:50:19.866093 139858871207680 logging_writer.py:48] [43157] accumulated_eval_time=3205.701399, accumulated_logging_time=1.350769, accumulated_submission_time=38935.628347, global_step=43157, preemption_count=0, score=38935.628347, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=42144.497828, train/ctc_loss=1873.3466796875, train/wer=0.940142, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 07:54:56.240061 139858871207680 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.0, loss=1777.8887939453125
I0216 08:02:51.737749 139858862814976 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.0, loss=1898.81494140625
I0216 08:10:42.091762 139858871207680 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.0, loss=1838.4073486328125
I0216 08:14:20.434159 140014697482048 spec.py:321] Evaluating on the training split.
I0216 08:14:58.647040 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 08:15:46.339720 140014697482048 spec.py:349] Evaluating on the test split.
I0216 08:16:10.214195 140014697482048 submission_runner.py:408] Time since start: 43694.88s, 	Step: 44745, 	{'train/ctc_loss': Array(1866.5626, dtype=float32), 'train/wer': 0.9415328062295403, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 40376.11959147453, 'total_duration': 43694.88478589058, 'accumulated_submission_time': 40376.11959147453, 'accumulated_eval_time': 3315.475694656372, 'accumulated_logging_time': 1.404447078704834}
I0216 08:16:10.248827 139858871207680 logging_writer.py:48] [44745] accumulated_eval_time=3315.475695, accumulated_logging_time=1.404447, accumulated_submission_time=40376.119591, global_step=44745, preemption_count=0, score=40376.119591, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=43694.884786, train/ctc_loss=1866.5626220703125, train/wer=0.941533, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 08:19:47.733639 139858862814976 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.0, loss=1806.23876953125
I0216 08:27:30.950013 139858871207680 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.0, loss=1858.8311767578125
I0216 08:35:12.491051 139858862814976 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.0, loss=1874.3463134765625
I0216 08:40:11.104880 140014697482048 spec.py:321] Evaluating on the training split.
I0216 08:40:49.104435 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 08:41:35.442167 140014697482048 spec.py:349] Evaluating on the test split.
I0216 08:41:59.025625 140014697482048 submission_runner.py:408] Time since start: 45243.70s, 	Step: 46301, 	{'train/ctc_loss': Array(1827.0233, dtype=float32), 'train/wer': 0.941605522275386, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 41816.90072131157, 'total_duration': 45243.69677758217, 'accumulated_submission_time': 41816.90072131157, 'accumulated_eval_time': 3423.3912692070007, 'accumulated_logging_time': 1.4543075561523438}
I0216 08:41:59.059816 139857990563584 logging_writer.py:48] [46301] accumulated_eval_time=3423.391269, accumulated_logging_time=1.454308, accumulated_submission_time=41816.900721, global_step=46301, preemption_count=0, score=41816.900721, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=45243.696778, train/ctc_loss=1827.0233154296875, train/wer=0.941606, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 08:44:39.439145 139857990563584 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.0, loss=1792.328125
I0216 08:52:21.506876 139857982170880 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.0, loss=1850.63427734375
I0216 09:00:18.852033 139857120163584 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.0, loss=1828.820068359375
I0216 09:06:00.250244 140014697482048 spec.py:321] Evaluating on the training split.
I0216 09:06:39.419153 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 09:07:27.112446 140014697482048 spec.py:349] Evaluating on the test split.
I0216 09:07:51.237917 140014697482048 submission_runner.py:408] Time since start: 46795.91s, 	Step: 47889, 	{'train/ctc_loss': Array(1805.656, dtype=float32), 'train/wer': 0.9422779591135544, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 43258.016634464264, 'total_duration': 46795.90790772438, 'accumulated_submission_time': 43258.016634464264, 'accumulated_eval_time': 3534.3726069927216, 'accumulated_logging_time': 1.5015861988067627}
I0216 09:07:51.276606 139856905123584 logging_writer.py:48] [47889] accumulated_eval_time=3534.372607, accumulated_logging_time=1.501586, accumulated_submission_time=43258.016634, global_step=47889, preemption_count=0, score=43258.016634, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=46795.907908, train/ctc_loss=1805.656005859375, train/wer=0.942278, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 09:09:16.068339 140014697482048 spec.py:321] Evaluating on the training split.
I0216 09:09:53.627655 140014697482048 spec.py:333] Evaluating on the validation split.
I0216 09:10:33.206464 140014697482048 spec.py:349] Evaluating on the test split.
I0216 09:10:53.072922 140014697482048 submission_runner.py:408] Time since start: 46977.75s, 	Step: 48000, 	{'train/ctc_loss': Array(1813.7828, dtype=float32), 'train/wer': 0.9416786903741633, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 43342.78263735771, 'total_duration': 46977.74662613869, 'accumulated_submission_time': 43342.78263735771, 'accumulated_eval_time': 3631.3745455741882, 'accumulated_logging_time': 1.5610098838806152}
I0216 09:10:53.097906 139857478559488 logging_writer.py:48] [48000] accumulated_eval_time=3631.374546, accumulated_logging_time=1.561010, accumulated_submission_time=43342.782637, global_step=48000, preemption_count=0, score=43342.782637, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=46977.746626, train/ctc_loss=1813.7828369140625, train/wer=0.941679, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0216 09:10:53.117692 139857470166784 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=43342.782637
I0216 09:10:53.338055 140014697482048 checkpoints.py:490] Saving checkpoint at step: 48000
I0216 09:10:54.319339 140014697482048 checkpoints.py:422] Saved checkpoint at /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_tanh_jax/trial_1/checkpoint_48000
I0216 09:10:54.339764 140014697482048 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_tanh_jax/trial_1/checkpoint_48000.
I0216 09:10:55.809489 140014697482048 submission_runner.py:583] Tuning trial 1/1
I0216 09:10:55.809740 140014697482048 submission_runner.py:584] Hyperparameters: Hyperparameters(learning_rate=4.131896390902391, beta1=0.9274758113254791, beta2=0.9978504782314613, warmup_steps=6999, decay_steps_factor=0.9007765761611038, end_factor=0.001, weight_decay=5.6687777311501786e-06, label_smoothing=0.2)
I0216 09:10:55.822995 140014697482048 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(32.390755, dtype=float32), 'train/wer': 4.916930429169357, 'validation/ctc_loss': Array(30.971638, dtype=float32), 'validation/wer': 4.484287052144781, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.25611, dtype=float32), 'test/wer': 4.7169581378343794, 'test/num_examples': 2472, 'score': 44.44989728927612, 'total_duration': 317.6483597755432, 'accumulated_submission_time': 44.44989728927612, 'accumulated_eval_time': 273.1984131336212, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1568, {'train/ctc_loss': Array(1743.7164, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(3002.7078, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2862.2117, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1484.367047548294, 'total_duration': 1867.507966041565, 'accumulated_submission_time': 1484.367047548294, 'accumulated_eval_time': 383.03476428985596, 'accumulated_logging_time': 0.04356122016906738, 'global_step': 1568, 'preemption_count': 0}), (3105, {'train/ctc_loss': Array(1761.4772, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': Array(3355.5432, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3188.455, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2924.839037179947, 'total_duration': 3416.7955255508423, 'accumulated_submission_time': 2924.839037179947, 'accumulated_eval_time': 491.7250773906708, 'accumulated_logging_time': 0.10099291801452637, 'global_step': 3105, 'preemption_count': 0}), (4693, {'train/ctc_loss': Array(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4365.939663410187, 'total_duration': 4965.9764885902405, 'accumulated_submission_time': 4365.939663410187, 'accumulated_eval_time': 599.6957597732544, 'accumulated_logging_time': 0.14436078071594238, 'global_step': 4693, 'preemption_count': 0}), (6259, {'train/ctc_loss': Array(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 5806.31437253952, 'total_duration': 6515.386138916016, 'accumulated_submission_time': 5806.31437253952, 'accumulated_eval_time': 708.6031692028046, 'accumulated_logging_time': 0.20008563995361328, 'global_step': 6259, 'preemption_count': 0}), (7828, {'train/ctc_loss': Array(1832.9288, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7247.225501060486, 'total_duration': 8065.873816013336, 'accumulated_submission_time': 7247.225501060486, 'accumulated_eval_time': 818.0614292621613, 'accumulated_logging_time': 0.2494363784790039, 'global_step': 7828, 'preemption_count': 0}), (9366, {'train/ctc_loss': Array(1752.8004, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 8687.782214164734, 'total_duration': 9615.12493777275, 'accumulated_submission_time': 8687.782214164734, 'accumulated_eval_time': 926.628751039505, 'accumulated_logging_time': 0.3049924373626709, 'global_step': 9366, 'preemption_count': 0}), (10927, {'train/ctc_loss': Array(1746.111, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 10128.221009969711, 'total_duration': 11170.00968503952, 'accumulated_submission_time': 10128.221009969711, 'accumulated_eval_time': 1040.9642670154572, 'accumulated_logging_time': 0.34908127784729004, 'global_step': 10927, 'preemption_count': 0}), (12476, {'train/ctc_loss': Array(1733.7393, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 11568.53969836235, 'total_duration': 12719.939601182938, 'accumulated_submission_time': 11568.53969836235, 'accumulated_eval_time': 1150.4276537895203, 'accumulated_logging_time': 0.4226975440979004, 'global_step': 12476, 'preemption_count': 0}), (14032, {'train/ctc_loss': Array(1786.8647, dtype=float32), 'train/wer': 0.9427990785714666, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 13009.017201900482, 'total_duration': 14271.31238245964, 'accumulated_submission_time': 13009.017201900482, 'accumulated_eval_time': 1261.1964766979218, 'accumulated_logging_time': 0.4794437885284424, 'global_step': 14032, 'preemption_count': 0}), (15638, {'train/ctc_loss': Array(1755.9379, dtype=float32), 'train/wer': 0.9423383225986367, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14448.97100520134, 'total_duration': 15817.968080759048, 'accumulated_submission_time': 14448.97100520134, 'accumulated_eval_time': 1367.7788231372833, 'accumulated_logging_time': 0.5296204090118408, 'global_step': 15638, 'preemption_count': 0}), (17247, {'train/ctc_loss': Array(1731.2493, dtype=float32), 'train/wer': 0.9431396916893625, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15889.21781039238, 'total_duration': 17366.065151691437, 'accumulated_submission_time': 15889.21781039238, 'accumulated_eval_time': 1475.5289466381073, 'accumulated_logging_time': 0.5665364265441895, 'global_step': 17247, 'preemption_count': 0}), (18886, {'train/ctc_loss': Array(1763.6166, dtype=float32), 'train/wer': 0.9432716912443612, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 17329.50151705742, 'total_duration': 18913.84265613556, 'accumulated_submission_time': 17329.50151705742, 'accumulated_eval_time': 1582.9052891731262, 'accumulated_logging_time': 0.6175887584686279, 'global_step': 18886, 'preemption_count': 0}), (20497, {'train/ctc_loss': Array(1739.3486, dtype=float32), 'train/wer': 0.944685667249717, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 18769.421981573105, 'total_duration': 20462.040531396866, 'accumulated_submission_time': 18769.421981573105, 'accumulated_eval_time': 1691.0756063461304, 'accumulated_logging_time': 0.6611666679382324, 'global_step': 20497, 'preemption_count': 0}), (22147, {'train/ctc_loss': Array(1769.4735, dtype=float32), 'train/wer': 0.9432456399645285, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 20209.35424208641, 'total_duration': 22009.295882701874, 'accumulated_submission_time': 20209.35424208641, 'accumulated_eval_time': 1798.2731993198395, 'accumulated_logging_time': 0.7187881469726562, 'global_step': 22147, 'preemption_count': 0}), (23747, {'train/ctc_loss': Array(1736.9211, dtype=float32), 'train/wer': 0.9439109001278072, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 21649.83579683304, 'total_duration': 23555.906695127487, 'accumulated_submission_time': 21649.83579683304, 'accumulated_eval_time': 1904.2872927188873, 'accumulated_logging_time': 0.7687721252441406, 'global_step': 23747, 'preemption_count': 0}), (25402, {'train/ctc_loss': Array(1715.2471, dtype=float32), 'train/wer': 0.9450143703143059, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 23089.824274778366, 'total_duration': 25104.85315823555, 'accumulated_submission_time': 23089.824274778366, 'accumulated_eval_time': 2013.1436035633087, 'accumulated_logging_time': 0.8072834014892578, 'global_step': 25402, 'preemption_count': 0}), (27035, {'train/ctc_loss': Array(1783.4977, dtype=float32), 'train/wer': 0.9417576703068122, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 24529.986037015915, 'total_duration': 26653.03680229187, 'accumulated_submission_time': 24529.986037015915, 'accumulated_eval_time': 2121.0457701683044, 'accumulated_logging_time': 0.8580546379089355, 'global_step': 27035, 'preemption_count': 0}), (28643, {'train/ctc_loss': Array(1824.6918, dtype=float32), 'train/wer': 0.9416600198590335, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 25971.34206390381, 'total_duration': 28202.42156791687, 'accumulated_submission_time': 25971.34206390381, 'accumulated_eval_time': 2228.9691021442413, 'accumulated_logging_time': 0.8990771770477295, 'global_step': 28643, 'preemption_count': 0}), (30295, {'train/ctc_loss': Array(1692.7738, dtype=float32), 'train/wer': 0.9447677853176417, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 27411.942588567734, 'total_duration': 29751.078408956528, 'accumulated_submission_time': 27411.942588567734, 'accumulated_eval_time': 2336.9099802970886, 'accumulated_logging_time': 0.9452264308929443, 'global_step': 30295, 'preemption_count': 0}), (31897, {'train/ctc_loss': Array(1787.1797, dtype=float32), 'train/wer': 0.9427091658940503, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 28852.50607395172, 'total_duration': 31299.69019818306, 'accumulated_submission_time': 28852.50607395172, 'accumulated_eval_time': 2444.841779232025, 'accumulated_logging_time': 0.9952611923217773, 'global_step': 31897, 'preemption_count': 0}), (33515, {'train/ctc_loss': Array(1714.3282, dtype=float32), 'train/wer': 0.9448971433842748, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 30292.97639656067, 'total_duration': 32848.77649331093, 'accumulated_submission_time': 30292.97639656067, 'accumulated_eval_time': 2553.334731578827, 'accumulated_logging_time': 1.0508801937103271, 'global_step': 33515, 'preemption_count': 0}), (35138, {'train/ctc_loss': Array(1760.688, dtype=float32), 'train/wer': 0.9432324554919642, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 31732.911568164825, 'total_duration': 34396.11934876442, 'accumulated_submission_time': 31732.911568164825, 'accumulated_eval_time': 2660.624226331711, 'accumulated_logging_time': 1.1010878086090088, 'global_step': 35138, 'preemption_count': 0}), (36770, {'train/ctc_loss': Array(1852.6235, dtype=float32), 'train/wer': 0.941680272071945, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 33173.48407006264, 'total_duration': 35946.50369143486, 'accumulated_submission_time': 33173.48407006264, 'accumulated_eval_time': 2770.329491376877, 'accumulated_logging_time': 1.1431169509887695, 'global_step': 36770, 'preemption_count': 0}), (38394, {'train/ctc_loss': Array(1901.3732, dtype=float32), 'train/wer': 0.940312095793757, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 34614.00135946274, 'total_duration': 37494.92159795761, 'accumulated_submission_time': 34614.00135946274, 'accumulated_eval_time': 2878.1027014255524, 'accumulated_logging_time': 1.2007122039794922, 'global_step': 38394, 'preemption_count': 0}), (39983, {'train/ctc_loss': Array(1959.8696, dtype=float32), 'train/wer': 0.9371047844119075, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 36054.20668363571, 'total_duration': 39043.88767552376, 'accumulated_submission_time': 36054.20668363571, 'accumulated_eval_time': 2986.7606551647186, 'accumulated_logging_time': 1.2392642498016357, 'global_step': 39983, 'preemption_count': 0}), (41600, {'train/ctc_loss': Array(2009.2771, dtype=float32), 'train/wer': 0.9366967129626904, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 37494.25252366066, 'total_duration': 40592.99976372719, 'accumulated_submission_time': 37494.25252366066, 'accumulated_eval_time': 3095.704182624817, 'accumulated_logging_time': 1.292348861694336, 'global_step': 41600, 'preemption_count': 0}), (43157, {'train/ctc_loss': Array(1873.3467, dtype=float32), 'train/wer': 0.9401422956587576, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 38935.62834739685, 'total_duration': 42144.497827768326, 'accumulated_submission_time': 38935.62834739685, 'accumulated_eval_time': 3205.701399087906, 'accumulated_logging_time': 1.350768804550171, 'global_step': 43157, 'preemption_count': 0}), (44745, {'train/ctc_loss': Array(1866.5626, dtype=float32), 'train/wer': 0.9415328062295403, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 40376.11959147453, 'total_duration': 43694.88478589058, 'accumulated_submission_time': 40376.11959147453, 'accumulated_eval_time': 3315.475694656372, 'accumulated_logging_time': 1.404447078704834, 'global_step': 44745, 'preemption_count': 0}), (46301, {'train/ctc_loss': Array(1827.0233, dtype=float32), 'train/wer': 0.941605522275386, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 41816.90072131157, 'total_duration': 45243.69677758217, 'accumulated_submission_time': 41816.90072131157, 'accumulated_eval_time': 3423.3912692070007, 'accumulated_logging_time': 1.4543075561523438, 'global_step': 46301, 'preemption_count': 0}), (47889, {'train/ctc_loss': Array(1805.656, dtype=float32), 'train/wer': 0.9422779591135544, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 43258.016634464264, 'total_duration': 46795.90790772438, 'accumulated_submission_time': 43258.016634464264, 'accumulated_eval_time': 3534.3726069927216, 'accumulated_logging_time': 1.5015861988067627, 'global_step': 47889, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(1813.7828, dtype=float32), 'train/wer': 0.9416786903741633, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 43342.78263735771, 'total_duration': 46977.74662613869, 'accumulated_submission_time': 43342.78263735771, 'accumulated_eval_time': 3631.3745455741882, 'accumulated_logging_time': 1.5610098838806152, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0216 09:10:55.823232 140014697482048 submission_runner.py:586] Timing: 43342.78263735771
I0216 09:10:55.823314 140014697482048 submission_runner.py:588] Total number of evals: 32
I0216 09:10:55.823382 140014697482048 submission_runner.py:589] ====================
I0216 09:10:55.827292 140014697482048 submission_runner.py:673] Final librispeech_deepspeech_tanh score: 43342.78263735771
