python3 submission_runner.py --framework=jax --workload=imagenet_resnet_gelu --submission_path=reference_algorithms/target_setting_algorithms/jax_momentum.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --rng_seed=2511472366 --max_global_steps=186666 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=external --tuning_search_space=reference_algorithms/target_setting_algorithms/imagenet_resnet_gelu/tuning_search_space.json --num_tuning_trials=1 2>&1 | tee -a /logs/imagenet_resnet_gelu_jax_03-15-2024-02-12-09.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0315 02:12:30.519844 139666979968832 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/imagenet_resnet_gelu_jax.
I0315 02:12:31.538043 139666979968832 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0315 02:12:31.539682 139666979968832 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0315 02:12:31.539860 139666979968832 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0315 02:12:31.546167 139666979968832 submission_runner.py:554] Using RNG seed 2511472366
I0315 02:12:32.696124 139666979968832 submission_runner.py:563] --- Tuning run 1/1 ---
I0315 02:12:32.696366 139666979968832 submission_runner.py:568] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/imagenet_resnet_gelu_jax/trial_1.
I0315 02:12:32.696553 139666979968832 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/imagenet_resnet_gelu_jax/trial_1/hparams.json.
I0315 02:12:32.876711 139666979968832 submission_runner.py:209] Initializing dataset.
I0315 02:12:32.892657 139666979968832 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0315 02:12:32.903834 139666979968832 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0315 02:12:33.279973 139666979968832 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0315 02:12:34.465339 139666979968832 submission_runner.py:220] Initializing model.
I0315 02:12:44.877267 139666979968832 submission_runner.py:262] Initializing optimizer.
I0315 02:12:46.424067 139666979968832 submission_runner.py:269] Initializing metrics bundle.
I0315 02:12:46.424306 139666979968832 submission_runner.py:287] Initializing checkpoint and logger.
I0315 02:12:46.425763 139666979968832 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/imagenet_resnet_gelu_jax/trial_1 with prefix checkpoint_
I0315 02:12:46.425942 139666979968832 submission_runner.py:307] Saving meta data to /experiment_runs/variants_target_setting/study_0/imagenet_resnet_gelu_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0315 02:12:46.717522 139666979968832 logger_utils.py:220] Unable to record git information. Continuing without it.
I0315 02:12:46.990071 139666979968832 submission_runner.py:311] Saving flags to /experiment_runs/variants_target_setting/study_0/imagenet_resnet_gelu_jax/trial_1/flags_0.json.
I0315 02:12:47.000360 139666979968832 submission_runner.py:321] Starting training loop.
I0315 02:13:26.462594 139504580228864 logging_writer.py:48] [0] global_step=0, grad_norm=0.45109087228775024, loss=6.914555549621582
I0315 02:13:26.479265 139666979968832 spec.py:321] Evaluating on the training split.
I0315 02:13:27.428659 139666979968832 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0315 02:13:27.438623 139666979968832 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0315 02:13:27.534708 139666979968832 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0315 02:13:45.680802 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 02:13:46.935221 139666979968832 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0315 02:13:46.957584 139666979968832 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0315 02:13:47.025084 139666979968832 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0315 02:14:03.495799 139666979968832 spec.py:349] Evaluating on the test split.
I0315 02:14:04.283931 139666979968832 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0315 02:14:04.291575 139666979968832 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0315 02:14:04.330111 139666979968832 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0315 02:14:08.836417 139666979968832 submission_runner.py:420] Time since start: 81.84s, 	Step: 1, 	{'train/accuracy': 0.0008968430920504034, 'train/loss': 6.907759666442871, 'validation/accuracy': 0.0009199999622069299, 'validation/loss': 6.907748699188232, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.907728672027588, 'test/num_examples': 10000, 'score': 39.47880697250366, 'total_duration': 81.83598804473877, 'accumulated_submission_time': 39.47880697250366, 'accumulated_eval_time': 42.357078313827515, 'accumulated_logging_time': 0}
I0315 02:14:08.856089 139487510259456 logging_writer.py:48] [1] accumulated_eval_time=42.357078, accumulated_logging_time=0, accumulated_submission_time=39.478807, global_step=1, preemption_count=0, score=39.478807, test/accuracy=0.000900, test/loss=6.907729, test/num_examples=10000, total_duration=81.835988, train/accuracy=0.000897, train/loss=6.907760, validation/accuracy=0.000920, validation/loss=6.907749, validation/num_examples=50000
I0315 02:14:09.052730 139487501866752 logging_writer.py:48] [1] global_step=1, grad_norm=0.4924682080745697, loss=6.918041229248047
I0315 02:14:09.244436 139487510259456 logging_writer.py:48] [2] global_step=2, grad_norm=0.4717642664909363, loss=6.924528121948242
I0315 02:14:09.437907 139487501866752 logging_writer.py:48] [3] global_step=3, grad_norm=0.464079350233078, loss=6.9162797927856445
I0315 02:14:09.634258 139487510259456 logging_writer.py:48] [4] global_step=4, grad_norm=0.49309465289115906, loss=6.914824485778809
I0315 02:14:09.830128 139487501866752 logging_writer.py:48] [5] global_step=5, grad_norm=0.4575231075286865, loss=6.914912223815918
I0315 02:14:10.024880 139487510259456 logging_writer.py:48] [6] global_step=6, grad_norm=0.46342116594314575, loss=6.915288925170898
I0315 02:14:10.217490 139487501866752 logging_writer.py:48] [7] global_step=7, grad_norm=0.47111910581588745, loss=6.920736312866211
I0315 02:14:10.413193 139487510259456 logging_writer.py:48] [8] global_step=8, grad_norm=0.45104309916496277, loss=6.915964126586914
I0315 02:14:10.607625 139487501866752 logging_writer.py:48] [9] global_step=9, grad_norm=0.451616495847702, loss=6.912591934204102
I0315 02:14:10.802256 139487510259456 logging_writer.py:48] [10] global_step=10, grad_norm=0.4697999656200409, loss=6.909755706787109
I0315 02:14:10.997979 139487501866752 logging_writer.py:48] [11] global_step=11, grad_norm=0.4797881841659546, loss=6.912376880645752
I0315 02:14:11.198517 139487510259456 logging_writer.py:48] [12] global_step=12, grad_norm=0.4686944782733917, loss=6.916291236877441
I0315 02:14:11.391865 139487501866752 logging_writer.py:48] [13] global_step=13, grad_norm=0.4875860810279846, loss=6.923865795135498
I0315 02:14:11.587344 139487510259456 logging_writer.py:48] [14] global_step=14, grad_norm=0.46783605217933655, loss=6.916301250457764
I0315 02:14:11.780213 139487501866752 logging_writer.py:48] [15] global_step=15, grad_norm=0.4628860652446747, loss=6.915982246398926
I0315 02:14:11.973619 139487510259456 logging_writer.py:48] [16] global_step=16, grad_norm=0.4422926902770996, loss=6.917388916015625
I0315 02:14:12.170881 139487501866752 logging_writer.py:48] [17] global_step=17, grad_norm=0.4639987349510193, loss=6.917855262756348
I0315 02:14:12.370384 139487510259456 logging_writer.py:48] [18] global_step=18, grad_norm=0.43755704164505005, loss=6.9128828048706055
I0315 02:14:12.562457 139487501866752 logging_writer.py:48] [19] global_step=19, grad_norm=0.4703456163406372, loss=6.918854713439941
I0315 02:14:12.754458 139487510259456 logging_writer.py:48] [20] global_step=20, grad_norm=0.4564877152442932, loss=6.915417194366455
I0315 02:14:12.950346 139487501866752 logging_writer.py:48] [21] global_step=21, grad_norm=0.4618018567562103, loss=6.909186840057373
I0315 02:14:13.143276 139487510259456 logging_writer.py:48] [22] global_step=22, grad_norm=0.45547711849212646, loss=6.915698528289795
I0315 02:14:13.339267 139487501866752 logging_writer.py:48] [23] global_step=23, grad_norm=0.44061970710754395, loss=6.915600299835205
I0315 02:14:13.547990 139487510259456 logging_writer.py:48] [24] global_step=24, grad_norm=0.47435879707336426, loss=6.916828155517578
I0315 02:14:13.750588 139487501866752 logging_writer.py:48] [25] global_step=25, grad_norm=0.4636573791503906, loss=6.916322708129883
I0315 02:14:13.946810 139487510259456 logging_writer.py:48] [26] global_step=26, grad_norm=0.47281455993652344, loss=6.915593147277832
I0315 02:14:14.143063 139487501866752 logging_writer.py:48] [27] global_step=27, grad_norm=0.4753452241420746, loss=6.921272277832031
I0315 02:14:14.335310 139487510259456 logging_writer.py:48] [28] global_step=28, grad_norm=0.47356581687927246, loss=6.911547660827637
I0315 02:14:14.528384 139487501866752 logging_writer.py:48] [29] global_step=29, grad_norm=0.5025587677955627, loss=6.91286039352417
I0315 02:14:14.724982 139487510259456 logging_writer.py:48] [30] global_step=30, grad_norm=0.460034042596817, loss=6.910801410675049
I0315 02:14:14.920420 139487501866752 logging_writer.py:48] [31] global_step=31, grad_norm=0.45688748359680176, loss=6.906607627868652
I0315 02:14:15.117331 139487510259456 logging_writer.py:48] [32] global_step=32, grad_norm=0.46020016074180603, loss=6.910977363586426
I0315 02:14:15.311596 139487501866752 logging_writer.py:48] [33] global_step=33, grad_norm=0.46627339720726013, loss=6.911861419677734
I0315 02:14:15.506258 139487510259456 logging_writer.py:48] [34] global_step=34, grad_norm=0.4564346373081207, loss=6.912149906158447
I0315 02:14:15.702113 139487501866752 logging_writer.py:48] [35] global_step=35, grad_norm=0.4494117200374603, loss=6.9121222496032715
I0315 02:14:15.894542 139487510259456 logging_writer.py:48] [36] global_step=36, grad_norm=0.4764162302017212, loss=6.90786600112915
I0315 02:14:16.084822 139487501866752 logging_writer.py:48] [37] global_step=37, grad_norm=0.4670153260231018, loss=6.9068145751953125
I0315 02:14:16.281522 139487510259456 logging_writer.py:48] [38] global_step=38, grad_norm=0.4717005789279938, loss=6.915088653564453
I0315 02:14:16.473758 139487501866752 logging_writer.py:48] [39] global_step=39, grad_norm=0.44066357612609863, loss=6.905196189880371
I0315 02:14:16.668395 139487510259456 logging_writer.py:48] [40] global_step=40, grad_norm=0.463358074426651, loss=6.9121551513671875
I0315 02:14:16.860180 139487501866752 logging_writer.py:48] [41] global_step=41, grad_norm=0.5111523270606995, loss=6.900805473327637
I0315 02:14:17.055339 139487510259456 logging_writer.py:48] [42] global_step=42, grad_norm=0.4659053683280945, loss=6.903397083282471
I0315 02:14:17.250036 139487501866752 logging_writer.py:48] [43] global_step=43, grad_norm=0.49687618017196655, loss=6.898117542266846
I0315 02:14:17.444772 139487510259456 logging_writer.py:48] [44] global_step=44, grad_norm=0.4553769528865814, loss=6.905263900756836
I0315 02:14:17.637834 139487501866752 logging_writer.py:48] [45] global_step=45, grad_norm=0.4615108072757721, loss=6.908215045928955
I0315 02:14:17.837362 139487510259456 logging_writer.py:48] [46] global_step=46, grad_norm=0.46795836091041565, loss=6.908315658569336
I0315 02:14:18.030992 139487501866752 logging_writer.py:48] [47] global_step=47, grad_norm=0.46108153462409973, loss=6.902163028717041
I0315 02:14:18.226373 139487510259456 logging_writer.py:48] [48] global_step=48, grad_norm=0.45635470747947693, loss=6.902677536010742
I0315 02:14:18.422873 139487501866752 logging_writer.py:48] [49] global_step=49, grad_norm=0.44179797172546387, loss=6.9054718017578125
I0315 02:14:18.622590 139487510259456 logging_writer.py:48] [50] global_step=50, grad_norm=0.44604113698005676, loss=6.901742935180664
I0315 02:14:18.818938 139487501866752 logging_writer.py:48] [51] global_step=51, grad_norm=0.4387686848640442, loss=6.909313678741455
I0315 02:14:19.009758 139487510259456 logging_writer.py:48] [52] global_step=52, grad_norm=0.46516910195350647, loss=6.909279823303223
I0315 02:14:19.200296 139487501866752 logging_writer.py:48] [53] global_step=53, grad_norm=0.44328972697257996, loss=6.904409885406494
I0315 02:14:19.394587 139487510259456 logging_writer.py:48] [54] global_step=54, grad_norm=0.4764648675918579, loss=6.896123886108398
I0315 02:14:19.590643 139487501866752 logging_writer.py:48] [55] global_step=55, grad_norm=0.45947474241256714, loss=6.8954362869262695
I0315 02:14:19.784197 139487510259456 logging_writer.py:48] [56] global_step=56, grad_norm=0.453841894865036, loss=6.899471282958984
I0315 02:14:19.980841 139487501866752 logging_writer.py:48] [57] global_step=57, grad_norm=0.46468907594680786, loss=6.889203071594238
I0315 02:14:20.173512 139487510259456 logging_writer.py:48] [58] global_step=58, grad_norm=0.45630401372909546, loss=6.897167205810547
I0315 02:14:20.367121 139487501866752 logging_writer.py:48] [59] global_step=59, grad_norm=0.4819408059120178, loss=6.89306640625
I0315 02:14:20.568161 139487510259456 logging_writer.py:48] [60] global_step=60, grad_norm=0.42916563153266907, loss=6.896327018737793
I0315 02:14:20.760306 139487501866752 logging_writer.py:48] [61] global_step=61, grad_norm=0.4665355980396271, loss=6.8814592361450195
I0315 02:14:20.960298 139487510259456 logging_writer.py:48] [62] global_step=62, grad_norm=0.4556010067462921, loss=6.893756866455078
I0315 02:14:21.154028 139487501866752 logging_writer.py:48] [63] global_step=63, grad_norm=0.48346298933029175, loss=6.895323753356934
I0315 02:14:21.345301 139487510259456 logging_writer.py:48] [64] global_step=64, grad_norm=0.4597512483596802, loss=6.892162322998047
I0315 02:14:21.537755 139487501866752 logging_writer.py:48] [65] global_step=65, grad_norm=0.46558642387390137, loss=6.8916754722595215
I0315 02:14:21.732659 139487510259456 logging_writer.py:48] [66] global_step=66, grad_norm=0.4623255729675293, loss=6.886163711547852
I0315 02:14:21.928360 139487501866752 logging_writer.py:48] [67] global_step=67, grad_norm=0.4632783830165863, loss=6.883843898773193
I0315 02:14:22.119071 139487510259456 logging_writer.py:48] [68] global_step=68, grad_norm=0.4441315531730652, loss=6.901140213012695
I0315 02:14:22.315347 139487501866752 logging_writer.py:48] [69] global_step=69, grad_norm=0.4802294373512268, loss=6.888705730438232
I0315 02:14:22.510890 139487510259456 logging_writer.py:48] [70] global_step=70, grad_norm=0.463704377412796, loss=6.894723415374756
I0315 02:14:22.703027 139487501866752 logging_writer.py:48] [71] global_step=71, grad_norm=0.44235819578170776, loss=6.895810127258301
I0315 02:14:22.901713 139487510259456 logging_writer.py:48] [72] global_step=72, grad_norm=0.4430457055568695, loss=6.891446113586426
I0315 02:14:23.096513 139487501866752 logging_writer.py:48] [73] global_step=73, grad_norm=0.4672733247280121, loss=6.895183563232422
I0315 02:14:23.289913 139487510259456 logging_writer.py:48] [74] global_step=74, grad_norm=0.4496121108531952, loss=6.89423131942749
I0315 02:14:23.485719 139487501866752 logging_writer.py:48] [75] global_step=75, grad_norm=0.43303173780441284, loss=6.889346122741699
I0315 02:14:23.681993 139487510259456 logging_writer.py:48] [76] global_step=76, grad_norm=0.4589328169822693, loss=6.886869430541992
I0315 02:14:23.877668 139487501866752 logging_writer.py:48] [77] global_step=77, grad_norm=0.4507331848144531, loss=6.886878967285156
I0315 02:14:24.075612 139487510259456 logging_writer.py:48] [78] global_step=78, grad_norm=0.4716525077819824, loss=6.892312526702881
I0315 02:14:24.272156 139487501866752 logging_writer.py:48] [79] global_step=79, grad_norm=0.46506577730178833, loss=6.8847432136535645
I0315 02:14:24.466885 139487510259456 logging_writer.py:48] [80] global_step=80, grad_norm=0.4428040683269501, loss=6.8898773193359375
I0315 02:14:24.659581 139487501866752 logging_writer.py:48] [81] global_step=81, grad_norm=0.46331822872161865, loss=6.883700370788574
I0315 02:14:24.851667 139487510259456 logging_writer.py:48] [82] global_step=82, grad_norm=0.4537663757801056, loss=6.877094268798828
I0315 02:14:25.049705 139487501866752 logging_writer.py:48] [83] global_step=83, grad_norm=0.4301806390285492, loss=6.88917875289917
I0315 02:14:25.244945 139487510259456 logging_writer.py:48] [84] global_step=84, grad_norm=0.47787439823150635, loss=6.880514144897461
I0315 02:14:25.446527 139487501866752 logging_writer.py:48] [85] global_step=85, grad_norm=0.4747883081436157, loss=6.887233734130859
I0315 02:14:25.643847 139487510259456 logging_writer.py:48] [86] global_step=86, grad_norm=0.4806898236274719, loss=6.886919975280762
I0315 02:14:25.835868 139487501866752 logging_writer.py:48] [87] global_step=87, grad_norm=0.4569828510284424, loss=6.88740873336792
I0315 02:14:26.034853 139487510259456 logging_writer.py:48] [88] global_step=88, grad_norm=0.48335570096969604, loss=6.887027263641357
I0315 02:14:26.237039 139487501866752 logging_writer.py:48] [89] global_step=89, grad_norm=0.44241955876350403, loss=6.879003524780273
I0315 02:14:26.429862 139487510259456 logging_writer.py:48] [90] global_step=90, grad_norm=0.4839761257171631, loss=6.882107734680176
I0315 02:14:26.623546 139487501866752 logging_writer.py:48] [91] global_step=91, grad_norm=0.4826143682003021, loss=6.868088722229004
I0315 02:14:26.818545 139487510259456 logging_writer.py:48] [92] global_step=92, grad_norm=0.46838247776031494, loss=6.877178192138672
I0315 02:14:27.012217 139487501866752 logging_writer.py:48] [93] global_step=93, grad_norm=0.4758879542350769, loss=6.886288642883301
I0315 02:14:27.207764 139487510259456 logging_writer.py:48] [94] global_step=94, grad_norm=0.4717923700809479, loss=6.878839492797852
I0315 02:14:27.402371 139487501866752 logging_writer.py:48] [95] global_step=95, grad_norm=0.44383224844932556, loss=6.888631820678711
I0315 02:14:27.599255 139487510259456 logging_writer.py:48] [96] global_step=96, grad_norm=0.4697971045970917, loss=6.888572692871094
I0315 02:14:27.793314 139487501866752 logging_writer.py:48] [97] global_step=97, grad_norm=0.45809584856033325, loss=6.868486404418945
I0315 02:14:27.987871 139487510259456 logging_writer.py:48] [98] global_step=98, grad_norm=0.4632987380027771, loss=6.890031814575195
I0315 02:14:28.179764 139487501866752 logging_writer.py:48] [99] global_step=99, grad_norm=0.45529642701148987, loss=6.865653991699219
I0315 02:14:28.369658 139487510259456 logging_writer.py:48] [100] global_step=100, grad_norm=0.44404274225234985, loss=6.892746448516846
I0315 02:15:40.959597 139487501866752 logging_writer.py:48] [500] global_step=500, grad_norm=0.5344305038452148, loss=6.5214433670043945
I0315 02:17:11.771114 139487510259456 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.45848050713539124, loss=6.096764087677002
I0315 02:18:42.598631 139487501866752 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.47936156392097473, loss=5.7077460289001465
I0315 02:20:13.459223 139487510259456 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.501164972782135, loss=5.509013652801514
I0315 02:21:44.273247 139487501866752 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.4943593740463257, loss=5.188870429992676
I0315 02:22:38.862503 139666979968832 spec.py:321] Evaluating on the training split.
I0315 02:22:46.235793 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 02:22:54.686362 139666979968832 spec.py:349] Evaluating on the test split.
I0315 02:22:57.033166 139666979968832 submission_runner.py:420] Time since start: 610.03s, 	Step: 2802, 	{'train/accuracy': 0.2709462642669678, 'train/loss': 3.717517614364624, 'validation/accuracy': 0.22853998839855194, 'validation/loss': 3.955676555633545, 'validation/num_examples': 50000, 'test/accuracy': 0.16760000586509705, 'test/loss': 4.43997859954834, 'test/num_examples': 10000, 'score': 549.3842663764954, 'total_duration': 610.0327498912811, 'accumulated_submission_time': 549.3842663764954, 'accumulated_eval_time': 60.52774119377136, 'accumulated_logging_time': 0.028769254684448242}
I0315 02:22:57.050574 139487518652160 logging_writer.py:48] [2802] accumulated_eval_time=60.527741, accumulated_logging_time=0.028769, accumulated_submission_time=549.384266, global_step=2802, preemption_count=0, score=549.384266, test/accuracy=0.167600, test/loss=4.439979, test/num_examples=10000, total_duration=610.032750, train/accuracy=0.270946, train/loss=3.717518, validation/accuracy=0.228540, validation/loss=3.955677, validation/num_examples=50000
I0315 02:23:33.204910 139487669622528 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.48764368891716003, loss=5.006917953491211
I0315 02:25:04.032124 139487518652160 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.5424694418907166, loss=4.861527919769287
I0315 02:26:34.874041 139487669622528 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.4301193356513977, loss=4.781474590301514
I0315 02:28:05.733508 139487518652160 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.43257445096969604, loss=4.733984470367432
I0315 02:29:36.580951 139487669622528 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.41288912296295166, loss=4.541444778442383
I0315 02:31:07.414126 139487518652160 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.41719698905944824, loss=4.609151840209961
I0315 02:31:27.095511 139666979968832 spec.py:321] Evaluating on the training split.
I0315 02:31:34.352082 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 02:31:42.903742 139666979968832 spec.py:349] Evaluating on the test split.
I0315 02:31:45.232351 139666979968832 submission_runner.py:420] Time since start: 1138.23s, 	Step: 5610, 	{'train/accuracy': 0.3941924273967743, 'train/loss': 2.9730653762817383, 'validation/accuracy': 0.35558000206947327, 'validation/loss': 3.1851818561553955, 'validation/num_examples': 50000, 'test/accuracy': 0.26990002393722534, 'test/loss': 3.7601563930511475, 'test/num_examples': 10000, 'score': 1059.3302872180939, 'total_duration': 1138.2319378852844, 'accumulated_submission_time': 1059.3302872180939, 'accumulated_eval_time': 78.66454458236694, 'accumulated_logging_time': 0.05573558807373047}
I0315 02:31:45.249269 139503580698368 logging_writer.py:48] [5610] accumulated_eval_time=78.664545, accumulated_logging_time=0.055736, accumulated_submission_time=1059.330287, global_step=5610, preemption_count=0, score=1059.330287, test/accuracy=0.269900, test/loss=3.760156, test/num_examples=10000, total_duration=1138.231938, train/accuracy=0.394192, train/loss=2.973065, validation/accuracy=0.355580, validation/loss=3.185182, validation/num_examples=50000
I0315 02:32:56.294336 139503589091072 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.4068850874900818, loss=4.495718479156494
I0315 02:34:27.143320 139503580698368 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.4203276038169861, loss=4.518587589263916
I0315 02:35:58.003314 139503589091072 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.39803561568260193, loss=4.461722373962402
I0315 02:37:28.817632 139503580698368 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.3848549425601959, loss=4.327892303466797
I0315 02:38:59.637847 139503589091072 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.43543875217437744, loss=4.495856285095215
I0315 02:40:15.250275 139666979968832 spec.py:321] Evaluating on the training split.
I0315 02:40:22.624341 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 02:40:31.306799 139666979968832 spec.py:349] Evaluating on the test split.
I0315 02:40:33.593823 139666979968832 submission_runner.py:420] Time since start: 1666.59s, 	Step: 8418, 	{'train/accuracy': 0.44437578320503235, 'train/loss': 2.6151835918426514, 'validation/accuracy': 0.41373997926712036, 'validation/loss': 2.7880170345306396, 'validation/num_examples': 50000, 'test/accuracy': 0.318200021982193, 'test/loss': 3.4120326042175293, 'test/num_examples': 10000, 'score': 1569.233282327652, 'total_duration': 1666.59339761734, 'accumulated_submission_time': 1569.233282327652, 'accumulated_eval_time': 97.00804924964905, 'accumulated_logging_time': 0.08312249183654785}
I0315 02:40:33.610871 139504664950528 logging_writer.py:48] [8418] accumulated_eval_time=97.008049, accumulated_logging_time=0.083122, accumulated_submission_time=1569.233282, global_step=8418, preemption_count=0, score=1569.233282, test/accuracy=0.318200, test/loss=3.412033, test/num_examples=10000, total_duration=1666.593398, train/accuracy=0.444376, train/loss=2.615184, validation/accuracy=0.413740, validation/loss=2.788017, validation/num_examples=50000
I0315 02:40:48.694985 139504673343232 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.41257545351982117, loss=4.444210529327393
I0315 02:42:19.494230 139504664950528 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.3897859752178192, loss=4.246007442474365
I0315 02:43:50.300331 139504673343232 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.41757893562316895, loss=4.332126617431641
I0315 02:45:21.088330 139504664950528 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.4166509509086609, loss=4.229737281799316
I0315 02:46:51.889559 139504673343232 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.4016558825969696, loss=4.1730852127075195
I0315 02:48:22.693772 139504664950528 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.4056699872016907, loss=4.1870880126953125
I0315 02:49:03.610772 139666979968832 spec.py:321] Evaluating on the training split.
I0315 02:49:10.962202 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 02:49:19.585832 139666979968832 spec.py:349] Evaluating on the test split.
I0315 02:49:21.885313 139666979968832 submission_runner.py:420] Time since start: 2194.88s, 	Step: 11227, 	{'train/accuracy': 0.4833187162876129, 'train/loss': 2.470959424972534, 'validation/accuracy': 0.4481000006198883, 'validation/loss': 2.6501293182373047, 'validation/num_examples': 50000, 'test/accuracy': 0.3416000306606293, 'test/loss': 3.2876079082489014, 'test/num_examples': 10000, 'score': 2079.135133266449, 'total_duration': 2194.8848955631256, 'accumulated_submission_time': 2079.135133266449, 'accumulated_eval_time': 115.28256273269653, 'accumulated_logging_time': 0.10933995246887207}
I0315 02:49:21.905283 139504664950528 logging_writer.py:48] [11227] accumulated_eval_time=115.282563, accumulated_logging_time=0.109340, accumulated_submission_time=2079.135133, global_step=11227, preemption_count=0, score=2079.135133, test/accuracy=0.341600, test/loss=3.287608, test/num_examples=10000, total_duration=2194.884896, train/accuracy=0.483319, train/loss=2.470959, validation/accuracy=0.448100, validation/loss=2.650129, validation/num_examples=50000
I0315 02:50:11.692377 139504673343232 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.40844404697418213, loss=4.151320934295654
I0315 02:51:42.507943 139504664950528 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.4109969735145569, loss=4.179677486419678
I0315 02:53:13.361763 139504673343232 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.40737301111221313, loss=4.163122177124023
I0315 02:54:44.219631 139504664950528 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.40794482827186584, loss=4.07799768447876
I0315 02:56:15.040993 139504673343232 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.42242881655693054, loss=4.157648086547852
I0315 02:57:45.822585 139504664950528 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.40715938806533813, loss=4.197688102722168
I0315 02:57:52.051400 139666979968832 spec.py:321] Evaluating on the training split.
I0315 02:57:59.459705 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 02:58:07.826355 139666979968832 spec.py:349] Evaluating on the test split.
I0315 02:58:10.163884 139666979968832 submission_runner.py:420] Time since start: 2723.16s, 	Step: 14036, 	{'train/accuracy': 0.49284517765045166, 'train/loss': 2.44167160987854, 'validation/accuracy': 0.4616599977016449, 'validation/loss': 2.5923407077789307, 'validation/num_examples': 50000, 'test/accuracy': 0.352400004863739, 'test/loss': 3.253528118133545, 'test/num_examples': 10000, 'score': 2589.1811945438385, 'total_duration': 2723.163457632065, 'accumulated_submission_time': 2589.1811945438385, 'accumulated_eval_time': 133.39499020576477, 'accumulated_logging_time': 0.13920998573303223}
I0315 02:58:10.182436 139504698521344 logging_writer.py:48] [14036] accumulated_eval_time=133.394990, accumulated_logging_time=0.139210, accumulated_submission_time=2589.181195, global_step=14036, preemption_count=0, score=2589.181195, test/accuracy=0.352400, test/loss=3.253528, test/num_examples=10000, total_duration=2723.163458, train/accuracy=0.492845, train/loss=2.441672, validation/accuracy=0.461660, validation/loss=2.592341, validation/num_examples=50000
I0315 02:59:34.674741 139504910305024 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.42654088139533997, loss=4.231132507324219
I0315 03:01:05.453898 139504698521344 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.4219535291194916, loss=4.229187965393066
I0315 03:02:36.226897 139504910305024 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.4073978066444397, loss=4.104714870452881
I0315 03:04:07.052255 139504698521344 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.41648897528648376, loss=4.102248668670654
I0315 03:05:37.816961 139504910305024 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.41565045714378357, loss=4.032482624053955
I0315 03:06:40.163948 139666979968832 spec.py:321] Evaluating on the training split.
I0315 03:06:47.496782 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 03:06:56.083661 139666979968832 spec.py:349] Evaluating on the test split.
I0315 03:06:58.391052 139666979968832 submission_runner.py:420] Time since start: 3251.39s, 	Step: 16845, 	{'train/accuracy': 0.5042849183082581, 'train/loss': 2.3759348392486572, 'validation/accuracy': 0.4754199981689453, 'validation/loss': 2.5212173461914062, 'validation/num_examples': 50000, 'test/accuracy': 0.3671000301837921, 'test/loss': 3.1625282764434814, 'test/num_examples': 10000, 'score': 3099.06081533432, 'total_duration': 3251.3906412124634, 'accumulated_submission_time': 3099.06081533432, 'accumulated_eval_time': 151.62206530570984, 'accumulated_logging_time': 0.17100739479064941}
I0315 03:06:58.408258 139505101174528 logging_writer.py:48] [16845] accumulated_eval_time=151.622065, accumulated_logging_time=0.171007, accumulated_submission_time=3099.060815, global_step=16845, preemption_count=0, score=3099.060815, test/accuracy=0.367100, test/loss=3.162528, test/num_examples=10000, total_duration=3251.390641, train/accuracy=0.504285, train/loss=2.375935, validation/accuracy=0.475420, validation/loss=2.521217, validation/num_examples=50000
I0315 03:07:26.773921 139505185019648 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.44621962308883667, loss=4.162196159362793
I0315 03:08:57.456937 139505101174528 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.41193118691444397, loss=4.111296653747559
I0315 03:10:28.240910 139505185019648 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.410794734954834, loss=3.9940266609191895
I0315 03:11:58.972258 139505101174528 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.4142394959926605, loss=4.025853633880615
I0315 03:13:29.723860 139505185019648 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.4227679371833801, loss=4.00069522857666
I0315 03:15:00.530750 139505101174528 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.43136245012283325, loss=3.986876964569092
I0315 03:15:28.394855 139666979968832 spec.py:321] Evaluating on the training split.
I0315 03:15:35.998160 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 03:15:44.628784 139666979968832 spec.py:349] Evaluating on the test split.
I0315 03:15:46.920458 139666979968832 submission_runner.py:420] Time since start: 3779.92s, 	Step: 19655, 	{'train/accuracy': 0.516621470451355, 'train/loss': 2.2864809036254883, 'validation/accuracy': 0.4909399747848511, 'validation/loss': 2.4297704696655273, 'validation/num_examples': 50000, 'test/accuracy': 0.3792000114917755, 'test/loss': 3.102672815322876, 'test/num_examples': 10000, 'score': 3608.9477570056915, 'total_duration': 3779.92001414299, 'accumulated_submission_time': 3608.9477570056915, 'accumulated_eval_time': 170.14759874343872, 'accumulated_logging_time': 0.19753265380859375}
I0315 03:15:46.938900 139505185019648 logging_writer.py:48] [19655] accumulated_eval_time=170.147599, accumulated_logging_time=0.197533, accumulated_submission_time=3608.947757, global_step=19655, preemption_count=0, score=3608.947757, test/accuracy=0.379200, test/loss=3.102673, test/num_examples=10000, total_duration=3779.920014, train/accuracy=0.516621, train/loss=2.286481, validation/accuracy=0.490940, validation/loss=2.429770, validation/num_examples=50000
I0315 03:16:49.778006 139505193412352 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.4221958816051483, loss=3.9761595726013184
I0315 03:18:20.524574 139505185019648 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.41577330231666565, loss=4.0645270347595215
I0315 03:19:51.224290 139505193412352 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.42742419242858887, loss=4.092314720153809
I0315 03:21:21.992488 139505185019648 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.4340587258338928, loss=4.070718765258789
I0315 03:22:52.788065 139505193412352 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.4214714467525482, loss=3.864144802093506
I0315 03:24:17.057435 139666979968832 spec.py:321] Evaluating on the training split.
I0315 03:24:24.682554 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 03:24:33.425742 139666979968832 spec.py:349] Evaluating on the test split.
I0315 03:24:35.747904 139666979968832 submission_runner.py:420] Time since start: 4308.75s, 	Step: 22466, 	{'train/accuracy': 0.5379264950752258, 'train/loss': 2.1756951808929443, 'validation/accuracy': 0.5106399655342102, 'validation/loss': 2.30892014503479, 'validation/num_examples': 50000, 'test/accuracy': 0.3899000287055969, 'test/loss': 2.973999261856079, 'test/num_examples': 10000, 'score': 4118.965543985367, 'total_duration': 4308.747456789017, 'accumulated_submission_time': 4118.965543985367, 'accumulated_eval_time': 188.8380012512207, 'accumulated_logging_time': 0.22706365585327148}
I0315 03:24:35.770491 139505218590464 logging_writer.py:48] [22466] accumulated_eval_time=188.838001, accumulated_logging_time=0.227064, accumulated_submission_time=4118.965544, global_step=22466, preemption_count=0, score=4118.965544, test/accuracy=0.389900, test/loss=2.973999, test/num_examples=10000, total_duration=4308.747457, train/accuracy=0.537926, train/loss=2.175695, validation/accuracy=0.510640, validation/loss=2.308920, validation/num_examples=50000
I0315 03:24:42.147333 139505235375872 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.42494267225265503, loss=4.061704635620117
I0315 03:26:12.983159 139505218590464 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.43301859498023987, loss=4.021878242492676
I0315 03:27:43.797345 139505235375872 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.43116745352745056, loss=4.097570419311523
I0315 03:29:14.600304 139505218590464 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.42853355407714844, loss=4.022608757019043
I0315 03:30:45.391812 139505235375872 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.41819626092910767, loss=3.9678802490234375
I0315 03:32:16.223819 139505218590464 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.44438931345939636, loss=3.9693100452423096
I0315 03:33:05.865205 139666979968832 spec.py:321] Evaluating on the training split.
I0315 03:33:13.331874 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 03:33:24.492110 139666979968832 spec.py:349] Evaluating on the test split.
I0315 03:33:26.673755 139666979968832 submission_runner.py:420] Time since start: 4839.67s, 	Step: 25275, 	{'train/accuracy': 0.5290178656578064, 'train/loss': 2.225160837173462, 'validation/accuracy': 0.5007199645042419, 'validation/loss': 2.3718740940093994, 'validation/num_examples': 50000, 'test/accuracy': 0.3817000091075897, 'test/loss': 3.0522758960723877, 'test/num_examples': 10000, 'score': 4628.95613360405, 'total_duration': 4839.673331737518, 'accumulated_submission_time': 4628.95613360405, 'accumulated_eval_time': 209.6465151309967, 'accumulated_logging_time': 0.2606830596923828}
I0315 03:33:26.699269 139505226983168 logging_writer.py:48] [25275] accumulated_eval_time=209.646515, accumulated_logging_time=0.260683, accumulated_submission_time=4628.956134, global_step=25275, preemption_count=0, score=4628.956134, test/accuracy=0.381700, test/loss=3.052276, test/num_examples=10000, total_duration=4839.673332, train/accuracy=0.529018, train/loss=2.225161, validation/accuracy=0.500720, validation/loss=2.371874, validation/num_examples=50000
I0315 03:34:07.761088 139505235375872 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.43390434980392456, loss=3.930065631866455
I0315 03:35:38.526944 139505226983168 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.4261137843132019, loss=3.9978649616241455
I0315 03:37:09.348989 139505235375872 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.4329090416431427, loss=4.0109333992004395
I0315 03:38:40.159601 139505226983168 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.4399823248386383, loss=3.921456813812256
I0315 03:40:10.957388 139505235375872 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.4327853322029114, loss=3.8997137546539307
I0315 03:41:41.759404 139505226983168 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.4421869218349457, loss=3.9884843826293945
I0315 03:41:56.732488 139666979968832 spec.py:321] Evaluating on the training split.
I0315 03:42:04.316358 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 03:42:15.941402 139666979968832 spec.py:349] Evaluating on the test split.
I0315 03:42:18.218235 139666979968832 submission_runner.py:420] Time since start: 5371.22s, 	Step: 28084, 	{'train/accuracy': 0.5485690236091614, 'train/loss': 2.1369452476501465, 'validation/accuracy': 0.5232999920845032, 'validation/loss': 2.2611072063446045, 'validation/num_examples': 50000, 'test/accuracy': 0.40610000491142273, 'test/loss': 2.9257686138153076, 'test/num_examples': 10000, 'score': 5138.88924074173, 'total_duration': 5371.2177946567535, 'accumulated_submission_time': 5138.88924074173, 'accumulated_eval_time': 231.13219547271729, 'accumulated_logging_time': 0.2956113815307617}
I0315 03:42:18.242152 139505193412352 logging_writer.py:48] [28084] accumulated_eval_time=231.132195, accumulated_logging_time=0.295611, accumulated_submission_time=5138.889241, global_step=28084, preemption_count=0, score=5138.889241, test/accuracy=0.406100, test/loss=2.925769, test/num_examples=10000, total_duration=5371.217795, train/accuracy=0.548569, train/loss=2.136945, validation/accuracy=0.523300, validation/loss=2.261107, validation/num_examples=50000
I0315 03:43:33.892736 139505201805056 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.42671364545822144, loss=4.034271717071533
I0315 03:45:04.762835 139505193412352 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.4363898038864136, loss=3.824418544769287
I0315 03:46:35.641835 139505201805056 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.4401015639305115, loss=3.8871328830718994
I0315 03:48:06.480011 139505193412352 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.44419148564338684, loss=3.8775579929351807
I0315 03:49:37.334713 139505201805056 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.4375098943710327, loss=3.8731627464294434
I0315 03:50:48.266284 139666979968832 spec.py:321] Evaluating on the training split.
I0315 03:50:56.024060 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 03:51:06.004690 139666979968832 spec.py:349] Evaluating on the test split.
I0315 03:51:08.284756 139666979968832 submission_runner.py:420] Time since start: 5901.28s, 	Step: 30892, 	{'train/accuracy': 0.5383848547935486, 'train/loss': 2.1873598098754883, 'validation/accuracy': 0.5109400153160095, 'validation/loss': 2.30798602104187, 'validation/num_examples': 50000, 'test/accuracy': 0.396200031042099, 'test/loss': 2.9860281944274902, 'test/num_examples': 10000, 'score': 5648.814496278763, 'total_duration': 5901.284316062927, 'accumulated_submission_time': 5648.814496278763, 'accumulated_eval_time': 251.15060806274414, 'accumulated_logging_time': 0.32982945442199707}
I0315 03:51:08.305235 139504690128640 logging_writer.py:48] [30892] accumulated_eval_time=251.150608, accumulated_logging_time=0.329829, accumulated_submission_time=5648.814496, global_step=30892, preemption_count=0, score=5648.814496, test/accuracy=0.396200, test/loss=2.986028, test/num_examples=10000, total_duration=5901.284316, train/accuracy=0.538385, train/loss=2.187360, validation/accuracy=0.510940, validation/loss=2.307986, validation/num_examples=50000
I0315 03:51:28.112309 139504698521344 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.4379841089248657, loss=3.921891689300537
I0315 03:52:58.939590 139504690128640 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.4385581612586975, loss=3.8294100761413574
I0315 03:54:29.785326 139504698521344 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.43998822569847107, loss=4.051339149475098
I0315 03:56:00.685149 139504690128640 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.4530026316642761, loss=3.8267405033111572
I0315 03:57:31.557965 139504698521344 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.43714261054992676, loss=3.9257616996765137
I0315 03:59:02.456344 139504690128640 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.4414078891277313, loss=3.90946626663208
I0315 03:59:38.335211 139666979968832 spec.py:321] Evaluating on the training split.
I0315 03:59:46.699530 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 03:59:56.636850 139666979968832 spec.py:349] Evaluating on the test split.
I0315 03:59:59.000018 139666979968832 submission_runner.py:420] Time since start: 6432.00s, 	Step: 33699, 	{'train/accuracy': 0.5612045526504517, 'train/loss': 2.0527260303497314, 'validation/accuracy': 0.5278599858283997, 'validation/loss': 2.2167325019836426, 'validation/num_examples': 50000, 'test/accuracy': 0.41290003061294556, 'test/loss': 2.867882251739502, 'test/num_examples': 10000, 'score': 6158.743594884872, 'total_duration': 6431.999586820602, 'accumulated_submission_time': 6158.743594884872, 'accumulated_eval_time': 271.8153614997864, 'accumulated_logging_time': 0.3610966205596924}
I0315 03:59:59.035836 139504698521344 logging_writer.py:48] [33699] accumulated_eval_time=271.815361, accumulated_logging_time=0.361097, accumulated_submission_time=6158.743595, global_step=33699, preemption_count=0, score=6158.743595, test/accuracy=0.412900, test/loss=2.867882, test/num_examples=10000, total_duration=6431.999587, train/accuracy=0.561205, train/loss=2.052726, validation/accuracy=0.527860, validation/loss=2.216733, validation/num_examples=50000
I0315 04:00:53.871037 139505201805056 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.4451175928115845, loss=3.8999757766723633
I0315 04:02:24.680525 139504698521344 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.44944342970848083, loss=3.9428210258483887
I0315 04:03:55.440753 139505201805056 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.43189653754234314, loss=3.744201898574829
I0315 04:05:26.213061 139504698521344 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.4426514506340027, loss=4.022737979888916
I0315 04:06:57.025765 139505201805056 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.4418545663356781, loss=3.7055490016937256
I0315 04:08:27.812822 139504698521344 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.4308079481124878, loss=3.794255018234253
I0315 04:08:29.149601 139666979968832 spec.py:321] Evaluating on the training split.
I0315 04:08:37.126330 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 04:08:46.203891 139666979968832 spec.py:349] Evaluating on the test split.
I0315 04:08:48.447454 139666979968832 submission_runner.py:420] Time since start: 6961.45s, 	Step: 36509, 	{'train/accuracy': 0.6098333597183228, 'train/loss': 1.8721224069595337, 'validation/accuracy': 0.5256400108337402, 'validation/loss': 2.249420166015625, 'validation/num_examples': 50000, 'test/accuracy': 0.4086000323295593, 'test/loss': 2.9109206199645996, 'test/num_examples': 10000, 'score': 6668.75258231163, 'total_duration': 6961.447009801865, 'accumulated_submission_time': 6668.75258231163, 'accumulated_eval_time': 291.11314058303833, 'accumulated_logging_time': 0.4061269760131836}
I0315 04:08:48.467589 139505193412352 logging_writer.py:48] [36509] accumulated_eval_time=291.113141, accumulated_logging_time=0.406127, accumulated_submission_time=6668.752582, global_step=36509, preemption_count=0, score=6668.752582, test/accuracy=0.408600, test/loss=2.910921, test/num_examples=10000, total_duration=6961.447010, train/accuracy=0.609833, train/loss=1.872122, validation/accuracy=0.525640, validation/loss=2.249420, validation/num_examples=50000
I0315 04:10:17.837920 139505226983168 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.4443533420562744, loss=3.862952470779419
I0315 04:11:48.614275 139505193412352 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.44917669892311096, loss=3.9266955852508545
I0315 04:13:19.463260 139505226983168 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.4500300884246826, loss=3.9084272384643555
I0315 04:14:50.244388 139505193412352 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.4557567238807678, loss=3.8927435874938965
I0315 04:16:21.097615 139505226983168 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.4440543055534363, loss=3.853992223739624
I0315 04:17:18.559365 139666979968832 spec.py:321] Evaluating on the training split.
I0315 04:17:27.161938 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 04:17:37.199151 139666979968832 spec.py:349] Evaluating on the test split.
I0315 04:17:39.493638 139666979968832 submission_runner.py:420] Time since start: 7492.49s, 	Step: 39318, 	{'train/accuracy': 0.5975764989852905, 'train/loss': 1.9000749588012695, 'validation/accuracy': 0.5384399890899658, 'validation/loss': 2.1795661449432373, 'validation/num_examples': 50000, 'test/accuracy': 0.4187000095844269, 'test/loss': 2.8545751571655273, 'test/num_examples': 10000, 'score': 7178.742904424667, 'total_duration': 7492.493213653564, 'accumulated_submission_time': 7178.742904424667, 'accumulated_eval_time': 312.04736709594727, 'accumulated_logging_time': 0.4363114833831787}
I0315 04:17:39.525531 139504698521344 logging_writer.py:48] [39318] accumulated_eval_time=312.047367, accumulated_logging_time=0.436311, accumulated_submission_time=7178.742904, global_step=39318, preemption_count=0, score=7178.742904, test/accuracy=0.418700, test/loss=2.854575, test/num_examples=10000, total_duration=7492.493214, train/accuracy=0.597576, train/loss=1.900075, validation/accuracy=0.538440, validation/loss=2.179566, validation/num_examples=50000
I0315 04:18:12.835613 139505185019648 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.4545994699001312, loss=3.74385929107666
I0315 04:19:43.680791 139504698521344 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.455983966588974, loss=3.944957733154297
I0315 04:21:14.556571 139505185019648 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.4309309124946594, loss=3.8574042320251465
I0315 04:22:45.435384 139504698521344 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.4393574297428131, loss=3.6805579662323
I0315 04:24:16.248770 139505185019648 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.4673980474472046, loss=3.816016435623169
I0315 04:25:47.023469 139504698521344 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.45627641677856445, loss=3.809817314147949
I0315 04:26:09.602535 139666979968832 spec.py:321] Evaluating on the training split.
I0315 04:26:17.445148 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 04:26:28.776782 139666979968832 spec.py:349] Evaluating on the test split.
I0315 04:26:31.054002 139666979968832 submission_runner.py:420] Time since start: 8024.05s, 	Step: 42126, 	{'train/accuracy': 0.5855787396430969, 'train/loss': 1.9755046367645264, 'validation/accuracy': 0.5410799980163574, 'validation/loss': 2.1894428730010986, 'validation/num_examples': 50000, 'test/accuracy': 0.414900004863739, 'test/loss': 2.8620636463165283, 'test/num_examples': 10000, 'score': 7688.719514369965, 'total_duration': 8024.053572177887, 'accumulated_submission_time': 7688.719514369965, 'accumulated_eval_time': 333.4987795352936, 'accumulated_logging_time': 0.477977991104126}
I0315 04:26:31.073882 139504698521344 logging_writer.py:48] [42126] accumulated_eval_time=333.498780, accumulated_logging_time=0.477978, accumulated_submission_time=7688.719514, global_step=42126, preemption_count=0, score=7688.719514, test/accuracy=0.414900, test/loss=2.862064, test/num_examples=10000, total_duration=8024.053572, train/accuracy=0.585579, train/loss=1.975505, validation/accuracy=0.541080, validation/loss=2.189443, validation/num_examples=50000
I0315 04:27:39.155600 139505218590464 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.4470858573913574, loss=3.8175880908966064
I0315 04:29:09.925342 139504698521344 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.4447547197341919, loss=3.8422765731811523
I0315 04:30:40.750827 139505218590464 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.4592312276363373, loss=3.7325034141540527
I0315 04:32:11.585937 139504698521344 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.46413084864616394, loss=3.8731555938720703
I0315 04:33:42.372527 139505218590464 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.4505401849746704, loss=3.870870351791382
I0315 04:35:01.106961 139666979968832 spec.py:321] Evaluating on the training split.
I0315 04:35:09.440702 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 04:35:23.382822 139666979968832 spec.py:349] Evaluating on the test split.
I0315 04:35:25.639011 139666979968832 submission_runner.py:420] Time since start: 8558.64s, 	Step: 44935, 	{'train/accuracy': 0.6030572056770325, 'train/loss': 1.8343355655670166, 'validation/accuracy': 0.5600599646568298, 'validation/loss': 2.044644832611084, 'validation/num_examples': 50000, 'test/accuracy': 0.44050002098083496, 'test/loss': 2.7189035415649414, 'test/num_examples': 10000, 'score': 8198.649980545044, 'total_duration': 8558.638590335846, 'accumulated_submission_time': 8198.649980545044, 'accumulated_eval_time': 358.0307970046997, 'accumulated_logging_time': 0.5081648826599121}
I0315 04:35:25.665704 139505185019648 logging_writer.py:48] [44935] accumulated_eval_time=358.030797, accumulated_logging_time=0.508165, accumulated_submission_time=8198.649981, global_step=44935, preemption_count=0, score=8198.649981, test/accuracy=0.440500, test/loss=2.718904, test/num_examples=10000, total_duration=8558.638590, train/accuracy=0.603057, train/loss=1.834336, validation/accuracy=0.560060, validation/loss=2.044645, validation/num_examples=50000
I0315 04:35:37.663079 139505193412352 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.4548704922199249, loss=3.729936122894287
I0315 04:37:08.430122 139505185019648 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.4582090675830841, loss=3.8199453353881836
I0315 04:38:39.176214 139505193412352 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.4586244225502014, loss=3.8747997283935547
I0315 04:40:09.962813 139505185019648 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.4345490038394928, loss=3.6889402866363525
I0315 04:41:40.791407 139505193412352 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.46784552931785583, loss=3.8299736976623535
I0315 04:43:11.603743 139505185019648 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.449909508228302, loss=3.7551612854003906
I0315 04:43:55.670266 139666979968832 spec.py:321] Evaluating on the training split.
I0315 04:44:03.621658 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 04:44:15.751155 139666979968832 spec.py:349] Evaluating on the test split.
I0315 04:44:18.030158 139666979968832 submission_runner.py:420] Time since start: 9091.03s, 	Step: 47744, 	{'train/accuracy': 0.5924545526504517, 'train/loss': 1.927284836769104, 'validation/accuracy': 0.5535599589347839, 'validation/loss': 2.119499683380127, 'validation/num_examples': 50000, 'test/accuracy': 0.4309000074863434, 'test/loss': 2.7724545001983643, 'test/num_examples': 10000, 'score': 8708.554129123688, 'total_duration': 9091.029710054398, 'accumulated_submission_time': 8708.554129123688, 'accumulated_eval_time': 380.3906214237213, 'accumulated_logging_time': 0.5432441234588623}
I0315 04:44:18.052513 139504690128640 logging_writer.py:48] [47744] accumulated_eval_time=380.390621, accumulated_logging_time=0.543244, accumulated_submission_time=8708.554129, global_step=47744, preemption_count=0, score=8708.554129, test/accuracy=0.430900, test/loss=2.772455, test/num_examples=10000, total_duration=9091.029710, train/accuracy=0.592455, train/loss=1.927285, validation/accuracy=0.553560, validation/loss=2.119500, validation/num_examples=50000
I0315 04:45:04.699353 139504698521344 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.46537482738494873, loss=3.8092024326324463
I0315 04:46:35.468510 139504690128640 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.44200441241264343, loss=3.6653809547424316
I0315 04:48:06.257844 139504698521344 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.4548720121383667, loss=3.76608943939209
I0315 04:49:37.032339 139504690128640 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.451851487159729, loss=3.7528133392333984
I0315 04:51:07.799169 139504698521344 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.4646117687225342, loss=3.724093437194824
I0315 04:52:38.575213 139504690128640 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.463814377784729, loss=3.7580738067626953
I0315 04:52:48.077132 139666979968832 spec.py:321] Evaluating on the training split.
I0315 04:52:56.104131 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 04:53:08.593181 139666979968832 spec.py:349] Evaluating on the test split.
I0315 04:53:10.910332 139666979968832 submission_runner.py:420] Time since start: 9623.91s, 	Step: 50554, 	{'train/accuracy': 0.5901028513908386, 'train/loss': 1.929429292678833, 'validation/accuracy': 0.5543199777603149, 'validation/loss': 2.0966458320617676, 'validation/num_examples': 50000, 'test/accuracy': 0.42660000920295715, 'test/loss': 2.7711341381073, 'test/num_examples': 10000, 'score': 9218.479825258255, 'total_duration': 9623.909902572632, 'accumulated_submission_time': 9218.479825258255, 'accumulated_eval_time': 403.22376585006714, 'accumulated_logging_time': 0.5750327110290527}
I0315 04:53:10.927401 139505201805056 logging_writer.py:48] [50554] accumulated_eval_time=403.223766, accumulated_logging_time=0.575033, accumulated_submission_time=9218.479825, global_step=50554, preemption_count=0, score=9218.479825, test/accuracy=0.426600, test/loss=2.771134, test/num_examples=10000, total_duration=9623.909903, train/accuracy=0.590103, train/loss=1.929429, validation/accuracy=0.554320, validation/loss=2.096646, validation/num_examples=50000
I0315 04:54:32.314920 139505218590464 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.4575602412223816, loss=3.636962652206421
I0315 04:56:03.070240 139505201805056 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.46803200244903564, loss=3.756047487258911
I0315 04:57:33.837143 139505218590464 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.46669700741767883, loss=3.7666232585906982
I0315 04:59:04.592746 139505201805056 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.48468825221061707, loss=3.9201693534851074
I0315 05:00:35.317510 139505218590464 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.4630124866962433, loss=3.7694478034973145
I0315 05:01:40.912710 139666979968832 spec.py:321] Evaluating on the training split.
I0315 05:01:48.956323 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 05:02:03.171025 139666979968832 spec.py:349] Evaluating on the test split.
I0315 05:02:05.485517 139666979968832 submission_runner.py:420] Time since start: 10158.49s, 	Step: 53363, 	{'train/accuracy': 0.5937101244926453, 'train/loss': 1.9251199960708618, 'validation/accuracy': 0.5542399883270264, 'validation/loss': 2.095377206802368, 'validation/num_examples': 50000, 'test/accuracy': 0.4374000132083893, 'test/loss': 2.754514217376709, 'test/num_examples': 10000, 'score': 9728.36764883995, 'total_duration': 10158.485095024109, 'accumulated_submission_time': 9728.36764883995, 'accumulated_eval_time': 427.7965428829193, 'accumulated_logging_time': 0.6011092662811279}
I0315 05:02:05.503215 139505185019648 logging_writer.py:48] [53363] accumulated_eval_time=427.796543, accumulated_logging_time=0.601109, accumulated_submission_time=9728.367649, global_step=53363, preemption_count=0, score=9728.367649, test/accuracy=0.437400, test/loss=2.754514, test/num_examples=10000, total_duration=10158.485095, train/accuracy=0.593710, train/loss=1.925120, validation/accuracy=0.554240, validation/loss=2.095377, validation/num_examples=50000
I0315 05:02:30.567474 139505193412352 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.47902777791023254, loss=3.8936688899993896
I0315 05:04:01.355334 139505185019648 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.47869157791137695, loss=3.7803025245666504
I0315 05:05:32.147695 139505193412352 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.4625481367111206, loss=3.7035160064697266
I0315 05:07:03.017987 139505185019648 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.4771355986595154, loss=3.698803186416626
I0315 05:08:33.872914 139505193412352 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.4700334370136261, loss=3.703057289123535
I0315 05:10:04.723519 139505185019648 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.47489506006240845, loss=3.8550050258636475
I0315 05:10:35.662208 139666979968832 spec.py:321] Evaluating on the training split.
I0315 05:10:43.715310 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 05:10:56.961791 139666979968832 spec.py:349] Evaluating on the test split.
I0315 05:10:59.223247 139666979968832 submission_runner.py:420] Time since start: 10692.22s, 	Step: 56172, 	{'train/accuracy': 0.6036351919174194, 'train/loss': 1.8868541717529297, 'validation/accuracy': 0.5629400014877319, 'validation/loss': 2.0698935985565186, 'validation/num_examples': 50000, 'test/accuracy': 0.43810001015663147, 'test/loss': 2.715358018875122, 'test/num_examples': 10000, 'score': 10238.427752017975, 'total_duration': 10692.222826719284, 'accumulated_submission_time': 10238.427752017975, 'accumulated_eval_time': 451.3575351238251, 'accumulated_logging_time': 0.6278159618377686}
I0315 05:10:59.246215 139504673343232 logging_writer.py:48] [56172] accumulated_eval_time=451.357535, accumulated_logging_time=0.627816, accumulated_submission_time=10238.427752, global_step=56172, preemption_count=0, score=10238.427752, test/accuracy=0.438100, test/loss=2.715358, test/num_examples=10000, total_duration=10692.222827, train/accuracy=0.603635, train/loss=1.886854, validation/accuracy=0.562940, validation/loss=2.069894, validation/num_examples=50000
I0315 05:11:58.990874 139504681735936 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.4839246869087219, loss=3.7353017330169678
I0315 05:13:29.773918 139504673343232 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.4742888808250427, loss=3.6666269302368164
I0315 05:15:00.579051 139504681735936 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.4682901203632355, loss=3.714341163635254
I0315 05:16:31.376582 139504673343232 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.4919937551021576, loss=3.8289129734039307
I0315 05:18:02.209841 139504681735936 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.47536733746528625, loss=3.6809401512145996
I0315 05:19:29.278857 139666979968832 spec.py:321] Evaluating on the training split.
I0315 05:19:37.619804 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 05:19:50.702004 139666979968832 spec.py:349] Evaluating on the test split.
I0315 05:19:53.005558 139666979968832 submission_runner.py:420] Time since start: 11226.01s, 	Step: 58981, 	{'train/accuracy': 0.6097536683082581, 'train/loss': 1.8503715991973877, 'validation/accuracy': 0.575980007648468, 'validation/loss': 2.001864433288574, 'validation/num_examples': 50000, 'test/accuracy': 0.45660001039505005, 'test/loss': 2.6520166397094727, 'test/num_examples': 10000, 'score': 10748.354861974716, 'total_duration': 11226.00513100624, 'accumulated_submission_time': 10748.354861974716, 'accumulated_eval_time': 475.08420515060425, 'accumulated_logging_time': 0.6614036560058594}
I0315 05:19:53.027087 139505839322880 logging_writer.py:48] [58981] accumulated_eval_time=475.084205, accumulated_logging_time=0.661404, accumulated_submission_time=10748.354862, global_step=58981, preemption_count=0, score=10748.354862, test/accuracy=0.456600, test/loss=2.652017, test/num_examples=10000, total_duration=11226.005131, train/accuracy=0.609754, train/loss=1.850372, validation/accuracy=0.575980, validation/loss=2.001864, validation/num_examples=50000
I0315 05:19:56.670159 139505847715584 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.4803417921066284, loss=3.7076830863952637
I0315 05:21:27.436535 139505839322880 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.48749926686286926, loss=3.6775755882263184
I0315 05:22:58.160786 139505847715584 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.4866623282432556, loss=3.693726062774658
I0315 05:24:28.911057 139505839322880 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.48648354411125183, loss=3.713502883911133
I0315 05:25:59.683735 139505847715584 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.4735240638256073, loss=3.7246599197387695
I0315 05:27:30.429909 139505839322880 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.48106905817985535, loss=3.6630616188049316
I0315 05:28:23.139467 139666979968832 spec.py:321] Evaluating on the training split.
I0315 05:28:31.364385 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 05:28:44.569378 139666979968832 spec.py:349] Evaluating on the test split.
I0315 05:28:46.853008 139666979968832 submission_runner.py:420] Time since start: 11759.85s, 	Step: 61792, 	{'train/accuracy': 0.6163105964660645, 'train/loss': 1.8129554986953735, 'validation/accuracy': 0.5788999795913696, 'validation/loss': 1.9795621633529663, 'validation/num_examples': 50000, 'test/accuracy': 0.4531000256538391, 'test/loss': 2.6504082679748535, 'test/num_examples': 10000, 'score': 11258.367656707764, 'total_duration': 11759.852581262589, 'accumulated_submission_time': 11258.367656707764, 'accumulated_eval_time': 498.79771184921265, 'accumulated_logging_time': 0.6938827037811279}
I0315 05:28:46.880153 139505805752064 logging_writer.py:48] [61792] accumulated_eval_time=498.797712, accumulated_logging_time=0.693883, accumulated_submission_time=11258.367657, global_step=61792, preemption_count=0, score=11258.367657, test/accuracy=0.453100, test/loss=2.650408, test/num_examples=10000, total_duration=11759.852581, train/accuracy=0.616311, train/loss=1.812955, validation/accuracy=0.578900, validation/loss=1.979562, validation/num_examples=50000
I0315 05:29:24.848371 139505814144768 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.4777449369430542, loss=3.6378235816955566
I0315 05:30:55.646845 139505805752064 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.48543962836265564, loss=3.716531276702881
I0315 05:32:26.425514 139505814144768 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.47771576046943665, loss=3.6879079341888428
I0315 05:33:57.222346 139505805752064 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.49621373414993286, loss=3.705489158630371
I0315 05:35:27.994649 139505814144768 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.47975486516952515, loss=3.6292519569396973
I0315 05:36:58.820754 139505805752064 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.47528281807899475, loss=3.633680820465088
I0315 05:37:16.868681 139666979968832 spec.py:321] Evaluating on the training split.
I0315 05:37:25.044610 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 05:37:37.759669 139666979968832 spec.py:349] Evaluating on the test split.
I0315 05:37:40.023023 139666979968832 submission_runner.py:420] Time since start: 12293.02s, 	Step: 64601, 	{'train/accuracy': 0.6098533272743225, 'train/loss': 1.857517957687378, 'validation/accuracy': 0.5766000151634216, 'validation/loss': 2.0250394344329834, 'validation/num_examples': 50000, 'test/accuracy': 0.4572000205516815, 'test/loss': 2.6506733894348145, 'test/num_examples': 10000, 'score': 11768.252495288849, 'total_duration': 12293.022606134415, 'accumulated_submission_time': 11768.252495288849, 'accumulated_eval_time': 521.952006816864, 'accumulated_logging_time': 0.7330019474029541}
I0315 05:37:40.043949 139505830930176 logging_writer.py:48] [64601] accumulated_eval_time=521.952007, accumulated_logging_time=0.733002, accumulated_submission_time=11768.252495, global_step=64601, preemption_count=0, score=11768.252495, test/accuracy=0.457200, test/loss=2.650673, test/num_examples=10000, total_duration=12293.022606, train/accuracy=0.609853, train/loss=1.857518, validation/accuracy=0.576600, validation/loss=2.025039, validation/num_examples=50000
I0315 05:38:52.706226 139505839322880 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.4827740490436554, loss=3.657785654067993
I0315 05:40:23.462937 139505830930176 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.501490592956543, loss=3.8000431060791016
I0315 05:41:54.337134 139505839322880 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.49184170365333557, loss=3.580012798309326
I0315 05:43:25.123382 139505830930176 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.47213485836982727, loss=3.616406202316284
I0315 05:44:55.942055 139505839322880 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.509770393371582, loss=3.7157604694366455
I0315 05:46:10.108762 139666979968832 spec.py:321] Evaluating on the training split.
I0315 05:46:18.657009 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 05:46:32.041711 139666979968832 spec.py:349] Evaluating on the test split.
I0315 05:46:34.258702 139666979968832 submission_runner.py:420] Time since start: 12827.26s, 	Step: 67410, 	{'train/accuracy': 0.6324737071990967, 'train/loss': 1.7457120418548584, 'validation/accuracy': 0.5887399911880493, 'validation/loss': 1.9348591566085815, 'validation/num_examples': 50000, 'test/accuracy': 0.46880000829696655, 'test/loss': 2.5784528255462646, 'test/num_examples': 10000, 'score': 12278.215480327606, 'total_duration': 12827.258270025253, 'accumulated_submission_time': 12278.215480327606, 'accumulated_eval_time': 546.1019096374512, 'accumulated_logging_time': 0.7637128829956055}
I0315 05:46:34.280505 139505814144768 logging_writer.py:48] [67410] accumulated_eval_time=546.101910, accumulated_logging_time=0.763713, accumulated_submission_time=12278.215480, global_step=67410, preemption_count=0, score=12278.215480, test/accuracy=0.468800, test/loss=2.578453, test/num_examples=10000, total_duration=12827.258270, train/accuracy=0.632474, train/loss=1.745712, validation/accuracy=0.588740, validation/loss=1.934859, validation/num_examples=50000
I0315 05:46:50.852241 139505822537472 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.5086227059364319, loss=3.690887451171875
I0315 05:48:21.666092 139505814144768 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.5147702693939209, loss=3.6330888271331787
I0315 05:49:52.506001 139505822537472 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.49427667260169983, loss=3.5434064865112305
I0315 05:51:23.568480 139505814144768 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.49650150537490845, loss=3.6133854389190674
I0315 05:52:54.409437 139505822537472 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.4788539409637451, loss=3.5502820014953613
I0315 05:54:25.270622 139505814144768 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.5085625648498535, loss=3.682237148284912
I0315 05:55:04.381389 139666979968832 spec.py:321] Evaluating on the training split.
I0315 05:55:11.801053 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 05:55:23.743285 139666979968832 spec.py:349] Evaluating on the test split.
I0315 05:55:26.031532 139666979968832 submission_runner.py:420] Time since start: 13359.03s, 	Step: 70217, 	{'train/accuracy': 0.6731704473495483, 'train/loss': 1.5815236568450928, 'validation/accuracy': 0.5878599882125854, 'validation/loss': 1.9684792757034302, 'validation/num_examples': 50000, 'test/accuracy': 0.46980002522468567, 'test/loss': 2.615736484527588, 'test/num_examples': 10000, 'score': 12788.216607809067, 'total_duration': 13359.0311024189, 'accumulated_submission_time': 12788.216607809067, 'accumulated_eval_time': 567.7520003318787, 'accumulated_logging_time': 0.7960903644561768}
I0315 05:55:26.055330 139505847715584 logging_writer.py:48] [70217] accumulated_eval_time=567.752000, accumulated_logging_time=0.796090, accumulated_submission_time=12788.216608, global_step=70217, preemption_count=0, score=12788.216608, test/accuracy=0.469800, test/loss=2.615736, test/num_examples=10000, total_duration=13359.031102, train/accuracy=0.673170, train/loss=1.581524, validation/accuracy=0.587860, validation/loss=1.968479, validation/num_examples=50000
I0315 05:56:17.607985 139505856108288 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.4934457540512085, loss=3.618790864944458
I0315 05:57:48.365342 139505847715584 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.507309079170227, loss=3.592686414718628
I0315 05:59:19.133729 139505856108288 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.5020313858985901, loss=3.671973705291748
I0315 06:00:49.973763 139505847715584 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.514112114906311, loss=3.643946886062622
I0315 06:02:20.775787 139505856108288 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.5103568434715271, loss=3.6932175159454346
I0315 06:03:51.519459 139505847715584 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.5184692144393921, loss=3.6265766620635986
I0315 06:03:56.121439 139666979968832 spec.py:321] Evaluating on the training split.
I0315 06:04:03.490824 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 06:04:15.039568 139666979968832 spec.py:349] Evaluating on the test split.
I0315 06:04:17.320658 139666979968832 submission_runner.py:420] Time since start: 13890.32s, 	Step: 73027, 	{'train/accuracy': 0.6520248651504517, 'train/loss': 1.6730637550354004, 'validation/accuracy': 0.5866999626159668, 'validation/loss': 1.9701958894729614, 'validation/num_examples': 50000, 'test/accuracy': 0.4586000144481659, 'test/loss': 2.622561454772949, 'test/num_examples': 10000, 'score': 13298.16697025299, 'total_duration': 13890.320236682892, 'accumulated_submission_time': 13298.16697025299, 'accumulated_eval_time': 588.9511861801147, 'accumulated_logging_time': 0.8440396785736084}
I0315 06:04:17.344000 139505822537472 logging_writer.py:48] [73027] accumulated_eval_time=588.951186, accumulated_logging_time=0.844040, accumulated_submission_time=13298.166970, global_step=73027, preemption_count=0, score=13298.166970, test/accuracy=0.458600, test/loss=2.622561, test/num_examples=10000, total_duration=13890.320237, train/accuracy=0.652025, train/loss=1.673064, validation/accuracy=0.586700, validation/loss=1.970196, validation/num_examples=50000
I0315 06:05:43.450195 139505830930176 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.5049759149551392, loss=3.6771304607391357
I0315 06:07:14.228225 139505822537472 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.5082008838653564, loss=3.6197056770324707
I0315 06:08:45.038619 139505830930176 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.4945923984050751, loss=3.5537309646606445
I0315 06:10:15.839491 139505822537472 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.510779857635498, loss=3.586975574493408
I0315 06:11:46.614680 139505830930176 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.5092573761940002, loss=3.488884449005127
I0315 06:12:47.343838 139666979968832 spec.py:321] Evaluating on the training split.
I0315 06:12:54.399172 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 06:13:05.885893 139666979968832 spec.py:349] Evaluating on the test split.
I0315 06:13:08.228870 139666979968832 submission_runner.py:420] Time since start: 14421.23s, 	Step: 75836, 	{'train/accuracy': 0.6688257455825806, 'train/loss': 1.5871577262878418, 'validation/accuracy': 0.6103799939155579, 'validation/loss': 1.8433527946472168, 'validation/num_examples': 50000, 'test/accuracy': 0.48410001397132874, 'test/loss': 2.5036115646362305, 'test/num_examples': 10000, 'score': 13808.067230701447, 'total_duration': 14421.228022098541, 'accumulated_submission_time': 13808.067230701447, 'accumulated_eval_time': 609.835752248764, 'accumulated_logging_time': 0.8771841526031494}
I0315 06:13:08.252103 139505847715584 logging_writer.py:48] [75836] accumulated_eval_time=609.835752, accumulated_logging_time=0.877184, accumulated_submission_time=13808.067231, global_step=75836, preemption_count=0, score=13808.067231, test/accuracy=0.484100, test/loss=2.503612, test/num_examples=10000, total_duration=14421.228022, train/accuracy=0.668826, train/loss=1.587158, validation/accuracy=0.610380, validation/loss=1.843353, validation/num_examples=50000
I0315 06:13:38.191600 139505856108288 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.5207840800285339, loss=3.559321880340576
I0315 06:15:08.963303 139505847715584 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.5078983902931213, loss=3.493366003036499
I0315 06:16:39.765204 139505856108288 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.5294417142868042, loss=3.596846342086792
I0315 06:18:10.493619 139505847715584 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.5214678645133972, loss=3.5499701499938965
I0315 06:19:41.263686 139505856108288 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.5017728805541992, loss=3.5830700397491455
I0315 06:21:12.007262 139505847715584 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.5106944441795349, loss=3.5523500442504883
I0315 06:21:38.402330 139666979968832 spec.py:321] Evaluating on the training split.
I0315 06:21:45.267166 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 06:22:03.377746 139666979968832 spec.py:349] Evaluating on the test split.
I0315 06:22:05.747692 139666979968832 submission_runner.py:420] Time since start: 14958.75s, 	Step: 78647, 	{'train/accuracy': 0.6624082922935486, 'train/loss': 1.617139220237732, 'validation/accuracy': 0.6099599599838257, 'validation/loss': 1.8513147830963135, 'validation/num_examples': 50000, 'test/accuracy': 0.48660001158714294, 'test/loss': 2.4859347343444824, 'test/num_examples': 10000, 'score': 14318.115080595016, 'total_duration': 14958.747246026993, 'accumulated_submission_time': 14318.115080595016, 'accumulated_eval_time': 637.1810598373413, 'accumulated_logging_time': 0.9124436378479004}
I0315 06:22:05.769698 139505822537472 logging_writer.py:48] [78647] accumulated_eval_time=637.181060, accumulated_logging_time=0.912444, accumulated_submission_time=14318.115081, global_step=78647, preemption_count=0, score=14318.115081, test/accuracy=0.486600, test/loss=2.485935, test/num_examples=10000, total_duration=14958.747246, train/accuracy=0.662408, train/loss=1.617139, validation/accuracy=0.609960, validation/loss=1.851315, validation/num_examples=50000
I0315 06:23:10.078259 139505830930176 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.5111814737319946, loss=3.6047682762145996
I0315 06:24:40.875751 139505822537472 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.5158072113990784, loss=3.5802555084228516
I0315 06:26:11.678477 139505830930176 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.5320464372634888, loss=3.5948972702026367
I0315 06:27:42.494307 139505822537472 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.5044762492179871, loss=3.4577672481536865
I0315 06:29:13.340997 139505830930176 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.5357000827789307, loss=3.639005661010742
I0315 06:30:35.857480 139666979968832 spec.py:321] Evaluating on the training split.
I0315 06:30:42.681148 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 06:30:53.591156 139666979968832 spec.py:349] Evaluating on the test split.
I0315 06:30:55.916005 139666979968832 submission_runner.py:420] Time since start: 15488.92s, 	Step: 81456, 	{'train/accuracy': 0.645906388759613, 'train/loss': 1.6796780824661255, 'validation/accuracy': 0.6014800071716309, 'validation/loss': 1.8915008306503296, 'validation/num_examples': 50000, 'test/accuracy': 0.4756000339984894, 'test/loss': 2.5513505935668945, 'test/num_examples': 10000, 'score': 14828.103749036789, 'total_duration': 15488.91557765007, 'accumulated_submission_time': 14828.103749036789, 'accumulated_eval_time': 657.2395513057709, 'accumulated_logging_time': 0.9447917938232422}
I0315 06:30:55.938196 139505814144768 logging_writer.py:48] [81456] accumulated_eval_time=657.239551, accumulated_logging_time=0.944792, accumulated_submission_time=14828.103749, global_step=81456, preemption_count=0, score=14828.103749, test/accuracy=0.475600, test/loss=2.551351, test/num_examples=10000, total_duration=15488.915578, train/accuracy=0.645906, train/loss=1.679678, validation/accuracy=0.601480, validation/loss=1.891501, validation/num_examples=50000
I0315 06:31:04.125194 139505822537472 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.5436627864837646, loss=3.657877206802368
I0315 06:32:34.880519 139505814144768 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.5421013832092285, loss=3.5762698650360107
I0315 06:34:05.658782 139505822537472 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.5481625199317932, loss=3.487623691558838
I0315 06:35:36.401242 139505814144768 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.5331203937530518, loss=3.4825220108032227
I0315 06:37:07.088988 139505822537472 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.5149747729301453, loss=3.4415273666381836
I0315 06:38:37.820830 139505814144768 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.5467699766159058, loss=3.495847225189209
I0315 06:39:25.991937 139666979968832 spec.py:321] Evaluating on the training split.
I0315 06:39:32.632182 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 06:39:45.259576 139666979968832 spec.py:349] Evaluating on the test split.
I0315 06:39:47.547486 139666979968832 submission_runner.py:420] Time since start: 16020.55s, 	Step: 84267, 	{'train/accuracy': 0.6578842401504517, 'train/loss': 1.6404576301574707, 'validation/accuracy': 0.6136800050735474, 'validation/loss': 1.8449496030807495, 'validation/num_examples': 50000, 'test/accuracy': 0.48600003123283386, 'test/loss': 2.4789185523986816, 'test/num_examples': 10000, 'score': 15338.051067590714, 'total_duration': 16020.547025203705, 'accumulated_submission_time': 15338.051067590714, 'accumulated_eval_time': 678.7950310707092, 'accumulated_logging_time': 0.9780173301696777}
I0315 06:39:47.570144 139505814144768 logging_writer.py:48] [84267] accumulated_eval_time=678.795031, accumulated_logging_time=0.978017, accumulated_submission_time=15338.051068, global_step=84267, preemption_count=0, score=15338.051068, test/accuracy=0.486000, test/loss=2.478919, test/num_examples=10000, total_duration=16020.547025, train/accuracy=0.657884, train/loss=1.640458, validation/accuracy=0.613680, validation/loss=1.844950, validation/num_examples=50000
I0315 06:40:30.060391 139505839322880 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.5606246590614319, loss=3.6103057861328125
I0315 06:42:00.854915 139505814144768 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.5389822721481323, loss=3.573310613632202
I0315 06:43:31.606095 139505839322880 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.5675534009933472, loss=3.6020002365112305
I0315 06:45:02.360427 139505814144768 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.5443429946899414, loss=3.4920899868011475
I0315 06:46:33.155254 139505839322880 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.5355276465415955, loss=3.5105419158935547
I0315 06:48:03.953078 139505814144768 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.5493457913398743, loss=3.387798309326172
I0315 06:48:17.638811 139666979968832 spec.py:321] Evaluating on the training split.
I0315 06:48:23.982896 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 06:48:35.489884 139666979968832 spec.py:349] Evaluating on the test split.
I0315 06:48:37.870365 139666979968832 submission_runner.py:420] Time since start: 16550.87s, 	Step: 87077, 	{'train/accuracy': 0.6678889989852905, 'train/loss': 1.5783772468566895, 'validation/accuracy': 0.6217399835586548, 'validation/loss': 1.7732410430908203, 'validation/num_examples': 50000, 'test/accuracy': 0.49660003185272217, 'test/loss': 2.4108152389526367, 'test/num_examples': 10000, 'score': 15848.019825696945, 'total_duration': 16550.869858264923, 'accumulated_submission_time': 15848.019825696945, 'accumulated_eval_time': 699.0264530181885, 'accumulated_logging_time': 1.0111169815063477}
I0315 06:48:37.895256 139505822537472 logging_writer.py:48] [87077] accumulated_eval_time=699.026453, accumulated_logging_time=1.011117, accumulated_submission_time=15848.019826, global_step=87077, preemption_count=0, score=15848.019826, test/accuracy=0.496600, test/loss=2.410815, test/num_examples=10000, total_duration=16550.869858, train/accuracy=0.667889, train/loss=1.578377, validation/accuracy=0.621740, validation/loss=1.773241, validation/num_examples=50000
I0315 06:49:54.873450 139505830930176 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.5407306551933289, loss=3.4624414443969727
I0315 06:51:25.638662 139505822537472 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.5558260083198547, loss=3.483635902404785
I0315 06:52:56.410495 139505830930176 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.5681768655776978, loss=3.5153121948242188
I0315 06:54:27.174369 139505822537472 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.5443007349967957, loss=3.4123575687408447
I0315 06:55:57.958722 139505830930176 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.5543050169944763, loss=3.515416145324707
I0315 06:57:07.942368 139666979968832 spec.py:321] Evaluating on the training split.
I0315 06:57:14.328133 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 06:57:27.633008 139666979968832 spec.py:349] Evaluating on the test split.
I0315 06:57:29.910096 139666979968832 submission_runner.py:420] Time since start: 17082.91s, 	Step: 89887, 	{'train/accuracy': 0.6729910373687744, 'train/loss': 1.5459370613098145, 'validation/accuracy': 0.6239199638366699, 'validation/loss': 1.7690690755844116, 'validation/num_examples': 50000, 'test/accuracy': 0.5028000473976135, 'test/loss': 2.4182331562042236, 'test/num_examples': 10000, 'score': 16357.967458724976, 'total_duration': 17082.90966629982, 'accumulated_submission_time': 16357.967458724976, 'accumulated_eval_time': 720.9941301345825, 'accumulated_logging_time': 1.046532154083252}
I0315 06:57:29.929247 139505814144768 logging_writer.py:48] [89887] accumulated_eval_time=720.994130, accumulated_logging_time=1.046532, accumulated_submission_time=16357.967459, global_step=89887, preemption_count=0, score=16357.967459, test/accuracy=0.502800, test/loss=2.418233, test/num_examples=10000, total_duration=17082.909666, train/accuracy=0.672991, train/loss=1.545937, validation/accuracy=0.623920, validation/loss=1.769069, validation/num_examples=50000
I0315 06:57:50.651079 139505822537472 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.5367056727409363, loss=3.4293019771575928
I0315 06:59:21.389254 139505814144768 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.5485970377922058, loss=3.5083131790161133
I0315 07:00:52.149645 139505822537472 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.5524765849113464, loss=3.3877532482147217
I0315 07:02:22.915025 139505814144768 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.5487313866615295, loss=3.5509939193725586
I0315 07:03:53.680130 139505822537472 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.5496495962142944, loss=3.492816925048828
I0315 07:05:24.421143 139505814144768 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.555522084236145, loss=3.438063383102417
I0315 07:06:00.088605 139666979968832 spec.py:321] Evaluating on the training split.
I0315 07:06:06.295025 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 07:06:16.398403 139666979968832 spec.py:349] Evaluating on the test split.
I0315 07:06:19.060146 139666979968832 submission_runner.py:420] Time since start: 17612.06s, 	Step: 92698, 	{'train/accuracy': 0.6764190196990967, 'train/loss': 1.5440995693206787, 'validation/accuracy': 0.6328999996185303, 'validation/loss': 1.74272620677948, 'validation/num_examples': 50000, 'test/accuracy': 0.5101000070571899, 'test/loss': 2.3821768760681152, 'test/num_examples': 10000, 'score': 16868.0272295475, 'total_duration': 17612.059515476227, 'accumulated_submission_time': 16868.0272295475, 'accumulated_eval_time': 739.9654176235199, 'accumulated_logging_time': 1.0766267776489258}
I0315 07:06:19.085096 139505847715584 logging_writer.py:48] [92698] accumulated_eval_time=739.965418, accumulated_logging_time=1.076627, accumulated_submission_time=16868.027230, global_step=92698, preemption_count=0, score=16868.027230, test/accuracy=0.510100, test/loss=2.382177, test/num_examples=10000, total_duration=17612.059515, train/accuracy=0.676419, train/loss=1.544100, validation/accuracy=0.632900, validation/loss=1.742726, validation/num_examples=50000
I0315 07:07:14.048439 139505856108288 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.5740060210227966, loss=3.4348878860473633
I0315 07:08:44.777858 139505847715584 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.5556179285049438, loss=3.4935405254364014
I0315 07:10:15.496336 139505856108288 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.5558562874794006, loss=3.4001266956329346
I0315 07:11:46.311353 139505847715584 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.5666016936302185, loss=3.3812828063964844
I0315 07:13:16.990033 139505856108288 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.576926052570343, loss=3.476687431335449
I0315 07:14:47.733467 139505847715584 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.5735338926315308, loss=3.3234055042266846
I0315 07:14:49.066278 139666979968832 spec.py:321] Evaluating on the training split.
I0315 07:14:55.344078 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 07:15:04.964593 139666979968832 spec.py:349] Evaluating on the test split.
I0315 07:15:07.271408 139666979968832 submission_runner.py:420] Time since start: 18140.27s, 	Step: 95509, 	{'train/accuracy': 0.6886360049247742, 'train/loss': 1.4965471029281616, 'validation/accuracy': 0.6438599824905396, 'validation/loss': 1.6948471069335938, 'validation/num_examples': 50000, 'test/accuracy': 0.5175000429153442, 'test/loss': 2.3338916301727295, 'test/num_examples': 10000, 'score': 17377.905303001404, 'total_duration': 18140.27098608017, 'accumulated_submission_time': 17377.905303001404, 'accumulated_eval_time': 758.1704969406128, 'accumulated_logging_time': 1.1112191677093506}
I0315 07:15:07.294376 139505797359360 logging_writer.py:48] [95509] accumulated_eval_time=758.170497, accumulated_logging_time=1.111219, accumulated_submission_time=17377.905303, global_step=95509, preemption_count=0, score=17377.905303, test/accuracy=0.517500, test/loss=2.333892, test/num_examples=10000, total_duration=18140.270986, train/accuracy=0.688636, train/loss=1.496547, validation/accuracy=0.643860, validation/loss=1.694847, validation/num_examples=50000
I0315 07:16:36.614814 139505805752064 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.5830269455909729, loss=3.514838457107544
I0315 07:18:07.340579 139505797359360 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.6036698818206787, loss=3.4916043281555176
I0315 07:19:38.098092 139505805752064 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.5750137567520142, loss=3.4513959884643555
I0315 07:21:08.800127 139505797359360 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.5893276929855347, loss=3.4421305656433105
I0315 07:22:39.538136 139505805752064 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.6076564788818359, loss=3.379394292831421
I0315 07:23:37.281218 139666979968832 spec.py:321] Evaluating on the training split.
I0315 07:23:43.549932 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 07:23:53.123198 139666979968832 spec.py:349] Evaluating on the test split.
I0315 07:23:55.449937 139666979968832 submission_runner.py:420] Time since start: 18668.45s, 	Step: 98320, 	{'train/accuracy': 0.6938377022743225, 'train/loss': 1.4277307987213135, 'validation/accuracy': 0.6497200131416321, 'validation/loss': 1.628403663635254, 'validation/num_examples': 50000, 'test/accuracy': 0.5182999968528748, 'test/loss': 2.2841415405273438, 'test/num_examples': 10000, 'score': 17887.787921190262, 'total_duration': 18668.44917845726, 'accumulated_submission_time': 17887.787921190262, 'accumulated_eval_time': 776.3388483524323, 'accumulated_logging_time': 1.1502656936645508}
I0315 07:23:55.473224 139505839322880 logging_writer.py:48] [98320] accumulated_eval_time=776.338848, accumulated_logging_time=1.150266, accumulated_submission_time=17887.787921, global_step=98320, preemption_count=0, score=17887.787921, test/accuracy=0.518300, test/loss=2.284142, test/num_examples=10000, total_duration=18668.449178, train/accuracy=0.693838, train/loss=1.427731, validation/accuracy=0.649720, validation/loss=1.628404, validation/num_examples=50000
I0315 07:24:28.356452 139505847715584 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.5924690961837769, loss=3.3998048305511475
I0315 07:25:59.103619 139505839322880 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.5870499610900879, loss=3.392606019973755
I0315 07:27:29.929381 139505847715584 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.6047101020812988, loss=3.4255237579345703
I0315 07:29:00.793561 139505839322880 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.6257623434066772, loss=3.411337375640869
I0315 07:30:31.576481 139505847715584 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.584034264087677, loss=3.4183459281921387
I0315 07:32:02.386554 139505839322880 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.5751879215240479, loss=3.302088499069214
I0315 07:32:25.540831 139666979968832 spec.py:321] Evaluating on the training split.
I0315 07:32:31.819339 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 07:32:40.863386 139666979968832 spec.py:349] Evaluating on the test split.
I0315 07:32:43.148797 139666979968832 submission_runner.py:420] Time since start: 19196.15s, 	Step: 101129, 	{'train/accuracy': 0.7052973508834839, 'train/loss': 1.406061053276062, 'validation/accuracy': 0.6511200070381165, 'validation/loss': 1.633562445640564, 'validation/num_examples': 50000, 'test/accuracy': 0.522599995136261, 'test/loss': 2.281493663787842, 'test/num_examples': 10000, 'score': 18397.755425691605, 'total_duration': 19196.148114442825, 'accumulated_submission_time': 18397.755425691605, 'accumulated_eval_time': 793.9465079307556, 'accumulated_logging_time': 1.1844494342803955}
I0315 07:32:43.171200 139505814144768 logging_writer.py:48] [101129] accumulated_eval_time=793.946508, accumulated_logging_time=1.184449, accumulated_submission_time=18397.755426, global_step=101129, preemption_count=0, score=18397.755426, test/accuracy=0.522600, test/loss=2.281494, test/num_examples=10000, total_duration=19196.148114, train/accuracy=0.705297, train/loss=1.406061, validation/accuracy=0.651120, validation/loss=1.633562, validation/num_examples=50000
I0315 07:33:50.684469 139505822537472 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.6150788068771362, loss=3.3951125144958496
I0315 07:35:21.456271 139505814144768 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.5916683077812195, loss=3.35392427444458
I0315 07:36:52.269481 139505822537472 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.6117551922798157, loss=3.423734426498413
I0315 07:38:23.064942 139505814144768 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.5840719938278198, loss=3.362096071243286
I0315 07:39:53.852671 139505822537472 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.5938588976860046, loss=3.3498568534851074
I0315 07:41:13.246915 139666979968832 spec.py:321] Evaluating on the training split.
I0315 07:41:19.489356 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 07:41:29.130711 139666979968832 spec.py:349] Evaluating on the test split.
I0315 07:41:31.437247 139666979968832 submission_runner.py:420] Time since start: 19724.44s, 	Step: 103939, 	{'train/accuracy': 0.7524513602256775, 'train/loss': 1.2095069885253906, 'validation/accuracy': 0.6575999855995178, 'validation/loss': 1.598398208618164, 'validation/num_examples': 50000, 'test/accuracy': 0.5339000225067139, 'test/loss': 2.2238636016845703, 'test/num_examples': 10000, 'score': 18907.731110334396, 'total_duration': 19724.436819791794, 'accumulated_submission_time': 18907.731110334396, 'accumulated_eval_time': 812.1368012428284, 'accumulated_logging_time': 1.2169291973114014}
I0315 07:41:31.462212 139505797359360 logging_writer.py:48] [103939] accumulated_eval_time=812.136801, accumulated_logging_time=1.216929, accumulated_submission_time=18907.731110, global_step=103939, preemption_count=0, score=18907.731110, test/accuracy=0.533900, test/loss=2.223864, test/num_examples=10000, total_duration=19724.436820, train/accuracy=0.752451, train/loss=1.209507, validation/accuracy=0.657600, validation/loss=1.598398, validation/num_examples=50000
I0315 07:41:42.744444 139505805752064 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.5996352434158325, loss=3.29832124710083
I0315 07:43:13.517698 139505797359360 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.6094701290130615, loss=3.3493800163269043
I0315 07:44:44.269983 139505805752064 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.6351799368858337, loss=3.374814987182617
I0315 07:46:15.051923 139505797359360 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.6278290152549744, loss=3.3541321754455566
I0315 07:47:45.786236 139505805752064 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.6324911117553711, loss=3.3475139141082764
I0315 07:49:16.588540 139505797359360 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.6067432165145874, loss=3.262071132659912
I0315 07:50:01.458134 139666979968832 spec.py:321] Evaluating on the training split.
I0315 07:50:07.724448 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 07:50:16.734587 139666979968832 spec.py:349] Evaluating on the test split.
I0315 07:50:19.001425 139666979968832 submission_runner.py:420] Time since start: 20252.00s, 	Step: 106749, 	{'train/accuracy': 0.7403738498687744, 'train/loss': 1.256012201309204, 'validation/accuracy': 0.6581000089645386, 'validation/loss': 1.60061776638031, 'validation/num_examples': 50000, 'test/accuracy': 0.5315000414848328, 'test/loss': 2.2344987392425537, 'test/num_examples': 10000, 'score': 19417.62467956543, 'total_duration': 20252.0008995533, 'accumulated_submission_time': 19417.62467956543, 'accumulated_eval_time': 829.6799504756927, 'accumulated_logging_time': 1.2548072338104248}
I0315 07:50:19.026185 139505839322880 logging_writer.py:48] [106749] accumulated_eval_time=829.679950, accumulated_logging_time=1.254807, accumulated_submission_time=19417.624680, global_step=106749, preemption_count=0, score=19417.624680, test/accuracy=0.531500, test/loss=2.234499, test/num_examples=10000, total_duration=20252.000900, train/accuracy=0.740374, train/loss=1.256012, validation/accuracy=0.658100, validation/loss=1.600618, validation/num_examples=50000
I0315 07:51:04.781023 139505847715584 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.6326605081558228, loss=3.2577273845672607
I0315 07:52:35.584463 139505839322880 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.6099247336387634, loss=3.254615306854248
I0315 07:54:06.320480 139505847715584 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.6209153532981873, loss=3.2599711418151855
I0315 07:55:37.098355 139505839322880 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.6284964680671692, loss=3.3003463745117188
I0315 07:57:07.828417 139505847715584 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.6279189586639404, loss=3.290050983428955
I0315 07:58:38.524012 139505839322880 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.6539137363433838, loss=3.275315284729004
I0315 07:58:49.113064 139666979968832 spec.py:321] Evaluating on the training split.
I0315 07:58:55.321311 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 07:59:04.413223 139666979968832 spec.py:349] Evaluating on the test split.
I0315 07:59:06.726996 139666979968832 submission_runner.py:420] Time since start: 20779.73s, 	Step: 109560, 	{'train/accuracy': 0.7405133843421936, 'train/loss': 1.2657791376113892, 'validation/accuracy': 0.6663399934768677, 'validation/loss': 1.5837370157241821, 'validation/num_examples': 50000, 'test/accuracy': 0.5448000431060791, 'test/loss': 2.2128961086273193, 'test/num_examples': 10000, 'score': 19927.61045193672, 'total_duration': 20779.726460456848, 'accumulated_submission_time': 19927.61045193672, 'accumulated_eval_time': 847.2937185764313, 'accumulated_logging_time': 1.289489507675171}
I0315 07:59:06.753122 139505814144768 logging_writer.py:48] [109560] accumulated_eval_time=847.293719, accumulated_logging_time=1.289490, accumulated_submission_time=19927.610452, global_step=109560, preemption_count=0, score=19927.610452, test/accuracy=0.544800, test/loss=2.212896, test/num_examples=10000, total_duration=20779.726460, train/accuracy=0.740513, train/loss=1.265779, validation/accuracy=0.666340, validation/loss=1.583737, validation/num_examples=50000
I0315 08:00:26.822880 139505822537472 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.6709796786308289, loss=3.372910261154175
I0315 08:01:57.587998 139505814144768 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.6647735238075256, loss=3.340064287185669
I0315 08:03:28.359440 139505822537472 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.6390460729598999, loss=3.2633209228515625
I0315 08:04:59.187323 139505814144768 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.6674402952194214, loss=3.3406293392181396
I0315 08:06:29.995265 139505822537472 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.6376989483833313, loss=3.159301280975342
I0315 08:07:36.851782 139666979968832 spec.py:321] Evaluating on the training split.
I0315 08:07:43.031109 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 08:07:52.066972 139666979968832 spec.py:349] Evaluating on the test split.
I0315 08:07:54.420199 139666979968832 submission_runner.py:420] Time since start: 21307.42s, 	Step: 112370, 	{'train/accuracy': 0.7446388602256775, 'train/loss': 1.229624629020691, 'validation/accuracy': 0.6768800020217896, 'validation/loss': 1.5161625146865845, 'validation/num_examples': 50000, 'test/accuracy': 0.5543000102043152, 'test/loss': 2.1517488956451416, 'test/num_examples': 10000, 'score': 20437.608496189117, 'total_duration': 21307.419775009155, 'accumulated_submission_time': 20437.608496189117, 'accumulated_eval_time': 864.8620934486389, 'accumulated_logging_time': 1.326627492904663}
I0315 08:07:54.443940 139505797359360 logging_writer.py:48] [112370] accumulated_eval_time=864.862093, accumulated_logging_time=1.326627, accumulated_submission_time=20437.608496, global_step=112370, preemption_count=0, score=20437.608496, test/accuracy=0.554300, test/loss=2.151749, test/num_examples=10000, total_duration=21307.419775, train/accuracy=0.744639, train/loss=1.229625, validation/accuracy=0.676880, validation/loss=1.516163, validation/num_examples=50000
I0315 08:08:18.241888 139505805752064 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.6534831523895264, loss=3.202890396118164
I0315 08:09:49.014973 139505797359360 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.6799449324607849, loss=3.2637763023376465
I0315 08:11:19.780859 139505805752064 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.6822146773338318, loss=3.2099757194519043
I0315 08:12:50.548201 139505797359360 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.6710571050643921, loss=3.1941640377044678
I0315 08:14:21.272575 139505805752064 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.6732839345932007, loss=3.223296642303467
I0315 08:15:52.032753 139505797359360 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.68071448802948, loss=3.132054328918457
I0315 08:16:24.581042 139666979968832 spec.py:321] Evaluating on the training split.
I0315 08:16:30.795510 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 08:16:39.895553 139666979968832 spec.py:349] Evaluating on the test split.
I0315 08:16:42.215282 139666979968832 submission_runner.py:420] Time since start: 21835.21s, 	Step: 115181, 	{'train/accuracy': 0.7556201815605164, 'train/loss': 1.190289855003357, 'validation/accuracy': 0.6910399794578552, 'validation/loss': 1.4795939922332764, 'validation/num_examples': 50000, 'test/accuracy': 0.5596000552177429, 'test/loss': 2.106682300567627, 'test/num_examples': 10000, 'score': 20947.64518761635, 'total_duration': 21835.214857578278, 'accumulated_submission_time': 20947.64518761635, 'accumulated_eval_time': 882.4962873458862, 'accumulated_logging_time': 1.360917568206787}
I0315 08:16:42.240429 139505822537472 logging_writer.py:48] [115181] accumulated_eval_time=882.496287, accumulated_logging_time=1.360918, accumulated_submission_time=20947.645188, global_step=115181, preemption_count=0, score=20947.645188, test/accuracy=0.559600, test/loss=2.106682, test/num_examples=10000, total_duration=21835.214858, train/accuracy=0.755620, train/loss=1.190290, validation/accuracy=0.691040, validation/loss=1.479594, validation/num_examples=50000
I0315 08:17:40.362469 139505830930176 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.7392204403877258, loss=3.319584608078003
I0315 08:19:11.144185 139505822537472 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.7070185542106628, loss=3.1646475791931152
I0315 08:20:41.849579 139505830930176 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.6656545400619507, loss=3.156933307647705
I0315 08:22:12.650617 139505822537472 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.6973899006843567, loss=3.1256749629974365
I0315 08:23:43.388016 139505830930176 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.6849806308746338, loss=3.1202428340911865
I0315 08:25:12.232301 139666979968832 spec.py:321] Evaluating on the training split.
I0315 08:25:18.464911 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 08:25:27.491193 139666979968832 spec.py:349] Evaluating on the test split.
I0315 08:25:29.815652 139666979968832 submission_runner.py:420] Time since start: 22362.82s, 	Step: 117991, 	{'train/accuracy': 0.7688137888908386, 'train/loss': 1.1277391910552979, 'validation/accuracy': 0.6998199820518494, 'validation/loss': 1.4276926517486572, 'validation/num_examples': 50000, 'test/accuracy': 0.5718000531196594, 'test/loss': 2.0509040355682373, 'test/num_examples': 10000, 'score': 21457.53855085373, 'total_duration': 22362.81521010399, 'accumulated_submission_time': 21457.53855085373, 'accumulated_eval_time': 900.0796000957489, 'accumulated_logging_time': 1.3957161903381348}
I0315 08:25:29.839120 139505797359360 logging_writer.py:48] [117991] accumulated_eval_time=900.079600, accumulated_logging_time=1.395716, accumulated_submission_time=21457.538551, global_step=117991, preemption_count=0, score=21457.538551, test/accuracy=0.571800, test/loss=2.050904, test/num_examples=10000, total_duration=22362.815210, train/accuracy=0.768814, train/loss=1.127739, validation/accuracy=0.699820, validation/loss=1.427693, validation/num_examples=50000
I0315 08:25:31.674666 139505805752064 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.6945940256118774, loss=3.143674373626709
I0315 08:27:02.507606 139505797359360 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.6975460052490234, loss=3.1531784534454346
I0315 08:28:33.318945 139505805752064 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.7113035321235657, loss=3.10469388961792
I0315 08:30:04.074764 139505797359360 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.7319753170013428, loss=3.145764112472534
I0315 08:31:34.813908 139505805752064 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.7173941135406494, loss=3.003526210784912
I0315 08:33:05.647519 139505797359360 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.6854261159896851, loss=3.0443098545074463
I0315 08:33:59.822609 139666979968832 spec.py:321] Evaluating on the training split.
I0315 08:34:05.990681 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 08:34:18.093344 139666979968832 spec.py:349] Evaluating on the test split.
I0315 08:34:20.360878 139666979968832 submission_runner.py:420] Time since start: 22893.36s, 	Step: 120800, 	{'train/accuracy': 0.7773038744926453, 'train/loss': 1.0774314403533936, 'validation/accuracy': 0.7059400081634521, 'validation/loss': 1.373658299446106, 'validation/num_examples': 50000, 'test/accuracy': 0.5796000361442566, 'test/loss': 1.9862518310546875, 'test/num_examples': 10000, 'score': 21967.416944026947, 'total_duration': 22893.36046242714, 'accumulated_submission_time': 21967.416944026947, 'accumulated_eval_time': 920.617835521698, 'accumulated_logging_time': 1.4305720329284668}
I0315 08:34:20.380748 139505797359360 logging_writer.py:48] [120800] accumulated_eval_time=920.617836, accumulated_logging_time=1.430572, accumulated_submission_time=21967.416944, global_step=120800, preemption_count=0, score=21967.416944, test/accuracy=0.579600, test/loss=1.986252, test/num_examples=10000, total_duration=22893.360462, train/accuracy=0.777304, train/loss=1.077431, validation/accuracy=0.705940, validation/loss=1.373658, validation/num_examples=50000
I0315 08:34:56.873422 139505822537472 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.7327407598495483, loss=3.0194573402404785
I0315 08:36:27.656984 139505797359360 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.7286644577980042, loss=3.0481247901916504
I0315 08:37:58.430489 139505822537472 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.7372355461120605, loss=3.0655102729797363
I0315 08:39:29.217302 139505797359360 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.7456064820289612, loss=3.106062650680542
I0315 08:41:00.011951 139505822537472 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.7539805173873901, loss=3.028301954269409
I0315 08:42:30.787418 139505797359360 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.7543778419494629, loss=3.076482057571411
I0315 08:42:50.420752 139666979968832 spec.py:321] Evaluating on the training split.
I0315 08:42:56.610316 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 08:43:05.912918 139666979968832 spec.py:349] Evaluating on the test split.
I0315 08:43:08.204402 139666979968832 submission_runner.py:420] Time since start: 23421.20s, 	Step: 123610, 	{'train/accuracy': 0.7887236475944519, 'train/loss': 1.0411092042922974, 'validation/accuracy': 0.7205599546432495, 'validation/loss': 1.342443823814392, 'validation/num_examples': 50000, 'test/accuracy': 0.5943000316619873, 'test/loss': 1.9445018768310547, 'test/num_examples': 10000, 'score': 22477.35790538788, 'total_duration': 23421.203782081604, 'accumulated_submission_time': 22477.35790538788, 'accumulated_eval_time': 938.4012403488159, 'accumulated_logging_time': 1.460312843322754}
I0315 08:43:08.229476 139505856108288 logging_writer.py:48] [123610] accumulated_eval_time=938.401240, accumulated_logging_time=1.460313, accumulated_submission_time=22477.357905, global_step=123610, preemption_count=0, score=22477.357905, test/accuracy=0.594300, test/loss=1.944502, test/num_examples=10000, total_duration=23421.203782, train/accuracy=0.788724, train/loss=1.041109, validation/accuracy=0.720560, validation/loss=1.342444, validation/num_examples=50000
I0315 08:44:19.236428 139505864500992 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.811130940914154, loss=3.1291608810424805
I0315 08:45:50.027286 139505856108288 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.7687680721282959, loss=2.939025402069092
I0315 08:47:20.775374 139505864500992 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.7975581288337708, loss=3.051192283630371
I0315 08:48:51.542084 139505856108288 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.7892603278160095, loss=3.024719476699829
I0315 08:50:22.209388 139505864500992 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.7952897548675537, loss=2.9289095401763916
I0315 08:51:38.330325 139666979968832 spec.py:321] Evaluating on the training split.
I0315 08:51:44.526970 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 08:51:54.337244 139666979968832 spec.py:349] Evaluating on the test split.
I0315 08:51:56.725372 139666979968832 submission_runner.py:420] Time since start: 23949.72s, 	Step: 126421, 	{'train/accuracy': 0.8101881146430969, 'train/loss': 0.9542239904403687, 'validation/accuracy': 0.7309399843215942, 'validation/loss': 1.2795382738113403, 'validation/num_examples': 50000, 'test/accuracy': 0.6060000061988831, 'test/loss': 1.8781458139419556, 'test/num_examples': 10000, 'score': 22987.358740091324, 'total_duration': 23949.724945545197, 'accumulated_submission_time': 22987.358740091324, 'accumulated_eval_time': 956.7962579727173, 'accumulated_logging_time': 1.4970099925994873}
I0315 08:51:56.751588 139505864500992 logging_writer.py:48] [126421] accumulated_eval_time=956.796258, accumulated_logging_time=1.497010, accumulated_submission_time=22987.358740, global_step=126421, preemption_count=0, score=22987.358740, test/accuracy=0.606000, test/loss=1.878146, test/num_examples=10000, total_duration=23949.724946, train/accuracy=0.810188, train/loss=0.954224, validation/accuracy=0.730940, validation/loss=1.279538, validation/num_examples=50000
I0315 08:52:11.283437 139505872893696 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.7836793661117554, loss=2.9348344802856445
I0315 08:53:42.004328 139505864500992 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.8004403710365295, loss=2.949413299560547
I0315 08:55:12.795349 139505872893696 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.8243253827095032, loss=2.9221959114074707
I0315 08:56:43.566666 139505864500992 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.8009209632873535, loss=2.9369747638702393
I0315 08:58:14.241187 139505872893696 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.877152144908905, loss=3.043929100036621
I0315 08:59:45.002158 139505864500992 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.8561382293701172, loss=2.9639508724212646
I0315 09:00:26.814954 139666979968832 spec.py:321] Evaluating on the training split.
I0315 09:00:33.061690 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 09:00:42.078490 139666979968832 spec.py:349] Evaluating on the test split.
I0315 09:00:44.397237 139666979968832 submission_runner.py:420] Time since start: 24477.40s, 	Step: 129232, 	{'train/accuracy': 0.8279455900192261, 'train/loss': 0.877056360244751, 'validation/accuracy': 0.7427600026130676, 'validation/loss': 1.2180293798446655, 'validation/num_examples': 50000, 'test/accuracy': 0.6212000250816345, 'test/loss': 1.8021321296691895, 'test/num_examples': 10000, 'score': 23497.319289684296, 'total_duration': 24477.39676475525, 'accumulated_submission_time': 23497.319289684296, 'accumulated_eval_time': 974.3784441947937, 'accumulated_logging_time': 1.533355712890625}
I0315 09:00:44.424171 139505805752064 logging_writer.py:48] [129232] accumulated_eval_time=974.378444, accumulated_logging_time=1.533356, accumulated_submission_time=23497.319290, global_step=129232, preemption_count=0, score=23497.319290, test/accuracy=0.621200, test/loss=1.802132, test/num_examples=10000, total_duration=24477.396765, train/accuracy=0.827946, train/loss=0.877056, validation/accuracy=0.742760, validation/loss=1.218029, validation/num_examples=50000
I0315 09:01:33.226106 139505814144768 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.8442052006721497, loss=2.904426336288452
I0315 09:03:03.931227 139505805752064 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.7868227958679199, loss=2.793962001800537
I0315 09:04:34.633615 139505814144768 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.8136447072029114, loss=2.888620376586914
I0315 09:06:05.333692 139505805752064 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.8369268178939819, loss=2.858423948287964
I0315 09:07:35.997822 139505814144768 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.8916689157485962, loss=2.90483021736145
I0315 09:09:06.685371 139505805752064 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.8254868984222412, loss=2.8779139518737793
I0315 09:09:14.546057 139666979968832 spec.py:321] Evaluating on the training split.
I0315 09:09:20.833836 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 09:09:29.900336 139666979968832 spec.py:349] Evaluating on the test split.
I0315 09:09:32.197309 139666979968832 submission_runner.py:420] Time since start: 25005.20s, 	Step: 132045, 	{'train/accuracy': 0.8474768400192261, 'train/loss': 0.8029673099517822, 'validation/accuracy': 0.756879985332489, 'validation/loss': 1.1657541990280151, 'validation/num_examples': 50000, 'test/accuracy': 0.6394000053405762, 'test/loss': 1.7313377857208252, 'test/num_examples': 10000, 'score': 24007.340938329697, 'total_duration': 25005.19687461853, 'accumulated_submission_time': 24007.340938329697, 'accumulated_eval_time': 992.0296437740326, 'accumulated_logging_time': 1.5704536437988281}
I0315 09:09:32.221211 139505805752064 logging_writer.py:48] [132045] accumulated_eval_time=992.029644, accumulated_logging_time=1.570454, accumulated_submission_time=24007.340938, global_step=132045, preemption_count=0, score=24007.340938, test/accuracy=0.639400, test/loss=1.731338, test/num_examples=10000, total_duration=25005.196875, train/accuracy=0.847477, train/loss=0.802967, validation/accuracy=0.756880, validation/loss=1.165754, validation/num_examples=50000
I0315 09:10:54.975322 139505814144768 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.8424205780029297, loss=2.8087306022644043
I0315 09:12:25.718798 139505805752064 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.8362730741500854, loss=2.7962965965270996
I0315 09:13:56.415319 139505814144768 logging_writer.py:48] [133500] global_step=133500, grad_norm=0.8475813269615173, loss=2.7873430252075195
I0315 09:15:27.117282 139505805752064 logging_writer.py:48] [134000] global_step=134000, grad_norm=0.8763389587402344, loss=2.7893855571746826
I0315 09:16:57.840666 139505814144768 logging_writer.py:48] [134500] global_step=134500, grad_norm=0.8445538878440857, loss=2.762542724609375
I0315 09:18:02.296681 139666979968832 spec.py:321] Evaluating on the training split.
I0315 09:18:08.448324 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 09:18:17.436267 139666979968832 spec.py:349] Evaluating on the test split.
I0315 09:18:19.782156 139666979968832 submission_runner.py:420] Time since start: 25532.78s, 	Step: 134857, 	{'train/accuracy': 0.8592354655265808, 'train/loss': 0.7484070658683777, 'validation/accuracy': 0.7630999684333801, 'validation/loss': 1.1406275033950806, 'validation/num_examples': 50000, 'test/accuracy': 0.6477000117301941, 'test/loss': 1.6970221996307373, 'test/num_examples': 10000, 'score': 24517.31483054161, 'total_duration': 25532.781730413437, 'accumulated_submission_time': 24517.31483054161, 'accumulated_eval_time': 1009.5150735378265, 'accumulated_logging_time': 1.6044492721557617}
I0315 09:18:19.808881 139505830930176 logging_writer.py:48] [134857] accumulated_eval_time=1009.515074, accumulated_logging_time=1.604449, accumulated_submission_time=24517.314831, global_step=134857, preemption_count=0, score=24517.314831, test/accuracy=0.647700, test/loss=1.697022, test/num_examples=10000, total_duration=25532.781730, train/accuracy=0.859235, train/loss=0.748407, validation/accuracy=0.763100, validation/loss=1.140628, validation/num_examples=50000
I0315 09:18:45.981804 139505839322880 logging_writer.py:48] [135000] global_step=135000, grad_norm=0.8656408786773682, loss=2.816453218460083
I0315 09:20:16.687231 139505830930176 logging_writer.py:48] [135500] global_step=135500, grad_norm=0.865656316280365, loss=2.804154634475708
I0315 09:21:47.422790 139505839322880 logging_writer.py:48] [136000] global_step=136000, grad_norm=0.8632057905197144, loss=2.787586212158203
I0315 09:23:18.140420 139505830930176 logging_writer.py:48] [136500] global_step=136500, grad_norm=0.8476317524909973, loss=2.773934841156006
I0315 09:24:48.913498 139505839322880 logging_writer.py:48] [137000] global_step=137000, grad_norm=0.8200514316558838, loss=2.788311004638672
I0315 09:26:19.627707 139505830930176 logging_writer.py:48] [137500] global_step=137500, grad_norm=0.7690371870994568, loss=2.6944222450256348
I0315 09:26:49.814228 139666979968832 spec.py:321] Evaluating on the training split.
I0315 09:26:56.043856 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 09:27:05.090645 139666979968832 spec.py:349] Evaluating on the test split.
I0315 09:27:07.398793 139666979968832 submission_runner.py:420] Time since start: 26060.40s, 	Step: 137668, 	{'train/accuracy': 0.8620455861091614, 'train/loss': 0.7470542788505554, 'validation/accuracy': 0.763219952583313, 'validation/loss': 1.1393851041793823, 'validation/num_examples': 50000, 'test/accuracy': 0.6467000246047974, 'test/loss': 1.6975725889205933, 'test/num_examples': 10000, 'score': 25027.216710329056, 'total_duration': 26060.398368120193, 'accumulated_submission_time': 25027.216710329056, 'accumulated_eval_time': 1027.0995848178864, 'accumulated_logging_time': 1.6429941654205322}
I0315 09:27:07.423830 139505805752064 logging_writer.py:48] [137668] accumulated_eval_time=1027.099585, accumulated_logging_time=1.642994, accumulated_submission_time=25027.216710, global_step=137668, preemption_count=0, score=25027.216710, test/accuracy=0.646700, test/loss=1.697573, test/num_examples=10000, total_duration=26060.398368, train/accuracy=0.862046, train/loss=0.747054, validation/accuracy=0.763220, validation/loss=1.139385, validation/num_examples=50000
I0315 09:28:07.930467 139505814144768 logging_writer.py:48] [138000] global_step=138000, grad_norm=0.8407564163208008, loss=2.799201726913452
I0315 09:29:38.650904 139505805752064 logging_writer.py:48] [138500] global_step=138500, grad_norm=0.8337437510490417, loss=2.795427083969116
I0315 09:31:09.330558 139505814144768 logging_writer.py:48] [139000] global_step=139000, grad_norm=0.845933198928833, loss=2.7540793418884277
I0315 09:32:40.077855 139505805752064 logging_writer.py:48] [139500] global_step=139500, grad_norm=0.8607674837112427, loss=2.7802014350891113
I0315 09:34:10.764946 139505814144768 logging_writer.py:48] [140000] global_step=140000, grad_norm=0.8482858538627625, loss=2.8290364742279053
I0315 09:35:37.522176 139666979968832 spec.py:321] Evaluating on the training split.
I0315 09:35:44.351652 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 09:35:53.609931 139666979968832 spec.py:349] Evaluating on the test split.
I0315 09:35:55.930444 139666979968832 submission_runner.py:420] Time since start: 26588.93s, 	Step: 140480, 	{'train/accuracy': 0.8608497977256775, 'train/loss': 0.7480168342590332, 'validation/accuracy': 0.7636799812316895, 'validation/loss': 1.1389931440353394, 'validation/num_examples': 50000, 'test/accuracy': 0.6481000185012817, 'test/loss': 1.693626880645752, 'test/num_examples': 10000, 'score': 25537.2121386528, 'total_duration': 26588.929893493652, 'accumulated_submission_time': 25537.2121386528, 'accumulated_eval_time': 1045.5076894760132, 'accumulated_logging_time': 1.6779308319091797}
I0315 09:35:55.956335 139505805752064 logging_writer.py:48] [140480] accumulated_eval_time=1045.507689, accumulated_logging_time=1.677931, accumulated_submission_time=25537.212139, global_step=140480, preemption_count=0, score=25537.212139, test/accuracy=0.648100, test/loss=1.693627, test/num_examples=10000, total_duration=26588.929893, train/accuracy=0.860850, train/loss=0.748017, validation/accuracy=0.763680, validation/loss=1.138993, validation/num_examples=50000
I0315 09:35:59.788953 139505839322880 logging_writer.py:48] [140500] global_step=140500, grad_norm=0.8396462202072144, loss=2.7218666076660156
I0315 09:37:30.559297 139505805752064 logging_writer.py:48] [141000] global_step=141000, grad_norm=0.8636977672576904, loss=2.7770800590515137
I0315 09:39:01.333163 139505839322880 logging_writer.py:48] [141500] global_step=141500, grad_norm=0.8626682162284851, loss=2.7854857444763184
I0315 09:40:32.084591 139505805752064 logging_writer.py:48] [142000] global_step=142000, grad_norm=0.8984966278076172, loss=2.81001615524292
I0315 09:42:02.765784 139505839322880 logging_writer.py:48] [142500] global_step=142500, grad_norm=0.9053803086280823, loss=2.7667980194091797
I0315 09:43:33.544447 139505805752064 logging_writer.py:48] [143000] global_step=143000, grad_norm=0.8897578120231628, loss=2.8099303245544434
I0315 09:44:26.027369 139666979968832 spec.py:321] Evaluating on the training split.
I0315 09:44:32.355231 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 09:44:41.434215 139666979968832 spec.py:349] Evaluating on the test split.
I0315 09:44:43.765411 139666979968832 submission_runner.py:420] Time since start: 27116.76s, 	Step: 143291, 	{'train/accuracy': 0.8587571382522583, 'train/loss': 0.7474705576896667, 'validation/accuracy': 0.7634999752044678, 'validation/loss': 1.134859323501587, 'validation/num_examples': 50000, 'test/accuracy': 0.6470000147819519, 'test/loss': 1.6911301612854004, 'test/num_examples': 10000, 'score': 26047.181527137756, 'total_duration': 27116.764947652817, 'accumulated_submission_time': 26047.181527137756, 'accumulated_eval_time': 1063.2456469535828, 'accumulated_logging_time': 1.715416431427002}
I0315 09:44:43.800783 139505847715584 logging_writer.py:48] [143291] accumulated_eval_time=1063.245647, accumulated_logging_time=1.715416, accumulated_submission_time=26047.181527, global_step=143291, preemption_count=0, score=26047.181527, test/accuracy=0.647000, test/loss=1.691130, test/num_examples=10000, total_duration=27116.764948, train/accuracy=0.858757, train/loss=0.747471, validation/accuracy=0.763500, validation/loss=1.134859, validation/num_examples=50000
I0315 09:45:21.906684 139505856108288 logging_writer.py:48] [143500] global_step=143500, grad_norm=0.8652805685997009, loss=2.7608933448791504
I0315 09:46:52.682383 139505847715584 logging_writer.py:48] [144000] global_step=144000, grad_norm=0.901849627494812, loss=2.8506317138671875
I0315 09:48:23.476152 139505856108288 logging_writer.py:48] [144500] global_step=144500, grad_norm=0.9253881573677063, loss=2.8727569580078125
I0315 09:49:54.230485 139505847715584 logging_writer.py:48] [145000] global_step=145000, grad_norm=0.8417168259620667, loss=2.7111597061157227
I0315 09:51:24.970803 139505856108288 logging_writer.py:48] [145500] global_step=145500, grad_norm=0.9115751385688782, loss=2.792571544647217
I0315 09:52:55.723817 139505847715584 logging_writer.py:48] [146000] global_step=146000, grad_norm=0.8900361657142639, loss=2.83198881149292
I0315 09:53:13.930167 139666979968832 spec.py:321] Evaluating on the training split.
I0315 09:53:20.119725 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 09:53:29.523924 139666979968832 spec.py:349] Evaluating on the test split.
I0315 09:53:31.882111 139666979968832 submission_runner.py:420] Time since start: 27644.88s, 	Step: 146102, 	{'train/accuracy': 0.8621252775192261, 'train/loss': 0.7498049736022949, 'validation/accuracy': 0.7643599510192871, 'validation/loss': 1.1372393369674683, 'validation/num_examples': 50000, 'test/accuracy': 0.6493000388145447, 'test/loss': 1.694334626197815, 'test/num_examples': 10000, 'score': 26557.20747065544, 'total_duration': 27644.881666898727, 'accumulated_submission_time': 26557.20747065544, 'accumulated_eval_time': 1081.19753074646, 'accumulated_logging_time': 1.762721300125122}
I0315 09:53:31.909337 139505805752064 logging_writer.py:48] [146102] accumulated_eval_time=1081.197531, accumulated_logging_time=1.762721, accumulated_submission_time=26557.207471, global_step=146102, preemption_count=0, score=26557.207471, test/accuracy=0.649300, test/loss=1.694335, test/num_examples=10000, total_duration=27644.881667, train/accuracy=0.862125, train/loss=0.749805, validation/accuracy=0.764360, validation/loss=1.137239, validation/num_examples=50000
I0315 09:54:44.309741 139505814144768 logging_writer.py:48] [146500] global_step=146500, grad_norm=0.8829180598258972, loss=2.803954601287842
I0315 09:56:15.005146 139505805752064 logging_writer.py:48] [147000] global_step=147000, grad_norm=0.8643321990966797, loss=2.7647311687469482
I0315 09:57:45.746979 139505814144768 logging_writer.py:48] [147500] global_step=147500, grad_norm=0.9018634557723999, loss=2.800398111343384
I0315 09:59:16.460589 139505805752064 logging_writer.py:48] [148000] global_step=148000, grad_norm=0.8874695897102356, loss=2.749638319015503
I0315 10:00:47.207895 139505814144768 logging_writer.py:48] [148500] global_step=148500, grad_norm=0.8637458086013794, loss=2.7053439617156982
I0315 10:02:02.016837 139666979968832 spec.py:321] Evaluating on the training split.
I0315 10:02:08.168073 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 10:02:17.089281 139666979968832 spec.py:349] Evaluating on the test split.
I0315 10:02:19.453678 139666979968832 submission_runner.py:420] Time since start: 28172.45s, 	Step: 148914, 	{'train/accuracy': 0.8628228306770325, 'train/loss': 0.743279755115509, 'validation/accuracy': 0.7644199728965759, 'validation/loss': 1.1323521137237549, 'validation/num_examples': 50000, 'test/accuracy': 0.65010005235672, 'test/loss': 1.6888558864593506, 'test/num_examples': 10000, 'score': 27067.21181702614, 'total_duration': 28172.453258514404, 'accumulated_submission_time': 27067.21181702614, 'accumulated_eval_time': 1098.6343348026276, 'accumulated_logging_time': 1.8031542301177979}
I0315 10:02:19.482359 139505797359360 logging_writer.py:48] [148914] accumulated_eval_time=1098.634335, accumulated_logging_time=1.803154, accumulated_submission_time=27067.211817, global_step=148914, preemption_count=0, score=27067.211817, test/accuracy=0.650100, test/loss=1.688856, test/num_examples=10000, total_duration=28172.453259, train/accuracy=0.862823, train/loss=0.743280, validation/accuracy=0.764420, validation/loss=1.132352, validation/num_examples=50000
I0315 10:02:35.298440 139505805752064 logging_writer.py:48] [149000] global_step=149000, grad_norm=0.9215749502182007, loss=2.8402621746063232
I0315 10:04:06.060253 139505797359360 logging_writer.py:48] [149500] global_step=149500, grad_norm=0.9193609952926636, loss=2.789463520050049
I0315 10:05:36.823560 139505805752064 logging_writer.py:48] [150000] global_step=150000, grad_norm=0.9430112242698669, loss=2.794888496398926
I0315 10:07:07.612846 139505797359360 logging_writer.py:48] [150500] global_step=150500, grad_norm=0.8583337068557739, loss=2.825894832611084
I0315 10:08:38.398020 139505805752064 logging_writer.py:48] [151000] global_step=151000, grad_norm=0.8939598202705383, loss=2.83119535446167
I0315 10:10:09.210056 139505797359360 logging_writer.py:48] [151500] global_step=151500, grad_norm=0.8598804473876953, loss=2.7399442195892334
I0315 10:10:49.576507 139666979968832 spec.py:321] Evaluating on the training split.
I0315 10:10:55.790705 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 10:11:05.280875 139666979968832 spec.py:349] Evaluating on the test split.
I0315 10:11:07.601368 139666979968832 submission_runner.py:420] Time since start: 28700.60s, 	Step: 151724, 	{'train/accuracy': 0.8606704473495483, 'train/loss': 0.7461697459220886, 'validation/accuracy': 0.7648400068283081, 'validation/loss': 1.1311298608779907, 'validation/num_examples': 50000, 'test/accuracy': 0.6498000025749207, 'test/loss': 1.6885789632797241, 'test/num_examples': 10000, 'score': 27577.204789161682, 'total_duration': 28700.600935935974, 'accumulated_submission_time': 27577.204789161682, 'accumulated_eval_time': 1116.6591629981995, 'accumulated_logging_time': 1.8424439430236816}
I0315 10:11:07.626617 139505805752064 logging_writer.py:48] [151724] accumulated_eval_time=1116.659163, accumulated_logging_time=1.842444, accumulated_submission_time=27577.204789, global_step=151724, preemption_count=0, score=27577.204789, test/accuracy=0.649800, test/loss=1.688579, test/num_examples=10000, total_duration=28700.600936, train/accuracy=0.860670, train/loss=0.746170, validation/accuracy=0.764840, validation/loss=1.131130, validation/num_examples=50000
I0315 10:11:57.919001 139505814144768 logging_writer.py:48] [152000] global_step=152000, grad_norm=0.8811670541763306, loss=2.778866767883301
I0315 10:13:28.644489 139505805752064 logging_writer.py:48] [152500] global_step=152500, grad_norm=0.8851622939109802, loss=2.8173675537109375
I0315 10:14:59.430411 139505814144768 logging_writer.py:48] [153000] global_step=153000, grad_norm=0.8919475078582764, loss=2.7388811111450195
I0315 10:16:30.174278 139505805752064 logging_writer.py:48] [153500] global_step=153500, grad_norm=0.9171890616416931, loss=2.835202932357788
I0315 10:18:00.960009 139505814144768 logging_writer.py:48] [154000] global_step=154000, grad_norm=0.8642585277557373, loss=2.7068610191345215
I0315 10:19:31.718703 139505805752064 logging_writer.py:48] [154500] global_step=154500, grad_norm=0.83119136095047, loss=2.7004668712615967
I0315 10:19:37.773677 139666979968832 spec.py:321] Evaluating on the training split.
I0315 10:19:43.897162 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 10:19:52.881655 139666979968832 spec.py:349] Evaluating on the test split.
I0315 10:19:55.215605 139666979968832 submission_runner.py:420] Time since start: 29228.22s, 	Step: 154535, 	{'train/accuracy': 0.8603515625, 'train/loss': 0.746981143951416, 'validation/accuracy': 0.7651999592781067, 'validation/loss': 1.1362378597259521, 'validation/num_examples': 50000, 'test/accuracy': 0.6504000425338745, 'test/loss': 1.6936733722686768, 'test/num_examples': 10000, 'score': 28087.249876976013, 'total_duration': 29228.215181350708, 'accumulated_submission_time': 28087.249876976013, 'accumulated_eval_time': 1134.1010489463806, 'accumulated_logging_time': 1.8775477409362793}
I0315 10:19:55.243400 139505839322880 logging_writer.py:48] [154535] accumulated_eval_time=1134.101049, accumulated_logging_time=1.877548, accumulated_submission_time=28087.249877, global_step=154535, preemption_count=0, score=28087.249877, test/accuracy=0.650400, test/loss=1.693673, test/num_examples=10000, total_duration=29228.215181, train/accuracy=0.860352, train/loss=0.746981, validation/accuracy=0.765200, validation/loss=1.136238, validation/num_examples=50000
I0315 10:21:19.813729 139505847715584 logging_writer.py:48] [155000] global_step=155000, grad_norm=0.9272671937942505, loss=2.849215030670166
I0315 10:22:50.590671 139505839322880 logging_writer.py:48] [155500] global_step=155500, grad_norm=0.8802645206451416, loss=2.802144765853882
I0315 10:24:21.311712 139505847715584 logging_writer.py:48] [156000] global_step=156000, grad_norm=0.8986729383468628, loss=2.771080493927002
I0315 10:25:52.088052 139505839322880 logging_writer.py:48] [156500] global_step=156500, grad_norm=0.8512523770332336, loss=2.7488176822662354
I0315 10:27:22.869050 139505847715584 logging_writer.py:48] [157000] global_step=157000, grad_norm=0.8878965377807617, loss=2.7328312397003174
I0315 10:28:25.350004 139666979968832 spec.py:321] Evaluating on the training split.
I0315 10:28:31.493757 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 10:28:40.595098 139666979968832 spec.py:349] Evaluating on the test split.
I0315 10:28:42.984845 139666979968832 submission_runner.py:420] Time since start: 29755.98s, 	Step: 157346, 	{'train/accuracy': 0.8623843789100647, 'train/loss': 0.7468582391738892, 'validation/accuracy': 0.765019953250885, 'validation/loss': 1.1292741298675537, 'validation/num_examples': 50000, 'test/accuracy': 0.6503000259399414, 'test/loss': 1.6862452030181885, 'test/num_examples': 10000, 'score': 28597.254593849182, 'total_duration': 29755.98441886902, 'accumulated_submission_time': 28597.254593849182, 'accumulated_eval_time': 1151.7358434200287, 'accumulated_logging_time': 1.9162397384643555}
I0315 10:28:43.010219 139505805752064 logging_writer.py:48] [157346] accumulated_eval_time=1151.735843, accumulated_logging_time=1.916240, accumulated_submission_time=28597.254594, global_step=157346, preemption_count=0, score=28597.254594, test/accuracy=0.650300, test/loss=1.686245, test/num_examples=10000, total_duration=29755.984419, train/accuracy=0.862384, train/loss=0.746858, validation/accuracy=0.765020, validation/loss=1.129274, validation/num_examples=50000
I0315 10:29:11.165141 139505814144768 logging_writer.py:48] [157500] global_step=157500, grad_norm=0.8546867370605469, loss=2.7429213523864746
I0315 10:30:41.958451 139505805752064 logging_writer.py:48] [158000] global_step=158000, grad_norm=0.8958808183670044, loss=2.714869499206543
I0315 10:32:12.693336 139505814144768 logging_writer.py:48] [158500] global_step=158500, grad_norm=0.8741289377212524, loss=2.7485997676849365
I0315 10:33:43.387822 139505805752064 logging_writer.py:48] [159000] global_step=159000, grad_norm=0.9496151208877563, loss=2.855172634124756
I0315 10:35:14.098934 139505814144768 logging_writer.py:48] [159500] global_step=159500, grad_norm=0.8837188482284546, loss=2.7321417331695557
I0315 10:36:44.837149 139505805752064 logging_writer.py:48] [160000] global_step=160000, grad_norm=0.8809787034988403, loss=2.7693772315979004
I0315 10:37:13.027211 139666979968832 spec.py:321] Evaluating on the training split.
I0315 10:37:19.194922 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 10:37:28.047431 139666979968832 spec.py:349] Evaluating on the test split.
I0315 10:37:30.331539 139666979968832 submission_runner.py:420] Time since start: 30283.33s, 	Step: 160157, 	{'train/accuracy': 0.8634008169174194, 'train/loss': 0.7377191781997681, 'validation/accuracy': 0.7651999592781067, 'validation/loss': 1.1320921182632446, 'validation/num_examples': 50000, 'test/accuracy': 0.6502000093460083, 'test/loss': 1.6894065141677856, 'test/num_examples': 10000, 'score': 29107.170354127884, 'total_duration': 30283.3311150074, 'accumulated_submission_time': 29107.170354127884, 'accumulated_eval_time': 1169.0401241779327, 'accumulated_logging_time': 1.9524645805358887}
I0315 10:37:30.357270 139505805752064 logging_writer.py:48] [160157] accumulated_eval_time=1169.040124, accumulated_logging_time=1.952465, accumulated_submission_time=29107.170354, global_step=160157, preemption_count=0, score=29107.170354, test/accuracy=0.650200, test/loss=1.689407, test/num_examples=10000, total_duration=30283.331115, train/accuracy=0.863401, train/loss=0.737719, validation/accuracy=0.765200, validation/loss=1.132092, validation/num_examples=50000
I0315 10:38:32.850840 139505814144768 logging_writer.py:48] [160500] global_step=160500, grad_norm=0.9154512286186218, loss=2.7281031608581543
I0315 10:40:03.614171 139505805752064 logging_writer.py:48] [161000] global_step=161000, grad_norm=0.9206284880638123, loss=2.7667791843414307
I0315 10:41:34.324903 139505814144768 logging_writer.py:48] [161500] global_step=161500, grad_norm=0.8651291131973267, loss=2.7702536582946777
I0315 10:43:05.113402 139505805752064 logging_writer.py:48] [162000] global_step=162000, grad_norm=0.9238685369491577, loss=2.693998336791992
I0315 10:44:35.870743 139505814144768 logging_writer.py:48] [162500] global_step=162500, grad_norm=0.9018178582191467, loss=2.7955150604248047
I0315 10:46:00.339855 139666979968832 spec.py:321] Evaluating on the training split.
I0315 10:46:06.544365 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 10:46:15.432348 139666979968832 spec.py:349] Evaluating on the test split.
I0315 10:46:17.745073 139666979968832 submission_runner.py:420] Time since start: 30810.74s, 	Step: 162967, 	{'train/accuracy': 0.865632951259613, 'train/loss': 0.7323713898658752, 'validation/accuracy': 0.7657999992370605, 'validation/loss': 1.130832552909851, 'validation/num_examples': 50000, 'test/accuracy': 0.65010005235672, 'test/loss': 1.6847559213638306, 'test/num_examples': 10000, 'score': 29617.048443078995, 'total_duration': 30810.744643211365, 'accumulated_submission_time': 29617.048443078995, 'accumulated_eval_time': 1186.4453101158142, 'accumulated_logging_time': 1.990326166152954}
I0315 10:46:17.772615 139505797359360 logging_writer.py:48] [162967] accumulated_eval_time=1186.445310, accumulated_logging_time=1.990326, accumulated_submission_time=29617.048443, global_step=162967, preemption_count=0, score=29617.048443, test/accuracy=0.650100, test/loss=1.684756, test/num_examples=10000, total_duration=30810.744643, train/accuracy=0.865633, train/loss=0.732371, validation/accuracy=0.765800, validation/loss=1.130833, validation/num_examples=50000
I0315 10:46:23.965270 139505805752064 logging_writer.py:48] [163000] global_step=163000, grad_norm=0.8783966898918152, loss=2.729611396789551
I0315 10:47:54.728442 139505797359360 logging_writer.py:48] [163500] global_step=163500, grad_norm=0.8761545419692993, loss=2.7049665451049805
I0315 10:49:25.476326 139505805752064 logging_writer.py:48] [164000] global_step=164000, grad_norm=0.9431458711624146, loss=2.7986412048339844
I0315 10:50:56.206188 139505797359360 logging_writer.py:48] [164500] global_step=164500, grad_norm=0.9182494878768921, loss=2.8485217094421387
I0315 10:52:26.903378 139505805752064 logging_writer.py:48] [165000] global_step=165000, grad_norm=0.8798330426216125, loss=2.811490058898926
I0315 10:53:57.567929 139505797359360 logging_writer.py:48] [165500] global_step=165500, grad_norm=0.9140527844429016, loss=2.7848587036132812
I0315 10:54:47.887707 139666979968832 spec.py:321] Evaluating on the training split.
I0315 10:54:54.216300 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 10:55:03.393111 139666979968832 spec.py:349] Evaluating on the test split.
I0315 10:55:05.699477 139666979968832 submission_runner.py:420] Time since start: 31338.70s, 	Step: 165779, 	{'train/accuracy': 0.8631417155265808, 'train/loss': 0.737945020198822, 'validation/accuracy': 0.7654399871826172, 'validation/loss': 1.1309257745742798, 'validation/num_examples': 50000, 'test/accuracy': 0.6500000357627869, 'test/loss': 1.6848125457763672, 'test/num_examples': 10000, 'score': 30127.061368703842, 'total_duration': 31338.699058294296, 'accumulated_submission_time': 30127.061368703842, 'accumulated_eval_time': 1204.2570366859436, 'accumulated_logging_time': 2.0283799171447754}
I0315 10:55:05.725957 139505847715584 logging_writer.py:48] [165779] accumulated_eval_time=1204.257037, accumulated_logging_time=2.028380, accumulated_submission_time=30127.061369, global_step=165779, preemption_count=0, score=30127.061369, test/accuracy=0.650000, test/loss=1.684813, test/num_examples=10000, total_duration=31338.699058, train/accuracy=0.863142, train/loss=0.737945, validation/accuracy=0.765440, validation/loss=1.130926, validation/num_examples=50000
I0315 10:55:46.041204 139505856108288 logging_writer.py:48] [166000] global_step=166000, grad_norm=0.8699612617492676, loss=2.719329833984375
I0315 10:57:16.844073 139505847715584 logging_writer.py:48] [166500] global_step=166500, grad_norm=0.8844595551490784, loss=2.7314293384552
I0315 10:58:47.585271 139505856108288 logging_writer.py:48] [167000] global_step=167000, grad_norm=0.8421053886413574, loss=2.700662851333618
I0315 11:00:18.305591 139505847715584 logging_writer.py:48] [167500] global_step=167500, grad_norm=0.9331601858139038, loss=2.705841064453125
I0315 11:01:49.047129 139505856108288 logging_writer.py:48] [168000] global_step=168000, grad_norm=0.9271708130836487, loss=2.783656597137451
I0315 11:03:19.807274 139505847715584 logging_writer.py:48] [168500] global_step=168500, grad_norm=0.9147899746894836, loss=2.8057162761688232
I0315 11:03:35.852375 139666979968832 spec.py:321] Evaluating on the training split.
I0315 11:03:42.035496 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 11:03:51.414788 139666979968832 spec.py:349] Evaluating on the test split.
I0315 11:03:53.707879 139666979968832 submission_runner.py:420] Time since start: 31866.71s, 	Step: 168590, 	{'train/accuracy': 0.8643972873687744, 'train/loss': 0.7392303943634033, 'validation/accuracy': 0.7665999531745911, 'validation/loss': 1.1317466497421265, 'validation/num_examples': 50000, 'test/accuracy': 0.6504000425338745, 'test/loss': 1.6857174634933472, 'test/num_examples': 10000, 'score': 30637.082643270493, 'total_duration': 31866.707454681396, 'accumulated_submission_time': 30637.082643270493, 'accumulated_eval_time': 1222.1124918460846, 'accumulated_logging_time': 2.06549072265625}
I0315 11:03:53.735236 139505797359360 logging_writer.py:48] [168590] accumulated_eval_time=1222.112492, accumulated_logging_time=2.065491, accumulated_submission_time=30637.082643, global_step=168590, preemption_count=0, score=30637.082643, test/accuracy=0.650400, test/loss=1.685717, test/num_examples=10000, total_duration=31866.707455, train/accuracy=0.864397, train/loss=0.739230, validation/accuracy=0.766600, validation/loss=1.131747, validation/num_examples=50000
I0315 11:05:08.337123 139505805752064 logging_writer.py:48] [169000] global_step=169000, grad_norm=0.921103298664093, loss=2.7908148765563965
I0315 11:06:39.017004 139505797359360 logging_writer.py:48] [169500] global_step=169500, grad_norm=0.9206643104553223, loss=2.787036895751953
I0315 11:08:09.731717 139505805752064 logging_writer.py:48] [170000] global_step=170000, grad_norm=0.8845836520195007, loss=2.6909704208374023
I0315 11:09:40.449058 139505797359360 logging_writer.py:48] [170500] global_step=170500, grad_norm=0.8853431940078735, loss=2.7592668533325195
I0315 11:11:11.188841 139505805752064 logging_writer.py:48] [171000] global_step=171000, grad_norm=0.9118219017982483, loss=2.8232433795928955
I0315 11:12:23.791037 139666979968832 spec.py:321] Evaluating on the training split.
I0315 11:12:29.953083 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 11:12:38.969978 139666979968832 spec.py:349] Evaluating on the test split.
I0315 11:12:41.316748 139666979968832 submission_runner.py:420] Time since start: 32394.32s, 	Step: 171402, 	{'train/accuracy': 0.8664301633834839, 'train/loss': 0.7219376564025879, 'validation/accuracy': 0.7665599584579468, 'validation/loss': 1.1241357326507568, 'validation/num_examples': 50000, 'test/accuracy': 0.6508000493049622, 'test/loss': 1.6809180974960327, 'test/num_examples': 10000, 'score': 31147.037385463715, 'total_duration': 32394.316321849823, 'accumulated_submission_time': 31147.037385463715, 'accumulated_eval_time': 1239.6381602287292, 'accumulated_logging_time': 2.1022064685821533}
I0315 11:12:41.343675 139505797359360 logging_writer.py:48] [171402] accumulated_eval_time=1239.638160, accumulated_logging_time=2.102206, accumulated_submission_time=31147.037385, global_step=171402, preemption_count=0, score=31147.037385, test/accuracy=0.650800, test/loss=1.680918, test/num_examples=10000, total_duration=32394.316322, train/accuracy=0.866430, train/loss=0.721938, validation/accuracy=0.766560, validation/loss=1.124136, validation/num_examples=50000
I0315 11:12:59.330941 139505805752064 logging_writer.py:48] [171500] global_step=171500, grad_norm=0.9186168909072876, loss=2.730299949645996
I0315 11:14:30.081027 139505797359360 logging_writer.py:48] [172000] global_step=172000, grad_norm=0.885083794593811, loss=2.763489246368408
I0315 11:16:00.805572 139505805752064 logging_writer.py:48] [172500] global_step=172500, grad_norm=0.9138350486755371, loss=2.7555174827575684
I0315 11:17:31.480736 139505797359360 logging_writer.py:48] [173000] global_step=173000, grad_norm=0.8960179686546326, loss=2.7191085815429688
I0315 11:19:02.161054 139505805752064 logging_writer.py:48] [173500] global_step=173500, grad_norm=0.8814908862113953, loss=2.7328031063079834
I0315 11:20:32.856020 139505797359360 logging_writer.py:48] [174000] global_step=174000, grad_norm=0.9459393620491028, loss=2.848358392715454
I0315 11:21:11.381189 139666979968832 spec.py:321] Evaluating on the training split.
I0315 11:21:17.575229 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 11:21:26.541575 139666979968832 spec.py:349] Evaluating on the test split.
I0315 11:21:28.888838 139666979968832 submission_runner.py:420] Time since start: 32921.89s, 	Step: 174214, 	{'train/accuracy': 0.8652941584587097, 'train/loss': 0.734643816947937, 'validation/accuracy': 0.766979992389679, 'validation/loss': 1.1310449838638306, 'validation/num_examples': 50000, 'test/accuracy': 0.6514000296592712, 'test/loss': 1.6869580745697021, 'test/num_examples': 10000, 'score': 31656.975496053696, 'total_duration': 32921.88840174675, 'accumulated_submission_time': 31656.975496053696, 'accumulated_eval_time': 1257.1457471847534, 'accumulated_logging_time': 2.1405720710754395}
I0315 11:21:28.917786 139505797359360 logging_writer.py:48] [174214] accumulated_eval_time=1257.145747, accumulated_logging_time=2.140572, accumulated_submission_time=31656.975496, global_step=174214, preemption_count=0, score=31656.975496, test/accuracy=0.651400, test/loss=1.686958, test/num_examples=10000, total_duration=32921.888402, train/accuracy=0.865294, train/loss=0.734644, validation/accuracy=0.766980, validation/loss=1.131045, validation/num_examples=50000
I0315 11:22:21.029692 139505805752064 logging_writer.py:48] [174500] global_step=174500, grad_norm=0.8575941920280457, loss=2.763436794281006
I0315 11:23:51.788090 139505797359360 logging_writer.py:48] [175000] global_step=175000, grad_norm=0.9147648811340332, loss=2.7775607109069824
I0315 11:25:22.550838 139505805752064 logging_writer.py:48] [175500] global_step=175500, grad_norm=0.9388538002967834, loss=2.761561393737793
I0315 11:26:53.350000 139505797359360 logging_writer.py:48] [176000] global_step=176000, grad_norm=0.851759135723114, loss=2.647355079650879
I0315 11:28:24.076120 139505805752064 logging_writer.py:48] [176500] global_step=176500, grad_norm=0.9256106615066528, loss=2.7832295894622803
I0315 11:29:54.857700 139505797359360 logging_writer.py:48] [177000] global_step=177000, grad_norm=0.8793019652366638, loss=2.661421775817871
I0315 11:29:58.912749 139666979968832 spec.py:321] Evaluating on the training split.
I0315 11:30:05.088335 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 11:30:14.238568 139666979968832 spec.py:349] Evaluating on the test split.
I0315 11:30:16.555543 139666979968832 submission_runner.py:420] Time since start: 33449.56s, 	Step: 177024, 	{'train/accuracy': 0.8670280575752258, 'train/loss': 0.7283276915550232, 'validation/accuracy': 0.7667999863624573, 'validation/loss': 1.1249099969863892, 'validation/num_examples': 50000, 'test/accuracy': 0.6497000455856323, 'test/loss': 1.6827945709228516, 'test/num_examples': 10000, 'score': 32166.867676973343, 'total_duration': 33449.55511856079, 'accumulated_submission_time': 32166.867676973343, 'accumulated_eval_time': 1274.7884957790375, 'accumulated_logging_time': 2.1796064376831055}
I0315 11:30:16.583758 139505839322880 logging_writer.py:48] [177024] accumulated_eval_time=1274.788496, accumulated_logging_time=2.179606, accumulated_submission_time=32166.867677, global_step=177024, preemption_count=0, score=32166.867677, test/accuracy=0.649700, test/loss=1.682795, test/num_examples=10000, total_duration=33449.555119, train/accuracy=0.867028, train/loss=0.728328, validation/accuracy=0.766800, validation/loss=1.124910, validation/num_examples=50000
I0315 11:31:43.162743 139505847715584 logging_writer.py:48] [177500] global_step=177500, grad_norm=0.9020580649375916, loss=2.8176169395446777
I0315 11:33:13.890753 139505839322880 logging_writer.py:48] [178000] global_step=178000, grad_norm=0.9408790469169617, loss=2.8359017372131348
I0315 11:34:44.653967 139505847715584 logging_writer.py:48] [178500] global_step=178500, grad_norm=0.8754918575286865, loss=2.6758692264556885
I0315 11:36:15.313624 139505839322880 logging_writer.py:48] [179000] global_step=179000, grad_norm=0.9083448648452759, loss=2.7120821475982666
I0315 11:37:46.033559 139505847715584 logging_writer.py:48] [179500] global_step=179500, grad_norm=0.922818660736084, loss=2.7934203147888184
I0315 11:38:46.711829 139666979968832 spec.py:321] Evaluating on the training split.
I0315 11:38:52.933879 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 11:39:02.243122 139666979968832 spec.py:349] Evaluating on the test split.
I0315 11:39:04.547659 139666979968832 submission_runner.py:420] Time since start: 33977.55s, 	Step: 179836, 	{'train/accuracy': 0.8680245280265808, 'train/loss': 0.7234154939651489, 'validation/accuracy': 0.7670599818229675, 'validation/loss': 1.1276352405548096, 'validation/num_examples': 50000, 'test/accuracy': 0.6509000062942505, 'test/loss': 1.6827391386032104, 'test/num_examples': 10000, 'score': 32676.893333673477, 'total_duration': 33977.54694867134, 'accumulated_submission_time': 32676.893333673477, 'accumulated_eval_time': 1292.6239953041077, 'accumulated_logging_time': 2.2183167934417725}
I0315 11:39:04.575079 139505822537472 logging_writer.py:48] [179836] accumulated_eval_time=1292.623995, accumulated_logging_time=2.218317, accumulated_submission_time=32676.893334, global_step=179836, preemption_count=0, score=32676.893334, test/accuracy=0.650900, test/loss=1.682739, test/num_examples=10000, total_duration=33977.546949, train/accuracy=0.868025, train/loss=0.723415, validation/accuracy=0.767060, validation/loss=1.127635, validation/num_examples=50000
I0315 11:39:34.564589 139505830930176 logging_writer.py:48] [180000] global_step=180000, grad_norm=0.8753273487091064, loss=2.6860413551330566
I0315 11:41:05.335380 139505822537472 logging_writer.py:48] [180500] global_step=180500, grad_norm=0.9298751354217529, loss=2.7970545291900635
I0315 11:42:36.120306 139505830930176 logging_writer.py:48] [181000] global_step=181000, grad_norm=0.8960710763931274, loss=2.796980142593384
I0315 11:44:06.854022 139505822537472 logging_writer.py:48] [181500] global_step=181500, grad_norm=0.8895875215530396, loss=2.7043371200561523
I0315 11:45:37.632830 139505830930176 logging_writer.py:48] [182000] global_step=182000, grad_norm=0.9568189382553101, loss=2.8155622482299805
I0315 11:47:08.383332 139505822537472 logging_writer.py:48] [182500] global_step=182500, grad_norm=0.8891308903694153, loss=2.717888116836548
I0315 11:47:34.563524 139666979968832 spec.py:321] Evaluating on the training split.
I0315 11:47:40.767353 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 11:47:49.770820 139666979968832 spec.py:349] Evaluating on the test split.
I0315 11:47:52.056771 139666979968832 submission_runner.py:420] Time since start: 34505.06s, 	Step: 182646, 	{'train/accuracy': 0.8690608739852905, 'train/loss': 0.7239134311676025, 'validation/accuracy': 0.766979992389679, 'validation/loss': 1.1275781393051147, 'validation/num_examples': 50000, 'test/accuracy': 0.651900053024292, 'test/loss': 1.6822025775909424, 'test/num_examples': 10000, 'score': 33186.78197026253, 'total_duration': 34505.05634212494, 'accumulated_submission_time': 33186.78197026253, 'accumulated_eval_time': 1310.1172006130219, 'accumulated_logging_time': 2.256136178970337}
I0315 11:47:52.085420 139505847715584 logging_writer.py:48] [182646] accumulated_eval_time=1310.117201, accumulated_logging_time=2.256136, accumulated_submission_time=33186.781970, global_step=182646, preemption_count=0, score=33186.781970, test/accuracy=0.651900, test/loss=1.682203, test/num_examples=10000, total_duration=34505.056342, train/accuracy=0.869061, train/loss=0.723913, validation/accuracy=0.766980, validation/loss=1.127578, validation/num_examples=50000
I0315 11:48:56.514264 139505856108288 logging_writer.py:48] [183000] global_step=183000, grad_norm=0.9103553891181946, loss=2.7964868545532227
I0315 11:50:27.283671 139505847715584 logging_writer.py:48] [183500] global_step=183500, grad_norm=0.8934232592582703, loss=2.74849534034729
I0315 11:51:58.004037 139505856108288 logging_writer.py:48] [184000] global_step=184000, grad_norm=0.890331506729126, loss=2.657912254333496
I0315 11:53:28.684969 139505847715584 logging_writer.py:48] [184500] global_step=184500, grad_norm=0.9697122573852539, loss=2.7666523456573486
I0315 11:54:59.484431 139505856108288 logging_writer.py:48] [185000] global_step=185000, grad_norm=0.8909231424331665, loss=2.760700225830078
I0315 11:56:22.140692 139666979968832 spec.py:321] Evaluating on the training split.
I0315 11:56:28.292364 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 11:56:37.371260 139666979968832 spec.py:349] Evaluating on the test split.
I0315 11:56:39.673718 139666979968832 submission_runner.py:420] Time since start: 35032.67s, 	Step: 185457, 	{'train/accuracy': 0.8656130433082581, 'train/loss': 0.7277498841285706, 'validation/accuracy': 0.7676599621772766, 'validation/loss': 1.1302763223648071, 'validation/num_examples': 50000, 'test/accuracy': 0.650700032711029, 'test/loss': 1.6858309507369995, 'test/num_examples': 10000, 'score': 33696.73606610298, 'total_duration': 35032.67329096794, 'accumulated_submission_time': 33696.73606610298, 'accumulated_eval_time': 1327.6501822471619, 'accumulated_logging_time': 2.2953546047210693}
I0315 11:56:39.702836 139505805752064 logging_writer.py:48] [185457] accumulated_eval_time=1327.650182, accumulated_logging_time=2.295355, accumulated_submission_time=33696.736066, global_step=185457, preemption_count=0, score=33696.736066, test/accuracy=0.650700, test/loss=1.685831, test/num_examples=10000, total_duration=35032.673291, train/accuracy=0.865613, train/loss=0.727750, validation/accuracy=0.767660, validation/loss=1.130276, validation/num_examples=50000
I0315 11:56:47.706501 139505814144768 logging_writer.py:48] [185500] global_step=185500, grad_norm=0.8557350039482117, loss=2.6926233768463135
I0315 11:58:18.472552 139505805752064 logging_writer.py:48] [186000] global_step=186000, grad_norm=0.9043999314308167, loss=2.784578323364258
I0315 11:59:49.170618 139505814144768 logging_writer.py:48] [186500] global_step=186500, grad_norm=0.8583655953407288, loss=2.714482545852661
I0315 12:00:18.997755 139666979968832 spec.py:321] Evaluating on the training split.
I0315 12:00:25.143794 139666979968832 spec.py:333] Evaluating on the validation split.
I0315 12:00:34.018117 139666979968832 spec.py:349] Evaluating on the test split.
I0315 12:00:36.337519 139666979968832 submission_runner.py:420] Time since start: 35269.34s, 	Step: 186666, 	{'train/accuracy': 0.8672871589660645, 'train/loss': 0.7182402014732361, 'validation/accuracy': 0.7675199508666992, 'validation/loss': 1.1246577501296997, 'validation/num_examples': 50000, 'test/accuracy': 0.6503000259399414, 'test/loss': 1.683452844619751, 'test/num_examples': 10000, 'score': 33915.97921395302, 'total_duration': 35269.33709192276, 'accumulated_submission_time': 33915.97921395302, 'accumulated_eval_time': 1344.9899072647095, 'accumulated_logging_time': 2.334820508956909}
I0315 12:00:36.367229 139505805752064 logging_writer.py:48] [186666] accumulated_eval_time=1344.989907, accumulated_logging_time=2.334821, accumulated_submission_time=33915.979214, global_step=186666, preemption_count=0, score=33915.979214, test/accuracy=0.650300, test/loss=1.683453, test/num_examples=10000, total_duration=35269.337092, train/accuracy=0.867287, train/loss=0.718240, validation/accuracy=0.767520, validation/loss=1.124658, validation/num_examples=50000
I0315 12:00:36.392631 139505839322880 logging_writer.py:48] [186666] global_step=186666, preemption_count=0, score=33915.979214
I0315 12:00:36.628174 139666979968832 checkpoints.py:490] Saving checkpoint at step: 186666
I0315 12:00:37.519408 139666979968832 checkpoints.py:422] Saved checkpoint at /experiment_runs/variants_target_setting/study_0/imagenet_resnet_gelu_jax/trial_1/checkpoint_186666
I0315 12:00:37.538985 139666979968832 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/variants_target_setting/study_0/imagenet_resnet_gelu_jax/trial_1/checkpoint_186666.
I0315 12:00:38.027811 139666979968832 submission_runner.py:593] Tuning trial 1/1
I0315 12:00:38.029029 139666979968832 submission_runner.py:594] Hyperparameters: Hyperparameters(learning_rate=0.3850582234619253, beta1=0.9845129495436189, warmup_steps=6999, decay_steps_factor=0.9504205232618159, end_factor=0.001, weight_decay=1.7359160785435053e-05, label_smoothing=0.2)
I0315 12:00:38.032173 139666979968832 submission_runner.py:595] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008968430920504034, 'train/loss': 6.907759666442871, 'validation/accuracy': 0.0009199999622069299, 'validation/loss': 6.907748699188232, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.907728672027588, 'test/num_examples': 10000, 'score': 39.47880697250366, 'total_duration': 81.83598804473877, 'accumulated_submission_time': 39.47880697250366, 'accumulated_eval_time': 42.357078313827515, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2802, {'train/accuracy': 0.2709462642669678, 'train/loss': 3.717517614364624, 'validation/accuracy': 0.22853998839855194, 'validation/loss': 3.955676555633545, 'validation/num_examples': 50000, 'test/accuracy': 0.16760000586509705, 'test/loss': 4.43997859954834, 'test/num_examples': 10000, 'score': 549.3842663764954, 'total_duration': 610.0327498912811, 'accumulated_submission_time': 549.3842663764954, 'accumulated_eval_time': 60.52774119377136, 'accumulated_logging_time': 0.028769254684448242, 'global_step': 2802, 'preemption_count': 0}), (5610, {'train/accuracy': 0.3941924273967743, 'train/loss': 2.9730653762817383, 'validation/accuracy': 0.35558000206947327, 'validation/loss': 3.1851818561553955, 'validation/num_examples': 50000, 'test/accuracy': 0.26990002393722534, 'test/loss': 3.7601563930511475, 'test/num_examples': 10000, 'score': 1059.3302872180939, 'total_duration': 1138.2319378852844, 'accumulated_submission_time': 1059.3302872180939, 'accumulated_eval_time': 78.66454458236694, 'accumulated_logging_time': 0.05573558807373047, 'global_step': 5610, 'preemption_count': 0}), (8418, {'train/accuracy': 0.44437578320503235, 'train/loss': 2.6151835918426514, 'validation/accuracy': 0.41373997926712036, 'validation/loss': 2.7880170345306396, 'validation/num_examples': 50000, 'test/accuracy': 0.318200021982193, 'test/loss': 3.4120326042175293, 'test/num_examples': 10000, 'score': 1569.233282327652, 'total_duration': 1666.59339761734, 'accumulated_submission_time': 1569.233282327652, 'accumulated_eval_time': 97.00804924964905, 'accumulated_logging_time': 0.08312249183654785, 'global_step': 8418, 'preemption_count': 0}), (11227, {'train/accuracy': 0.4833187162876129, 'train/loss': 2.470959424972534, 'validation/accuracy': 0.4481000006198883, 'validation/loss': 2.6501293182373047, 'validation/num_examples': 50000, 'test/accuracy': 0.3416000306606293, 'test/loss': 3.2876079082489014, 'test/num_examples': 10000, 'score': 2079.135133266449, 'total_duration': 2194.8848955631256, 'accumulated_submission_time': 2079.135133266449, 'accumulated_eval_time': 115.28256273269653, 'accumulated_logging_time': 0.10933995246887207, 'global_step': 11227, 'preemption_count': 0}), (14036, {'train/accuracy': 0.49284517765045166, 'train/loss': 2.44167160987854, 'validation/accuracy': 0.4616599977016449, 'validation/loss': 2.5923407077789307, 'validation/num_examples': 50000, 'test/accuracy': 0.352400004863739, 'test/loss': 3.253528118133545, 'test/num_examples': 10000, 'score': 2589.1811945438385, 'total_duration': 2723.163457632065, 'accumulated_submission_time': 2589.1811945438385, 'accumulated_eval_time': 133.39499020576477, 'accumulated_logging_time': 0.13920998573303223, 'global_step': 14036, 'preemption_count': 0}), (16845, {'train/accuracy': 0.5042849183082581, 'train/loss': 2.3759348392486572, 'validation/accuracy': 0.4754199981689453, 'validation/loss': 2.5212173461914062, 'validation/num_examples': 50000, 'test/accuracy': 0.3671000301837921, 'test/loss': 3.1625282764434814, 'test/num_examples': 10000, 'score': 3099.06081533432, 'total_duration': 3251.3906412124634, 'accumulated_submission_time': 3099.06081533432, 'accumulated_eval_time': 151.62206530570984, 'accumulated_logging_time': 0.17100739479064941, 'global_step': 16845, 'preemption_count': 0}), (19655, {'train/accuracy': 0.516621470451355, 'train/loss': 2.2864809036254883, 'validation/accuracy': 0.4909399747848511, 'validation/loss': 2.4297704696655273, 'validation/num_examples': 50000, 'test/accuracy': 0.3792000114917755, 'test/loss': 3.102672815322876, 'test/num_examples': 10000, 'score': 3608.9477570056915, 'total_duration': 3779.92001414299, 'accumulated_submission_time': 3608.9477570056915, 'accumulated_eval_time': 170.14759874343872, 'accumulated_logging_time': 0.19753265380859375, 'global_step': 19655, 'preemption_count': 0}), (22466, {'train/accuracy': 0.5379264950752258, 'train/loss': 2.1756951808929443, 'validation/accuracy': 0.5106399655342102, 'validation/loss': 2.30892014503479, 'validation/num_examples': 50000, 'test/accuracy': 0.3899000287055969, 'test/loss': 2.973999261856079, 'test/num_examples': 10000, 'score': 4118.965543985367, 'total_duration': 4308.747456789017, 'accumulated_submission_time': 4118.965543985367, 'accumulated_eval_time': 188.8380012512207, 'accumulated_logging_time': 0.22706365585327148, 'global_step': 22466, 'preemption_count': 0}), (25275, {'train/accuracy': 0.5290178656578064, 'train/loss': 2.225160837173462, 'validation/accuracy': 0.5007199645042419, 'validation/loss': 2.3718740940093994, 'validation/num_examples': 50000, 'test/accuracy': 0.3817000091075897, 'test/loss': 3.0522758960723877, 'test/num_examples': 10000, 'score': 4628.95613360405, 'total_duration': 4839.673331737518, 'accumulated_submission_time': 4628.95613360405, 'accumulated_eval_time': 209.6465151309967, 'accumulated_logging_time': 0.2606830596923828, 'global_step': 25275, 'preemption_count': 0}), (28084, {'train/accuracy': 0.5485690236091614, 'train/loss': 2.1369452476501465, 'validation/accuracy': 0.5232999920845032, 'validation/loss': 2.2611072063446045, 'validation/num_examples': 50000, 'test/accuracy': 0.40610000491142273, 'test/loss': 2.9257686138153076, 'test/num_examples': 10000, 'score': 5138.88924074173, 'total_duration': 5371.2177946567535, 'accumulated_submission_time': 5138.88924074173, 'accumulated_eval_time': 231.13219547271729, 'accumulated_logging_time': 0.2956113815307617, 'global_step': 28084, 'preemption_count': 0}), (30892, {'train/accuracy': 0.5383848547935486, 'train/loss': 2.1873598098754883, 'validation/accuracy': 0.5109400153160095, 'validation/loss': 2.30798602104187, 'validation/num_examples': 50000, 'test/accuracy': 0.396200031042099, 'test/loss': 2.9860281944274902, 'test/num_examples': 10000, 'score': 5648.814496278763, 'total_duration': 5901.284316062927, 'accumulated_submission_time': 5648.814496278763, 'accumulated_eval_time': 251.15060806274414, 'accumulated_logging_time': 0.32982945442199707, 'global_step': 30892, 'preemption_count': 0}), (33699, {'train/accuracy': 0.5612045526504517, 'train/loss': 2.0527260303497314, 'validation/accuracy': 0.5278599858283997, 'validation/loss': 2.2167325019836426, 'validation/num_examples': 50000, 'test/accuracy': 0.41290003061294556, 'test/loss': 2.867882251739502, 'test/num_examples': 10000, 'score': 6158.743594884872, 'total_duration': 6431.999586820602, 'accumulated_submission_time': 6158.743594884872, 'accumulated_eval_time': 271.8153614997864, 'accumulated_logging_time': 0.3610966205596924, 'global_step': 33699, 'preemption_count': 0}), (36509, {'train/accuracy': 0.6098333597183228, 'train/loss': 1.8721224069595337, 'validation/accuracy': 0.5256400108337402, 'validation/loss': 2.249420166015625, 'validation/num_examples': 50000, 'test/accuracy': 0.4086000323295593, 'test/loss': 2.9109206199645996, 'test/num_examples': 10000, 'score': 6668.75258231163, 'total_duration': 6961.447009801865, 'accumulated_submission_time': 6668.75258231163, 'accumulated_eval_time': 291.11314058303833, 'accumulated_logging_time': 0.4061269760131836, 'global_step': 36509, 'preemption_count': 0}), (39318, {'train/accuracy': 0.5975764989852905, 'train/loss': 1.9000749588012695, 'validation/accuracy': 0.5384399890899658, 'validation/loss': 2.1795661449432373, 'validation/num_examples': 50000, 'test/accuracy': 0.4187000095844269, 'test/loss': 2.8545751571655273, 'test/num_examples': 10000, 'score': 7178.742904424667, 'total_duration': 7492.493213653564, 'accumulated_submission_time': 7178.742904424667, 'accumulated_eval_time': 312.04736709594727, 'accumulated_logging_time': 0.4363114833831787, 'global_step': 39318, 'preemption_count': 0}), (42126, {'train/accuracy': 0.5855787396430969, 'train/loss': 1.9755046367645264, 'validation/accuracy': 0.5410799980163574, 'validation/loss': 2.1894428730010986, 'validation/num_examples': 50000, 'test/accuracy': 0.414900004863739, 'test/loss': 2.8620636463165283, 'test/num_examples': 10000, 'score': 7688.719514369965, 'total_duration': 8024.053572177887, 'accumulated_submission_time': 7688.719514369965, 'accumulated_eval_time': 333.4987795352936, 'accumulated_logging_time': 0.477977991104126, 'global_step': 42126, 'preemption_count': 0}), (44935, {'train/accuracy': 0.6030572056770325, 'train/loss': 1.8343355655670166, 'validation/accuracy': 0.5600599646568298, 'validation/loss': 2.044644832611084, 'validation/num_examples': 50000, 'test/accuracy': 0.44050002098083496, 'test/loss': 2.7189035415649414, 'test/num_examples': 10000, 'score': 8198.649980545044, 'total_duration': 8558.638590335846, 'accumulated_submission_time': 8198.649980545044, 'accumulated_eval_time': 358.0307970046997, 'accumulated_logging_time': 0.5081648826599121, 'global_step': 44935, 'preemption_count': 0}), (47744, {'train/accuracy': 0.5924545526504517, 'train/loss': 1.927284836769104, 'validation/accuracy': 0.5535599589347839, 'validation/loss': 2.119499683380127, 'validation/num_examples': 50000, 'test/accuracy': 0.4309000074863434, 'test/loss': 2.7724545001983643, 'test/num_examples': 10000, 'score': 8708.554129123688, 'total_duration': 9091.029710054398, 'accumulated_submission_time': 8708.554129123688, 'accumulated_eval_time': 380.3906214237213, 'accumulated_logging_time': 0.5432441234588623, 'global_step': 47744, 'preemption_count': 0}), (50554, {'train/accuracy': 0.5901028513908386, 'train/loss': 1.929429292678833, 'validation/accuracy': 0.5543199777603149, 'validation/loss': 2.0966458320617676, 'validation/num_examples': 50000, 'test/accuracy': 0.42660000920295715, 'test/loss': 2.7711341381073, 'test/num_examples': 10000, 'score': 9218.479825258255, 'total_duration': 9623.909902572632, 'accumulated_submission_time': 9218.479825258255, 'accumulated_eval_time': 403.22376585006714, 'accumulated_logging_time': 0.5750327110290527, 'global_step': 50554, 'preemption_count': 0}), (53363, {'train/accuracy': 0.5937101244926453, 'train/loss': 1.9251199960708618, 'validation/accuracy': 0.5542399883270264, 'validation/loss': 2.095377206802368, 'validation/num_examples': 50000, 'test/accuracy': 0.4374000132083893, 'test/loss': 2.754514217376709, 'test/num_examples': 10000, 'score': 9728.36764883995, 'total_duration': 10158.485095024109, 'accumulated_submission_time': 9728.36764883995, 'accumulated_eval_time': 427.7965428829193, 'accumulated_logging_time': 0.6011092662811279, 'global_step': 53363, 'preemption_count': 0}), (56172, {'train/accuracy': 0.6036351919174194, 'train/loss': 1.8868541717529297, 'validation/accuracy': 0.5629400014877319, 'validation/loss': 2.0698935985565186, 'validation/num_examples': 50000, 'test/accuracy': 0.43810001015663147, 'test/loss': 2.715358018875122, 'test/num_examples': 10000, 'score': 10238.427752017975, 'total_duration': 10692.222826719284, 'accumulated_submission_time': 10238.427752017975, 'accumulated_eval_time': 451.3575351238251, 'accumulated_logging_time': 0.6278159618377686, 'global_step': 56172, 'preemption_count': 0}), (58981, {'train/accuracy': 0.6097536683082581, 'train/loss': 1.8503715991973877, 'validation/accuracy': 0.575980007648468, 'validation/loss': 2.001864433288574, 'validation/num_examples': 50000, 'test/accuracy': 0.45660001039505005, 'test/loss': 2.6520166397094727, 'test/num_examples': 10000, 'score': 10748.354861974716, 'total_duration': 11226.00513100624, 'accumulated_submission_time': 10748.354861974716, 'accumulated_eval_time': 475.08420515060425, 'accumulated_logging_time': 0.6614036560058594, 'global_step': 58981, 'preemption_count': 0}), (61792, {'train/accuracy': 0.6163105964660645, 'train/loss': 1.8129554986953735, 'validation/accuracy': 0.5788999795913696, 'validation/loss': 1.9795621633529663, 'validation/num_examples': 50000, 'test/accuracy': 0.4531000256538391, 'test/loss': 2.6504082679748535, 'test/num_examples': 10000, 'score': 11258.367656707764, 'total_duration': 11759.852581262589, 'accumulated_submission_time': 11258.367656707764, 'accumulated_eval_time': 498.79771184921265, 'accumulated_logging_time': 0.6938827037811279, 'global_step': 61792, 'preemption_count': 0}), (64601, {'train/accuracy': 0.6098533272743225, 'train/loss': 1.857517957687378, 'validation/accuracy': 0.5766000151634216, 'validation/loss': 2.0250394344329834, 'validation/num_examples': 50000, 'test/accuracy': 0.4572000205516815, 'test/loss': 2.6506733894348145, 'test/num_examples': 10000, 'score': 11768.252495288849, 'total_duration': 12293.022606134415, 'accumulated_submission_time': 11768.252495288849, 'accumulated_eval_time': 521.952006816864, 'accumulated_logging_time': 0.7330019474029541, 'global_step': 64601, 'preemption_count': 0}), (67410, {'train/accuracy': 0.6324737071990967, 'train/loss': 1.7457120418548584, 'validation/accuracy': 0.5887399911880493, 'validation/loss': 1.9348591566085815, 'validation/num_examples': 50000, 'test/accuracy': 0.46880000829696655, 'test/loss': 2.5784528255462646, 'test/num_examples': 10000, 'score': 12278.215480327606, 'total_duration': 12827.258270025253, 'accumulated_submission_time': 12278.215480327606, 'accumulated_eval_time': 546.1019096374512, 'accumulated_logging_time': 0.7637128829956055, 'global_step': 67410, 'preemption_count': 0}), (70217, {'train/accuracy': 0.6731704473495483, 'train/loss': 1.5815236568450928, 'validation/accuracy': 0.5878599882125854, 'validation/loss': 1.9684792757034302, 'validation/num_examples': 50000, 'test/accuracy': 0.46980002522468567, 'test/loss': 2.615736484527588, 'test/num_examples': 10000, 'score': 12788.216607809067, 'total_duration': 13359.0311024189, 'accumulated_submission_time': 12788.216607809067, 'accumulated_eval_time': 567.7520003318787, 'accumulated_logging_time': 0.7960903644561768, 'global_step': 70217, 'preemption_count': 0}), (73027, {'train/accuracy': 0.6520248651504517, 'train/loss': 1.6730637550354004, 'validation/accuracy': 0.5866999626159668, 'validation/loss': 1.9701958894729614, 'validation/num_examples': 50000, 'test/accuracy': 0.4586000144481659, 'test/loss': 2.622561454772949, 'test/num_examples': 10000, 'score': 13298.16697025299, 'total_duration': 13890.320236682892, 'accumulated_submission_time': 13298.16697025299, 'accumulated_eval_time': 588.9511861801147, 'accumulated_logging_time': 0.8440396785736084, 'global_step': 73027, 'preemption_count': 0}), (75836, {'train/accuracy': 0.6688257455825806, 'train/loss': 1.5871577262878418, 'validation/accuracy': 0.6103799939155579, 'validation/loss': 1.8433527946472168, 'validation/num_examples': 50000, 'test/accuracy': 0.48410001397132874, 'test/loss': 2.5036115646362305, 'test/num_examples': 10000, 'score': 13808.067230701447, 'total_duration': 14421.228022098541, 'accumulated_submission_time': 13808.067230701447, 'accumulated_eval_time': 609.835752248764, 'accumulated_logging_time': 0.8771841526031494, 'global_step': 75836, 'preemption_count': 0}), (78647, {'train/accuracy': 0.6624082922935486, 'train/loss': 1.617139220237732, 'validation/accuracy': 0.6099599599838257, 'validation/loss': 1.8513147830963135, 'validation/num_examples': 50000, 'test/accuracy': 0.48660001158714294, 'test/loss': 2.4859347343444824, 'test/num_examples': 10000, 'score': 14318.115080595016, 'total_duration': 14958.747246026993, 'accumulated_submission_time': 14318.115080595016, 'accumulated_eval_time': 637.1810598373413, 'accumulated_logging_time': 0.9124436378479004, 'global_step': 78647, 'preemption_count': 0}), (81456, {'train/accuracy': 0.645906388759613, 'train/loss': 1.6796780824661255, 'validation/accuracy': 0.6014800071716309, 'validation/loss': 1.8915008306503296, 'validation/num_examples': 50000, 'test/accuracy': 0.4756000339984894, 'test/loss': 2.5513505935668945, 'test/num_examples': 10000, 'score': 14828.103749036789, 'total_duration': 15488.91557765007, 'accumulated_submission_time': 14828.103749036789, 'accumulated_eval_time': 657.2395513057709, 'accumulated_logging_time': 0.9447917938232422, 'global_step': 81456, 'preemption_count': 0}), (84267, {'train/accuracy': 0.6578842401504517, 'train/loss': 1.6404576301574707, 'validation/accuracy': 0.6136800050735474, 'validation/loss': 1.8449496030807495, 'validation/num_examples': 50000, 'test/accuracy': 0.48600003123283386, 'test/loss': 2.4789185523986816, 'test/num_examples': 10000, 'score': 15338.051067590714, 'total_duration': 16020.547025203705, 'accumulated_submission_time': 15338.051067590714, 'accumulated_eval_time': 678.7950310707092, 'accumulated_logging_time': 0.9780173301696777, 'global_step': 84267, 'preemption_count': 0}), (87077, {'train/accuracy': 0.6678889989852905, 'train/loss': 1.5783772468566895, 'validation/accuracy': 0.6217399835586548, 'validation/loss': 1.7732410430908203, 'validation/num_examples': 50000, 'test/accuracy': 0.49660003185272217, 'test/loss': 2.4108152389526367, 'test/num_examples': 10000, 'score': 15848.019825696945, 'total_duration': 16550.869858264923, 'accumulated_submission_time': 15848.019825696945, 'accumulated_eval_time': 699.0264530181885, 'accumulated_logging_time': 1.0111169815063477, 'global_step': 87077, 'preemption_count': 0}), (89887, {'train/accuracy': 0.6729910373687744, 'train/loss': 1.5459370613098145, 'validation/accuracy': 0.6239199638366699, 'validation/loss': 1.7690690755844116, 'validation/num_examples': 50000, 'test/accuracy': 0.5028000473976135, 'test/loss': 2.4182331562042236, 'test/num_examples': 10000, 'score': 16357.967458724976, 'total_duration': 17082.90966629982, 'accumulated_submission_time': 16357.967458724976, 'accumulated_eval_time': 720.9941301345825, 'accumulated_logging_time': 1.046532154083252, 'global_step': 89887, 'preemption_count': 0}), (92698, {'train/accuracy': 0.6764190196990967, 'train/loss': 1.5440995693206787, 'validation/accuracy': 0.6328999996185303, 'validation/loss': 1.74272620677948, 'validation/num_examples': 50000, 'test/accuracy': 0.5101000070571899, 'test/loss': 2.3821768760681152, 'test/num_examples': 10000, 'score': 16868.0272295475, 'total_duration': 17612.059515476227, 'accumulated_submission_time': 16868.0272295475, 'accumulated_eval_time': 739.9654176235199, 'accumulated_logging_time': 1.0766267776489258, 'global_step': 92698, 'preemption_count': 0}), (95509, {'train/accuracy': 0.6886360049247742, 'train/loss': 1.4965471029281616, 'validation/accuracy': 0.6438599824905396, 'validation/loss': 1.6948471069335938, 'validation/num_examples': 50000, 'test/accuracy': 0.5175000429153442, 'test/loss': 2.3338916301727295, 'test/num_examples': 10000, 'score': 17377.905303001404, 'total_duration': 18140.27098608017, 'accumulated_submission_time': 17377.905303001404, 'accumulated_eval_time': 758.1704969406128, 'accumulated_logging_time': 1.1112191677093506, 'global_step': 95509, 'preemption_count': 0}), (98320, {'train/accuracy': 0.6938377022743225, 'train/loss': 1.4277307987213135, 'validation/accuracy': 0.6497200131416321, 'validation/loss': 1.628403663635254, 'validation/num_examples': 50000, 'test/accuracy': 0.5182999968528748, 'test/loss': 2.2841415405273438, 'test/num_examples': 10000, 'score': 17887.787921190262, 'total_duration': 18668.44917845726, 'accumulated_submission_time': 17887.787921190262, 'accumulated_eval_time': 776.3388483524323, 'accumulated_logging_time': 1.1502656936645508, 'global_step': 98320, 'preemption_count': 0}), (101129, {'train/accuracy': 0.7052973508834839, 'train/loss': 1.406061053276062, 'validation/accuracy': 0.6511200070381165, 'validation/loss': 1.633562445640564, 'validation/num_examples': 50000, 'test/accuracy': 0.522599995136261, 'test/loss': 2.281493663787842, 'test/num_examples': 10000, 'score': 18397.755425691605, 'total_duration': 19196.148114442825, 'accumulated_submission_time': 18397.755425691605, 'accumulated_eval_time': 793.9465079307556, 'accumulated_logging_time': 1.1844494342803955, 'global_step': 101129, 'preemption_count': 0}), (103939, {'train/accuracy': 0.7524513602256775, 'train/loss': 1.2095069885253906, 'validation/accuracy': 0.6575999855995178, 'validation/loss': 1.598398208618164, 'validation/num_examples': 50000, 'test/accuracy': 0.5339000225067139, 'test/loss': 2.2238636016845703, 'test/num_examples': 10000, 'score': 18907.731110334396, 'total_duration': 19724.436819791794, 'accumulated_submission_time': 18907.731110334396, 'accumulated_eval_time': 812.1368012428284, 'accumulated_logging_time': 1.2169291973114014, 'global_step': 103939, 'preemption_count': 0}), (106749, {'train/accuracy': 0.7403738498687744, 'train/loss': 1.256012201309204, 'validation/accuracy': 0.6581000089645386, 'validation/loss': 1.60061776638031, 'validation/num_examples': 50000, 'test/accuracy': 0.5315000414848328, 'test/loss': 2.2344987392425537, 'test/num_examples': 10000, 'score': 19417.62467956543, 'total_duration': 20252.0008995533, 'accumulated_submission_time': 19417.62467956543, 'accumulated_eval_time': 829.6799504756927, 'accumulated_logging_time': 1.2548072338104248, 'global_step': 106749, 'preemption_count': 0}), (109560, {'train/accuracy': 0.7405133843421936, 'train/loss': 1.2657791376113892, 'validation/accuracy': 0.6663399934768677, 'validation/loss': 1.5837370157241821, 'validation/num_examples': 50000, 'test/accuracy': 0.5448000431060791, 'test/loss': 2.2128961086273193, 'test/num_examples': 10000, 'score': 19927.61045193672, 'total_duration': 20779.726460456848, 'accumulated_submission_time': 19927.61045193672, 'accumulated_eval_time': 847.2937185764313, 'accumulated_logging_time': 1.289489507675171, 'global_step': 109560, 'preemption_count': 0}), (112370, {'train/accuracy': 0.7446388602256775, 'train/loss': 1.229624629020691, 'validation/accuracy': 0.6768800020217896, 'validation/loss': 1.5161625146865845, 'validation/num_examples': 50000, 'test/accuracy': 0.5543000102043152, 'test/loss': 2.1517488956451416, 'test/num_examples': 10000, 'score': 20437.608496189117, 'total_duration': 21307.419775009155, 'accumulated_submission_time': 20437.608496189117, 'accumulated_eval_time': 864.8620934486389, 'accumulated_logging_time': 1.326627492904663, 'global_step': 112370, 'preemption_count': 0}), (115181, {'train/accuracy': 0.7556201815605164, 'train/loss': 1.190289855003357, 'validation/accuracy': 0.6910399794578552, 'validation/loss': 1.4795939922332764, 'validation/num_examples': 50000, 'test/accuracy': 0.5596000552177429, 'test/loss': 2.106682300567627, 'test/num_examples': 10000, 'score': 20947.64518761635, 'total_duration': 21835.214857578278, 'accumulated_submission_time': 20947.64518761635, 'accumulated_eval_time': 882.4962873458862, 'accumulated_logging_time': 1.360917568206787, 'global_step': 115181, 'preemption_count': 0}), (117991, {'train/accuracy': 0.7688137888908386, 'train/loss': 1.1277391910552979, 'validation/accuracy': 0.6998199820518494, 'validation/loss': 1.4276926517486572, 'validation/num_examples': 50000, 'test/accuracy': 0.5718000531196594, 'test/loss': 2.0509040355682373, 'test/num_examples': 10000, 'score': 21457.53855085373, 'total_duration': 22362.81521010399, 'accumulated_submission_time': 21457.53855085373, 'accumulated_eval_time': 900.0796000957489, 'accumulated_logging_time': 1.3957161903381348, 'global_step': 117991, 'preemption_count': 0}), (120800, {'train/accuracy': 0.7773038744926453, 'train/loss': 1.0774314403533936, 'validation/accuracy': 0.7059400081634521, 'validation/loss': 1.373658299446106, 'validation/num_examples': 50000, 'test/accuracy': 0.5796000361442566, 'test/loss': 1.9862518310546875, 'test/num_examples': 10000, 'score': 21967.416944026947, 'total_duration': 22893.36046242714, 'accumulated_submission_time': 21967.416944026947, 'accumulated_eval_time': 920.617835521698, 'accumulated_logging_time': 1.4305720329284668, 'global_step': 120800, 'preemption_count': 0}), (123610, {'train/accuracy': 0.7887236475944519, 'train/loss': 1.0411092042922974, 'validation/accuracy': 0.7205599546432495, 'validation/loss': 1.342443823814392, 'validation/num_examples': 50000, 'test/accuracy': 0.5943000316619873, 'test/loss': 1.9445018768310547, 'test/num_examples': 10000, 'score': 22477.35790538788, 'total_duration': 23421.203782081604, 'accumulated_submission_time': 22477.35790538788, 'accumulated_eval_time': 938.4012403488159, 'accumulated_logging_time': 1.460312843322754, 'global_step': 123610, 'preemption_count': 0}), (126421, {'train/accuracy': 0.8101881146430969, 'train/loss': 0.9542239904403687, 'validation/accuracy': 0.7309399843215942, 'validation/loss': 1.2795382738113403, 'validation/num_examples': 50000, 'test/accuracy': 0.6060000061988831, 'test/loss': 1.8781458139419556, 'test/num_examples': 10000, 'score': 22987.358740091324, 'total_duration': 23949.724945545197, 'accumulated_submission_time': 22987.358740091324, 'accumulated_eval_time': 956.7962579727173, 'accumulated_logging_time': 1.4970099925994873, 'global_step': 126421, 'preemption_count': 0}), (129232, {'train/accuracy': 0.8279455900192261, 'train/loss': 0.877056360244751, 'validation/accuracy': 0.7427600026130676, 'validation/loss': 1.2180293798446655, 'validation/num_examples': 50000, 'test/accuracy': 0.6212000250816345, 'test/loss': 1.8021321296691895, 'test/num_examples': 10000, 'score': 23497.319289684296, 'total_duration': 24477.39676475525, 'accumulated_submission_time': 23497.319289684296, 'accumulated_eval_time': 974.3784441947937, 'accumulated_logging_time': 1.533355712890625, 'global_step': 129232, 'preemption_count': 0}), (132045, {'train/accuracy': 0.8474768400192261, 'train/loss': 0.8029673099517822, 'validation/accuracy': 0.756879985332489, 'validation/loss': 1.1657541990280151, 'validation/num_examples': 50000, 'test/accuracy': 0.6394000053405762, 'test/loss': 1.7313377857208252, 'test/num_examples': 10000, 'score': 24007.340938329697, 'total_duration': 25005.19687461853, 'accumulated_submission_time': 24007.340938329697, 'accumulated_eval_time': 992.0296437740326, 'accumulated_logging_time': 1.5704536437988281, 'global_step': 132045, 'preemption_count': 0}), (134857, {'train/accuracy': 0.8592354655265808, 'train/loss': 0.7484070658683777, 'validation/accuracy': 0.7630999684333801, 'validation/loss': 1.1406275033950806, 'validation/num_examples': 50000, 'test/accuracy': 0.6477000117301941, 'test/loss': 1.6970221996307373, 'test/num_examples': 10000, 'score': 24517.31483054161, 'total_duration': 25532.781730413437, 'accumulated_submission_time': 24517.31483054161, 'accumulated_eval_time': 1009.5150735378265, 'accumulated_logging_time': 1.6044492721557617, 'global_step': 134857, 'preemption_count': 0}), (137668, {'train/accuracy': 0.8620455861091614, 'train/loss': 0.7470542788505554, 'validation/accuracy': 0.763219952583313, 'validation/loss': 1.1393851041793823, 'validation/num_examples': 50000, 'test/accuracy': 0.6467000246047974, 'test/loss': 1.6975725889205933, 'test/num_examples': 10000, 'score': 25027.216710329056, 'total_duration': 26060.398368120193, 'accumulated_submission_time': 25027.216710329056, 'accumulated_eval_time': 1027.0995848178864, 'accumulated_logging_time': 1.6429941654205322, 'global_step': 137668, 'preemption_count': 0}), (140480, {'train/accuracy': 0.8608497977256775, 'train/loss': 0.7480168342590332, 'validation/accuracy': 0.7636799812316895, 'validation/loss': 1.1389931440353394, 'validation/num_examples': 50000, 'test/accuracy': 0.6481000185012817, 'test/loss': 1.693626880645752, 'test/num_examples': 10000, 'score': 25537.2121386528, 'total_duration': 26588.929893493652, 'accumulated_submission_time': 25537.2121386528, 'accumulated_eval_time': 1045.5076894760132, 'accumulated_logging_time': 1.6779308319091797, 'global_step': 140480, 'preemption_count': 0}), (143291, {'train/accuracy': 0.8587571382522583, 'train/loss': 0.7474705576896667, 'validation/accuracy': 0.7634999752044678, 'validation/loss': 1.134859323501587, 'validation/num_examples': 50000, 'test/accuracy': 0.6470000147819519, 'test/loss': 1.6911301612854004, 'test/num_examples': 10000, 'score': 26047.181527137756, 'total_duration': 27116.764947652817, 'accumulated_submission_time': 26047.181527137756, 'accumulated_eval_time': 1063.2456469535828, 'accumulated_logging_time': 1.715416431427002, 'global_step': 143291, 'preemption_count': 0}), (146102, {'train/accuracy': 0.8621252775192261, 'train/loss': 0.7498049736022949, 'validation/accuracy': 0.7643599510192871, 'validation/loss': 1.1372393369674683, 'validation/num_examples': 50000, 'test/accuracy': 0.6493000388145447, 'test/loss': 1.694334626197815, 'test/num_examples': 10000, 'score': 26557.20747065544, 'total_duration': 27644.881666898727, 'accumulated_submission_time': 26557.20747065544, 'accumulated_eval_time': 1081.19753074646, 'accumulated_logging_time': 1.762721300125122, 'global_step': 146102, 'preemption_count': 0}), (148914, {'train/accuracy': 0.8628228306770325, 'train/loss': 0.743279755115509, 'validation/accuracy': 0.7644199728965759, 'validation/loss': 1.1323521137237549, 'validation/num_examples': 50000, 'test/accuracy': 0.65010005235672, 'test/loss': 1.6888558864593506, 'test/num_examples': 10000, 'score': 27067.21181702614, 'total_duration': 28172.453258514404, 'accumulated_submission_time': 27067.21181702614, 'accumulated_eval_time': 1098.6343348026276, 'accumulated_logging_time': 1.8031542301177979, 'global_step': 148914, 'preemption_count': 0}), (151724, {'train/accuracy': 0.8606704473495483, 'train/loss': 0.7461697459220886, 'validation/accuracy': 0.7648400068283081, 'validation/loss': 1.1311298608779907, 'validation/num_examples': 50000, 'test/accuracy': 0.6498000025749207, 'test/loss': 1.6885789632797241, 'test/num_examples': 10000, 'score': 27577.204789161682, 'total_duration': 28700.600935935974, 'accumulated_submission_time': 27577.204789161682, 'accumulated_eval_time': 1116.6591629981995, 'accumulated_logging_time': 1.8424439430236816, 'global_step': 151724, 'preemption_count': 0}), (154535, {'train/accuracy': 0.8603515625, 'train/loss': 0.746981143951416, 'validation/accuracy': 0.7651999592781067, 'validation/loss': 1.1362378597259521, 'validation/num_examples': 50000, 'test/accuracy': 0.6504000425338745, 'test/loss': 1.6936733722686768, 'test/num_examples': 10000, 'score': 28087.249876976013, 'total_duration': 29228.215181350708, 'accumulated_submission_time': 28087.249876976013, 'accumulated_eval_time': 1134.1010489463806, 'accumulated_logging_time': 1.8775477409362793, 'global_step': 154535, 'preemption_count': 0}), (157346, {'train/accuracy': 0.8623843789100647, 'train/loss': 0.7468582391738892, 'validation/accuracy': 0.765019953250885, 'validation/loss': 1.1292741298675537, 'validation/num_examples': 50000, 'test/accuracy': 0.6503000259399414, 'test/loss': 1.6862452030181885, 'test/num_examples': 10000, 'score': 28597.254593849182, 'total_duration': 29755.98441886902, 'accumulated_submission_time': 28597.254593849182, 'accumulated_eval_time': 1151.7358434200287, 'accumulated_logging_time': 1.9162397384643555, 'global_step': 157346, 'preemption_count': 0}), (160157, {'train/accuracy': 0.8634008169174194, 'train/loss': 0.7377191781997681, 'validation/accuracy': 0.7651999592781067, 'validation/loss': 1.1320921182632446, 'validation/num_examples': 50000, 'test/accuracy': 0.6502000093460083, 'test/loss': 1.6894065141677856, 'test/num_examples': 10000, 'score': 29107.170354127884, 'total_duration': 30283.3311150074, 'accumulated_submission_time': 29107.170354127884, 'accumulated_eval_time': 1169.0401241779327, 'accumulated_logging_time': 1.9524645805358887, 'global_step': 160157, 'preemption_count': 0}), (162967, {'train/accuracy': 0.865632951259613, 'train/loss': 0.7323713898658752, 'validation/accuracy': 0.7657999992370605, 'validation/loss': 1.130832552909851, 'validation/num_examples': 50000, 'test/accuracy': 0.65010005235672, 'test/loss': 1.6847559213638306, 'test/num_examples': 10000, 'score': 29617.048443078995, 'total_duration': 30810.744643211365, 'accumulated_submission_time': 29617.048443078995, 'accumulated_eval_time': 1186.4453101158142, 'accumulated_logging_time': 1.990326166152954, 'global_step': 162967, 'preemption_count': 0}), (165779, {'train/accuracy': 0.8631417155265808, 'train/loss': 0.737945020198822, 'validation/accuracy': 0.7654399871826172, 'validation/loss': 1.1309257745742798, 'validation/num_examples': 50000, 'test/accuracy': 0.6500000357627869, 'test/loss': 1.6848125457763672, 'test/num_examples': 10000, 'score': 30127.061368703842, 'total_duration': 31338.699058294296, 'accumulated_submission_time': 30127.061368703842, 'accumulated_eval_time': 1204.2570366859436, 'accumulated_logging_time': 2.0283799171447754, 'global_step': 165779, 'preemption_count': 0}), (168590, {'train/accuracy': 0.8643972873687744, 'train/loss': 0.7392303943634033, 'validation/accuracy': 0.7665999531745911, 'validation/loss': 1.1317466497421265, 'validation/num_examples': 50000, 'test/accuracy': 0.6504000425338745, 'test/loss': 1.6857174634933472, 'test/num_examples': 10000, 'score': 30637.082643270493, 'total_duration': 31866.707454681396, 'accumulated_submission_time': 30637.082643270493, 'accumulated_eval_time': 1222.1124918460846, 'accumulated_logging_time': 2.06549072265625, 'global_step': 168590, 'preemption_count': 0}), (171402, {'train/accuracy': 0.8664301633834839, 'train/loss': 0.7219376564025879, 'validation/accuracy': 0.7665599584579468, 'validation/loss': 1.1241357326507568, 'validation/num_examples': 50000, 'test/accuracy': 0.6508000493049622, 'test/loss': 1.6809180974960327, 'test/num_examples': 10000, 'score': 31147.037385463715, 'total_duration': 32394.316321849823, 'accumulated_submission_time': 31147.037385463715, 'accumulated_eval_time': 1239.6381602287292, 'accumulated_logging_time': 2.1022064685821533, 'global_step': 171402, 'preemption_count': 0}), (174214, {'train/accuracy': 0.8652941584587097, 'train/loss': 0.734643816947937, 'validation/accuracy': 0.766979992389679, 'validation/loss': 1.1310449838638306, 'validation/num_examples': 50000, 'test/accuracy': 0.6514000296592712, 'test/loss': 1.6869580745697021, 'test/num_examples': 10000, 'score': 31656.975496053696, 'total_duration': 32921.88840174675, 'accumulated_submission_time': 31656.975496053696, 'accumulated_eval_time': 1257.1457471847534, 'accumulated_logging_time': 2.1405720710754395, 'global_step': 174214, 'preemption_count': 0}), (177024, {'train/accuracy': 0.8670280575752258, 'train/loss': 0.7283276915550232, 'validation/accuracy': 0.7667999863624573, 'validation/loss': 1.1249099969863892, 'validation/num_examples': 50000, 'test/accuracy': 0.6497000455856323, 'test/loss': 1.6827945709228516, 'test/num_examples': 10000, 'score': 32166.867676973343, 'total_duration': 33449.55511856079, 'accumulated_submission_time': 32166.867676973343, 'accumulated_eval_time': 1274.7884957790375, 'accumulated_logging_time': 2.1796064376831055, 'global_step': 177024, 'preemption_count': 0}), (179836, {'train/accuracy': 0.8680245280265808, 'train/loss': 0.7234154939651489, 'validation/accuracy': 0.7670599818229675, 'validation/loss': 1.1276352405548096, 'validation/num_examples': 50000, 'test/accuracy': 0.6509000062942505, 'test/loss': 1.6827391386032104, 'test/num_examples': 10000, 'score': 32676.893333673477, 'total_duration': 33977.54694867134, 'accumulated_submission_time': 32676.893333673477, 'accumulated_eval_time': 1292.6239953041077, 'accumulated_logging_time': 2.2183167934417725, 'global_step': 179836, 'preemption_count': 0}), (182646, {'train/accuracy': 0.8690608739852905, 'train/loss': 0.7239134311676025, 'validation/accuracy': 0.766979992389679, 'validation/loss': 1.1275781393051147, 'validation/num_examples': 50000, 'test/accuracy': 0.651900053024292, 'test/loss': 1.6822025775909424, 'test/num_examples': 10000, 'score': 33186.78197026253, 'total_duration': 34505.05634212494, 'accumulated_submission_time': 33186.78197026253, 'accumulated_eval_time': 1310.1172006130219, 'accumulated_logging_time': 2.256136178970337, 'global_step': 182646, 'preemption_count': 0}), (185457, {'train/accuracy': 0.8656130433082581, 'train/loss': 0.7277498841285706, 'validation/accuracy': 0.7676599621772766, 'validation/loss': 1.1302763223648071, 'validation/num_examples': 50000, 'test/accuracy': 0.650700032711029, 'test/loss': 1.6858309507369995, 'test/num_examples': 10000, 'score': 33696.73606610298, 'total_duration': 35032.67329096794, 'accumulated_submission_time': 33696.73606610298, 'accumulated_eval_time': 1327.6501822471619, 'accumulated_logging_time': 2.2953546047210693, 'global_step': 185457, 'preemption_count': 0}), (186666, {'train/accuracy': 0.8672871589660645, 'train/loss': 0.7182402014732361, 'validation/accuracy': 0.7675199508666992, 'validation/loss': 1.1246577501296997, 'validation/num_examples': 50000, 'test/accuracy': 0.6503000259399414, 'test/loss': 1.683452844619751, 'test/num_examples': 10000, 'score': 33915.97921395302, 'total_duration': 35269.33709192276, 'accumulated_submission_time': 33915.97921395302, 'accumulated_eval_time': 1344.9899072647095, 'accumulated_logging_time': 2.334820508956909, 'global_step': 186666, 'preemption_count': 0})], 'global_step': 186666}
I0315 12:00:38.032334 139666979968832 submission_runner.py:596] Timing: 33915.97921395302
I0315 12:00:38.032391 139666979968832 submission_runner.py:598] Total number of evals: 68
I0315 12:00:38.032436 139666979968832 submission_runner.py:599] ====================
I0315 12:00:38.032635 139666979968832 submission_runner.py:683] Final imagenet_resnet_gelu score: 0
