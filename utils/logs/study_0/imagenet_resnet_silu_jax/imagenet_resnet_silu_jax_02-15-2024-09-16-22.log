python3 submission_runner.py --framework=jax --workload=imagenet_resnet_silu --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/imagenet_resnet_silu/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --num_tuning_trials=1 --rng_seed=1884662704 --max_global_steps=186666 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_resnet_silu_jax_02-15-2024-09-16-22.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0215 09:16:48.716723 139913240401728 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/imagenet_resnet_silu_jax.
I0215 09:16:49.804002 139913240401728 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host CUDA Interpreter
I0215 09:16:49.804694 139913240401728 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0215 09:16:49.805058 139913240401728 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0215 09:16:49.812369 139913240401728 submission_runner.py:542] Using RNG seed 1884662704
I0215 09:16:51.041572 139913240401728 submission_runner.py:551] --- Tuning run 1/1 ---
I0215 09:16:51.041812 139913240401728 submission_runner.py:556] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/imagenet_resnet_silu_jax/trial_1.
I0215 09:16:51.042025 139913240401728 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/imagenet_resnet_silu_jax/trial_1/hparams.json.
I0215 09:16:51.221904 139913240401728 submission_runner.py:206] Initializing dataset.
I0215 09:16:51.241870 139913240401728 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0215 09:16:51.252864 139913240401728 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0215 09:16:51.643165 139913240401728 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0215 09:16:52.936169 139913240401728 submission_runner.py:213] Initializing model.
I0215 09:17:03.108378 139913240401728 submission_runner.py:255] Initializing optimizer.
I0215 09:17:04.811883 139913240401728 submission_runner.py:262] Initializing metrics bundle.
I0215 09:17:04.812071 139913240401728 submission_runner.py:280] Initializing checkpoint and logger.
I0215 09:17:04.813242 139913240401728 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/imagenet_resnet_silu_jax/trial_1 with prefix checkpoint_
I0215 09:17:04.813396 139913240401728 submission_runner.py:300] Saving meta data to /experiment_runs/variants_target_setting/study_0/imagenet_resnet_silu_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0215 09:17:05.124426 139913240401728 logger_utils.py:220] Unable to record git information. Continuing without it.
I0215 09:17:05.414107 139913240401728 submission_runner.py:304] Saving flags to /experiment_runs/variants_target_setting/study_0/imagenet_resnet_silu_jax/trial_1/flags_0.json.
I0215 09:17:05.423657 139913240401728 submission_runner.py:314] Starting training loop.
I0215 09:17:57.400702 139742981097216 logging_writer.py:48] [0] global_step=0, grad_norm=0.3408416211605072, loss=6.908958435058594
I0215 09:17:57.416870 139913240401728 spec.py:321] Evaluating on the training split.
I0215 09:17:58.364261 139913240401728 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0215 09:17:58.373253 139913240401728 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0215 09:17:58.454806 139913240401728 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0215 09:18:12.300866 139913240401728 spec.py:333] Evaluating on the validation split.
I0215 09:18:13.864929 139913240401728 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0215 09:18:13.896121 139913240401728 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0215 09:18:13.962713 139913240401728 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0215 09:18:29.798370 139913240401728 spec.py:349] Evaluating on the test split.
I0215 09:18:30.646858 139913240401728 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0215 09:18:30.653592 139913240401728 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0215 09:18:30.698124 139913240401728 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0215 09:18:34.773693 139913240401728 submission_runner.py:408] Time since start: 89.35s, 	Step: 1, 	{'train/accuracy': 0.0010363520123064518, 'train/loss': 6.9077534675598145, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.907754898071289, 'validation/num_examples': 50000, 'test/accuracy': 0.000800000037997961, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 51.99310660362244, 'total_duration': 89.34995818138123, 'accumulated_submission_time': 51.99310660362244, 'accumulated_eval_time': 37.356759786605835, 'accumulated_logging_time': 0}
I0215 09:18:34.795712 139728162645760 logging_writer.py:48] [1] accumulated_eval_time=37.356760, accumulated_logging_time=0, accumulated_submission_time=51.993107, global_step=1, preemption_count=0, score=51.993107, test/accuracy=0.000800, test/loss=6.907757, test/num_examples=10000, total_duration=89.349958, train/accuracy=0.001036, train/loss=6.907753, validation/accuracy=0.001000, validation/loss=6.907755, validation/num_examples=50000
2024-02-15 09:18:44.826055: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2469] Execution of replica 2 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 12103062728 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:  366.18MiB
              constant allocation:         8B
        maybe_live_out allocation:  292.68MiB
     preallocated temp allocation:   11.27GiB
  preallocated temp fragmentation:  367.79MiB (3.19%)
                 total allocation:   11.63GiB
              total fragmentation:  367.89MiB (3.09%)
Peak buffers:
	Buffer 1:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_2/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 2:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1978"
		XLA Label: fusion
		Shape: f32[128,256,56,56]
		==========================

	Buffer 3:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1978"
		XLA Label: fusion
		Shape: f32[128,256,56,56]
		==========================

	Buffer 4:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 5:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_0/Conv_proj/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 6:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_0/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 7:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/Conv_init/conv_general_dilated[window_strides=(2, 2) padding=((3, 3), (3, 3)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,64,112,112]
		==========================

	Buffer 8:
		Size: 203.06MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=59
		XLA Label: fusion
		Shape: f32[128,128,57,57]
		==========================

	Buffer 9:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_4/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1986"
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 10:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_4/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 11:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 12:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 13:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_proj/conv_general_dilated[window_strides=(2, 2) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 14:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 15:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,128,56,56]
		==========================


2024-02-15 09:18:44.826163: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2469] Execution of replica 1 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 12103062728 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:  366.18MiB
              constant allocation:         8B
        maybe_live_out allocation:  292.68MiB
     preallocated temp allocation:   11.27GiB
  preallocated temp fragmentation:  367.79MiB (3.19%)
                 total allocation:   11.63GiB
              total fragmentation:  367.89MiB (3.09%)
Peak buffers:
	Buffer 1:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_2/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 2:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1978"
		XLA Label: fusion
		Shape: f32[128,256,56,56]
		==========================

	Buffer 3:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1978"
		XLA Label: fusion
		Shape: f32[128,256,56,56]
		==========================

	Buffer 4:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 5:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_0/Conv_proj/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 6:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_0/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 7:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/Conv_init/conv_general_dilated[window_strides=(2, 2) padding=((3, 3), (3, 3)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,64,112,112]
		==========================

	Buffer 8:
		Size: 203.06MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=59
		XLA Label: fusion
		Shape: f32[128,128,57,57]
		==========================

	Buffer 9:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_4/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1986"
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 10:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_4/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 11:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 12:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 13:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_proj/conv_general_dilated[window_strides=(2, 2) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 14:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 15:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,128,56,56]
		==========================


2024-02-15 09:18:44.826435: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2469] Execution of replica 3 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 12103062728 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:  366.18MiB
              constant allocation:         8B
        maybe_live_out allocation:  292.68MiB
     preallocated temp allocation:   11.27GiB
  preallocated temp fragmentation:  367.79MiB (3.19%)
                 total allocation:   11.63GiB
              total fragmentation:  367.89MiB (3.09%)
Peak buffers:
	Buffer 1:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_2/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 2:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1978"
		XLA Label: fusion
		Shape: f32[128,256,56,56]
		==========================

	Buffer 3:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1978"
		XLA Label: fusion
		Shape: f32[128,256,56,56]
		==========================

	Buffer 4:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 5:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_0/Conv_proj/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 6:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_0/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 7:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/Conv_init/conv_general_dilated[window_strides=(2, 2) padding=((3, 3), (3, 3)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,64,112,112]
		==========================

	Buffer 8:
		Size: 203.06MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=59
		XLA Label: fusion
		Shape: f32[128,128,57,57]
		==========================

	Buffer 9:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_4/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1986"
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 10:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_4/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 11:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 12:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 13:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_proj/conv_general_dilated[window_strides=(2, 2) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 14:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 15:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,128,56,56]
		==========================


2024-02-15 09:18:44.826497: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2469] Execution of replica 0 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 12103062728 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:  366.18MiB
              constant allocation:         8B
        maybe_live_out allocation:  292.68MiB
     preallocated temp allocation:   11.27GiB
  preallocated temp fragmentation:  367.79MiB (3.19%)
                 total allocation:   11.63GiB
              total fragmentation:  367.89MiB (3.09%)
Peak buffers:
	Buffer 1:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_2/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 2:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1978"
		XLA Label: fusion
		Shape: f32[128,256,56,56]
		==========================

	Buffer 3:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1978"
		XLA Label: fusion
		Shape: f32[128,256,56,56]
		==========================

	Buffer 4:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 5:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_0/Conv_proj/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 6:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_0/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 7:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/Conv_init/conv_general_dilated[window_strides=(2, 2) padding=((3, 3), (3, 3)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,64,112,112]
		==========================

	Buffer 8:
		Size: 203.06MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=59
		XLA Label: fusion
		Shape: f32[128,128,57,57]
		==========================

	Buffer 9:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_4/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1986"
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 10:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_4/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 11:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 12:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 13:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_proj/conv_general_dilated[window_strides=(2, 2) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 14:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 15:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,128,56,56]
		==========================


2024-02-15 09:18:44.826891: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2469] Execution of replica 6 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 12103062728 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:  366.18MiB
              constant allocation:         8B
        maybe_live_out allocation:  292.68MiB
     preallocated temp allocation:   11.27GiB
  preallocated temp fragmentation:  367.79MiB (3.19%)
                 total allocation:   11.63GiB
              total fragmentation:  367.89MiB (3.09%)
Peak buffers:
	Buffer 1:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_2/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 2:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1978"
		XLA Label: fusion
		Shape: f32[128,256,56,56]
		==========================

	Buffer 3:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1978"
		XLA Label: fusion
		Shape: f32[128,256,56,56]
		==========================

	Buffer 4:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 5:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_0/Conv_proj/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 6:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_0/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 7:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/Conv_init/conv_general_dilated[window_strides=(2, 2) padding=((3, 3), (3, 3)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,64,112,112]
		==========================

	Buffer 8:
		Size: 203.06MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=59
		XLA Label: fusion
		Shape: f32[128,128,57,57]
		==========================

	Buffer 9:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_4/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1986"
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 10:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_4/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 11:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 12:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 13:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_proj/conv_general_dilated[window_strides=(2, 2) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 14:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 15:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,128,56,56]
		==========================


2024-02-15 09:18:44.830541: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2469] Execution of replica 5 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 12103062728 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:  366.18MiB
              constant allocation:         8B
        maybe_live_out allocation:  292.68MiB
     preallocated temp allocation:   11.27GiB
  preallocated temp fragmentation:  367.79MiB (3.19%)
                 total allocation:   11.63GiB
              total fragmentation:  367.89MiB (3.09%)
Peak buffers:
	Buffer 1:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_2/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 2:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1978"
		XLA Label: fusion
		Shape: f32[128,256,56,56]
		==========================

	Buffer 3:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1978"
		XLA Label: fusion
		Shape: f32[128,256,56,56]
		==========================

	Buffer 4:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 5:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_0/Conv_proj/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 6:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_0/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 7:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/Conv_init/conv_general_dilated[window_strides=(2, 2) padding=((3, 3), (3, 3)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,64,112,112]
		==========================

	Buffer 8:
		Size: 203.06MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=59
		XLA Label: fusion
		Shape: f32[128,128,57,57]
		==========================

	Buffer 9:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_4/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1986"
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 10:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_4/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 11:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 12:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 13:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_proj/conv_general_dilated[window_strides=(2, 2) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 14:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 15:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,128,56,56]
		==========================


2024-02-15 09:18:44.832364: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2469] Execution of replica 4 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 12103062728 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:  366.18MiB
              constant allocation:         8B
        maybe_live_out allocation:  292.68MiB
     preallocated temp allocation:   11.27GiB
  preallocated temp fragmentation:  367.79MiB (3.19%)
                 total allocation:   11.63GiB
              total fragmentation:  367.89MiB (3.09%)
Peak buffers:
	Buffer 1:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_2/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 2:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1978"
		XLA Label: fusion
		Shape: f32[128,256,56,56]
		==========================

	Buffer 3:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1978"
		XLA Label: fusion
		Shape: f32[128,256,56,56]
		==========================

	Buffer 4:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 5:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_0/Conv_proj/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 6:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_0/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 7:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/Conv_init/conv_general_dilated[window_strides=(2, 2) padding=((3, 3), (3, 3)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,64,112,112]
		==========================

	Buffer 8:
		Size: 203.06MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=59
		XLA Label: fusion
		Shape: f32[128,128,57,57]
		==========================

	Buffer 9:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_4/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1986"
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 10:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_4/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 11:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 12:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 13:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_proj/conv_general_dilated[window_strides=(2, 2) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 14:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 15:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,128,56,56]
		==========================


2024-02-15 09:18:44.832840: E external/xla/xla/pjrt/pjrt_stream_executor_client.cc:2469] Execution of replica 7 failed: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 12103062728 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:  366.18MiB
              constant allocation:         8B
        maybe_live_out allocation:  292.68MiB
     preallocated temp allocation:   11.27GiB
  preallocated temp fragmentation:  367.79MiB (3.19%)
                 total allocation:   11.63GiB
              total fragmentation:  367.89MiB (3.09%)
Peak buffers:
	Buffer 1:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_2/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 2:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1978"
		XLA Label: fusion
		Shape: f32[128,256,56,56]
		==========================

	Buffer 3:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1978"
		XLA Label: fusion
		Shape: f32[128,256,56,56]
		==========================

	Buffer 4:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 5:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_0/Conv_proj/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 6:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_0/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 7:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/Conv_init/conv_general_dilated[window_strides=(2, 2) padding=((3, 3), (3, 3)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,64,112,112]
		==========================

	Buffer 8:
		Size: 203.06MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=59
		XLA Label: fusion
		Shape: f32[128,128,57,57]
		==========================

	Buffer 9:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_4/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1986"
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 10:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_4/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 11:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 12:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 13:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_proj/conv_general_dilated[window_strides=(2, 2) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 14:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 15:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,128,56,56]
		==========================


Traceback (most recent call last):
  File "submission_runner.py", line 689, in <module>
    app.run(main)
  File "/usr/local/lib/python3.8/dist-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/usr/local/lib/python3.8/dist-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "submission_runner.py", line 657, in main
    score = score_submission_on_workload(
  File "submission_runner.py", line 568, in score_submission_on_workload
    timing, metrics = train_once(workload, workload_name,
  File "submission_runner.py", line 336, in train_once
    optimizer_state, model_params, model_state = update_params(
  File "/algorithmic-efficiency/reference_algorithms/target_setting_algorithms/jax_submission_base.py", line 98, in update_params
    new_optimizer_state, new_params, new_model_state, loss, grad_norm = pmapped_train_step( # pylint: disable=line-too-long
ValueError: RESOURCE_EXHAUSTED: Out of memory while trying to allocate 12103062728 bytes.
BufferAssignment OOM Debugging.
BufferAssignment stats:
             parameter allocation:  366.18MiB
              constant allocation:         8B
        maybe_live_out allocation:  292.68MiB
     preallocated temp allocation:   11.27GiB
  preallocated temp fragmentation:  367.79MiB (3.19%)
                 total allocation:   11.63GiB
              total fragmentation:  367.89MiB (3.09%)
Peak buffers:
	Buffer 1:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_2/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 2:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1978"
		XLA Label: fusion
		Shape: f32[128,256,56,56]
		==========================

	Buffer 3:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1978"
		XLA Label: fusion
		Shape: f32[128,256,56,56]
		==========================

	Buffer 4:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_1/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 5:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_0/Conv_proj/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 6:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_0/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,256,56,56]
		==========================

	Buffer 7:
		Size: 392.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/Conv_init/conv_general_dilated[window_strides=(2, 2) padding=((3, 3), (3, 3)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,64,112,112]
		==========================

	Buffer 8:
		Size: 203.06MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=59
		XLA Label: fusion
		Shape: f32[128,128,57,57]
		==========================

	Buffer 9:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_4/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72 deduplicated_name="fusion.1986"
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 10:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_4/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 11:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 12:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/jit(silu)/mul" source_file="/algorithmic-efficiency/algorithmic_efficiency/workloads/imagenet_resnet/imagenet_jax/models.py" source_line=72
		XLA Label: fusion
		Shape: f32[128,512,28,28]
		==========================

	Buffer 13:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_proj/conv_general_dilated[window_strides=(2, 2) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 14:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,512,28,28]
		==========================

	Buffer 15:
		Size: 196.00MiB
		Operator: op_name="pmap(pmapped_train_step)/jit(main)/jvp(ResNet)/BottleneckResNetBlock_3/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 precision=None preferred_element_type=None]" source_file="/usr/local/lib/python3.8/dist-packages/flax/linen/linear.py" source_line=449
		XLA Label: custom-call
		Shape: f32[128,128,56,56]
		==========================

: while running replica 0 and partition 0 of a replicated computation (other replicas may have failed as well).
