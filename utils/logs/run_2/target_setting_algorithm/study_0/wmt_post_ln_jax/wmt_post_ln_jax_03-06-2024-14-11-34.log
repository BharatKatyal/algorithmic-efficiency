python3 submission_runner.py --framework=jax --workload=wmt_post_ln --submission_path=reference_algorithms/target_setting_algorithms/jax_adamw.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --rng_seed=2465481527 --max_global_steps=133333 --tuning_ruleset=external --tuning_search_space=reference_algorithms/target_setting_algorithms/wmt_post_ln/tuning_search_space.json --num_tuning_trials=1 2>&1 | tee -a /logs/wmt_post_ln_jax_03-06-2024-14-11-34.log
I0306 14:11:56.278451 140479251105600 logger_utils.py:61] Removing existing experiment directory /experiment_runs/variants_target_setting/study_0/wmt_post_ln_jax because --overwrite was set.
I0306 14:11:56.281053 140479251105600 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/wmt_post_ln_jax.
I0306 14:11:57.256083 140479251105600 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0306 14:11:57.257404 140479251105600 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0306 14:11:57.257559 140479251105600 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0306 14:11:57.264030 140479251105600 submission_runner.py:547] Using RNG seed 2465481527
I0306 14:11:58.354773 140479251105600 submission_runner.py:556] --- Tuning run 1/1 ---
I0306 14:11:58.354994 140479251105600 submission_runner.py:561] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/wmt_post_ln_jax/trial_1.
I0306 14:11:58.355196 140479251105600 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/wmt_post_ln_jax/trial_1/hparams.json.
I0306 14:11:58.539598 140479251105600 submission_runner.py:206] Initializing dataset.
I0306 14:11:58.550410 140479251105600 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0306 14:11:58.554425 140479251105600 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0306 14:11:58.707868 140479251105600 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0306 14:12:00.639981 140479251105600 submission_runner.py:213] Initializing model.
I0306 14:12:09.301574 140479251105600 submission_runner.py:255] Initializing optimizer.
I0306 14:12:10.343835 140479251105600 submission_runner.py:262] Initializing metrics bundle.
I0306 14:12:10.344059 140479251105600 submission_runner.py:280] Initializing checkpoint and logger.
I0306 14:12:10.345112 140479251105600 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/wmt_post_ln_jax/trial_1 with prefix checkpoint_
I0306 14:12:10.345251 140479251105600 submission_runner.py:300] Saving meta data to /experiment_runs/variants_target_setting/study_0/wmt_post_ln_jax/trial_1/meta_data_0.json.
I0306 14:12:10.345442 140479251105600 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0306 14:12:10.345503 140479251105600 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0306 14:12:10.667418 140479251105600 logger_utils.py:220] Unable to record git information. Continuing without it.
I0306 14:12:10.963131 140479251105600 submission_runner.py:304] Saving flags to /experiment_runs/variants_target_setting/study_0/wmt_post_ln_jax/trial_1/flags_0.json.
I0306 14:12:10.973399 140479251105600 submission_runner.py:314] Starting training loop.
I0306 14:12:47.980530 140318071269120 logging_writer.py:48] [0] global_step=0, grad_norm=7.2136993408203125, loss=10.687430381774902
I0306 14:12:47.999941 140479251105600 spec.py:321] Evaluating on the training split.
I0306 14:12:48.003472 140479251105600 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0306 14:12:48.006717 140479251105600 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0306 14:12:48.044375 140479251105600 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0306 14:12:55.758312 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 14:17:48.979801 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 14:17:48.983661 140479251105600 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0306 14:17:48.987261 140479251105600 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0306 14:17:49.023071 140479251105600 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0306 14:17:55.825863 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 14:22:37.258891 140479251105600 spec.py:349] Evaluating on the test split.
I0306 14:22:37.261644 140479251105600 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0306 14:22:37.265083 140479251105600 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0306 14:22:37.298108 140479251105600 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0306 14:22:40.068530 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 14:27:21.125002 140479251105600 submission_runner.py:413] Time since start: 910.15s, 	Step: 1, 	{'train/accuracy': 4.5538377889897674e-05, 'train/loss': 10.680941581726074, 'train/bleu': 0.0, 'validation/accuracy': 4.959640864399262e-05, 'validation/loss': 10.719841003417969, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 3.4861426684074104e-05, 'test/loss': 10.719524383544922, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 37.026495695114136, 'total_duration': 910.1514933109283, 'accumulated_submission_time': 37.026495695114136, 'accumulated_eval_time': 873.1249454021454, 'accumulated_logging_time': 0}
I0306 14:27:21.143080 140309584385792 logging_writer.py:48] [1] accumulated_eval_time=873.124945, accumulated_logging_time=0, accumulated_submission_time=37.026496, global_step=1, preemption_count=0, score=37.026496, test/accuracy=0.000035, test/bleu=0.000000, test/loss=10.719524, test/num_examples=3003, total_duration=910.151493, train/accuracy=0.000046, train/bleu=0.000000, train/loss=10.680942, validation/accuracy=0.000050, validation/bleu=0.000000, validation/loss=10.719841, validation/num_examples=3000
I0306 14:27:21.511780 140309575993088 logging_writer.py:48] [1] global_step=1, grad_norm=7.098105430603027, loss=10.706395149230957
I0306 14:27:21.878948 140309584385792 logging_writer.py:48] [2] global_step=2, grad_norm=7.392730712890625, loss=10.703821182250977
I0306 14:27:22.245727 140309575993088 logging_writer.py:48] [3] global_step=3, grad_norm=7.2464470863342285, loss=10.690839767456055
I0306 14:27:22.608521 140309584385792 logging_writer.py:48] [4] global_step=4, grad_norm=6.9925994873046875, loss=10.70234489440918
I0306 14:27:22.974685 140309575993088 logging_writer.py:48] [5] global_step=5, grad_norm=7.122506141662598, loss=10.69025993347168
I0306 14:27:23.341176 140309584385792 logging_writer.py:48] [6] global_step=6, grad_norm=7.341054916381836, loss=10.68748664855957
I0306 14:27:23.705873 140309575993088 logging_writer.py:48] [7] global_step=7, grad_norm=7.191071033477783, loss=10.685700416564941
I0306 14:27:24.069846 140309584385792 logging_writer.py:48] [8] global_step=8, grad_norm=7.3742594718933105, loss=10.679591178894043
I0306 14:27:24.432252 140309575993088 logging_writer.py:48] [9] global_step=9, grad_norm=7.173933029174805, loss=10.676324844360352
I0306 14:27:24.796638 140309584385792 logging_writer.py:48] [10] global_step=10, grad_norm=7.184872627258301, loss=10.661116600036621
I0306 14:27:25.159922 140309575993088 logging_writer.py:48] [11] global_step=11, grad_norm=7.333924770355225, loss=10.64927864074707
I0306 14:27:25.524556 140309584385792 logging_writer.py:48] [12] global_step=12, grad_norm=7.089618682861328, loss=10.639477729797363
I0306 14:27:25.889917 140309575993088 logging_writer.py:48] [13] global_step=13, grad_norm=7.242841720581055, loss=10.609759330749512
I0306 14:27:26.255576 140309584385792 logging_writer.py:48] [14] global_step=14, grad_norm=7.084028244018555, loss=10.612539291381836
I0306 14:27:26.619975 140309575993088 logging_writer.py:48] [15] global_step=15, grad_norm=7.177300453186035, loss=10.586563110351562
I0306 14:27:26.987183 140309584385792 logging_writer.py:48] [16] global_step=16, grad_norm=7.176005840301514, loss=10.572297096252441
I0306 14:27:27.351953 140309575993088 logging_writer.py:48] [17] global_step=17, grad_norm=7.082228183746338, loss=10.564882278442383
I0306 14:27:27.719832 140309584385792 logging_writer.py:48] [18] global_step=18, grad_norm=7.067374229431152, loss=10.538495063781738
I0306 14:27:28.084139 140309575993088 logging_writer.py:48] [19] global_step=19, grad_norm=6.97371768951416, loss=10.527589797973633
I0306 14:27:28.450425 140309584385792 logging_writer.py:48] [20] global_step=20, grad_norm=6.985790729522705, loss=10.50092601776123
I0306 14:27:28.815586 140309575993088 logging_writer.py:48] [21] global_step=21, grad_norm=6.961642265319824, loss=10.483687400817871
I0306 14:27:29.180773 140309584385792 logging_writer.py:48] [22] global_step=22, grad_norm=7.007136344909668, loss=10.467529296875
I0306 14:27:29.544858 140309575993088 logging_writer.py:48] [23] global_step=23, grad_norm=6.742757320404053, loss=10.442947387695312
I0306 14:27:29.911484 140309584385792 logging_writer.py:48] [24] global_step=24, grad_norm=6.728331089019775, loss=10.428299903869629
I0306 14:27:30.276626 140309575993088 logging_writer.py:48] [25] global_step=25, grad_norm=6.738191604614258, loss=10.398856163024902
I0306 14:27:30.642796 140309584385792 logging_writer.py:48] [26] global_step=26, grad_norm=6.835859775543213, loss=10.36361312866211
I0306 14:27:31.011006 140309575993088 logging_writer.py:48] [27] global_step=27, grad_norm=6.556154727935791, loss=10.353245735168457
I0306 14:27:31.375086 140309584385792 logging_writer.py:48] [28] global_step=28, grad_norm=6.449226379394531, loss=10.327701568603516
I0306 14:27:31.737296 140309575993088 logging_writer.py:48] [29] global_step=29, grad_norm=6.4965081214904785, loss=10.288636207580566
I0306 14:27:32.100226 140309584385792 logging_writer.py:48] [30] global_step=30, grad_norm=6.428353309631348, loss=10.276585578918457
I0306 14:27:32.463639 140309575993088 logging_writer.py:48] [31] global_step=31, grad_norm=6.348318576812744, loss=10.241658210754395
I0306 14:27:32.829214 140309584385792 logging_writer.py:48] [32] global_step=32, grad_norm=6.299001216888428, loss=10.212778091430664
I0306 14:27:33.196305 140309575993088 logging_writer.py:48] [33] global_step=33, grad_norm=6.098438739776611, loss=10.198832511901855
I0306 14:27:33.561753 140309584385792 logging_writer.py:48] [34] global_step=34, grad_norm=6.0182294845581055, loss=10.162965774536133
I0306 14:27:33.927174 140309575993088 logging_writer.py:48] [35] global_step=35, grad_norm=5.940240383148193, loss=10.130602836608887
I0306 14:27:34.294438 140309584385792 logging_writer.py:48] [36] global_step=36, grad_norm=5.649970054626465, loss=10.133440017700195
I0306 14:27:34.659358 140309575993088 logging_writer.py:48] [37] global_step=37, grad_norm=5.6548027992248535, loss=10.091397285461426
I0306 14:27:35.024732 140309584385792 logging_writer.py:48] [38] global_step=38, grad_norm=5.567826271057129, loss=10.04821491241455
I0306 14:27:35.390669 140309575993088 logging_writer.py:48] [39] global_step=39, grad_norm=5.4450483322143555, loss=10.04481029510498
I0306 14:27:35.755175 140309584385792 logging_writer.py:48] [40] global_step=40, grad_norm=5.342918872833252, loss=10.027556419372559
I0306 14:27:36.121598 140309575993088 logging_writer.py:48] [41] global_step=41, grad_norm=5.204575538635254, loss=10.008675575256348
I0306 14:27:36.485185 140309584385792 logging_writer.py:48] [42] global_step=42, grad_norm=5.188549518585205, loss=9.96284008026123
I0306 14:27:36.847362 140309575993088 logging_writer.py:48] [43] global_step=43, grad_norm=5.173580169677734, loss=9.918031692504883
I0306 14:27:37.210308 140309584385792 logging_writer.py:48] [44] global_step=44, grad_norm=4.9750542640686035, loss=9.905654907226562
I0306 14:27:37.572458 140309575993088 logging_writer.py:48] [45] global_step=45, grad_norm=4.9390106201171875, loss=9.880003929138184
I0306 14:27:37.936758 140309584385792 logging_writer.py:48] [46] global_step=46, grad_norm=4.855099201202393, loss=9.840615272521973
I0306 14:27:38.299126 140309575993088 logging_writer.py:48] [47] global_step=47, grad_norm=4.65938663482666, loss=9.854293823242188
I0306 14:27:38.663029 140309584385792 logging_writer.py:48] [48] global_step=48, grad_norm=4.638850688934326, loss=9.796590805053711
I0306 14:27:39.026893 140309575993088 logging_writer.py:48] [49] global_step=49, grad_norm=4.4722208976745605, loss=9.78386402130127
I0306 14:27:39.391837 140309584385792 logging_writer.py:48] [50] global_step=50, grad_norm=4.377757549285889, loss=9.780315399169922
I0306 14:27:39.757284 140309575993088 logging_writer.py:48] [51] global_step=51, grad_norm=4.384498596191406, loss=9.741464614868164
I0306 14:27:40.121110 140309584385792 logging_writer.py:48] [52] global_step=52, grad_norm=4.276758193969727, loss=9.72278881072998
I0306 14:27:40.485563 140309575993088 logging_writer.py:48] [53] global_step=53, grad_norm=4.1449737548828125, loss=9.708431243896484
I0306 14:27:40.854199 140309584385792 logging_writer.py:48] [54] global_step=54, grad_norm=4.045312881469727, loss=9.699304580688477
I0306 14:27:41.222098 140309575993088 logging_writer.py:48] [55] global_step=55, grad_norm=3.996156930923462, loss=9.684819221496582
I0306 14:27:41.589887 140309584385792 logging_writer.py:48] [56] global_step=56, grad_norm=3.9444899559020996, loss=9.669644355773926
I0306 14:27:41.956665 140309575993088 logging_writer.py:48] [57] global_step=57, grad_norm=3.901707887649536, loss=9.634252548217773
I0306 14:27:42.322320 140309584385792 logging_writer.py:48] [58] global_step=58, grad_norm=3.8518481254577637, loss=9.608305931091309
I0306 14:27:42.687762 140309575993088 logging_writer.py:48] [59] global_step=59, grad_norm=3.8242125511169434, loss=9.584051132202148
I0306 14:27:43.052281 140309584385792 logging_writer.py:48] [60] global_step=60, grad_norm=3.7086048126220703, loss=9.609981536865234
I0306 14:27:43.418182 140309575993088 logging_writer.py:48] [61] global_step=61, grad_norm=3.7352192401885986, loss=9.596358299255371
I0306 14:27:43.785992 140309584385792 logging_writer.py:48] [62] global_step=62, grad_norm=3.758117437362671, loss=9.579859733581543
I0306 14:27:44.151401 140309575993088 logging_writer.py:48] [63] global_step=63, grad_norm=3.785747766494751, loss=9.548202514648438
I0306 14:27:44.517537 140309584385792 logging_writer.py:48] [64] global_step=64, grad_norm=3.7153847217559814, loss=9.532920837402344
I0306 14:27:44.887391 140309575993088 logging_writer.py:48] [65] global_step=65, grad_norm=3.801353931427002, loss=9.530864715576172
I0306 14:27:45.253980 140309584385792 logging_writer.py:48] [66] global_step=66, grad_norm=3.772901773452759, loss=9.517580032348633
I0306 14:27:45.619195 140309575993088 logging_writer.py:48] [67] global_step=67, grad_norm=3.7400686740875244, loss=9.458470344543457
I0306 14:27:45.984991 140309584385792 logging_writer.py:48] [68] global_step=68, grad_norm=3.716747999191284, loss=9.463706970214844
I0306 14:27:46.351397 140309575993088 logging_writer.py:48] [69] global_step=69, grad_norm=3.735560894012451, loss=9.46036148071289
I0306 14:27:46.720127 140309584385792 logging_writer.py:48] [70] global_step=70, grad_norm=3.682072162628174, loss=9.425314903259277
I0306 14:27:47.086911 140309575993088 logging_writer.py:48] [71] global_step=71, grad_norm=3.6310043334960938, loss=9.456644058227539
I0306 14:27:47.453836 140309584385792 logging_writer.py:48] [72] global_step=72, grad_norm=3.58602237701416, loss=9.423656463623047
I0306 14:27:47.820665 140309575993088 logging_writer.py:48] [73] global_step=73, grad_norm=3.453869104385376, loss=9.38523006439209
I0306 14:27:48.185521 140309584385792 logging_writer.py:48] [74] global_step=74, grad_norm=3.5001449584960938, loss=9.377205848693848
I0306 14:27:48.549484 140309575993088 logging_writer.py:48] [75] global_step=75, grad_norm=3.411566972732544, loss=9.381948471069336
I0306 14:27:48.913323 140309584385792 logging_writer.py:48] [76] global_step=76, grad_norm=3.260591983795166, loss=9.35274887084961
I0306 14:27:49.275689 140309575993088 logging_writer.py:48] [77] global_step=77, grad_norm=3.1721744537353516, loss=9.371143341064453
I0306 14:27:49.641551 140309584385792 logging_writer.py:48] [78] global_step=78, grad_norm=3.047499895095825, loss=9.306694984436035
I0306 14:27:50.008423 140309575993088 logging_writer.py:48] [79] global_step=79, grad_norm=3.061830997467041, loss=9.337932586669922
I0306 14:27:50.373257 140309584385792 logging_writer.py:48] [80] global_step=80, grad_norm=2.806466817855835, loss=9.346122741699219
I0306 14:27:50.736723 140309575993088 logging_writer.py:48] [81] global_step=81, grad_norm=2.8360960483551025, loss=9.321490287780762
I0306 14:27:51.100563 140309584385792 logging_writer.py:48] [82] global_step=82, grad_norm=2.799835681915283, loss=9.238258361816406
I0306 14:27:51.466043 140309575993088 logging_writer.py:48] [83] global_step=83, grad_norm=2.6344242095947266, loss=9.296112060546875
I0306 14:27:51.832399 140309584385792 logging_writer.py:48] [84] global_step=84, grad_norm=2.593073606491089, loss=9.258550643920898
I0306 14:27:52.199474 140309575993088 logging_writer.py:48] [85] global_step=85, grad_norm=2.493056535720825, loss=9.249207496643066
I0306 14:27:52.565611 140309584385792 logging_writer.py:48] [86] global_step=86, grad_norm=2.4585204124450684, loss=9.244669914245605
I0306 14:27:52.930887 140309575993088 logging_writer.py:48] [87] global_step=87, grad_norm=2.468748092651367, loss=9.224716186523438
I0306 14:27:53.298374 140309584385792 logging_writer.py:48] [88] global_step=88, grad_norm=2.4362611770629883, loss=9.20272445678711
I0306 14:27:53.665234 140309575993088 logging_writer.py:48] [89] global_step=89, grad_norm=2.4199352264404297, loss=9.192239761352539
I0306 14:27:54.033445 140309584385792 logging_writer.py:48] [90] global_step=90, grad_norm=2.391284465789795, loss=9.182950019836426
I0306 14:27:54.397686 140309575993088 logging_writer.py:48] [91] global_step=91, grad_norm=2.3514723777770996, loss=9.199082374572754
I0306 14:27:54.762828 140309584385792 logging_writer.py:48] [92] global_step=92, grad_norm=2.3726284503936768, loss=9.16748046875
I0306 14:27:55.126844 140309575993088 logging_writer.py:48] [93] global_step=93, grad_norm=2.3289597034454346, loss=9.201050758361816
I0306 14:27:55.492652 140309584385792 logging_writer.py:48] [94] global_step=94, grad_norm=2.3350465297698975, loss=9.17084789276123
I0306 14:27:55.857516 140309575993088 logging_writer.py:48] [95] global_step=95, grad_norm=2.4192240238189697, loss=9.172669410705566
I0306 14:27:56.221657 140309584385792 logging_writer.py:48] [96] global_step=96, grad_norm=2.4137649536132812, loss=9.12883186340332
I0306 14:27:56.586205 140309575993088 logging_writer.py:48] [97] global_step=97, grad_norm=2.360398054122925, loss=9.150495529174805
I0306 14:27:56.951691 140309584385792 logging_writer.py:48] [98] global_step=98, grad_norm=2.3418407440185547, loss=9.127884864807129
I0306 14:27:57.318690 140309575993088 logging_writer.py:48] [99] global_step=99, grad_norm=2.2313106060028076, loss=9.138040542602539
I0306 14:27:57.685778 140309584385792 logging_writer.py:48] [100] global_step=100, grad_norm=2.278280735015869, loss=9.103652000427246
I0306 14:30:19.165651 140309575993088 logging_writer.py:48] [500] global_step=500, grad_norm=0.2291177362203598, loss=8.493425369262695
I0306 14:33:16.052032 140309584385792 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.1800968050956726, loss=8.090132713317871
I0306 14:36:12.868367 140309575993088 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.7404835820198059, loss=7.446267604827881
I0306 14:39:09.917386 140309584385792 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.3948464095592499, loss=6.737453937530518
I0306 14:41:21.353032 140479251105600 spec.py:321] Evaluating on the training split.
I0306 14:41:24.301380 140479251105600 workload.py:181] Translating evaluation dataset.
W0306 14:46:04.476173 140479251105600 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0306 14:46:04.476419 140479251105600 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0306 14:46:04.476487 140479251105600 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0306 14:46:05.439907 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 14:46:08.092921 140479251105600 workload.py:181] Translating evaluation dataset.
W0306 14:50:47.478770 140479251105600 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0306 14:50:47.478993 140479251105600 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0306 14:50:47.479044 140479251105600 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0306 14:50:48.490249 140479251105600 spec.py:349] Evaluating on the test split.
I0306 14:50:51.149303 140479251105600 workload.py:181] Translating evaluation dataset.
W0306 14:55:30.719079 140479251105600 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0306 14:55:30.719298 140479251105600 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0306 14:55:30.719347 140479251105600 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0306 14:55:31.946799 140479251105600 submission_runner.py:413] Time since start: 2600.97s, 	Step: 2373, 	{'train/accuracy': 0.16554959118366241, 'train/loss': 6.375943660736084, 'train/bleu': 0.5055459874411922, 'validation/accuracy': 0.16157269477844238, 'validation/loss': 6.475502967834473, 'validation/bleu': 0.08346499592423524, 'validation/num_examples': 3000, 'test/accuracy': 0.14870722591876984, 'test/loss': 6.718201637268066, 'test/bleu': 0.10198142982398664, 'test/num_examples': 3003, 'score': 877.1536650657654, 'total_duration': 2600.9733667373657, 'accumulated_submission_time': 877.1536650657654, 'accumulated_eval_time': 1723.718737602234, 'accumulated_logging_time': 0.027268409729003906}
I0306 14:55:31.963146 140309575993088 logging_writer.py:48] [2373] accumulated_eval_time=1723.718738, accumulated_logging_time=0.027268, accumulated_submission_time=877.153665, global_step=2373, preemption_count=0, score=877.153665, test/accuracy=0.148707, test/bleu=0.101981, test/loss=6.718202, test/num_examples=3003, total_duration=2600.973367, train/accuracy=0.165550, train/bleu=0.505546, train/loss=6.375944, validation/accuracy=0.161573, validation/bleu=0.083465, validation/loss=6.475503, validation/num_examples=3000
I0306 14:56:17.175452 140309584385792 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.5896186828613281, loss=6.314456939697266
I0306 14:59:14.212327 140309575993088 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8288478255271912, loss=5.881222248077393
I0306 15:02:11.414001 140309584385792 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.5749441385269165, loss=5.482122898101807
I0306 15:05:08.531228 140309575993088 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.6423479914665222, loss=5.273394584655762
I0306 15:08:05.614129 140309584385792 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.637602686882019, loss=4.962014198303223
I0306 15:09:32.037050 140479251105600 spec.py:321] Evaluating on the training split.
I0306 15:09:34.986841 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 15:14:05.734453 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 15:14:08.390341 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 15:18:47.522383 140479251105600 spec.py:349] Evaluating on the test split.
I0306 15:18:50.184287 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 15:23:30.410779 140479251105600 submission_runner.py:413] Time since start: 4279.44s, 	Step: 4746, 	{'train/accuracy': 0.29963192343711853, 'train/loss': 4.757896900177002, 'train/bleu': 5.589181539208054, 'validation/accuracy': 0.26796939969062805, 'validation/loss': 5.071068286895752, 'validation/bleu': 2.313022269605274, 'validation/num_examples': 3000, 'test/accuracy': 0.24799257516860962, 'test/loss': 5.372460842132568, 'test/bleu': 1.7368712242060385, 'test/num_examples': 3003, 'score': 1717.140832901001, 'total_duration': 4279.43731546402, 'accumulated_submission_time': 1717.140832901001, 'accumulated_eval_time': 2562.092456817627, 'accumulated_logging_time': 0.054433345794677734}
I0306 15:23:30.425985 140309575993088 logging_writer.py:48] [4746] accumulated_eval_time=2562.092457, accumulated_logging_time=0.054433, accumulated_submission_time=1717.140833, global_step=4746, preemption_count=0, score=1717.140833, test/accuracy=0.247993, test/bleu=1.736871, test/loss=5.372461, test/num_examples=3003, total_duration=4279.437315, train/accuracy=0.299632, train/bleu=5.589182, train/loss=4.757897, validation/accuracy=0.267969, validation/bleu=2.313022, validation/loss=5.071068, validation/num_examples=3000
I0306 15:25:00.595181 140309584385792 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6792809963226318, loss=4.724565029144287
I0306 15:27:57.679777 140309575993088 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6139013171195984, loss=4.648838043212891
I0306 15:30:54.685912 140309584385792 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6453903317451477, loss=4.3775529861450195
I0306 15:33:51.759391 140309575993088 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.691727876663208, loss=4.287588119506836
I0306 15:36:48.811617 140309584385792 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6904833912849426, loss=4.191510200500488
I0306 15:37:30.698109 140479251105600 spec.py:321] Evaluating on the training split.
I0306 15:37:33.661778 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 15:42:00.897675 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 15:42:03.570295 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 15:46:36.433146 140479251105600 spec.py:349] Evaluating on the test split.
I0306 15:46:39.077797 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 15:51:10.972638 140479251105600 submission_runner.py:413] Time since start: 5940.00s, 	Step: 7120, 	{'train/accuracy': 0.3576729893684387, 'train/loss': 3.9961116313934326, 'train/bleu': 7.724495868299385, 'validation/accuracy': 0.3177393972873688, 'validation/loss': 4.359506607055664, 'validation/bleu': 3.3767921191211205, 'validation/num_examples': 3000, 'test/accuracy': 0.29782116413116455, 'test/loss': 4.646053314208984, 'test/bleu': 2.3950492710240585, 'test/num_examples': 3003, 'score': 2557.3294196128845, 'total_duration': 5939.9991092681885, 'accumulated_submission_time': 2557.3294196128845, 'accumulated_eval_time': 3382.366870164871, 'accumulated_logging_time': 0.0795748233795166}
I0306 15:51:10.990769 140309575993088 logging_writer.py:48] [7120] accumulated_eval_time=3382.366870, accumulated_logging_time=0.079575, accumulated_submission_time=2557.329420, global_step=7120, preemption_count=0, score=2557.329420, test/accuracy=0.297821, test/bleu=2.395049, test/loss=4.646053, test/num_examples=3003, total_duration=5939.999109, train/accuracy=0.357673, train/bleu=7.724496, train/loss=3.996112, validation/accuracy=0.317739, validation/bleu=3.376792, validation/loss=4.359507, validation/num_examples=3000
I0306 15:53:25.819630 140309584385792 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6656438708305359, loss=4.030068397521973
I0306 15:56:22.934889 140309575993088 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.6320470571517944, loss=4.057509422302246
I0306 15:59:20.032691 140309584385792 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.6533686518669128, loss=3.8746485710144043
I0306 16:02:17.133293 140309575993088 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6143706440925598, loss=3.8036000728607178
I0306 16:05:11.258378 140479251105600 spec.py:321] Evaluating on the training split.
I0306 16:05:14.200455 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 16:09:37.110835 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 16:09:39.760684 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 16:14:05.190824 140479251105600 spec.py:349] Evaluating on the test split.
I0306 16:14:07.841485 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 16:18:28.235962 140479251105600 submission_runner.py:413] Time since start: 7577.26s, 	Step: 9493, 	{'train/accuracy': 0.3939822316169739, 'train/loss': 3.5680007934570312, 'train/bleu': 8.959164110927713, 'validation/accuracy': 0.3531884253025055, 'validation/loss': 3.897632122039795, 'validation/bleu': 4.1326592238074635, 'validation/num_examples': 3000, 'test/accuracy': 0.3317297101020813, 'test/loss': 4.168331623077393, 'test/bleu': 2.939828795868217, 'test/num_examples': 3003, 'score': 3397.510332107544, 'total_duration': 7577.262491941452, 'accumulated_submission_time': 3397.510332107544, 'accumulated_eval_time': 4179.34440946579, 'accumulated_logging_time': 0.10790038108825684}
I0306 16:18:28.251482 140309584385792 logging_writer.py:48] [9493] accumulated_eval_time=4179.344409, accumulated_logging_time=0.107900, accumulated_submission_time=3397.510332, global_step=9493, preemption_count=0, score=3397.510332, test/accuracy=0.331730, test/bleu=2.939829, test/loss=4.168332, test/num_examples=3003, total_duration=7577.262492, train/accuracy=0.393982, train/bleu=8.959164, train/loss=3.568001, validation/accuracy=0.353188, validation/bleu=4.132659, validation/loss=3.897632, validation/num_examples=3000
I0306 16:18:31.076070 140309575993088 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.6472241282463074, loss=3.6292595863342285
I0306 16:21:27.861144 140309584385792 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7445535659790039, loss=3.4463844299316406
I0306 16:24:24.942060 140309575993088 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.729714572429657, loss=3.3228135108947754
I0306 16:27:21.989491 140309584385792 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7042681574821472, loss=3.1168313026428223
I0306 16:30:19.170373 140309575993088 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6451075673103333, loss=2.944286346435547
I0306 16:32:28.517400 140479251105600 spec.py:321] Evaluating on the training split.
I0306 16:32:31.459826 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 16:35:44.373047 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 16:35:47.032295 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 16:38:30.654075 140479251105600 spec.py:349] Evaluating on the test split.
I0306 16:38:33.305341 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 16:41:10.661159 140479251105600 submission_runner.py:413] Time since start: 8939.69s, 	Step: 11867, 	{'train/accuracy': 0.5276615619659424, 'train/loss': 2.6731984615325928, 'train/bleu': 21.670140635051332, 'validation/accuracy': 0.5194851756095886, 'validation/loss': 2.7354586124420166, 'validation/bleu': 16.803285917460464, 'validation/num_examples': 3000, 'test/accuracy': 0.5176340937614441, 'test/loss': 2.7942471504211426, 'test/bleu': 15.075098528653777, 'test/num_examples': 3003, 'score': 4237.694143295288, 'total_duration': 8939.687692403793, 'accumulated_submission_time': 4237.694143295288, 'accumulated_eval_time': 4701.488126993179, 'accumulated_logging_time': 0.13354754447937012}
I0306 16:41:10.676489 140309584385792 logging_writer.py:48] [11867] accumulated_eval_time=4701.488127, accumulated_logging_time=0.133548, accumulated_submission_time=4237.694143, global_step=11867, preemption_count=0, score=4237.694143, test/accuracy=0.517634, test/bleu=15.075099, test/loss=2.794247, test/num_examples=3003, total_duration=8939.687692, train/accuracy=0.527662, train/bleu=21.670141, train/loss=2.673198, validation/accuracy=0.519485, validation/bleu=16.803286, validation/loss=2.735459, validation/num_examples=3000
I0306 16:41:57.999813 140309575993088 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6580386757850647, loss=2.8558294773101807
I0306 16:44:55.203848 140309584385792 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6168602705001831, loss=2.821976900100708
I0306 16:47:52.462429 140309575993088 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.601364254951477, loss=2.6203784942626953
I0306 16:50:49.728145 140309584385792 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.5676829218864441, loss=2.6331279277801514
I0306 16:53:46.787245 140309575993088 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.5278019905090332, loss=2.483091354370117
I0306 16:55:10.947400 140479251105600 spec.py:321] Evaluating on the training split.
I0306 16:55:13.895180 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 16:57:56.494466 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 16:57:59.147105 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 17:00:35.757072 140479251105600 spec.py:349] Evaluating on the test split.
I0306 17:00:38.415700 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 17:03:01.620137 140479251105600 submission_runner.py:413] Time since start: 10250.65s, 	Step: 14239, 	{'train/accuracy': 0.5713105797767639, 'train/loss': 2.3388614654541016, 'train/bleu': 26.30931954488054, 'validation/accuracy': 0.5742024183273315, 'validation/loss': 2.3315088748931885, 'validation/bleu': 21.292113283628584, 'validation/num_examples': 3000, 'test/accuracy': 0.5747603178024292, 'test/loss': 2.337750196456909, 'test/bleu': 19.65655123385915, 'test/num_examples': 3003, 'score': 5077.872762441635, 'total_duration': 10250.646673440933, 'accumulated_submission_time': 5077.872762441635, 'accumulated_eval_time': 5172.160847663879, 'accumulated_logging_time': 0.15973806381225586}
I0306 17:03:01.635798 140309584385792 logging_writer.py:48] [14239] accumulated_eval_time=5172.160848, accumulated_logging_time=0.159738, accumulated_submission_time=5077.872762, global_step=14239, preemption_count=0, score=5077.872762, test/accuracy=0.574760, test/bleu=19.656551, test/loss=2.337750, test/num_examples=3003, total_duration=10250.646673, train/accuracy=0.571311, train/bleu=26.309320, train/loss=2.338861, validation/accuracy=0.574202, validation/bleu=21.292113, validation/loss=2.331509, validation/num_examples=3000
I0306 17:04:34.269965 140309575993088 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.5344547033309937, loss=2.4722940921783447
I0306 17:07:31.350630 140309584385792 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.4974493980407715, loss=2.373084306716919
I0306 17:10:28.611865 140309575993088 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.49227648973464966, loss=2.3129594326019287
I0306 17:13:25.747021 140309584385792 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.4948059916496277, loss=2.3515801429748535
I0306 17:16:22.777458 140309575993088 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.49476081132888794, loss=2.4235692024230957
I0306 17:17:01.809207 140479251105600 spec.py:321] Evaluating on the training split.
I0306 17:17:04.751472 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 17:19:21.024726 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 17:19:23.680948 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 17:21:47.949809 140479251105600 spec.py:349] Evaluating on the test split.
I0306 17:21:50.620062 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 17:24:02.937915 140479251105600 submission_runner.py:413] Time since start: 11511.96s, 	Step: 16612, 	{'train/accuracy': 0.590408980846405, 'train/loss': 2.192633628845215, 'train/bleu': 27.654218287671327, 'validation/accuracy': 0.5975747108459473, 'validation/loss': 2.1420791149139404, 'validation/bleu': 23.389768987724974, 'validation/num_examples': 3000, 'test/accuracy': 0.6012085676193237, 'test/loss': 2.1240668296813965, 'test/bleu': 21.892491493610297, 'test/num_examples': 3003, 'score': 5917.961942195892, 'total_duration': 11511.964399814606, 'accumulated_submission_time': 5917.961942195892, 'accumulated_eval_time': 5593.289460659027, 'accumulated_logging_time': 0.18412065505981445}
I0306 17:24:02.957693 140309584385792 logging_writer.py:48] [16612] accumulated_eval_time=5593.289461, accumulated_logging_time=0.184121, accumulated_submission_time=5917.961942, global_step=16612, preemption_count=0, score=5917.961942, test/accuracy=0.601209, test/bleu=21.892491, test/loss=2.124067, test/num_examples=3003, total_duration=11511.964400, train/accuracy=0.590409, train/bleu=27.654218, train/loss=2.192634, validation/accuracy=0.597575, validation/bleu=23.389769, validation/loss=2.142079, validation/num_examples=3000
I0306 17:26:20.628828 140309575993088 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.4774453043937683, loss=2.365086078643799
I0306 17:29:17.705130 140309584385792 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.44141802191734314, loss=2.3382081985473633
I0306 17:32:14.899266 140309575993088 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.4469325542449951, loss=2.2428183555603027
I0306 17:35:11.980064 140309584385792 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.4535312056541443, loss=2.285977363586426
I0306 17:38:03.195768 140479251105600 spec.py:321] Evaluating on the training split.
I0306 17:38:06.148816 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 17:40:25.752776 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 17:40:28.409904 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 17:42:57.410170 140479251105600 spec.py:349] Evaluating on the test split.
I0306 17:43:00.065648 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 17:45:08.185323 140479251105600 submission_runner.py:413] Time since start: 12777.21s, 	Step: 18985, 	{'train/accuracy': 0.6170119643211365, 'train/loss': 1.9861655235290527, 'train/bleu': 29.130296003246443, 'validation/accuracy': 0.612292468547821, 'validation/loss': 2.0235533714294434, 'validation/bleu': 24.460743028793953, 'validation/num_examples': 3000, 'test/accuracy': 0.6166289448738098, 'test/loss': 1.9903719425201416, 'test/bleu': 23.1778212618428, 'test/num_examples': 3003, 'score': 6758.1156215667725, 'total_duration': 12777.2118537426, 'accumulated_submission_time': 6758.1156215667725, 'accumulated_eval_time': 6018.278962135315, 'accumulated_logging_time': 0.21383881568908691}
I0306 17:45:08.200862 140309575993088 logging_writer.py:48] [18985] accumulated_eval_time=6018.278962, accumulated_logging_time=0.213839, accumulated_submission_time=6758.115622, global_step=18985, preemption_count=0, score=6758.115622, test/accuracy=0.616629, test/bleu=23.177821, test/loss=1.990372, test/num_examples=3003, total_duration=12777.211854, train/accuracy=0.617012, train/bleu=29.130296, train/loss=1.986166, validation/accuracy=0.612292, validation/bleu=24.460743, validation/loss=2.023553, validation/num_examples=3000
I0306 17:45:13.948185 140309584385792 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.4350185692310333, loss=2.122929334640503
I0306 17:48:10.942549 140309575993088 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.4350809156894684, loss=2.1649630069732666
I0306 17:51:08.163387 140309584385792 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.43898382782936096, loss=2.2324488162994385
I0306 17:54:05.391234 140309575993088 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.4610227942466736, loss=2.140974283218384
I0306 17:57:02.553713 140309584385792 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.402227520942688, loss=2.101217031478882
I0306 17:59:08.459990 140479251105600 spec.py:321] Evaluating on the training split.
I0306 17:59:11.411107 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 18:02:07.618025 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 18:02:10.276423 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 18:04:36.641703 140479251105600 spec.py:349] Evaluating on the test split.
I0306 18:04:39.305198 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 18:06:56.809167 140479251105600 submission_runner.py:413] Time since start: 14085.84s, 	Step: 21357, 	{'train/accuracy': 0.6118479371070862, 'train/loss': 2.020354747772217, 'train/bleu': 29.323991583626714, 'validation/accuracy': 0.6203518509864807, 'validation/loss': 1.9466265439987183, 'validation/bleu': 25.223630033063053, 'validation/num_examples': 3000, 'test/accuracy': 0.62579745054245, 'test/loss': 1.9064064025878906, 'test/bleu': 23.811507225729045, 'test/num_examples': 3003, 'score': 7598.2155549526215, 'total_duration': 14085.8356487751, 'accumulated_submission_time': 7598.2155549526215, 'accumulated_eval_time': 6486.62804889679, 'accumulated_logging_time': 0.312896728515625}
I0306 18:06:56.828748 140309575993088 logging_writer.py:48] [21357] accumulated_eval_time=6486.628049, accumulated_logging_time=0.312897, accumulated_submission_time=7598.215555, global_step=21357, preemption_count=0, score=7598.215555, test/accuracy=0.625797, test/bleu=23.811507, test/loss=1.906406, test/num_examples=3003, total_duration=14085.835649, train/accuracy=0.611848, train/bleu=29.323992, train/loss=2.020355, validation/accuracy=0.620352, validation/bleu=25.223630, validation/loss=1.946627, validation/num_examples=3000
I0306 18:07:47.743376 140309584385792 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.42541372776031494, loss=2.089949131011963
I0306 18:10:44.889438 140309575993088 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.45051416754722595, loss=2.2172036170959473
I0306 18:13:42.169276 140309584385792 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.4348074793815613, loss=2.0474307537078857
I0306 18:16:39.286115 140309575993088 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.4028286635875702, loss=2.050990104675293
I0306 18:19:36.448728 140309584385792 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.41596850752830505, loss=2.0443408489227295
I0306 18:20:57.010341 140479251105600 spec.py:321] Evaluating on the training split.
I0306 18:20:59.968561 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 18:24:16.519853 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 18:24:19.184601 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 18:26:43.061204 140479251105600 spec.py:349] Evaluating on the test split.
I0306 18:26:45.713999 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 18:28:57.319673 140479251105600 submission_runner.py:413] Time since start: 15406.35s, 	Step: 23729, 	{'train/accuracy': 0.6130566000938416, 'train/loss': 1.9910234212875366, 'train/bleu': 29.008670164352342, 'validation/accuracy': 0.6288080811500549, 'validation/loss': 1.8797252178192139, 'validation/bleu': 25.45348854405818, 'validation/num_examples': 3000, 'test/accuracy': 0.6346173882484436, 'test/loss': 1.8363406658172607, 'test/bleu': 24.40175743920315, 'test/num_examples': 3003, 'score': 8438.309606075287, 'total_duration': 15406.34621143341, 'accumulated_submission_time': 8438.309606075287, 'accumulated_eval_time': 6966.937338113785, 'accumulated_logging_time': 0.34282708168029785}
I0306 18:28:57.336509 140309575993088 logging_writer.py:48] [23729] accumulated_eval_time=6966.937338, accumulated_logging_time=0.342827, accumulated_submission_time=8438.309606, global_step=23729, preemption_count=0, score=8438.309606, test/accuracy=0.634617, test/bleu=24.401757, test/loss=1.836341, test/num_examples=3003, total_duration=15406.346211, train/accuracy=0.613057, train/bleu=29.008670, train/loss=1.991023, validation/accuracy=0.628808, validation/bleu=25.453489, validation/loss=1.879725, validation/num_examples=3000
I0306 18:30:33.490862 140309584385792 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.40716201066970825, loss=2.0701584815979004
I0306 18:33:30.586515 140309575993088 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.3939305245876312, loss=2.016266345977783
I0306 18:36:27.910691 140309584385792 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.3943007290363312, loss=2.063375473022461
I0306 18:39:25.172246 140309575993088 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.40488317608833313, loss=1.9790847301483154
I0306 18:42:22.455751 140309584385792 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.3863474726676941, loss=2.0469417572021484
I0306 18:42:57.638308 140479251105600 spec.py:321] Evaluating on the training split.
I0306 18:43:00.597466 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 18:45:34.226823 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 18:45:36.881698 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 18:47:59.285539 140479251105600 spec.py:349] Evaluating on the test split.
I0306 18:48:01.951009 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 18:50:17.761423 140479251105600 submission_runner.py:413] Time since start: 16686.79s, 	Step: 26101, 	{'train/accuracy': 0.6259080767631531, 'train/loss': 1.8868838548660278, 'train/bleu': 30.179083379715863, 'validation/accuracy': 0.6362971067428589, 'validation/loss': 1.8235551118850708, 'validation/bleu': 26.384664324348677, 'validation/num_examples': 3000, 'test/accuracy': 0.6441578269004822, 'test/loss': 1.772401213645935, 'test/bleu': 25.32138446752374, 'test/num_examples': 3003, 'score': 9278.52301311493, 'total_duration': 16686.78796339035, 'accumulated_submission_time': 9278.52301311493, 'accumulated_eval_time': 7407.060416698456, 'accumulated_logging_time': 0.3685746192932129}
I0306 18:50:17.778180 140309575993088 logging_writer.py:48] [26101] accumulated_eval_time=7407.060417, accumulated_logging_time=0.368575, accumulated_submission_time=9278.523013, global_step=26101, preemption_count=0, score=9278.523013, test/accuracy=0.644158, test/bleu=25.321384, test/loss=1.772401, test/num_examples=3003, total_duration=16686.787963, train/accuracy=0.625908, train/bleu=30.179083, train/loss=1.886884, validation/accuracy=0.636297, validation/bleu=26.384664, validation/loss=1.823555, validation/num_examples=3000
I0306 18:52:39.166716 140309584385792 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.4116877317428589, loss=2.0905303955078125
I0306 18:55:36.231966 140309575993088 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.40770992636680603, loss=2.006343126296997
I0306 18:58:33.339180 140309584385792 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.40887826681137085, loss=2.049363851547241
I0306 19:01:30.415643 140309575993088 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.40183529257774353, loss=2.0271170139312744
I0306 19:04:17.793888 140479251105600 spec.py:321] Evaluating on the training split.
I0306 19:04:20.749136 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 19:07:07.948590 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 19:07:10.610528 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 19:09:33.094112 140479251105600 spec.py:349] Evaluating on the test split.
I0306 19:09:35.760233 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 19:11:52.708928 140479251105600 submission_runner.py:413] Time since start: 17981.74s, 	Step: 28474, 	{'train/accuracy': 0.6278741359710693, 'train/loss': 1.8764451742172241, 'train/bleu': 30.580050059388913, 'validation/accuracy': 0.6410459876060486, 'validation/loss': 1.7856191396713257, 'validation/bleu': 26.55775990002107, 'validation/num_examples': 3000, 'test/accuracy': 0.6514670848846436, 'test/loss': 1.719996690750122, 'test/bleu': 25.71419687547752, 'test/num_examples': 3003, 'score': 10118.453328609467, 'total_duration': 17981.735468387604, 'accumulated_submission_time': 10118.453328609467, 'accumulated_eval_time': 7861.975425481796, 'accumulated_logging_time': 0.39550185203552246}
I0306 19:11:52.725418 140309584385792 logging_writer.py:48] [28474] accumulated_eval_time=7861.975425, accumulated_logging_time=0.395502, accumulated_submission_time=10118.453329, global_step=28474, preemption_count=0, score=10118.453329, test/accuracy=0.651467, test/bleu=25.714197, test/loss=1.719997, test/num_examples=3003, total_duration=17981.735468, train/accuracy=0.627874, train/bleu=30.580050, train/loss=1.876445, validation/accuracy=0.641046, validation/bleu=26.557760, validation/loss=1.785619, validation/num_examples=3000
I0306 19:12:02.257571 140309575993088 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.3988455533981323, loss=2.037787675857544
I0306 19:14:59.195387 140309584385792 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.41056540608406067, loss=1.9933382272720337
I0306 19:17:56.310030 140309575993088 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.39453405141830444, loss=2.028569459915161
I0306 19:20:53.402226 140309584385792 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.37768134474754333, loss=1.9977213144302368
I0306 19:23:50.477592 140309575993088 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.3739776313304901, loss=1.8821674585342407
I0306 19:25:52.954196 140479251105600 spec.py:321] Evaluating on the training split.
I0306 19:25:55.923216 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 19:28:26.586024 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 19:28:29.266184 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 19:30:48.580904 140479251105600 spec.py:349] Evaluating on the test split.
I0306 19:30:51.253395 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 19:33:04.181700 140479251105600 submission_runner.py:413] Time since start: 19253.21s, 	Step: 30847, 	{'train/accuracy': 0.6271517276763916, 'train/loss': 1.8758031129837036, 'train/bleu': 30.385887548990254, 'validation/accuracy': 0.645683228969574, 'validation/loss': 1.7459243535995483, 'validation/bleu': 27.356207782527754, 'validation/num_examples': 3000, 'test/accuracy': 0.6540584564208984, 'test/loss': 1.6935328245162964, 'test/bleu': 26.00806679697413, 'test/num_examples': 3003, 'score': 10958.59934592247, 'total_duration': 19253.208206653595, 'accumulated_submission_time': 10958.59934592247, 'accumulated_eval_time': 8293.202862262726, 'accumulated_logging_time': 0.42104101181030273}
I0306 19:33:04.198028 140309584385792 logging_writer.py:48] [30847] accumulated_eval_time=8293.202862, accumulated_logging_time=0.421041, accumulated_submission_time=10958.599346, global_step=30847, preemption_count=0, score=10958.599346, test/accuracy=0.654058, test/bleu=26.008067, test/loss=1.693533, test/num_examples=3003, total_duration=19253.208207, train/accuracy=0.627152, train/bleu=30.385888, train/loss=1.875803, validation/accuracy=0.645683, validation/bleu=27.356208, validation/loss=1.745924, validation/num_examples=3000
I0306 19:33:58.528637 140309575993088 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.36723050475120544, loss=1.9203052520751953
I0306 19:36:55.654654 140309584385792 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.372490257024765, loss=1.9191173315048218
I0306 19:39:52.786107 140309575993088 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.3679746389389038, loss=1.9551442861557007
I0306 19:42:49.911994 140309584385792 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.36089688539505005, loss=1.810562014579773
I0306 19:45:46.978715 140309575993088 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.37401264905929565, loss=1.9738640785217285
I0306 19:47:04.313691 140479251105600 spec.py:321] Evaluating on the training split.
I0306 19:47:07.267295 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 19:50:26.060564 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 19:50:28.719619 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 19:53:03.869491 140479251105600 spec.py:349] Evaluating on the test split.
I0306 19:53:06.531993 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 19:55:16.493656 140479251105600 submission_runner.py:413] Time since start: 20585.52s, 	Step: 33220, 	{'train/accuracy': 0.6357178092002869, 'train/loss': 1.8116439580917358, 'train/bleu': 30.964162543206665, 'validation/accuracy': 0.6494277715682983, 'validation/loss': 1.7206015586853027, 'validation/bleu': 27.538105689602112, 'validation/num_examples': 3000, 'test/accuracy': 0.6602521538734436, 'test/loss': 1.6538362503051758, 'test/bleu': 26.550878491057826, 'test/num_examples': 3003, 'score': 11798.631483793259, 'total_duration': 20585.520178079605, 'accumulated_submission_time': 11798.631483793259, 'accumulated_eval_time': 8785.382771253586, 'accumulated_logging_time': 0.4463164806365967}
I0306 19:55:16.510660 140309584385792 logging_writer.py:48] [33220] accumulated_eval_time=8785.382771, accumulated_logging_time=0.446316, accumulated_submission_time=11798.631484, global_step=33220, preemption_count=0, score=11798.631484, test/accuracy=0.660252, test/bleu=26.550878, test/loss=1.653836, test/num_examples=3003, total_duration=20585.520178, train/accuracy=0.635718, train/bleu=30.964163, train/loss=1.811644, validation/accuracy=0.649428, validation/bleu=27.538106, validation/loss=1.720602, validation/num_examples=3000
I0306 19:56:55.801357 140309575993088 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.36411240696907043, loss=1.975624918937683
I0306 19:59:52.937684 140309584385792 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.35091090202331543, loss=1.8769197463989258
I0306 20:02:50.062998 140309575993088 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.36283913254737854, loss=1.8629965782165527
I0306 20:05:47.243957 140309584385792 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.3501899540424347, loss=1.8704278469085693
I0306 20:08:44.374124 140309575993088 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.35955753922462463, loss=1.9329522848129272
I0306 20:09:16.709992 140479251105600 spec.py:321] Evaluating on the training split.
I0306 20:09:19.666486 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 20:11:53.746093 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 20:11:56.427347 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 20:14:27.925868 140479251105600 spec.py:349] Evaluating on the test split.
I0306 20:14:30.607004 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 20:17:02.107670 140479251105600 submission_runner.py:413] Time since start: 21891.13s, 	Step: 35593, 	{'train/accuracy': 0.6376006007194519, 'train/loss': 1.795326590538025, 'train/bleu': 31.053307670989863, 'validation/accuracy': 0.6520687937736511, 'validation/loss': 1.6875280141830444, 'validation/bleu': 27.53264112337824, 'validation/num_examples': 3000, 'test/accuracy': 0.663203775882721, 'test/loss': 1.6211374998092651, 'test/bleu': 26.717511510370425, 'test/num_examples': 3003, 'score': 12638.748220205307, 'total_duration': 21891.13416814804, 'accumulated_submission_time': 12638.748220205307, 'accumulated_eval_time': 9250.780358076096, 'accumulated_logging_time': 0.4722471237182617}
I0306 20:17:02.127493 140309584385792 logging_writer.py:48] [35593] accumulated_eval_time=9250.780358, accumulated_logging_time=0.472247, accumulated_submission_time=12638.748220, global_step=35593, preemption_count=0, score=12638.748220, test/accuracy=0.663204, test/bleu=26.717512, test/loss=1.621137, test/num_examples=3003, total_duration=21891.134168, train/accuracy=0.637601, train/bleu=31.053308, train/loss=1.795327, validation/accuracy=0.652069, validation/bleu=27.532641, validation/loss=1.687528, validation/num_examples=3000
I0306 20:19:26.528723 140309575993088 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.35856619477272034, loss=1.936647891998291
I0306 20:22:23.684176 140309584385792 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.3535686135292053, loss=1.899878978729248
I0306 20:25:20.902044 140309575993088 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.3628014922142029, loss=1.9979157447814941
I0306 20:28:18.138593 140309584385792 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.4306061863899231, loss=1.962827205657959
I0306 20:31:02.279897 140479251105600 spec.py:321] Evaluating on the training split.
I0306 20:31:05.230341 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 20:34:36.020288 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 20:34:38.679437 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 20:37:54.860334 140479251105600 spec.py:349] Evaluating on the test split.
I0306 20:37:57.521884 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 20:40:51.103432 140479251105600 submission_runner.py:413] Time since start: 23320.13s, 	Step: 37965, 	{'train/accuracy': 0.6497527360916138, 'train/loss': 1.7091151475906372, 'train/bleu': 32.04856828987185, 'validation/accuracy': 0.6565572619438171, 'validation/loss': 1.6644614934921265, 'validation/bleu': 27.71397684530375, 'validation/num_examples': 3000, 'test/accuracy': 0.6659926772117615, 'test/loss': 1.5949139595031738, 'test/bleu': 27.09271560065603, 'test/num_examples': 3003, 'score': 13478.81368470192, 'total_duration': 23320.12997364998, 'accumulated_submission_time': 13478.81368470192, 'accumulated_eval_time': 9839.603850126266, 'accumulated_logging_time': 0.5021743774414062}
I0306 20:40:51.120124 140309575993088 logging_writer.py:48] [37965] accumulated_eval_time=9839.603850, accumulated_logging_time=0.502174, accumulated_submission_time=13478.813685, global_step=37965, preemption_count=0, score=13478.813685, test/accuracy=0.665993, test/bleu=27.092716, test/loss=1.594914, test/num_examples=3003, total_duration=23320.129974, train/accuracy=0.649753, train/bleu=32.048568, train/loss=1.709115, validation/accuracy=0.656557, validation/bleu=27.713977, validation/loss=1.664461, validation/num_examples=3000
I0306 20:41:03.813790 140309584385792 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.33661627769470215, loss=1.8848719596862793
I0306 20:44:00.715547 140309575993088 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.3607332706451416, loss=1.862841248512268
I0306 20:46:57.791165 140309584385792 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.34439754486083984, loss=1.8310518264770508
I0306 20:49:54.963799 140309575993088 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.35273122787475586, loss=1.8657147884368896
I0306 20:52:52.108047 140309584385792 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.33652883768081665, loss=1.7868651151657104
I0306 20:54:51.236225 140479251105600 spec.py:321] Evaluating on the training split.
I0306 20:54:54.188843 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 20:57:49.540215 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 20:57:52.199057 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 21:00:20.709128 140479251105600 spec.py:349] Evaluating on the test split.
I0306 21:00:23.367136 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 21:02:34.159150 140479251105600 submission_runner.py:413] Time since start: 24623.19s, 	Step: 40338, 	{'train/accuracy': 0.6435055732727051, 'train/loss': 1.7565072774887085, 'train/bleu': 31.808233138610035, 'validation/accuracy': 0.6595454216003418, 'validation/loss': 1.6422971487045288, 'validation/bleu': 28.078354347189922, 'validation/num_examples': 3000, 'test/accuracy': 0.6697461009025574, 'test/loss': 1.574199914932251, 'test/bleu': 27.43604887143621, 'test/num_examples': 3003, 'score': 14318.843627214432, 'total_duration': 24623.185683488846, 'accumulated_submission_time': 14318.843627214432, 'accumulated_eval_time': 10302.526732206345, 'accumulated_logging_time': 0.5289435386657715}
I0306 21:02:34.175957 140309575993088 logging_writer.py:48] [40338] accumulated_eval_time=10302.526732, accumulated_logging_time=0.528944, accumulated_submission_time=14318.843627, global_step=40338, preemption_count=0, score=14318.843627, test/accuracy=0.669746, test/bleu=27.436049, test/loss=1.574200, test/num_examples=3003, total_duration=24623.185683, train/accuracy=0.643506, train/bleu=31.808233, train/loss=1.756507, validation/accuracy=0.659545, validation/bleu=28.078354, validation/loss=1.642297, validation/num_examples=3000
I0306 21:03:31.836100 140309584385792 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.3553236126899719, loss=1.8799103498458862
I0306 21:06:28.946018 140309575993088 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.36086514592170715, loss=1.7987347841262817
I0306 21:09:26.106518 140309584385792 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.34534627199172974, loss=1.8551493883132935
I0306 21:12:23.197579 140309575993088 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.365476131439209, loss=1.8413023948669434
I0306 21:15:20.392112 140309584385792 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.34384411573410034, loss=1.78587007522583
I0306 21:16:34.504394 140479251105600 spec.py:321] Evaluating on the training split.
I0306 21:16:37.453799 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 21:19:37.056698 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 21:19:39.718658 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 21:21:55.294631 140479251105600 spec.py:349] Evaluating on the test split.
I0306 21:21:57.953763 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 21:23:59.079088 140479251105600 submission_runner.py:413] Time since start: 25908.11s, 	Step: 42711, 	{'train/accuracy': 0.6435038447380066, 'train/loss': 1.744189977645874, 'train/bleu': 31.650799330190253, 'validation/accuracy': 0.6613433361053467, 'validation/loss': 1.6276651620864868, 'validation/bleu': 28.393053224055592, 'validation/num_examples': 3000, 'test/accuracy': 0.6725001335144043, 'test/loss': 1.5530000925064087, 'test/bleu': 27.643341196206578, 'test/num_examples': 3003, 'score': 15159.088337182999, 'total_duration': 25908.105601787567, 'accumulated_submission_time': 15159.088337182999, 'accumulated_eval_time': 10747.101356744766, 'accumulated_logging_time': 0.5549688339233398}
I0306 21:23:59.096308 140309575993088 logging_writer.py:48] [42711] accumulated_eval_time=10747.101357, accumulated_logging_time=0.554969, accumulated_submission_time=15159.088337, global_step=42711, preemption_count=0, score=15159.088337, test/accuracy=0.672500, test/bleu=27.643341, test/loss=1.553000, test/num_examples=3003, total_duration=25908.105602, train/accuracy=0.643504, train/bleu=31.650799, train/loss=1.744190, validation/accuracy=0.661343, validation/bleu=28.393053, validation/loss=1.627665, validation/num_examples=3000
I0306 21:25:41.795290 140309584385792 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.3397316634654999, loss=1.777140498161316
I0306 21:28:38.984918 140309575993088 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.335319846868515, loss=1.8455564975738525
I0306 21:31:36.098677 140309584385792 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.3402193486690521, loss=1.7657151222229004
I0306 21:34:33.264634 140309575993088 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.32474285364151, loss=1.8497207164764404
I0306 21:37:30.519561 140309584385792 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.33185330033302307, loss=1.8027498722076416
I0306 21:37:59.289570 140479251105600 spec.py:321] Evaluating on the training split.
I0306 21:38:02.252466 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 21:41:13.319087 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 21:41:15.975367 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 21:43:54.251017 140479251105600 spec.py:349] Evaluating on the test split.
I0306 21:43:56.907554 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 21:46:21.280043 140479251105600 submission_runner.py:413] Time since start: 27250.31s, 	Step: 45083, 	{'train/accuracy': 0.6504939198493958, 'train/loss': 1.6879643201828003, 'train/bleu': 32.02476908456034, 'validation/accuracy': 0.6653234362602234, 'validation/loss': 1.6043541431427002, 'validation/bleu': 28.198855326407323, 'validation/num_examples': 3000, 'test/accuracy': 0.6762651801109314, 'test/loss': 1.5304776430130005, 'test/bleu': 27.993240158895745, 'test/num_examples': 3003, 'score': 15999.193945884705, 'total_duration': 27250.30659008026, 'accumulated_submission_time': 15999.193945884705, 'accumulated_eval_time': 11249.091786384583, 'accumulated_logging_time': 0.5832390785217285}
I0306 21:46:21.296780 140309575993088 logging_writer.py:48] [45083] accumulated_eval_time=11249.091786, accumulated_logging_time=0.583239, accumulated_submission_time=15999.193946, global_step=45083, preemption_count=0, score=15999.193946, test/accuracy=0.676265, test/bleu=27.993240, test/loss=1.530478, test/num_examples=3003, total_duration=27250.306590, train/accuracy=0.650494, train/bleu=32.024769, train/loss=1.687964, validation/accuracy=0.665323, validation/bleu=28.198855, validation/loss=1.604354, validation/num_examples=3000
I0306 21:48:49.176828 140309584385792 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.31832876801490784, loss=1.790682315826416
I0306 21:51:46.347433 140309575993088 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.3579023778438568, loss=1.7795794010162354
I0306 21:54:43.453495 140309584385792 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.3244406580924988, loss=1.8371187448501587
I0306 21:57:40.710130 140309575993088 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.34465187788009644, loss=1.8375182151794434
I0306 22:00:21.299428 140479251105600 spec.py:321] Evaluating on the training split.
I0306 22:00:24.256541 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 22:03:38.986677 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 22:03:41.639942 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 22:06:33.609966 140479251105600 spec.py:349] Evaluating on the test split.
I0306 22:06:36.265513 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 22:09:04.241755 140479251105600 submission_runner.py:413] Time since start: 28613.27s, 	Step: 47455, 	{'train/accuracy': 0.6478411555290222, 'train/loss': 1.712612509727478, 'train/bleu': 32.04224538935785, 'validation/accuracy': 0.6665757298469543, 'validation/loss': 1.591001033782959, 'validation/bleu': 28.425250850587027, 'validation/num_examples': 3000, 'test/accuracy': 0.6780896186828613, 'test/loss': 1.5132921934127808, 'test/bleu': 27.913060390684564, 'test/num_examples': 3003, 'score': 16839.112189769745, 'total_duration': 28613.26828479767, 'accumulated_submission_time': 16839.112189769745, 'accumulated_eval_time': 11772.034071445465, 'accumulated_logging_time': 0.608893632888794}
I0306 22:09:04.259673 140309584385792 logging_writer.py:48] [47455] accumulated_eval_time=11772.034071, accumulated_logging_time=0.608894, accumulated_submission_time=16839.112190, global_step=47455, preemption_count=0, score=16839.112190, test/accuracy=0.678090, test/bleu=27.913060, test/loss=1.513292, test/num_examples=3003, total_duration=28613.268285, train/accuracy=0.647841, train/bleu=32.042245, train/loss=1.712613, validation/accuracy=0.666576, validation/bleu=28.425251, validation/loss=1.591001, validation/num_examples=3000
I0306 22:09:20.480773 140309575993088 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.33284494280815125, loss=1.8116638660430908
I0306 22:12:17.535771 140309584385792 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.327313095331192, loss=1.8376730680465698
I0306 22:15:14.765493 140309575993088 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.3285442590713501, loss=1.8032344579696655
I0306 22:18:11.809859 140309584385792 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.32303696870803833, loss=1.750587821006775
I0306 22:21:08.825285 140309575993088 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.3104236125946045, loss=1.8092037439346313
I0306 22:23:04.334902 140479251105600 spec.py:321] Evaluating on the training split.
I0306 22:23:07.288127 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 22:25:51.797849 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 22:25:54.477379 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 22:28:13.313184 140479251105600 spec.py:349] Evaluating on the test split.
I0306 22:28:15.992455 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 22:30:25.152402 140479251105600 submission_runner.py:413] Time since start: 29894.18s, 	Step: 49828, 	{'train/accuracy': 0.6558026075363159, 'train/loss': 1.6667882204055786, 'train/bleu': 32.43539331962923, 'validation/accuracy': 0.6666625142097473, 'validation/loss': 1.5765550136566162, 'validation/bleu': 28.982425479616044, 'validation/num_examples': 3000, 'test/accuracy': 0.6810412406921387, 'test/loss': 1.493761420249939, 'test/bleu': 28.460115633355112, 'test/num_examples': 3003, 'score': 17679.10280394554, 'total_duration': 29894.178914308548, 'accumulated_submission_time': 17679.10280394554, 'accumulated_eval_time': 12212.851514816284, 'accumulated_logging_time': 0.63606858253479}
I0306 22:30:25.172755 140309584385792 logging_writer.py:48] [49828] accumulated_eval_time=12212.851515, accumulated_logging_time=0.636069, accumulated_submission_time=17679.102804, global_step=49828, preemption_count=0, score=17679.102804, test/accuracy=0.681041, test/bleu=28.460116, test/loss=1.493761, test/num_examples=3003, total_duration=29894.178914, train/accuracy=0.655803, train/bleu=32.435393, train/loss=1.666788, validation/accuracy=0.666663, validation/bleu=28.982425, validation/loss=1.576555, validation/num_examples=3000
I0306 22:31:26.281410 140309575993088 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.3539482653141022, loss=1.7832984924316406
I0306 22:34:23.417030 140309584385792 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.3588986098766327, loss=1.7332861423492432
I0306 22:37:20.482726 140309575993088 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.3415563106536865, loss=1.7394421100616455
I0306 22:40:17.552115 140309584385792 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.32498645782470703, loss=1.7247591018676758
I0306 22:43:14.652507 140309575993088 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.3208390176296234, loss=1.7309554815292358
I0306 22:44:25.194881 140479251105600 spec.py:321] Evaluating on the training split.
I0306 22:44:28.147238 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 22:46:57.979646 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 22:47:00.648679 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 22:49:18.897441 140479251105600 spec.py:349] Evaluating on the test split.
I0306 22:49:21.580838 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 22:51:37.429674 140479251105600 submission_runner.py:413] Time since start: 31166.46s, 	Step: 52201, 	{'train/accuracy': 0.654120922088623, 'train/loss': 1.6606074571609497, 'train/bleu': 32.24839582043509, 'validation/accuracy': 0.667902410030365, 'validation/loss': 1.5687613487243652, 'validation/bleu': 28.613720812629968, 'validation/num_examples': 3000, 'test/accuracy': 0.6803556084632874, 'test/loss': 1.4891008138656616, 'test/bleu': 28.43649313404824, 'test/num_examples': 3003, 'score': 18519.04047203064, 'total_duration': 31166.456148386, 'accumulated_submission_time': 18519.04047203064, 'accumulated_eval_time': 12645.08620762825, 'accumulated_logging_time': 0.6676373481750488}
I0306 22:51:37.451285 140309584385792 logging_writer.py:48] [52201] accumulated_eval_time=12645.086208, accumulated_logging_time=0.667637, accumulated_submission_time=18519.040472, global_step=52201, preemption_count=0, score=18519.040472, test/accuracy=0.680356, test/bleu=28.436493, test/loss=1.489101, test/num_examples=3003, total_duration=31166.456148, train/accuracy=0.654121, train/bleu=32.248396, train/loss=1.660607, validation/accuracy=0.667902, validation/bleu=28.613721, validation/loss=1.568761, validation/num_examples=3000
I0306 22:53:23.533421 140309575993088 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.33781757950782776, loss=1.7944890260696411
I0306 22:56:20.670121 140309584385792 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.35715630650520325, loss=1.8455181121826172
I0306 22:59:17.787554 140309575993088 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.3360423743724823, loss=1.774147629737854
I0306 23:02:14.907243 140309584385792 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.31852373480796814, loss=1.7183159589767456
I0306 23:05:12.021636 140309575993088 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.31513890624046326, loss=1.6770246028900146
I0306 23:05:37.616913 140479251105600 spec.py:321] Evaluating on the training split.
I0306 23:05:40.566733 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 23:08:35.676768 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 23:08:38.330765 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 23:10:59.746326 140479251105600 spec.py:349] Evaluating on the test split.
I0306 23:11:02.406965 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 23:13:07.096166 140479251105600 submission_runner.py:413] Time since start: 32456.12s, 	Step: 54574, 	{'train/accuracy': 0.658129870891571, 'train/loss': 1.6468077898025513, 'train/bleu': 31.868311532831648, 'validation/accuracy': 0.6713742017745972, 'validation/loss': 1.5561473369598389, 'validation/bleu': 29.04187550718383, 'validation/num_examples': 3000, 'test/accuracy': 0.6837720274925232, 'test/loss': 1.4734342098236084, 'test/bleu': 28.643117462667007, 'test/num_examples': 3003, 'score': 19359.120665550232, 'total_duration': 32456.122641563416, 'accumulated_submission_time': 19359.120665550232, 'accumulated_eval_time': 13094.565346717834, 'accumulated_logging_time': 0.7001116275787354}
I0306 23:13:07.117765 140309584385792 logging_writer.py:48] [54574] accumulated_eval_time=13094.565347, accumulated_logging_time=0.700112, accumulated_submission_time=19359.120666, global_step=54574, preemption_count=0, score=19359.120666, test/accuracy=0.683772, test/bleu=28.643117, test/loss=1.473434, test/num_examples=3003, total_duration=32456.122642, train/accuracy=0.658130, train/bleu=31.868312, train/loss=1.646808, validation/accuracy=0.671374, validation/bleu=29.041876, validation/loss=1.556147, validation/num_examples=3000
I0306 23:15:38.345968 140309575993088 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.32110682129859924, loss=1.6654421091079712
I0306 23:18:35.460277 140309584385792 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.3160155415534973, loss=1.7083956003189087
I0306 23:21:32.505640 140309575993088 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.3252699375152588, loss=1.780690312385559
I0306 23:24:29.615191 140309584385792 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.31464385986328125, loss=1.65984308719635
I0306 23:27:07.350013 140479251105600 spec.py:321] Evaluating on the training split.
I0306 23:27:10.303709 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 23:30:25.744680 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 23:30:28.397772 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 23:33:01.566037 140479251105600 spec.py:349] Evaluating on the test split.
I0306 23:33:04.230634 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 23:35:48.333336 140479251105600 submission_runner.py:413] Time since start: 33817.36s, 	Step: 56947, 	{'train/accuracy': 0.6687179207801819, 'train/loss': 1.5735982656478882, 'train/bleu': 33.15393052251328, 'validation/accuracy': 0.6729984879493713, 'validation/loss': 1.5422427654266357, 'validation/bleu': 29.182041544121475, 'validation/num_examples': 3000, 'test/accuracy': 0.6852943301200867, 'test/loss': 1.4579284191131592, 'test/bleu': 28.667188622555706, 'test/num_examples': 3003, 'score': 20199.2664103508, 'total_duration': 33817.35985803604, 'accumulated_submission_time': 20199.2664103508, 'accumulated_eval_time': 13615.548609256744, 'accumulated_logging_time': 0.7321228981018066}
I0306 23:35:48.351773 140309575993088 logging_writer.py:48] [56947] accumulated_eval_time=13615.548609, accumulated_logging_time=0.732123, accumulated_submission_time=20199.266410, global_step=56947, preemption_count=0, score=20199.266410, test/accuracy=0.685294, test/bleu=28.667189, test/loss=1.457928, test/num_examples=3003, total_duration=33817.359858, train/accuracy=0.668718, train/bleu=33.153931, train/loss=1.573598, validation/accuracy=0.672998, validation/bleu=29.182042, validation/loss=1.542243, validation/num_examples=3000
I0306 23:36:07.378596 140309584385792 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.32303154468536377, loss=1.8032450675964355
I0306 23:39:04.335057 140309575993088 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.3152700960636139, loss=1.7173653841018677
I0306 23:42:01.554307 140309584385792 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.3278312385082245, loss=1.6744803190231323
I0306 23:44:58.576073 140309575993088 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.32635387778282166, loss=1.825263500213623
I0306 23:47:55.635015 140309584385792 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.3164329528808594, loss=1.7288309335708618
I0306 23:49:48.389451 140479251105600 spec.py:321] Evaluating on the training split.
I0306 23:49:51.352443 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 23:52:47.989203 140479251105600 spec.py:333] Evaluating on the validation split.
I0306 23:52:50.645946 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 23:55:25.807492 140479251105600 spec.py:349] Evaluating on the test split.
I0306 23:55:28.466459 140479251105600 workload.py:181] Translating evaluation dataset.
I0306 23:58:04.781197 140479251105600 submission_runner.py:413] Time since start: 35153.81s, 	Step: 59320, 	{'train/accuracy': 0.661736011505127, 'train/loss': 1.6122812032699585, 'train/bleu': 32.59593845161208, 'validation/accuracy': 0.6744615435600281, 'validation/loss': 1.5311570167541504, 'validation/bleu': 29.223824721728594, 'validation/num_examples': 3000, 'test/accuracy': 0.6868746876716614, 'test/loss': 1.4448662996292114, 'test/bleu': 29.00632413735738, 'test/num_examples': 3003, 'score': 21039.218721151352, 'total_duration': 35153.80774140358, 'accumulated_submission_time': 21039.218721151352, 'accumulated_eval_time': 14111.940336704254, 'accumulated_logging_time': 0.7595162391662598}
I0306 23:58:04.798732 140309575993088 logging_writer.py:48] [59320] accumulated_eval_time=14111.940337, accumulated_logging_time=0.759516, accumulated_submission_time=21039.218721, global_step=59320, preemption_count=0, score=21039.218721, test/accuracy=0.686875, test/bleu=29.006324, test/loss=1.444866, test/num_examples=3003, total_duration=35153.807741, train/accuracy=0.661736, train/bleu=32.595938, train/loss=1.612281, validation/accuracy=0.674462, validation/bleu=29.223825, validation/loss=1.531157, validation/num_examples=3000
I0306 23:59:08.710330 140309584385792 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.32784342765808105, loss=1.818208932876587
I0307 00:02:05.857186 140309575993088 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.3435942530632019, loss=1.8082817792892456
I0307 00:05:03.004531 140309584385792 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.3150452971458435, loss=1.704044222831726
I0307 00:08:00.221254 140309575993088 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.3247474133968353, loss=1.706058382987976
I0307 00:10:57.400939 140309584385792 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.3183252513408661, loss=1.6154170036315918
I0307 00:12:04.795084 140479251105600 spec.py:321] Evaluating on the training split.
I0307 00:12:07.740564 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 00:14:37.931420 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 00:14:40.595962 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 00:17:01.316387 140479251105600 spec.py:349] Evaluating on the test split.
I0307 00:17:03.977224 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 00:19:17.126570 140479251105600 submission_runner.py:413] Time since start: 36426.15s, 	Step: 61692, 	{'train/accuracy': 0.6636953949928284, 'train/loss': 1.6126148700714111, 'train/bleu': 32.661907874709684, 'validation/accuracy': 0.6747963428497314, 'validation/loss': 1.5258315801620483, 'validation/bleu': 29.24441031085027, 'validation/num_examples': 3000, 'test/accuracy': 0.6882110238075256, 'test/loss': 1.4359838962554932, 'test/bleu': 29.06128226487907, 'test/num_examples': 3003, 'score': 21879.126792669296, 'total_duration': 36426.15311074257, 'accumulated_submission_time': 21879.126792669296, 'accumulated_eval_time': 14544.27177977562, 'accumulated_logging_time': 0.786595344543457}
I0307 00:19:17.144541 140309575993088 logging_writer.py:48] [61692] accumulated_eval_time=14544.271780, accumulated_logging_time=0.786595, accumulated_submission_time=21879.126793, global_step=61692, preemption_count=0, score=21879.126793, test/accuracy=0.688211, test/bleu=29.061282, test/loss=1.435984, test/num_examples=3003, total_duration=36426.153111, train/accuracy=0.663695, train/bleu=32.661908, train/loss=1.612615, validation/accuracy=0.674796, validation/bleu=29.244410, validation/loss=1.525832, validation/num_examples=3000
I0307 00:21:06.415891 140309584385792 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.3247671127319336, loss=1.7815625667572021
I0307 00:24:03.563967 140309575993088 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.3174595534801483, loss=1.6814815998077393
I0307 00:27:00.721299 140309584385792 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.3573123812675476, loss=1.744907259941101
I0307 00:29:57.847738 140309575993088 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.3188588619232178, loss=1.7803473472595215
I0307 00:32:54.971933 140309584385792 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.316617876291275, loss=1.6730901002883911
I0307 00:33:17.388268 140479251105600 spec.py:321] Evaluating on the training split.
I0307 00:33:20.341190 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 00:36:19.289612 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 00:36:21.951651 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 00:38:50.450860 140479251105600 spec.py:349] Evaluating on the test split.
I0307 00:38:53.110882 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 00:41:13.295505 140479251105600 submission_runner.py:413] Time since start: 37742.32s, 	Step: 64065, 	{'train/accuracy': 0.6689755320549011, 'train/loss': 1.560241937637329, 'train/bleu': 33.6577532470055, 'validation/accuracy': 0.6772513389587402, 'validation/loss': 1.5159430503845215, 'validation/bleu': 29.241512861505495, 'validation/num_examples': 3000, 'test/accuracy': 0.6905002593994141, 'test/loss': 1.430458903312683, 'test/bleu': 29.194278802519776, 'test/num_examples': 3003, 'score': 22719.286415100098, 'total_duration': 37742.32204914093, 'accumulated_submission_time': 22719.286415100098, 'accumulated_eval_time': 15020.178970813751, 'accumulated_logging_time': 0.8134200572967529}
I0307 00:41:13.313410 140309575993088 logging_writer.py:48] [64065] accumulated_eval_time=15020.178971, accumulated_logging_time=0.813420, accumulated_submission_time=22719.286415, global_step=64065, preemption_count=0, score=22719.286415, test/accuracy=0.690500, test/bleu=29.194279, test/loss=1.430459, test/num_examples=3003, total_duration=37742.322049, train/accuracy=0.668976, train/bleu=33.657753, train/loss=1.560242, validation/accuracy=0.677251, validation/bleu=29.241513, validation/loss=1.515943, validation/num_examples=3000
I0307 00:43:47.451002 140309584385792 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.3151841163635254, loss=1.6471728086471558
I0307 00:46:44.508440 140309575993088 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.317330002784729, loss=1.756293773651123
I0307 00:49:41.775989 140309584385792 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.3154086768627167, loss=1.6288785934448242
I0307 00:52:38.785669 140309575993088 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.30987995862960815, loss=1.5835288763046265
I0307 00:55:13.649013 140479251105600 spec.py:321] Evaluating on the training split.
I0307 00:55:16.600674 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 00:58:48.831339 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 00:58:51.488139 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 01:01:51.206077 140479251105600 spec.py:349] Evaluating on the test split.
I0307 01:01:53.871170 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 01:04:43.933821 140479251105600 submission_runner.py:413] Time since start: 39152.96s, 	Step: 66439, 	{'train/accuracy': 0.6681381464004517, 'train/loss': 1.5711647272109985, 'train/bleu': 32.870854886084715, 'validation/accuracy': 0.6773505806922913, 'validation/loss': 1.5075639486312866, 'validation/bleu': 29.533497605061257, 'validation/num_examples': 3000, 'test/accuracy': 0.6923828125, 'test/loss': 1.4150910377502441, 'test/bleu': 29.547095741952447, 'test/num_examples': 3003, 'score': 23559.536057949066, 'total_duration': 39152.96033978462, 'accumulated_submission_time': 23559.536057949066, 'accumulated_eval_time': 15590.463738679886, 'accumulated_logging_time': 0.8402137756347656}
I0307 01:04:43.953520 140309584385792 logging_writer.py:48] [66439] accumulated_eval_time=15590.463739, accumulated_logging_time=0.840214, accumulated_submission_time=23559.536058, global_step=66439, preemption_count=0, score=23559.536058, test/accuracy=0.692383, test/bleu=29.547096, test/loss=1.415091, test/num_examples=3003, total_duration=39152.960340, train/accuracy=0.668138, train/bleu=32.870855, train/loss=1.571165, validation/accuracy=0.677351, validation/bleu=29.533498, validation/loss=1.507564, validation/num_examples=3000
I0307 01:05:05.828755 140309575993088 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.3219011425971985, loss=1.640763759613037
I0307 01:08:02.990077 140309584385792 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.31286901235580444, loss=1.6961641311645508
I0307 01:11:00.194470 140309575993088 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.3167341649532318, loss=1.651058316230774
I0307 01:13:57.247498 140309584385792 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.31153222918510437, loss=1.6256084442138672
I0307 01:16:54.352376 140309575993088 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.3260321319103241, loss=1.6834595203399658
I0307 01:18:44.233566 140479251105600 spec.py:321] Evaluating on the training split.
I0307 01:18:47.182278 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 01:21:59.394796 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 01:22:02.056896 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 01:24:11.873479 140479251105600 spec.py:349] Evaluating on the test split.
I0307 01:24:14.553889 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 01:26:32.907353 140479251105600 submission_runner.py:413] Time since start: 40461.93s, 	Step: 68812, 	{'train/accuracy': 0.6660295128822327, 'train/loss': 1.5897455215454102, 'train/bleu': 33.2229564491977, 'validation/accuracy': 0.6795203685760498, 'validation/loss': 1.4992176294326782, 'validation/bleu': 29.4938680018353, 'validation/num_examples': 3000, 'test/accuracy': 0.693428635597229, 'test/loss': 1.4072777032852173, 'test/bleu': 29.36758891332158, 'test/num_examples': 3003, 'score': 24399.730229377747, 'total_duration': 40461.933876514435, 'accumulated_submission_time': 24399.730229377747, 'accumulated_eval_time': 16059.137459516525, 'accumulated_logging_time': 0.8695645332336426}
I0307 01:26:32.927633 140309584385792 logging_writer.py:48] [68812] accumulated_eval_time=16059.137460, accumulated_logging_time=0.869565, accumulated_submission_time=24399.730229, global_step=68812, preemption_count=0, score=24399.730229, test/accuracy=0.693429, test/bleu=29.367589, test/loss=1.407278, test/num_examples=3003, total_duration=40461.933877, train/accuracy=0.666030, train/bleu=33.222956, train/loss=1.589746, validation/accuracy=0.679520, validation/bleu=29.493868, validation/loss=1.499218, validation/num_examples=3000
I0307 01:27:39.687903 140309575993088 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.3184095621109009, loss=1.6762205362319946
I0307 01:30:36.888513 140309584385792 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.31907904148101807, loss=1.6923617124557495
I0307 01:33:34.021413 140309575993088 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.3208447992801666, loss=1.7786316871643066
I0307 01:36:31.122051 140309584385792 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.31351473927497864, loss=1.6256266832351685
I0307 01:39:28.304822 140309575993088 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.315652072429657, loss=1.6598352193832397
I0307 01:40:33.202882 140479251105600 spec.py:321] Evaluating on the training split.
I0307 01:40:36.146438 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 01:44:16.122778 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 01:44:18.777856 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 01:47:16.975533 140479251105600 spec.py:349] Evaluating on the test split.
I0307 01:47:19.627970 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 01:50:23.766164 140479251105600 submission_runner.py:413] Time since start: 41892.79s, 	Step: 71185, 	{'train/accuracy': 0.6706438660621643, 'train/loss': 1.548822283744812, 'train/bleu': 33.63335498095293, 'validation/accuracy': 0.6798427700996399, 'validation/loss': 1.4899640083312988, 'validation/bleu': 29.584248418353056, 'validation/num_examples': 3000, 'test/accuracy': 0.6949160695075989, 'test/loss': 1.400790810585022, 'test/bleu': 29.551375837013723, 'test/num_examples': 3003, 'score': 25239.921297311783, 'total_duration': 41892.79270076752, 'accumulated_submission_time': 25239.921297311783, 'accumulated_eval_time': 16649.700699329376, 'accumulated_logging_time': 0.8990745544433594}
I0307 01:50:23.784502 140309584385792 logging_writer.py:48] [71185] accumulated_eval_time=16649.700699, accumulated_logging_time=0.899075, accumulated_submission_time=25239.921297, global_step=71185, preemption_count=0, score=25239.921297, test/accuracy=0.694916, test/bleu=29.551376, test/loss=1.400791, test/num_examples=3003, total_duration=41892.792701, train/accuracy=0.670644, train/bleu=33.633355, train/loss=1.548822, validation/accuracy=0.679843, validation/bleu=29.584248, validation/loss=1.489964, validation/num_examples=3000
I0307 01:52:15.462647 140309575993088 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.31168079376220703, loss=1.5890018939971924
I0307 01:55:12.733656 140309584385792 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.32137686014175415, loss=1.683042049407959
I0307 01:58:09.816936 140309575993088 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.3053876757621765, loss=1.6300814151763916
I0307 02:01:06.822672 140309584385792 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.3127739727497101, loss=1.602814793586731
I0307 02:04:03.956912 140309575993088 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.32725241780281067, loss=1.6195095777511597
I0307 02:04:23.884394 140479251105600 spec.py:321] Evaluating on the training split.
I0307 02:04:26.836751 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 02:07:33.413827 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 02:07:36.061020 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 02:10:20.673067 140479251105600 spec.py:349] Evaluating on the test split.
I0307 02:10:23.331839 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 02:12:48.328384 140479251105600 submission_runner.py:413] Time since start: 43237.35s, 	Step: 73558, 	{'train/accuracy': 0.6686922311782837, 'train/loss': 1.5636966228485107, 'train/bleu': 33.61162310909176, 'validation/accuracy': 0.6811322569847107, 'validation/loss': 1.487377643585205, 'validation/bleu': 29.614756219300876, 'validation/num_examples': 3000, 'test/accuracy': 0.6956132650375366, 'test/loss': 1.3960987329483032, 'test/bleu': 29.679354624505628, 'test/num_examples': 3003, 'score': 26079.9373755455, 'total_duration': 43237.35492491722, 'accumulated_submission_time': 26079.9373755455, 'accumulated_eval_time': 17154.144641160965, 'accumulated_logging_time': 0.9275655746459961}
I0307 02:12:48.346662 140309584385792 logging_writer.py:48] [73558] accumulated_eval_time=17154.144641, accumulated_logging_time=0.927566, accumulated_submission_time=26079.937376, global_step=73558, preemption_count=0, score=26079.937376, test/accuracy=0.695613, test/bleu=29.679355, test/loss=1.396099, test/num_examples=3003, total_duration=43237.354925, train/accuracy=0.668692, train/bleu=33.611623, train/loss=1.563697, validation/accuracy=0.681132, validation/bleu=29.614756, validation/loss=1.487378, validation/num_examples=3000
I0307 02:15:25.086718 140309575993088 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.314877986907959, loss=1.6173666715621948
I0307 02:18:22.114516 140309584385792 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.3213317096233368, loss=1.6392759084701538
I0307 02:21:19.267048 140309575993088 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.3114393353462219, loss=1.5560617446899414
I0307 02:24:16.581303 140309584385792 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.3181919455528259, loss=1.6705670356750488
I0307 02:26:48.609175 140479251105600 spec.py:321] Evaluating on the training split.
I0307 02:26:51.561078 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 02:30:38.853025 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 02:30:41.504678 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 02:33:44.158931 140479251105600 spec.py:349] Evaluating on the test split.
I0307 02:33:46.820581 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 02:36:57.907940 140479251105600 submission_runner.py:413] Time since start: 44686.93s, 	Step: 75931, 	{'train/accuracy': 0.6778443455696106, 'train/loss': 1.5113569498062134, 'train/bleu': 33.68908874111985, 'validation/accuracy': 0.6824589967727661, 'validation/loss': 1.4826048612594604, 'validation/bleu': 29.80243295526676, 'validation/num_examples': 3000, 'test/accuracy': 0.6972866654396057, 'test/loss': 1.387231707572937, 'test/bleu': 29.684434673409008, 'test/num_examples': 3003, 'score': 26920.11353945732, 'total_duration': 44686.934475660324, 'accumulated_submission_time': 26920.11353945732, 'accumulated_eval_time': 17763.443356752396, 'accumulated_logging_time': 0.9555902481079102}
I0307 02:36:57.926775 140309575993088 logging_writer.py:48] [75931] accumulated_eval_time=17763.443357, accumulated_logging_time=0.955590, accumulated_submission_time=26920.113539, global_step=75931, preemption_count=0, score=26920.113539, test/accuracy=0.697287, test/bleu=29.684435, test/loss=1.387232, test/num_examples=3003, total_duration=44686.934476, train/accuracy=0.677844, train/bleu=33.689089, train/loss=1.511357, validation/accuracy=0.682459, validation/bleu=29.802433, validation/loss=1.482605, validation/num_examples=3000
I0307 02:37:22.626688 140309584385792 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.30791616439819336, loss=1.6561193466186523
I0307 02:40:19.594382 140309575993088 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.3238961398601532, loss=1.6473009586334229
I0307 02:43:16.755481 140309584385792 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.31173548102378845, loss=1.667137861251831
I0307 02:46:13.927244 140309575993088 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.30657336115837097, loss=1.5407966375350952
I0307 02:49:10.991528 140309584385792 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.3148750066757202, loss=1.66209876537323
I0307 02:50:58.032077 140479251105600 spec.py:321] Evaluating on the training split.
I0307 02:51:00.996206 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 02:54:30.676076 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 02:54:33.332348 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 02:57:09.289937 140479251105600 spec.py:349] Evaluating on the test split.
I0307 02:57:11.955449 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 02:59:55.318232 140479251105600 submission_runner.py:413] Time since start: 46064.34s, 	Step: 78304, 	{'train/accuracy': 0.6727930307388306, 'train/loss': 1.5448284149169922, 'train/bleu': 33.363913091301995, 'validation/accuracy': 0.6829673647880554, 'validation/loss': 1.4761617183685303, 'validation/bleu': 29.7455915346154, 'validation/num_examples': 3000, 'test/accuracy': 0.696694016456604, 'test/loss': 1.3837041854858398, 'test/bleu': 29.876523596683413, 'test/num_examples': 3003, 'score': 27760.13487482071, 'total_duration': 46064.34475827217, 'accumulated_submission_time': 27760.13487482071, 'accumulated_eval_time': 18300.72947216034, 'accumulated_logging_time': 0.9833755493164062}
I0307 02:59:55.336513 140309575993088 logging_writer.py:48] [78304] accumulated_eval_time=18300.729472, accumulated_logging_time=0.983376, accumulated_submission_time=27760.134875, global_step=78304, preemption_count=0, score=27760.134875, test/accuracy=0.696694, test/bleu=29.876524, test/loss=1.383704, test/num_examples=3003, total_duration=46064.344758, train/accuracy=0.672793, train/bleu=33.363913, train/loss=1.544828, validation/accuracy=0.682967, validation/bleu=29.745592, validation/loss=1.476162, validation/num_examples=3000
I0307 03:01:04.984949 140309584385792 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.3123401999473572, loss=1.6222591400146484
I0307 03:04:02.144195 140309575993088 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.31414589285850525, loss=1.5957518815994263
I0307 03:06:59.280738 140309584385792 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.3137916922569275, loss=1.592552661895752
I0307 03:09:56.501470 140309575993088 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.3263888657093048, loss=1.5972579717636108
I0307 03:12:53.764444 140309584385792 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.31890127062797546, loss=1.5388703346252441
I0307 03:13:55.505070 140479251105600 spec.py:321] Evaluating on the training split.
I0307 03:13:58.463271 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 03:17:17.695244 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 03:17:20.358574 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 03:19:58.682045 140479251105600 spec.py:349] Evaluating on the test split.
I0307 03:20:01.338067 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 03:22:37.228481 140479251105600 submission_runner.py:413] Time since start: 47426.26s, 	Step: 80676, 	{'train/accuracy': 0.6747324466705322, 'train/loss': 1.5321621894836426, 'train/bleu': 33.81755577223068, 'validation/accuracy': 0.683078944683075, 'validation/loss': 1.474095106124878, 'validation/bleu': 29.818120434164623, 'validation/num_examples': 3000, 'test/accuracy': 0.6976119875907898, 'test/loss': 1.3782261610031128, 'test/bleu': 30.000250726081184, 'test/num_examples': 3003, 'score': 28600.219145298004, 'total_duration': 47426.25502705574, 'accumulated_submission_time': 28600.219145298004, 'accumulated_eval_time': 18822.452847719193, 'accumulated_logging_time': 1.0107650756835938}
I0307 03:22:37.247223 140309575993088 logging_writer.py:48] [80676] accumulated_eval_time=18822.452848, accumulated_logging_time=1.010765, accumulated_submission_time=28600.219145, global_step=80676, preemption_count=0, score=28600.219145, test/accuracy=0.697612, test/bleu=30.000251, test/loss=1.378226, test/num_examples=3003, total_duration=47426.255027, train/accuracy=0.674732, train/bleu=33.817556, train/loss=1.532162, validation/accuracy=0.683079, validation/bleu=29.818120, validation/loss=1.474095, validation/num_examples=3000
I0307 03:24:32.179282 140309584385792 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.30893340706825256, loss=1.5477744340896606
I0307 03:27:29.383425 140309575993088 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.3140519857406616, loss=1.6298619508743286
I0307 03:30:26.627009 140309584385792 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.3089870512485504, loss=1.5926564931869507
I0307 03:33:23.819505 140309575993088 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.3239552974700928, loss=1.5717151165008545
I0307 03:36:20.951616 140309584385792 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.32214948534965515, loss=1.5990607738494873
I0307 03:36:37.320352 140479251105600 spec.py:321] Evaluating on the training split.
I0307 03:36:40.286317 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 03:39:49.612105 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 03:39:52.262392 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 03:42:39.571693 140479251105600 spec.py:349] Evaluating on the test split.
I0307 03:42:42.230047 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 03:45:15.250267 140479251105600 submission_runner.py:413] Time since start: 48784.28s, 	Step: 83048, 	{'train/accuracy': 0.6766062378883362, 'train/loss': 1.5232235193252563, 'train/bleu': 34.390865962752926, 'validation/accuracy': 0.6842568516731262, 'validation/loss': 1.4705488681793213, 'validation/bleu': 29.83519080667452, 'validation/num_examples': 3000, 'test/accuracy': 0.6981349587440491, 'test/loss': 1.374583125114441, 'test/bleu': 29.859794249510102, 'test/num_examples': 3003, 'score': 29440.208112716675, 'total_duration': 48784.276811122894, 'accumulated_submission_time': 29440.208112716675, 'accumulated_eval_time': 19340.382719755173, 'accumulated_logging_time': 1.038370132446289}
I0307 03:45:15.268738 140309575993088 logging_writer.py:48] [83048] accumulated_eval_time=19340.382720, accumulated_logging_time=1.038370, accumulated_submission_time=29440.208113, global_step=83048, preemption_count=0, score=29440.208113, test/accuracy=0.698135, test/bleu=29.859794, test/loss=1.374583, test/num_examples=3003, total_duration=48784.276811, train/accuracy=0.676606, train/bleu=34.390866, train/loss=1.523224, validation/accuracy=0.684257, validation/bleu=29.835191, validation/loss=1.470549, validation/num_examples=3000
I0307 03:47:55.799483 140309584385792 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.32050836086273193, loss=1.6229897737503052
I0307 03:50:53.082849 140309575993088 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.3096342980861664, loss=1.6129652261734009
I0307 03:53:50.326190 140309584385792 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.31486833095550537, loss=1.627374529838562
I0307 03:56:47.429802 140309575993088 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.320308655500412, loss=1.5595591068267822
I0307 03:59:15.266753 140479251105600 spec.py:321] Evaluating on the training split.
I0307 03:59:18.215755 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 04:02:26.788501 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 04:02:29.444868 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 04:05:23.682130 140479251105600 spec.py:349] Evaluating on the test split.
I0307 04:05:26.342798 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 04:08:12.967193 140479251105600 submission_runner.py:413] Time since start: 50161.99s, 	Step: 85419, 	{'train/accuracy': 0.6752600073814392, 'train/loss': 1.518908143043518, 'train/bleu': 34.22081710561856, 'validation/accuracy': 0.6839096546173096, 'validation/loss': 1.466374397277832, 'validation/bleu': 29.964729017171297, 'validation/num_examples': 3000, 'test/accuracy': 0.6996572017669678, 'test/loss': 1.369691014289856, 'test/bleu': 30.03802438606821, 'test/num_examples': 3003, 'score': 30280.11798620224, 'total_duration': 50161.99370551109, 'accumulated_submission_time': 30280.11798620224, 'accumulated_eval_time': 19878.083097696304, 'accumulated_logging_time': 1.0668854713439941}
I0307 04:08:12.985861 140309584385792 logging_writer.py:48] [85419] accumulated_eval_time=19878.083098, accumulated_logging_time=1.066885, accumulated_submission_time=30280.117986, global_step=85419, preemption_count=0, score=30280.117986, test/accuracy=0.699657, test/bleu=30.038024, test/loss=1.369691, test/num_examples=3003, total_duration=50161.993706, train/accuracy=0.675260, train/bleu=34.220817, train/loss=1.518908, validation/accuracy=0.683910, validation/bleu=29.964729, validation/loss=1.466374, validation/num_examples=3000
I0307 04:08:41.937011 140309575993088 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.31659725308418274, loss=1.6520882844924927
I0307 04:11:39.074160 140309584385792 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.31345126032829285, loss=1.6067500114440918
I0307 04:14:36.271541 140309575993088 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.3205452263355255, loss=1.5975373983383179
I0307 04:17:33.406867 140309584385792 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.31892406940460205, loss=1.6163254976272583
I0307 04:20:30.585440 140309575993088 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.30993393063545227, loss=1.5193102359771729
I0307 04:22:13.082117 140479251105600 spec.py:321] Evaluating on the training split.
I0307 04:22:16.034126 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 04:25:30.705363 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 04:25:33.347725 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 04:28:03.989190 140479251105600 spec.py:349] Evaluating on the test split.
I0307 04:28:06.652696 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 04:30:33.605561 140479251105600 submission_runner.py:413] Time since start: 51502.63s, 	Step: 87791, 	{'train/accuracy': 0.6782928109169006, 'train/loss': 1.506111741065979, 'train/bleu': 34.47983844493076, 'validation/accuracy': 0.6847032308578491, 'validation/loss': 1.4656473398208618, 'validation/bleu': 29.90656084595846, 'validation/num_examples': 3000, 'test/accuracy': 0.7003660798072815, 'test/loss': 1.368316411972046, 'test/bleu': 30.115225070781918, 'test/num_examples': 3003, 'score': 31120.12797641754, 'total_duration': 51502.63209462166, 'accumulated_submission_time': 31120.12797641754, 'accumulated_eval_time': 20378.60650396347, 'accumulated_logging_time': 1.095641851425171}
I0307 04:30:33.624455 140309584385792 logging_writer.py:48] [87791] accumulated_eval_time=20378.606504, accumulated_logging_time=1.095642, accumulated_submission_time=31120.127976, global_step=87791, preemption_count=0, score=31120.127976, test/accuracy=0.700366, test/bleu=30.115225, test/loss=1.368316, test/num_examples=3003, total_duration=51502.632095, train/accuracy=0.678293, train/bleu=34.479838, train/loss=1.506112, validation/accuracy=0.684703, validation/bleu=29.906561, validation/loss=1.465647, validation/num_examples=3000
I0307 04:31:47.845113 140309575993088 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.32411712408065796, loss=1.6203542947769165
I0307 04:34:44.996885 140309584385792 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.3118041157722473, loss=1.602889895439148
I0307 04:37:42.136669 140309575993088 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.30889251828193665, loss=1.5530833005905151
I0307 04:40:39.267477 140309584385792 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.31563442945480347, loss=1.571964979171753
I0307 04:43:36.447918 140309575993088 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.31588688492774963, loss=1.6211953163146973
I0307 04:44:33.939365 140479251105600 spec.py:321] Evaluating on the training split.
I0307 04:44:36.901084 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 04:47:59.300199 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 04:48:01.961984 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 04:50:37.501342 140479251105600 spec.py:349] Evaluating on the test split.
I0307 04:50:40.163463 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 04:53:37.038318 140479251105600 submission_runner.py:413] Time since start: 52886.06s, 	Step: 90164, 	{'train/accuracy': 0.6785022616386414, 'train/loss': 1.500885009765625, 'train/bleu': 34.0069950951964, 'validation/accuracy': 0.684566855430603, 'validation/loss': 1.4633512496948242, 'validation/bleu': 30.077972365339207, 'validation/num_examples': 3000, 'test/accuracy': 0.7001801133155823, 'test/loss': 1.3668028116226196, 'test/bleu': 30.131782785340032, 'test/num_examples': 3003, 'score': 31960.361180067062, 'total_duration': 52886.064812898636, 'accumulated_submission_time': 31960.361180067062, 'accumulated_eval_time': 20921.705367088318, 'accumulated_logging_time': 1.1235229969024658}
I0307 04:53:37.060480 140309584385792 logging_writer.py:48] [90164] accumulated_eval_time=20921.705367, accumulated_logging_time=1.123523, accumulated_submission_time=31960.361180, global_step=90164, preemption_count=0, score=31960.361180, test/accuracy=0.700180, test/bleu=30.131783, test/loss=1.366803, test/num_examples=3003, total_duration=52886.064813, train/accuracy=0.678502, train/bleu=34.006995, train/loss=1.500885, validation/accuracy=0.684567, validation/bleu=30.077972, validation/loss=1.463351, validation/num_examples=3000
I0307 04:55:36.213135 140309575993088 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.3152606785297394, loss=1.6101808547973633
I0307 04:58:33.286965 140309584385792 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.3174334466457367, loss=1.634118914604187
I0307 05:01:30.387783 140309575993088 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.3182571828365326, loss=1.5934722423553467
I0307 05:04:27.558689 140309584385792 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.32581257820129395, loss=1.6343681812286377
I0307 05:07:24.861705 140309575993088 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.32017695903778076, loss=1.640524983406067
I0307 05:07:37.336335 140479251105600 spec.py:321] Evaluating on the training split.
I0307 05:07:40.289378 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 05:11:02.728300 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 05:11:05.391051 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 05:13:45.976474 140479251105600 spec.py:349] Evaluating on the test split.
I0307 05:13:48.653367 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 05:16:40.236663 140479251105600 submission_runner.py:413] Time since start: 54269.26s, 	Step: 92537, 	{'train/accuracy': 0.6780723333358765, 'train/loss': 1.508441686630249, 'train/bleu': 34.27605743081869, 'validation/accuracy': 0.6850255727767944, 'validation/loss': 1.4646774530410767, 'validation/bleu': 30.021923551313247, 'validation/num_examples': 3000, 'test/accuracy': 0.6999012231826782, 'test/loss': 1.3655339479446411, 'test/bleu': 30.015570893508283, 'test/num_examples': 3003, 'score': 32800.55123567581, 'total_duration': 54269.263169288635, 'accumulated_submission_time': 32800.55123567581, 'accumulated_eval_time': 21464.605616807938, 'accumulated_logging_time': 1.155937671661377}
I0307 05:16:40.255875 140309584385792 logging_writer.py:48] [92537] accumulated_eval_time=21464.605617, accumulated_logging_time=1.155938, accumulated_submission_time=32800.551236, global_step=92537, preemption_count=0, score=32800.551236, test/accuracy=0.699901, test/bleu=30.015571, test/loss=1.365534, test/num_examples=3003, total_duration=54269.263169, train/accuracy=0.678072, train/bleu=34.276057, train/loss=1.508442, validation/accuracy=0.685026, validation/bleu=30.021924, validation/loss=1.464677, validation/num_examples=3000
I0307 05:19:24.393518 140309575993088 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.3161085546016693, loss=1.5759910345077515
I0307 05:22:21.446808 140309584385792 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.31473588943481445, loss=1.5680009126663208
I0307 05:25:18.649601 140309575993088 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.3138124346733093, loss=1.60970938205719
I0307 05:28:15.782503 140309584385792 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.3115036189556122, loss=1.5963670015335083
I0307 05:30:40.382613 140479251105600 spec.py:321] Evaluating on the training split.
I0307 05:30:43.332592 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 05:34:04.623858 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 05:34:07.284315 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 05:36:36.201296 140479251105600 spec.py:349] Evaluating on the test split.
I0307 05:36:38.858003 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 05:39:33.345512 140479251105600 submission_runner.py:413] Time since start: 55642.37s, 	Step: 94910, 	{'train/accuracy': 0.6803591251373291, 'train/loss': 1.4964025020599365, 'train/bleu': 34.10109996382132, 'validation/accuracy': 0.6847900152206421, 'validation/loss': 1.4636088609695435, 'validation/bleu': 29.947799046860744, 'validation/num_examples': 3000, 'test/accuracy': 0.7000290751457214, 'test/loss': 1.3650816679000854, 'test/bleu': 29.967965550167904, 'test/num_examples': 3003, 'score': 33640.595079422, 'total_duration': 55642.37203788757, 'accumulated_submission_time': 33640.595079422, 'accumulated_eval_time': 21997.568466424942, 'accumulated_logging_time': 1.1839983463287354}
I0307 05:39:33.365328 140309575993088 logging_writer.py:48] [94910] accumulated_eval_time=21997.568466, accumulated_logging_time=1.183998, accumulated_submission_time=33640.595079, global_step=94910, preemption_count=0, score=33640.595079, test/accuracy=0.700029, test/bleu=29.967966, test/loss=1.365082, test/num_examples=3003, total_duration=55642.372038, train/accuracy=0.680359, train/bleu=34.101100, train/loss=1.496403, validation/accuracy=0.684790, validation/bleu=29.947799, validation/loss=1.463609, validation/num_examples=3000
I0307 05:40:05.486865 140309584385792 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.3171844482421875, loss=1.685557246208191
I0307 05:43:02.443491 140309575993088 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.31193482875823975, loss=1.5428423881530762
I0307 05:45:59.509363 140309584385792 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.3010806441307068, loss=1.531423568725586
I0307 05:48:56.644323 140309575993088 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.3143003284931183, loss=1.5878081321716309
I0307 05:51:53.715259 140309584385792 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.3070593774318695, loss=1.5771656036376953
I0307 05:53:33.691711 140479251105600 spec.py:321] Evaluating on the training split.
I0307 05:53:36.656300 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 05:56:55.539833 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 05:56:58.186339 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 05:59:28.315779 140479251105600 spec.py:349] Evaluating on the test split.
I0307 05:59:30.976812 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 06:02:10.488256 140479251105600 submission_runner.py:413] Time since start: 56999.51s, 	Step: 97284, 	{'train/accuracy': 0.6767615079879761, 'train/loss': 1.5081969499588013, 'train/bleu': 34.15721278833713, 'validation/accuracy': 0.6848148107528687, 'validation/loss': 1.4631707668304443, 'validation/bleu': 29.917024016626115, 'validation/num_examples': 3000, 'test/accuracy': 0.7001336812973022, 'test/loss': 1.3649258613586426, 'test/bleu': 30.09773101381347, 'test/num_examples': 3003, 'score': 34480.83657312393, 'total_duration': 56999.51479077339, 'accumulated_submission_time': 34480.83657312393, 'accumulated_eval_time': 22514.36498761177, 'accumulated_logging_time': 1.2141282558441162}
I0307 06:02:10.508431 140309575993088 logging_writer.py:48] [97284] accumulated_eval_time=22514.364988, accumulated_logging_time=1.214128, accumulated_submission_time=34480.836573, global_step=97284, preemption_count=0, score=34480.836573, test/accuracy=0.700134, test/bleu=30.097731, test/loss=1.364926, test/num_examples=3003, total_duration=56999.514791, train/accuracy=0.676762, train/bleu=34.157213, train/loss=1.508197, validation/accuracy=0.684815, validation/bleu=29.917024, validation/loss=1.463171, validation/num_examples=3000
I0307 06:03:27.275388 140309584385792 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.3075369596481323, loss=1.5392663478851318
I0307 06:06:24.364044 140309575993088 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.31370115280151367, loss=1.5956599712371826
I0307 06:09:21.442218 140309584385792 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.3132186233997345, loss=1.6668801307678223
I0307 06:12:18.517049 140309575993088 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.3132290542125702, loss=1.5602890253067017
I0307 06:15:15.747412 140309584385792 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.31370025873184204, loss=1.5932587385177612
I0307 06:16:10.712387 140479251105600 spec.py:321] Evaluating on the training split.
I0307 06:16:13.667981 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 06:19:27.777050 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 06:19:30.440232 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 06:22:00.014461 140479251105600 spec.py:349] Evaluating on the test split.
I0307 06:22:02.682778 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 06:24:47.965958 140479251105600 submission_runner.py:413] Time since start: 58356.99s, 	Step: 99657, 	{'train/accuracy': 0.6754489541053772, 'train/loss': 1.5172375440597534, 'train/bleu': 34.423228851443, 'validation/accuracy': 0.6849759817123413, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002149820327759, 'test/loss': 1.3647725582122803, 'test/bleu': 30.122889271149255, 'test/num_examples': 3003, 'score': 35320.95359826088, 'total_duration': 58356.99248313904, 'accumulated_submission_time': 35320.95359826088, 'accumulated_eval_time': 23031.618523597717, 'accumulated_logging_time': 1.2446951866149902}
I0307 06:24:47.986038 140309575993088 logging_writer.py:48] [99657] accumulated_eval_time=23031.618524, accumulated_logging_time=1.244695, accumulated_submission_time=35320.953598, global_step=99657, preemption_count=0, score=35320.953598, test/accuracy=0.700215, test/bleu=30.122889, test/loss=1.364773, test/num_examples=3003, total_duration=58356.992483, train/accuracy=0.675449, train/bleu=34.423229, train/loss=1.517238, validation/accuracy=0.684976, validation/bleu=29.960544, validation/loss=1.462844, validation/num_examples=3000
I0307 06:26:49.640993 140309584385792 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.3085272014141083, loss=1.5689895153045654
I0307 06:29:46.806525 140309575993088 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.31544366478919983, loss=1.5846325159072876
I0307 06:32:44.035526 140309584385792 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.31495407223701477, loss=1.560318946838379
I0307 06:35:41.169697 140309575993088 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.3111412525177002, loss=1.544479489326477
I0307 06:38:38.303436 140309584385792 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.3050994277000427, loss=1.617846965789795
I0307 06:38:48.309670 140479251105600 spec.py:321] Evaluating on the training split.
I0307 06:38:51.255421 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 06:42:05.664419 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 06:42:08.324729 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 06:44:37.696499 140479251105600 spec.py:349] Evaluating on the test split.
I0307 06:44:40.349858 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 06:47:25.625172 140479251105600 submission_runner.py:413] Time since start: 59714.65s, 	Step: 102030, 	{'train/accuracy': 0.6781300902366638, 'train/loss': 1.497983455657959, 'train/bleu': 34.45385933482152, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 36161.19220638275, 'total_duration': 59714.65171122551, 'accumulated_submission_time': 36161.19220638275, 'accumulated_eval_time': 23548.933971881866, 'accumulated_logging_time': 1.2751150131225586}
I0307 06:47:25.644855 140309575993088 logging_writer.py:48] [102030] accumulated_eval_time=23548.933972, accumulated_logging_time=1.275115, accumulated_submission_time=36161.192206, global_step=102030, preemption_count=0, score=36161.192206, test/accuracy=0.700203, test/bleu=30.121028, test/loss=1.364772, test/num_examples=3003, total_duration=59714.651711, train/accuracy=0.678130, train/bleu=34.453859, train/loss=1.497983, validation/accuracy=0.684964, validation/bleu=29.960544, validation/loss=1.462844, validation/num_examples=3000
I0307 06:50:12.358579 140309584385792 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.30096185207366943, loss=1.4921919107437134
I0307 06:53:09.576160 140309575993088 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.3001369833946228, loss=1.5436122417449951
I0307 06:56:06.729403 140309584385792 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.3069141209125519, loss=1.4914129972457886
I0307 06:59:03.930046 140309575993088 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.31127986311912537, loss=1.5768684148788452
I0307 07:01:25.803018 140479251105600 spec.py:321] Evaluating on the training split.
I0307 07:01:28.750866 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 07:04:53.024833 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 07:04:55.682305 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 07:07:25.084766 140479251105600 spec.py:349] Evaluating on the test split.
I0307 07:07:27.739524 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 07:10:12.937401 140479251105600 submission_runner.py:413] Time since start: 61081.96s, 	Step: 104402, 	{'train/accuracy': 0.677732527256012, 'train/loss': 1.507550835609436, 'train/bleu': 34.567807180475455, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 37001.26561522484, 'total_duration': 61081.96391534805, 'accumulated_submission_time': 37001.26561522484, 'accumulated_eval_time': 24076.06829738617, 'accumulated_logging_time': 1.3037071228027344}
I0307 07:10:12.957189 140309584385792 logging_writer.py:48] [104402] accumulated_eval_time=24076.068297, accumulated_logging_time=1.303707, accumulated_submission_time=37001.265615, global_step=104402, preemption_count=0, score=37001.265615, test/accuracy=0.700203, test/bleu=30.121028, test/loss=1.364772, test/num_examples=3003, total_duration=61081.963915, train/accuracy=0.677733, train/bleu=34.567807, train/loss=1.507551, validation/accuracy=0.684964, validation/bleu=29.960544, validation/loss=1.462844, validation/num_examples=3000
I0307 07:10:47.902697 140309575993088 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.3136545419692993, loss=1.5602649450302124
I0307 07:13:45.199929 140309584385792 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.32421910762786865, loss=1.6304253339767456
I0307 07:16:42.304059 140309575993088 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.30986288189888, loss=1.626082420349121
I0307 07:19:39.442751 140309584385792 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.31525906920433044, loss=1.6144249439239502
I0307 07:22:36.610678 140309575993088 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.30701592564582825, loss=1.504830002784729
I0307 07:24:13.062794 140479251105600 spec.py:321] Evaluating on the training split.
I0307 07:24:16.011409 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 07:27:31.112560 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 07:27:33.752311 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 07:30:03.225780 140479251105600 spec.py:349] Evaluating on the test split.
I0307 07:30:05.882766 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 07:32:52.059040 140479251105600 submission_runner.py:413] Time since start: 62441.09s, 	Step: 106774, 	{'train/accuracy': 0.677811861038208, 'train/loss': 1.5082634687423706, 'train/bleu': 34.15026548586909, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 37841.28516578674, 'total_duration': 62441.08551955223, 'accumulated_submission_time': 37841.28516578674, 'accumulated_eval_time': 24595.06444334984, 'accumulated_logging_time': 1.3323338031768799}
I0307 07:32:52.082976 140309584385792 logging_writer.py:48] [106774] accumulated_eval_time=24595.064443, accumulated_logging_time=1.332334, accumulated_submission_time=37841.285166, global_step=106774, preemption_count=0, score=37841.285166, test/accuracy=0.700203, test/bleu=30.121028, test/loss=1.364772, test/num_examples=3003, total_duration=62441.085520, train/accuracy=0.677812, train/bleu=34.150265, train/loss=1.508263, validation/accuracy=0.684964, validation/bleu=29.960544, validation/loss=1.462844, validation/num_examples=3000
I0307 07:34:12.495887 140309575993088 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.3245542347431183, loss=1.606836199760437
I0307 07:37:09.550650 140309584385792 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.3074779212474823, loss=1.6072572469711304
I0307 07:40:06.674175 140309575993088 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.3169318437576294, loss=1.656589388847351
I0307 07:43:03.829674 140309584385792 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.31118401885032654, loss=1.5860271453857422
I0307 07:46:00.975533 140309575993088 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.32112637162208557, loss=1.559932827949524
I0307 07:46:52.078519 140479251105600 spec.py:321] Evaluating on the training split.
I0307 07:46:55.029492 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 07:50:04.668196 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 07:50:07.340068 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 07:52:38.495539 140479251105600 spec.py:349] Evaluating on the test split.
I0307 07:52:41.173653 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 07:55:28.224855 140479251105600 submission_runner.py:413] Time since start: 63797.25s, 	Step: 109146, 	{'train/accuracy': 0.6786259412765503, 'train/loss': 1.5085808038711548, 'train/bleu': 34.42038114135181, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 38681.19652748108, 'total_duration': 63797.2513551712, 'accumulated_submission_time': 38681.19652748108, 'accumulated_eval_time': 25111.210710525513, 'accumulated_logging_time': 1.3660976886749268}
I0307 07:55:28.248943 140309584385792 logging_writer.py:48] [109146] accumulated_eval_time=25111.210711, accumulated_logging_time=1.366098, accumulated_submission_time=38681.196527, global_step=109146, preemption_count=0, score=38681.196527, test/accuracy=0.700203, test/bleu=30.121028, test/loss=1.364772, test/num_examples=3003, total_duration=63797.251355, train/accuracy=0.678626, train/bleu=34.420381, train/loss=1.508581, validation/accuracy=0.684964, validation/bleu=29.960544, validation/loss=1.462844, validation/num_examples=3000
I0307 07:57:33.786491 140309575993088 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.30805015563964844, loss=1.609983205795288
I0307 08:00:30.962649 140309584385792 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.31651172041893005, loss=1.5789129734039307
I0307 08:03:28.095854 140309575993088 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.3112652599811554, loss=1.6193006038665771
I0307 08:06:25.198009 140309584385792 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.3120152950286865, loss=1.6807920932769775
I0307 08:09:22.462268 140309575993088 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.314198762178421, loss=1.6022312641143799
I0307 08:09:28.564460 140479251105600 spec.py:321] Evaluating on the training split.
I0307 08:09:31.527086 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 08:12:47.242104 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 08:12:49.897491 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 08:15:19.007183 140479251105600 spec.py:349] Evaluating on the test split.
I0307 08:15:21.668679 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 08:18:07.677168 140479251105600 submission_runner.py:413] Time since start: 65156.70s, 	Step: 111519, 	{'train/accuracy': 0.6782219409942627, 'train/loss': 1.5064390897750854, 'train/bleu': 34.122666928954075, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 39521.42624616623, 'total_duration': 65156.70369029045, 'accumulated_submission_time': 39521.42624616623, 'accumulated_eval_time': 25630.32334780693, 'accumulated_logging_time': 1.4000046253204346}
I0307 08:18:07.702668 140309584385792 logging_writer.py:48] [111519] accumulated_eval_time=25630.323348, accumulated_logging_time=1.400005, accumulated_submission_time=39521.426246, global_step=111519, preemption_count=0, score=39521.426246, test/accuracy=0.700203, test/bleu=30.121028, test/loss=1.364772, test/num_examples=3003, total_duration=65156.703690, train/accuracy=0.678222, train/bleu=34.122667, train/loss=1.506439, validation/accuracy=0.684964, validation/bleu=29.960544, validation/loss=1.462844, validation/num_examples=3000
I0307 08:20:58.244071 140309575993088 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.3163013458251953, loss=1.5353574752807617
I0307 08:23:55.588055 140309584385792 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.320705771446228, loss=1.650093674659729
I0307 08:26:52.754557 140309575993088 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.30541685223579407, loss=1.525527834892273
I0307 08:29:50.056449 140309584385792 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.31337177753448486, loss=1.5983388423919678
I0307 08:32:07.945711 140479251105600 spec.py:321] Evaluating on the training split.
I0307 08:32:10.893367 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 08:35:21.179538 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 08:35:23.844159 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 08:37:54.961886 140479251105600 spec.py:349] Evaluating on the test split.
I0307 08:37:57.635430 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 08:40:43.688470 140479251105600 submission_runner.py:413] Time since start: 66512.71s, 	Step: 113891, 	{'train/accuracy': 0.6761565208435059, 'train/loss': 1.5166107416152954, 'train/bleu': 34.253915567968384, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 40361.58182144165, 'total_duration': 66512.71496796608, 'accumulated_submission_time': 40361.58182144165, 'accumulated_eval_time': 26146.066030740738, 'accumulated_logging_time': 1.4352364540100098}
I0307 08:40:43.715380 140309575993088 logging_writer.py:48] [113891] accumulated_eval_time=26146.066031, accumulated_logging_time=1.435236, accumulated_submission_time=40361.581821, global_step=113891, preemption_count=0, score=40361.581821, test/accuracy=0.700203, test/bleu=30.121028, test/loss=1.364772, test/num_examples=3003, total_duration=66512.714968, train/accuracy=0.676157, train/bleu=34.253916, train/loss=1.516611, validation/accuracy=0.684964, validation/bleu=29.960544, validation/loss=1.462844, validation/num_examples=3000
I0307 08:41:22.589220 140309584385792 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.31190386414527893, loss=1.5127711296081543
I0307 08:44:19.701842 140309575993088 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.313215047121048, loss=1.6545466184616089
I0307 08:47:16.807417 140309584385792 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.3086375892162323, loss=1.5940653085708618
I0307 08:50:13.955507 140309575993088 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.3135894238948822, loss=1.625386118888855
I0307 08:53:11.091292 140309584385792 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.31723299622535706, loss=1.597096562385559
I0307 08:54:44.005441 140479251105600 spec.py:321] Evaluating on the training split.
I0307 08:54:46.973196 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 08:58:03.887680 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 08:58:06.542100 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 09:00:36.144757 140479251105600 spec.py:349] Evaluating on the test split.
I0307 09:00:38.804820 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 09:03:24.297219 140479251105600 submission_runner.py:413] Time since start: 67873.32s, 	Step: 116264, 	{'train/accuracy': 0.678873598575592, 'train/loss': 1.5006016492843628, 'train/bleu': 34.391596361759454, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 41201.78594636917, 'total_duration': 67873.323741436, 'accumulated_submission_time': 41201.78594636917, 'accumulated_eval_time': 26666.357748508453, 'accumulated_logging_time': 1.472506046295166}
I0307 09:03:24.318004 140309575993088 logging_writer.py:48] [116264] accumulated_eval_time=26666.357749, accumulated_logging_time=1.472506, accumulated_submission_time=41201.785946, global_step=116264, preemption_count=0, score=41201.785946, test/accuracy=0.700203, test/bleu=30.121028, test/loss=1.364772, test/num_examples=3003, total_duration=67873.323741, train/accuracy=0.678874, train/bleu=34.391596, train/loss=1.500602, validation/accuracy=0.684964, validation/bleu=29.960544, validation/loss=1.462844, validation/num_examples=3000
I0307 09:04:48.080843 140309584385792 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.3135196268558502, loss=1.5924912691116333
I0307 09:07:45.338190 140309575993088 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.3182486891746521, loss=1.5936720371246338
I0307 09:10:42.693307 140309584385792 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.3360934853553772, loss=1.6779721975326538
I0307 09:13:39.868203 140309575993088 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.31363919377326965, loss=1.5837124586105347
I0307 09:16:36.962348 140309584385792 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.3057556748390198, loss=1.5640097856521606
I0307 09:17:24.548349 140479251105600 spec.py:321] Evaluating on the training split.
I0307 09:17:27.501198 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 09:20:44.684972 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 09:20:47.351492 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 09:23:18.526746 140479251105600 spec.py:349] Evaluating on the test split.
I0307 09:23:21.194039 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 09:26:08.216116 140479251105600 submission_runner.py:413] Time since start: 69237.24s, 	Step: 118636, 	{'train/accuracy': 0.6766113042831421, 'train/loss': 1.5116180181503296, 'train/bleu': 34.05060337128362, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 42041.92815041542, 'total_duration': 69237.24261426926, 'accumulated_submission_time': 42041.92815041542, 'accumulated_eval_time': 27190.02543067932, 'accumulated_logging_time': 1.5034704208374023}
I0307 09:26:08.240157 140309575993088 logging_writer.py:48] [118636] accumulated_eval_time=27190.025431, accumulated_logging_time=1.503470, accumulated_submission_time=42041.928150, global_step=118636, preemption_count=0, score=42041.928150, test/accuracy=0.700203, test/bleu=30.121028, test/loss=1.364772, test/num_examples=3003, total_duration=69237.242614, train/accuracy=0.676611, train/bleu=34.050603, train/loss=1.511618, validation/accuracy=0.684964, validation/bleu=29.960544, validation/loss=1.462844, validation/num_examples=3000
I0307 09:28:17.418168 140309584385792 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.3110802173614502, loss=1.6002824306488037
I0307 09:31:14.638675 140309575993088 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.315927654504776, loss=1.6147371530532837
I0307 09:34:11.829335 140309584385792 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.3112921118736267, loss=1.5760115385055542
I0307 09:37:09.083574 140309575993088 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.31275221705436707, loss=1.5745177268981934
I0307 09:40:06.438281 140309584385792 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.3075489103794098, loss=1.565940260887146
I0307 09:40:08.298583 140479251105600 spec.py:321] Evaluating on the training split.
I0307 09:40:11.254336 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 09:43:28.386661 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 09:43:31.038844 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 09:46:00.387678 140479251105600 spec.py:349] Evaluating on the test split.
I0307 09:46:03.055647 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 09:48:48.155741 140479251105600 submission_runner.py:413] Time since start: 70597.18s, 	Step: 121007, 	{'train/accuracy': 0.6760412454605103, 'train/loss': 1.516676664352417, 'train/bleu': 34.55043134398056, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 42881.89913582802, 'total_duration': 70597.18227124214, 'accumulated_submission_time': 42881.89913582802, 'accumulated_eval_time': 27709.88254070282, 'accumulated_logging_time': 1.537132978439331}
I0307 09:48:48.176657 140309575993088 logging_writer.py:48] [121007] accumulated_eval_time=27709.882541, accumulated_logging_time=1.537133, accumulated_submission_time=42881.899136, global_step=121007, preemption_count=0, score=42881.899136, test/accuracy=0.700203, test/bleu=30.121028, test/loss=1.364772, test/num_examples=3003, total_duration=70597.182271, train/accuracy=0.676041, train/bleu=34.550431, train/loss=1.516677, validation/accuracy=0.684964, validation/bleu=29.960544, validation/loss=1.462844, validation/num_examples=3000
I0307 09:51:42.942097 140309584385792 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.30679500102996826, loss=1.5310295820236206
I0307 09:54:40.144272 140309575993088 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.30972495675086975, loss=1.5260353088378906
I0307 09:57:37.200028 140309584385792 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.30968576669692993, loss=1.5025560855865479
I0307 10:00:34.467124 140309575993088 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.31416502594947815, loss=1.5643130540847778
I0307 10:02:48.435847 140479251105600 spec.py:321] Evaluating on the training split.
I0307 10:02:51.382660 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 10:06:12.488769 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 10:06:15.145955 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 10:08:44.522162 140479251105600 spec.py:349] Evaluating on the test split.
I0307 10:08:47.185962 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 10:11:32.423671 140479251105600 submission_runner.py:413] Time since start: 71961.45s, 	Step: 123380, 	{'train/accuracy': 0.6774241328239441, 'train/loss': 1.5079680681228638, 'train/bleu': 34.12996794621817, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 43722.07191467285, 'total_duration': 71961.45019507408, 'accumulated_submission_time': 43722.07191467285, 'accumulated_eval_time': 28233.87031507492, 'accumulated_logging_time': 1.5666418075561523}
I0307 10:11:32.445686 140309584385792 logging_writer.py:48] [123380] accumulated_eval_time=28233.870315, accumulated_logging_time=1.566642, accumulated_submission_time=43722.071915, global_step=123380, preemption_count=0, score=43722.071915, test/accuracy=0.700203, test/bleu=30.121028, test/loss=1.364772, test/num_examples=3003, total_duration=71961.450195, train/accuracy=0.677424, train/bleu=34.129968, train/loss=1.507968, validation/accuracy=0.684964, validation/bleu=29.960544, validation/loss=1.462844, validation/num_examples=3000
I0307 10:12:15.169497 140309575993088 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.31772926449775696, loss=1.6267560720443726
I0307 10:15:12.268626 140309584385792 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.31064775586128235, loss=1.5632845163345337
I0307 10:18:09.520196 140309575993088 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.31587302684783936, loss=1.6094837188720703
I0307 10:21:06.692540 140309584385792 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.3183397352695465, loss=1.5389764308929443
I0307 10:24:03.858711 140309575993088 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.3176121413707733, loss=1.5831776857376099
I0307 10:25:32.608607 140479251105600 spec.py:321] Evaluating on the training split.
I0307 10:25:35.576220 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 10:28:54.890706 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 10:28:57.546320 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 10:31:26.743738 140479251105600 spec.py:349] Evaluating on the test split.
I0307 10:31:29.409280 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 10:34:14.520064 140479251105600 submission_runner.py:413] Time since start: 73323.55s, 	Step: 125752, 	{'train/accuracy': 0.6760739684104919, 'train/loss': 1.5220497846603394, 'train/bleu': 34.34472898918166, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 44562.15006542206, 'total_duration': 73323.54658699036, 'accumulated_submission_time': 44562.15006542206, 'accumulated_eval_time': 28755.78173518181, 'accumulated_logging_time': 1.5983860492706299}
I0307 10:34:14.541683 140309584385792 logging_writer.py:48] [125752] accumulated_eval_time=28755.781735, accumulated_logging_time=1.598386, accumulated_submission_time=44562.150065, global_step=125752, preemption_count=0, score=44562.150065, test/accuracy=0.700203, test/bleu=30.121028, test/loss=1.364772, test/num_examples=3003, total_duration=73323.546587, train/accuracy=0.676074, train/bleu=34.344729, train/loss=1.522050, validation/accuracy=0.684964, validation/bleu=29.960544, validation/loss=1.462844, validation/num_examples=3000
I0307 10:35:42.598179 140309575993088 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.30708423256874084, loss=1.4879969358444214
I0307 10:38:39.718435 140309584385792 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.3199574053287506, loss=1.588801622390747
I0307 10:41:36.885190 140309575993088 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.3049987852573395, loss=1.5321663618087769
I0307 10:44:33.987543 140309584385792 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.3124001622200012, loss=1.5426710844039917
I0307 10:47:31.176422 140309575993088 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.3186608850955963, loss=1.5990835428237915
I0307 10:48:14.861252 140479251105600 spec.py:321] Evaluating on the training split.
I0307 10:48:17.812673 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 10:51:28.632991 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 10:51:31.278305 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 10:54:00.470567 140479251105600 spec.py:349] Evaluating on the test split.
I0307 10:54:03.135626 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 10:56:48.254992 140479251105600 submission_runner.py:413] Time since start: 74677.28s, 	Step: 128125, 	{'train/accuracy': 0.6808525323867798, 'train/loss': 1.4908273220062256, 'train/bleu': 34.42436211487829, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 45402.38612151146, 'total_duration': 74677.28153204918, 'accumulated_submission_time': 45402.38612151146, 'accumulated_eval_time': 29269.17542886734, 'accumulated_logging_time': 1.6297407150268555}
I0307 10:56:48.276129 140309584385792 logging_writer.py:48] [128125] accumulated_eval_time=29269.175429, accumulated_logging_time=1.629741, accumulated_submission_time=45402.386122, global_step=128125, preemption_count=0, score=45402.386122, test/accuracy=0.700203, test/bleu=30.121028, test/loss=1.364772, test/num_examples=3003, total_duration=74677.281532, train/accuracy=0.680853, train/bleu=34.424362, train/loss=1.490827, validation/accuracy=0.684964, validation/bleu=29.960544, validation/loss=1.462844, validation/num_examples=3000
I0307 10:59:01.247637 140309575993088 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.3049291670322418, loss=1.5473827123641968
I0307 11:01:58.337871 140309584385792 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.3045428395271301, loss=1.589830994606018
I0307 11:04:55.639608 140309575993088 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.30626043677330017, loss=1.5627310276031494
I0307 11:07:52.810772 140309584385792 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.31210631132125854, loss=1.5822254419326782
I0307 11:10:48.265160 140479251105600 spec.py:321] Evaluating on the training split.
I0307 11:10:51.220242 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 11:14:05.002486 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 11:14:07.678913 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 11:16:38.723909 140479251105600 spec.py:349] Evaluating on the test split.
I0307 11:16:41.396681 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 11:19:26.454265 140479251105600 submission_runner.py:413] Time since start: 76035.48s, 	Step: 130497, 	{'train/accuracy': 0.6760064959526062, 'train/loss': 1.5168737173080444, 'train/bleu': 34.720106980678516, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 46242.28758740425, 'total_duration': 76035.48079037666, 'accumulated_submission_time': 46242.28758740425, 'accumulated_eval_time': 29787.36448264122, 'accumulated_logging_time': 1.6617002487182617}
I0307 11:19:26.475343 140309575993088 logging_writer.py:48] [130497] accumulated_eval_time=29787.364483, accumulated_logging_time=1.661700, accumulated_submission_time=46242.287587, global_step=130497, preemption_count=0, score=46242.287587, test/accuracy=0.700203, test/bleu=30.121028, test/loss=1.364772, test/num_examples=3003, total_duration=76035.480790, train/accuracy=0.676006, train/bleu=34.720107, train/loss=1.516874, validation/accuracy=0.684964, validation/bleu=29.960544, validation/loss=1.462844, validation/num_examples=3000
I0307 11:19:27.912016 140309584385792 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.302832692861557, loss=1.5606294870376587
I0307 11:22:24.895265 140309575993088 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.3038190007209778, loss=1.5204408168792725
I0307 11:25:22.037518 140309584385792 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.31779584288597107, loss=1.6136192083358765
I0307 11:28:19.187592 140309575993088 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.31085366010665894, loss=1.5924859046936035
I0307 11:31:16.495889 140309584385792 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.3075714111328125, loss=1.5697039365768433
I0307 11:33:26.524655 140479251105600 spec.py:321] Evaluating on the training split.
I0307 11:33:29.484220 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 11:36:43.748921 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 11:36:46.404520 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 11:39:16.040248 140479251105600 spec.py:349] Evaluating on the test split.
I0307 11:39:18.715179 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 11:42:03.940317 140479251105600 submission_runner.py:413] Time since start: 77392.97s, 	Step: 132869, 	{'train/accuracy': 0.6761490702629089, 'train/loss': 1.5160126686096191, 'train/bleu': 34.16542832748747, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 47082.251356601715, 'total_duration': 77392.96685242653, 'accumulated_submission_time': 47082.251356601715, 'accumulated_eval_time': 30304.78010368347, 'accumulated_logging_time': 1.691335916519165}
I0307 11:42:03.961205 140309575993088 logging_writer.py:48] [132869] accumulated_eval_time=30304.780104, accumulated_logging_time=1.691336, accumulated_submission_time=47082.251357, global_step=132869, preemption_count=0, score=47082.251357, test/accuracy=0.700203, test/bleu=30.121028, test/loss=1.364772, test/num_examples=3003, total_duration=77392.966852, train/accuracy=0.676149, train/bleu=34.165428, train/loss=1.516013, validation/accuracy=0.684964, validation/bleu=29.960544, validation/loss=1.462844, validation/num_examples=3000
I0307 11:42:50.583633 140309584385792 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.3183557987213135, loss=1.6048120260238647
I0307 11:44:47.860605 140479251105600 spec.py:321] Evaluating on the training split.
I0307 11:44:50.811097 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 11:48:01.340436 140479251105600 spec.py:333] Evaluating on the validation split.
I0307 11:48:03.999726 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 11:50:33.474526 140479251105600 spec.py:349] Evaluating on the test split.
I0307 11:50:36.138231 140479251105600 workload.py:181] Translating evaluation dataset.
I0307 11:53:21.190980 140479251105600 submission_runner.py:413] Time since start: 78070.22s, 	Step: 133333, 	{'train/accuracy': 0.6787363886833191, 'train/loss': 1.4981261491775513, 'train/bleu': 33.98268396215291, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 47246.126288414, 'total_duration': 78070.21750569344, 'accumulated_submission_time': 47246.126288414, 'accumulated_eval_time': 30818.110419511795, 'accumulated_logging_time': 1.7221424579620361}
I0307 11:53:21.211971 140309575993088 logging_writer.py:48] [133333] accumulated_eval_time=30818.110420, accumulated_logging_time=1.722142, accumulated_submission_time=47246.126288, global_step=133333, preemption_count=0, score=47246.126288, test/accuracy=0.700203, test/bleu=30.121028, test/loss=1.364772, test/num_examples=3003, total_duration=78070.217506, train/accuracy=0.678736, train/bleu=33.982684, train/loss=1.498126, validation/accuracy=0.684964, validation/bleu=29.960544, validation/loss=1.462844, validation/num_examples=3000
I0307 11:53:21.230626 140309584385792 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=47246.126288
I0307 11:53:22.346309 140479251105600 checkpoints.py:490] Saving checkpoint at step: 133333
I0307 11:53:26.170264 140479251105600 checkpoints.py:422] Saved checkpoint at /experiment_runs/variants_target_setting/study_0/wmt_post_ln_jax/trial_1/checkpoint_133333
I0307 11:53:26.175275 140479251105600 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/variants_target_setting/study_0/wmt_post_ln_jax/trial_1/checkpoint_133333.
I0307 11:53:26.216850 140479251105600 submission_runner.py:588] Tuning trial 1/1
I0307 11:53:26.217056 140479251105600 submission_runner.py:589] Hyperparameters: Hyperparameters(learning_rate=0.0003477912008450351, beta1=0.9936632117510711, beta2=0.9967873550453692, warmup_steps=9999, weight_decay=0.04120183162940475, label_smoothing=0.0)
I0307 11:53:26.222416 140479251105600 submission_runner.py:590] Metrics: {'eval_results': [(1, {'train/accuracy': 4.5538377889897674e-05, 'train/loss': 10.680941581726074, 'train/bleu': 0.0, 'validation/accuracy': 4.959640864399262e-05, 'validation/loss': 10.719841003417969, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 3.4861426684074104e-05, 'test/loss': 10.719524383544922, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 37.026495695114136, 'total_duration': 910.1514933109283, 'accumulated_submission_time': 37.026495695114136, 'accumulated_eval_time': 873.1249454021454, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2373, {'train/accuracy': 0.16554959118366241, 'train/loss': 6.375943660736084, 'train/bleu': 0.5055459874411922, 'validation/accuracy': 0.16157269477844238, 'validation/loss': 6.475502967834473, 'validation/bleu': 0.08346499592423524, 'validation/num_examples': 3000, 'test/accuracy': 0.14870722591876984, 'test/loss': 6.718201637268066, 'test/bleu': 0.10198142982398664, 'test/num_examples': 3003, 'score': 877.1536650657654, 'total_duration': 2600.9733667373657, 'accumulated_submission_time': 877.1536650657654, 'accumulated_eval_time': 1723.718737602234, 'accumulated_logging_time': 0.027268409729003906, 'global_step': 2373, 'preemption_count': 0}), (4746, {'train/accuracy': 0.29963192343711853, 'train/loss': 4.757896900177002, 'train/bleu': 5.589181539208054, 'validation/accuracy': 0.26796939969062805, 'validation/loss': 5.071068286895752, 'validation/bleu': 2.313022269605274, 'validation/num_examples': 3000, 'test/accuracy': 0.24799257516860962, 'test/loss': 5.372460842132568, 'test/bleu': 1.7368712242060385, 'test/num_examples': 3003, 'score': 1717.140832901001, 'total_duration': 4279.43731546402, 'accumulated_submission_time': 1717.140832901001, 'accumulated_eval_time': 2562.092456817627, 'accumulated_logging_time': 0.054433345794677734, 'global_step': 4746, 'preemption_count': 0}), (7120, {'train/accuracy': 0.3576729893684387, 'train/loss': 3.9961116313934326, 'train/bleu': 7.724495868299385, 'validation/accuracy': 0.3177393972873688, 'validation/loss': 4.359506607055664, 'validation/bleu': 3.3767921191211205, 'validation/num_examples': 3000, 'test/accuracy': 0.29782116413116455, 'test/loss': 4.646053314208984, 'test/bleu': 2.3950492710240585, 'test/num_examples': 3003, 'score': 2557.3294196128845, 'total_duration': 5939.9991092681885, 'accumulated_submission_time': 2557.3294196128845, 'accumulated_eval_time': 3382.366870164871, 'accumulated_logging_time': 0.0795748233795166, 'global_step': 7120, 'preemption_count': 0}), (9493, {'train/accuracy': 0.3939822316169739, 'train/loss': 3.5680007934570312, 'train/bleu': 8.959164110927713, 'validation/accuracy': 0.3531884253025055, 'validation/loss': 3.897632122039795, 'validation/bleu': 4.1326592238074635, 'validation/num_examples': 3000, 'test/accuracy': 0.3317297101020813, 'test/loss': 4.168331623077393, 'test/bleu': 2.939828795868217, 'test/num_examples': 3003, 'score': 3397.510332107544, 'total_duration': 7577.262491941452, 'accumulated_submission_time': 3397.510332107544, 'accumulated_eval_time': 4179.34440946579, 'accumulated_logging_time': 0.10790038108825684, 'global_step': 9493, 'preemption_count': 0}), (11867, {'train/accuracy': 0.5276615619659424, 'train/loss': 2.6731984615325928, 'train/bleu': 21.670140635051332, 'validation/accuracy': 0.5194851756095886, 'validation/loss': 2.7354586124420166, 'validation/bleu': 16.803285917460464, 'validation/num_examples': 3000, 'test/accuracy': 0.5176340937614441, 'test/loss': 2.7942471504211426, 'test/bleu': 15.075098528653777, 'test/num_examples': 3003, 'score': 4237.694143295288, 'total_duration': 8939.687692403793, 'accumulated_submission_time': 4237.694143295288, 'accumulated_eval_time': 4701.488126993179, 'accumulated_logging_time': 0.13354754447937012, 'global_step': 11867, 'preemption_count': 0}), (14239, {'train/accuracy': 0.5713105797767639, 'train/loss': 2.3388614654541016, 'train/bleu': 26.30931954488054, 'validation/accuracy': 0.5742024183273315, 'validation/loss': 2.3315088748931885, 'validation/bleu': 21.292113283628584, 'validation/num_examples': 3000, 'test/accuracy': 0.5747603178024292, 'test/loss': 2.337750196456909, 'test/bleu': 19.65655123385915, 'test/num_examples': 3003, 'score': 5077.872762441635, 'total_duration': 10250.646673440933, 'accumulated_submission_time': 5077.872762441635, 'accumulated_eval_time': 5172.160847663879, 'accumulated_logging_time': 0.15973806381225586, 'global_step': 14239, 'preemption_count': 0}), (16612, {'train/accuracy': 0.590408980846405, 'train/loss': 2.192633628845215, 'train/bleu': 27.654218287671327, 'validation/accuracy': 0.5975747108459473, 'validation/loss': 2.1420791149139404, 'validation/bleu': 23.389768987724974, 'validation/num_examples': 3000, 'test/accuracy': 0.6012085676193237, 'test/loss': 2.1240668296813965, 'test/bleu': 21.892491493610297, 'test/num_examples': 3003, 'score': 5917.961942195892, 'total_duration': 11511.964399814606, 'accumulated_submission_time': 5917.961942195892, 'accumulated_eval_time': 5593.289460659027, 'accumulated_logging_time': 0.18412065505981445, 'global_step': 16612, 'preemption_count': 0}), (18985, {'train/accuracy': 0.6170119643211365, 'train/loss': 1.9861655235290527, 'train/bleu': 29.130296003246443, 'validation/accuracy': 0.612292468547821, 'validation/loss': 2.0235533714294434, 'validation/bleu': 24.460743028793953, 'validation/num_examples': 3000, 'test/accuracy': 0.6166289448738098, 'test/loss': 1.9903719425201416, 'test/bleu': 23.1778212618428, 'test/num_examples': 3003, 'score': 6758.1156215667725, 'total_duration': 12777.2118537426, 'accumulated_submission_time': 6758.1156215667725, 'accumulated_eval_time': 6018.278962135315, 'accumulated_logging_time': 0.21383881568908691, 'global_step': 18985, 'preemption_count': 0}), (21357, {'train/accuracy': 0.6118479371070862, 'train/loss': 2.020354747772217, 'train/bleu': 29.323991583626714, 'validation/accuracy': 0.6203518509864807, 'validation/loss': 1.9466265439987183, 'validation/bleu': 25.223630033063053, 'validation/num_examples': 3000, 'test/accuracy': 0.62579745054245, 'test/loss': 1.9064064025878906, 'test/bleu': 23.811507225729045, 'test/num_examples': 3003, 'score': 7598.2155549526215, 'total_duration': 14085.8356487751, 'accumulated_submission_time': 7598.2155549526215, 'accumulated_eval_time': 6486.62804889679, 'accumulated_logging_time': 0.312896728515625, 'global_step': 21357, 'preemption_count': 0}), (23729, {'train/accuracy': 0.6130566000938416, 'train/loss': 1.9910234212875366, 'train/bleu': 29.008670164352342, 'validation/accuracy': 0.6288080811500549, 'validation/loss': 1.8797252178192139, 'validation/bleu': 25.45348854405818, 'validation/num_examples': 3000, 'test/accuracy': 0.6346173882484436, 'test/loss': 1.8363406658172607, 'test/bleu': 24.40175743920315, 'test/num_examples': 3003, 'score': 8438.309606075287, 'total_duration': 15406.34621143341, 'accumulated_submission_time': 8438.309606075287, 'accumulated_eval_time': 6966.937338113785, 'accumulated_logging_time': 0.34282708168029785, 'global_step': 23729, 'preemption_count': 0}), (26101, {'train/accuracy': 0.6259080767631531, 'train/loss': 1.8868838548660278, 'train/bleu': 30.179083379715863, 'validation/accuracy': 0.6362971067428589, 'validation/loss': 1.8235551118850708, 'validation/bleu': 26.384664324348677, 'validation/num_examples': 3000, 'test/accuracy': 0.6441578269004822, 'test/loss': 1.772401213645935, 'test/bleu': 25.32138446752374, 'test/num_examples': 3003, 'score': 9278.52301311493, 'total_duration': 16686.78796339035, 'accumulated_submission_time': 9278.52301311493, 'accumulated_eval_time': 7407.060416698456, 'accumulated_logging_time': 0.3685746192932129, 'global_step': 26101, 'preemption_count': 0}), (28474, {'train/accuracy': 0.6278741359710693, 'train/loss': 1.8764451742172241, 'train/bleu': 30.580050059388913, 'validation/accuracy': 0.6410459876060486, 'validation/loss': 1.7856191396713257, 'validation/bleu': 26.55775990002107, 'validation/num_examples': 3000, 'test/accuracy': 0.6514670848846436, 'test/loss': 1.719996690750122, 'test/bleu': 25.71419687547752, 'test/num_examples': 3003, 'score': 10118.453328609467, 'total_duration': 17981.735468387604, 'accumulated_submission_time': 10118.453328609467, 'accumulated_eval_time': 7861.975425481796, 'accumulated_logging_time': 0.39550185203552246, 'global_step': 28474, 'preemption_count': 0}), (30847, {'train/accuracy': 0.6271517276763916, 'train/loss': 1.8758031129837036, 'train/bleu': 30.385887548990254, 'validation/accuracy': 0.645683228969574, 'validation/loss': 1.7459243535995483, 'validation/bleu': 27.356207782527754, 'validation/num_examples': 3000, 'test/accuracy': 0.6540584564208984, 'test/loss': 1.6935328245162964, 'test/bleu': 26.00806679697413, 'test/num_examples': 3003, 'score': 10958.59934592247, 'total_duration': 19253.208206653595, 'accumulated_submission_time': 10958.59934592247, 'accumulated_eval_time': 8293.202862262726, 'accumulated_logging_time': 0.42104101181030273, 'global_step': 30847, 'preemption_count': 0}), (33220, {'train/accuracy': 0.6357178092002869, 'train/loss': 1.8116439580917358, 'train/bleu': 30.964162543206665, 'validation/accuracy': 0.6494277715682983, 'validation/loss': 1.7206015586853027, 'validation/bleu': 27.538105689602112, 'validation/num_examples': 3000, 'test/accuracy': 0.6602521538734436, 'test/loss': 1.6538362503051758, 'test/bleu': 26.550878491057826, 'test/num_examples': 3003, 'score': 11798.631483793259, 'total_duration': 20585.520178079605, 'accumulated_submission_time': 11798.631483793259, 'accumulated_eval_time': 8785.382771253586, 'accumulated_logging_time': 0.4463164806365967, 'global_step': 33220, 'preemption_count': 0}), (35593, {'train/accuracy': 0.6376006007194519, 'train/loss': 1.795326590538025, 'train/bleu': 31.053307670989863, 'validation/accuracy': 0.6520687937736511, 'validation/loss': 1.6875280141830444, 'validation/bleu': 27.53264112337824, 'validation/num_examples': 3000, 'test/accuracy': 0.663203775882721, 'test/loss': 1.6211374998092651, 'test/bleu': 26.717511510370425, 'test/num_examples': 3003, 'score': 12638.748220205307, 'total_duration': 21891.13416814804, 'accumulated_submission_time': 12638.748220205307, 'accumulated_eval_time': 9250.780358076096, 'accumulated_logging_time': 0.4722471237182617, 'global_step': 35593, 'preemption_count': 0}), (37965, {'train/accuracy': 0.6497527360916138, 'train/loss': 1.7091151475906372, 'train/bleu': 32.04856828987185, 'validation/accuracy': 0.6565572619438171, 'validation/loss': 1.6644614934921265, 'validation/bleu': 27.71397684530375, 'validation/num_examples': 3000, 'test/accuracy': 0.6659926772117615, 'test/loss': 1.5949139595031738, 'test/bleu': 27.09271560065603, 'test/num_examples': 3003, 'score': 13478.81368470192, 'total_duration': 23320.12997364998, 'accumulated_submission_time': 13478.81368470192, 'accumulated_eval_time': 9839.603850126266, 'accumulated_logging_time': 0.5021743774414062, 'global_step': 37965, 'preemption_count': 0}), (40338, {'train/accuracy': 0.6435055732727051, 'train/loss': 1.7565072774887085, 'train/bleu': 31.808233138610035, 'validation/accuracy': 0.6595454216003418, 'validation/loss': 1.6422971487045288, 'validation/bleu': 28.078354347189922, 'validation/num_examples': 3000, 'test/accuracy': 0.6697461009025574, 'test/loss': 1.574199914932251, 'test/bleu': 27.43604887143621, 'test/num_examples': 3003, 'score': 14318.843627214432, 'total_duration': 24623.185683488846, 'accumulated_submission_time': 14318.843627214432, 'accumulated_eval_time': 10302.526732206345, 'accumulated_logging_time': 0.5289435386657715, 'global_step': 40338, 'preemption_count': 0}), (42711, {'train/accuracy': 0.6435038447380066, 'train/loss': 1.744189977645874, 'train/bleu': 31.650799330190253, 'validation/accuracy': 0.6613433361053467, 'validation/loss': 1.6276651620864868, 'validation/bleu': 28.393053224055592, 'validation/num_examples': 3000, 'test/accuracy': 0.6725001335144043, 'test/loss': 1.5530000925064087, 'test/bleu': 27.643341196206578, 'test/num_examples': 3003, 'score': 15159.088337182999, 'total_duration': 25908.105601787567, 'accumulated_submission_time': 15159.088337182999, 'accumulated_eval_time': 10747.101356744766, 'accumulated_logging_time': 0.5549688339233398, 'global_step': 42711, 'preemption_count': 0}), (45083, {'train/accuracy': 0.6504939198493958, 'train/loss': 1.6879643201828003, 'train/bleu': 32.02476908456034, 'validation/accuracy': 0.6653234362602234, 'validation/loss': 1.6043541431427002, 'validation/bleu': 28.198855326407323, 'validation/num_examples': 3000, 'test/accuracy': 0.6762651801109314, 'test/loss': 1.5304776430130005, 'test/bleu': 27.993240158895745, 'test/num_examples': 3003, 'score': 15999.193945884705, 'total_duration': 27250.30659008026, 'accumulated_submission_time': 15999.193945884705, 'accumulated_eval_time': 11249.091786384583, 'accumulated_logging_time': 0.5832390785217285, 'global_step': 45083, 'preemption_count': 0}), (47455, {'train/accuracy': 0.6478411555290222, 'train/loss': 1.712612509727478, 'train/bleu': 32.04224538935785, 'validation/accuracy': 0.6665757298469543, 'validation/loss': 1.591001033782959, 'validation/bleu': 28.425250850587027, 'validation/num_examples': 3000, 'test/accuracy': 0.6780896186828613, 'test/loss': 1.5132921934127808, 'test/bleu': 27.913060390684564, 'test/num_examples': 3003, 'score': 16839.112189769745, 'total_duration': 28613.26828479767, 'accumulated_submission_time': 16839.112189769745, 'accumulated_eval_time': 11772.034071445465, 'accumulated_logging_time': 0.608893632888794, 'global_step': 47455, 'preemption_count': 0}), (49828, {'train/accuracy': 0.6558026075363159, 'train/loss': 1.6667882204055786, 'train/bleu': 32.43539331962923, 'validation/accuracy': 0.6666625142097473, 'validation/loss': 1.5765550136566162, 'validation/bleu': 28.982425479616044, 'validation/num_examples': 3000, 'test/accuracy': 0.6810412406921387, 'test/loss': 1.493761420249939, 'test/bleu': 28.460115633355112, 'test/num_examples': 3003, 'score': 17679.10280394554, 'total_duration': 29894.178914308548, 'accumulated_submission_time': 17679.10280394554, 'accumulated_eval_time': 12212.851514816284, 'accumulated_logging_time': 0.63606858253479, 'global_step': 49828, 'preemption_count': 0}), (52201, {'train/accuracy': 0.654120922088623, 'train/loss': 1.6606074571609497, 'train/bleu': 32.24839582043509, 'validation/accuracy': 0.667902410030365, 'validation/loss': 1.5687613487243652, 'validation/bleu': 28.613720812629968, 'validation/num_examples': 3000, 'test/accuracy': 0.6803556084632874, 'test/loss': 1.4891008138656616, 'test/bleu': 28.43649313404824, 'test/num_examples': 3003, 'score': 18519.04047203064, 'total_duration': 31166.456148386, 'accumulated_submission_time': 18519.04047203064, 'accumulated_eval_time': 12645.08620762825, 'accumulated_logging_time': 0.6676373481750488, 'global_step': 52201, 'preemption_count': 0}), (54574, {'train/accuracy': 0.658129870891571, 'train/loss': 1.6468077898025513, 'train/bleu': 31.868311532831648, 'validation/accuracy': 0.6713742017745972, 'validation/loss': 1.5561473369598389, 'validation/bleu': 29.04187550718383, 'validation/num_examples': 3000, 'test/accuracy': 0.6837720274925232, 'test/loss': 1.4734342098236084, 'test/bleu': 28.643117462667007, 'test/num_examples': 3003, 'score': 19359.120665550232, 'total_duration': 32456.122641563416, 'accumulated_submission_time': 19359.120665550232, 'accumulated_eval_time': 13094.565346717834, 'accumulated_logging_time': 0.7001116275787354, 'global_step': 54574, 'preemption_count': 0}), (56947, {'train/accuracy': 0.6687179207801819, 'train/loss': 1.5735982656478882, 'train/bleu': 33.15393052251328, 'validation/accuracy': 0.6729984879493713, 'validation/loss': 1.5422427654266357, 'validation/bleu': 29.182041544121475, 'validation/num_examples': 3000, 'test/accuracy': 0.6852943301200867, 'test/loss': 1.4579284191131592, 'test/bleu': 28.667188622555706, 'test/num_examples': 3003, 'score': 20199.2664103508, 'total_duration': 33817.35985803604, 'accumulated_submission_time': 20199.2664103508, 'accumulated_eval_time': 13615.548609256744, 'accumulated_logging_time': 0.7321228981018066, 'global_step': 56947, 'preemption_count': 0}), (59320, {'train/accuracy': 0.661736011505127, 'train/loss': 1.6122812032699585, 'train/bleu': 32.59593845161208, 'validation/accuracy': 0.6744615435600281, 'validation/loss': 1.5311570167541504, 'validation/bleu': 29.223824721728594, 'validation/num_examples': 3000, 'test/accuracy': 0.6868746876716614, 'test/loss': 1.4448662996292114, 'test/bleu': 29.00632413735738, 'test/num_examples': 3003, 'score': 21039.218721151352, 'total_duration': 35153.80774140358, 'accumulated_submission_time': 21039.218721151352, 'accumulated_eval_time': 14111.940336704254, 'accumulated_logging_time': 0.7595162391662598, 'global_step': 59320, 'preemption_count': 0}), (61692, {'train/accuracy': 0.6636953949928284, 'train/loss': 1.6126148700714111, 'train/bleu': 32.661907874709684, 'validation/accuracy': 0.6747963428497314, 'validation/loss': 1.5258315801620483, 'validation/bleu': 29.24441031085027, 'validation/num_examples': 3000, 'test/accuracy': 0.6882110238075256, 'test/loss': 1.4359838962554932, 'test/bleu': 29.06128226487907, 'test/num_examples': 3003, 'score': 21879.126792669296, 'total_duration': 36426.15311074257, 'accumulated_submission_time': 21879.126792669296, 'accumulated_eval_time': 14544.27177977562, 'accumulated_logging_time': 0.786595344543457, 'global_step': 61692, 'preemption_count': 0}), (64065, {'train/accuracy': 0.6689755320549011, 'train/loss': 1.560241937637329, 'train/bleu': 33.6577532470055, 'validation/accuracy': 0.6772513389587402, 'validation/loss': 1.5159430503845215, 'validation/bleu': 29.241512861505495, 'validation/num_examples': 3000, 'test/accuracy': 0.6905002593994141, 'test/loss': 1.430458903312683, 'test/bleu': 29.194278802519776, 'test/num_examples': 3003, 'score': 22719.286415100098, 'total_duration': 37742.32204914093, 'accumulated_submission_time': 22719.286415100098, 'accumulated_eval_time': 15020.178970813751, 'accumulated_logging_time': 0.8134200572967529, 'global_step': 64065, 'preemption_count': 0}), (66439, {'train/accuracy': 0.6681381464004517, 'train/loss': 1.5711647272109985, 'train/bleu': 32.870854886084715, 'validation/accuracy': 0.6773505806922913, 'validation/loss': 1.5075639486312866, 'validation/bleu': 29.533497605061257, 'validation/num_examples': 3000, 'test/accuracy': 0.6923828125, 'test/loss': 1.4150910377502441, 'test/bleu': 29.547095741952447, 'test/num_examples': 3003, 'score': 23559.536057949066, 'total_duration': 39152.96033978462, 'accumulated_submission_time': 23559.536057949066, 'accumulated_eval_time': 15590.463738679886, 'accumulated_logging_time': 0.8402137756347656, 'global_step': 66439, 'preemption_count': 0}), (68812, {'train/accuracy': 0.6660295128822327, 'train/loss': 1.5897455215454102, 'train/bleu': 33.2229564491977, 'validation/accuracy': 0.6795203685760498, 'validation/loss': 1.4992176294326782, 'validation/bleu': 29.4938680018353, 'validation/num_examples': 3000, 'test/accuracy': 0.693428635597229, 'test/loss': 1.4072777032852173, 'test/bleu': 29.36758891332158, 'test/num_examples': 3003, 'score': 24399.730229377747, 'total_duration': 40461.933876514435, 'accumulated_submission_time': 24399.730229377747, 'accumulated_eval_time': 16059.137459516525, 'accumulated_logging_time': 0.8695645332336426, 'global_step': 68812, 'preemption_count': 0}), (71185, {'train/accuracy': 0.6706438660621643, 'train/loss': 1.548822283744812, 'train/bleu': 33.63335498095293, 'validation/accuracy': 0.6798427700996399, 'validation/loss': 1.4899640083312988, 'validation/bleu': 29.584248418353056, 'validation/num_examples': 3000, 'test/accuracy': 0.6949160695075989, 'test/loss': 1.400790810585022, 'test/bleu': 29.551375837013723, 'test/num_examples': 3003, 'score': 25239.921297311783, 'total_duration': 41892.79270076752, 'accumulated_submission_time': 25239.921297311783, 'accumulated_eval_time': 16649.700699329376, 'accumulated_logging_time': 0.8990745544433594, 'global_step': 71185, 'preemption_count': 0}), (73558, {'train/accuracy': 0.6686922311782837, 'train/loss': 1.5636966228485107, 'train/bleu': 33.61162310909176, 'validation/accuracy': 0.6811322569847107, 'validation/loss': 1.487377643585205, 'validation/bleu': 29.614756219300876, 'validation/num_examples': 3000, 'test/accuracy': 0.6956132650375366, 'test/loss': 1.3960987329483032, 'test/bleu': 29.679354624505628, 'test/num_examples': 3003, 'score': 26079.9373755455, 'total_duration': 43237.35492491722, 'accumulated_submission_time': 26079.9373755455, 'accumulated_eval_time': 17154.144641160965, 'accumulated_logging_time': 0.9275655746459961, 'global_step': 73558, 'preemption_count': 0}), (75931, {'train/accuracy': 0.6778443455696106, 'train/loss': 1.5113569498062134, 'train/bleu': 33.68908874111985, 'validation/accuracy': 0.6824589967727661, 'validation/loss': 1.4826048612594604, 'validation/bleu': 29.80243295526676, 'validation/num_examples': 3000, 'test/accuracy': 0.6972866654396057, 'test/loss': 1.387231707572937, 'test/bleu': 29.684434673409008, 'test/num_examples': 3003, 'score': 26920.11353945732, 'total_duration': 44686.934475660324, 'accumulated_submission_time': 26920.11353945732, 'accumulated_eval_time': 17763.443356752396, 'accumulated_logging_time': 0.9555902481079102, 'global_step': 75931, 'preemption_count': 0}), (78304, {'train/accuracy': 0.6727930307388306, 'train/loss': 1.5448284149169922, 'train/bleu': 33.363913091301995, 'validation/accuracy': 0.6829673647880554, 'validation/loss': 1.4761617183685303, 'validation/bleu': 29.7455915346154, 'validation/num_examples': 3000, 'test/accuracy': 0.696694016456604, 'test/loss': 1.3837041854858398, 'test/bleu': 29.876523596683413, 'test/num_examples': 3003, 'score': 27760.13487482071, 'total_duration': 46064.34475827217, 'accumulated_submission_time': 27760.13487482071, 'accumulated_eval_time': 18300.72947216034, 'accumulated_logging_time': 0.9833755493164062, 'global_step': 78304, 'preemption_count': 0}), (80676, {'train/accuracy': 0.6747324466705322, 'train/loss': 1.5321621894836426, 'train/bleu': 33.81755577223068, 'validation/accuracy': 0.683078944683075, 'validation/loss': 1.474095106124878, 'validation/bleu': 29.818120434164623, 'validation/num_examples': 3000, 'test/accuracy': 0.6976119875907898, 'test/loss': 1.3782261610031128, 'test/bleu': 30.000250726081184, 'test/num_examples': 3003, 'score': 28600.219145298004, 'total_duration': 47426.25502705574, 'accumulated_submission_time': 28600.219145298004, 'accumulated_eval_time': 18822.452847719193, 'accumulated_logging_time': 1.0107650756835938, 'global_step': 80676, 'preemption_count': 0}), (83048, {'train/accuracy': 0.6766062378883362, 'train/loss': 1.5232235193252563, 'train/bleu': 34.390865962752926, 'validation/accuracy': 0.6842568516731262, 'validation/loss': 1.4705488681793213, 'validation/bleu': 29.83519080667452, 'validation/num_examples': 3000, 'test/accuracy': 0.6981349587440491, 'test/loss': 1.374583125114441, 'test/bleu': 29.859794249510102, 'test/num_examples': 3003, 'score': 29440.208112716675, 'total_duration': 48784.276811122894, 'accumulated_submission_time': 29440.208112716675, 'accumulated_eval_time': 19340.382719755173, 'accumulated_logging_time': 1.038370132446289, 'global_step': 83048, 'preemption_count': 0}), (85419, {'train/accuracy': 0.6752600073814392, 'train/loss': 1.518908143043518, 'train/bleu': 34.22081710561856, 'validation/accuracy': 0.6839096546173096, 'validation/loss': 1.466374397277832, 'validation/bleu': 29.964729017171297, 'validation/num_examples': 3000, 'test/accuracy': 0.6996572017669678, 'test/loss': 1.369691014289856, 'test/bleu': 30.03802438606821, 'test/num_examples': 3003, 'score': 30280.11798620224, 'total_duration': 50161.99370551109, 'accumulated_submission_time': 30280.11798620224, 'accumulated_eval_time': 19878.083097696304, 'accumulated_logging_time': 1.0668854713439941, 'global_step': 85419, 'preemption_count': 0}), (87791, {'train/accuracy': 0.6782928109169006, 'train/loss': 1.506111741065979, 'train/bleu': 34.47983844493076, 'validation/accuracy': 0.6847032308578491, 'validation/loss': 1.4656473398208618, 'validation/bleu': 29.90656084595846, 'validation/num_examples': 3000, 'test/accuracy': 0.7003660798072815, 'test/loss': 1.368316411972046, 'test/bleu': 30.115225070781918, 'test/num_examples': 3003, 'score': 31120.12797641754, 'total_duration': 51502.63209462166, 'accumulated_submission_time': 31120.12797641754, 'accumulated_eval_time': 20378.60650396347, 'accumulated_logging_time': 1.095641851425171, 'global_step': 87791, 'preemption_count': 0}), (90164, {'train/accuracy': 0.6785022616386414, 'train/loss': 1.500885009765625, 'train/bleu': 34.0069950951964, 'validation/accuracy': 0.684566855430603, 'validation/loss': 1.4633512496948242, 'validation/bleu': 30.077972365339207, 'validation/num_examples': 3000, 'test/accuracy': 0.7001801133155823, 'test/loss': 1.3668028116226196, 'test/bleu': 30.131782785340032, 'test/num_examples': 3003, 'score': 31960.361180067062, 'total_duration': 52886.064812898636, 'accumulated_submission_time': 31960.361180067062, 'accumulated_eval_time': 20921.705367088318, 'accumulated_logging_time': 1.1235229969024658, 'global_step': 90164, 'preemption_count': 0}), (92537, {'train/accuracy': 0.6780723333358765, 'train/loss': 1.508441686630249, 'train/bleu': 34.27605743081869, 'validation/accuracy': 0.6850255727767944, 'validation/loss': 1.4646774530410767, 'validation/bleu': 30.021923551313247, 'validation/num_examples': 3000, 'test/accuracy': 0.6999012231826782, 'test/loss': 1.3655339479446411, 'test/bleu': 30.015570893508283, 'test/num_examples': 3003, 'score': 32800.55123567581, 'total_duration': 54269.263169288635, 'accumulated_submission_time': 32800.55123567581, 'accumulated_eval_time': 21464.605616807938, 'accumulated_logging_time': 1.155937671661377, 'global_step': 92537, 'preemption_count': 0}), (94910, {'train/accuracy': 0.6803591251373291, 'train/loss': 1.4964025020599365, 'train/bleu': 34.10109996382132, 'validation/accuracy': 0.6847900152206421, 'validation/loss': 1.4636088609695435, 'validation/bleu': 29.947799046860744, 'validation/num_examples': 3000, 'test/accuracy': 0.7000290751457214, 'test/loss': 1.3650816679000854, 'test/bleu': 29.967965550167904, 'test/num_examples': 3003, 'score': 33640.595079422, 'total_duration': 55642.37203788757, 'accumulated_submission_time': 33640.595079422, 'accumulated_eval_time': 21997.568466424942, 'accumulated_logging_time': 1.1839983463287354, 'global_step': 94910, 'preemption_count': 0}), (97284, {'train/accuracy': 0.6767615079879761, 'train/loss': 1.5081969499588013, 'train/bleu': 34.15721278833713, 'validation/accuracy': 0.6848148107528687, 'validation/loss': 1.4631707668304443, 'validation/bleu': 29.917024016626115, 'validation/num_examples': 3000, 'test/accuracy': 0.7001336812973022, 'test/loss': 1.3649258613586426, 'test/bleu': 30.09773101381347, 'test/num_examples': 3003, 'score': 34480.83657312393, 'total_duration': 56999.51479077339, 'accumulated_submission_time': 34480.83657312393, 'accumulated_eval_time': 22514.36498761177, 'accumulated_logging_time': 1.2141282558441162, 'global_step': 97284, 'preemption_count': 0}), (99657, {'train/accuracy': 0.6754489541053772, 'train/loss': 1.5172375440597534, 'train/bleu': 34.423228851443, 'validation/accuracy': 0.6849759817123413, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002149820327759, 'test/loss': 1.3647725582122803, 'test/bleu': 30.122889271149255, 'test/num_examples': 3003, 'score': 35320.95359826088, 'total_duration': 58356.99248313904, 'accumulated_submission_time': 35320.95359826088, 'accumulated_eval_time': 23031.618523597717, 'accumulated_logging_time': 1.2446951866149902, 'global_step': 99657, 'preemption_count': 0}), (102030, {'train/accuracy': 0.6781300902366638, 'train/loss': 1.497983455657959, 'train/bleu': 34.45385933482152, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 36161.19220638275, 'total_duration': 59714.65171122551, 'accumulated_submission_time': 36161.19220638275, 'accumulated_eval_time': 23548.933971881866, 'accumulated_logging_time': 1.2751150131225586, 'global_step': 102030, 'preemption_count': 0}), (104402, {'train/accuracy': 0.677732527256012, 'train/loss': 1.507550835609436, 'train/bleu': 34.567807180475455, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 37001.26561522484, 'total_duration': 61081.96391534805, 'accumulated_submission_time': 37001.26561522484, 'accumulated_eval_time': 24076.06829738617, 'accumulated_logging_time': 1.3037071228027344, 'global_step': 104402, 'preemption_count': 0}), (106774, {'train/accuracy': 0.677811861038208, 'train/loss': 1.5082634687423706, 'train/bleu': 34.15026548586909, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 37841.28516578674, 'total_duration': 62441.08551955223, 'accumulated_submission_time': 37841.28516578674, 'accumulated_eval_time': 24595.06444334984, 'accumulated_logging_time': 1.3323338031768799, 'global_step': 106774, 'preemption_count': 0}), (109146, {'train/accuracy': 0.6786259412765503, 'train/loss': 1.5085808038711548, 'train/bleu': 34.42038114135181, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 38681.19652748108, 'total_duration': 63797.2513551712, 'accumulated_submission_time': 38681.19652748108, 'accumulated_eval_time': 25111.210710525513, 'accumulated_logging_time': 1.3660976886749268, 'global_step': 109146, 'preemption_count': 0}), (111519, {'train/accuracy': 0.6782219409942627, 'train/loss': 1.5064390897750854, 'train/bleu': 34.122666928954075, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 39521.42624616623, 'total_duration': 65156.70369029045, 'accumulated_submission_time': 39521.42624616623, 'accumulated_eval_time': 25630.32334780693, 'accumulated_logging_time': 1.4000046253204346, 'global_step': 111519, 'preemption_count': 0}), (113891, {'train/accuracy': 0.6761565208435059, 'train/loss': 1.5166107416152954, 'train/bleu': 34.253915567968384, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 40361.58182144165, 'total_duration': 66512.71496796608, 'accumulated_submission_time': 40361.58182144165, 'accumulated_eval_time': 26146.066030740738, 'accumulated_logging_time': 1.4352364540100098, 'global_step': 113891, 'preemption_count': 0}), (116264, {'train/accuracy': 0.678873598575592, 'train/loss': 1.5006016492843628, 'train/bleu': 34.391596361759454, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 41201.78594636917, 'total_duration': 67873.323741436, 'accumulated_submission_time': 41201.78594636917, 'accumulated_eval_time': 26666.357748508453, 'accumulated_logging_time': 1.472506046295166, 'global_step': 116264, 'preemption_count': 0}), (118636, {'train/accuracy': 0.6766113042831421, 'train/loss': 1.5116180181503296, 'train/bleu': 34.05060337128362, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 42041.92815041542, 'total_duration': 69237.24261426926, 'accumulated_submission_time': 42041.92815041542, 'accumulated_eval_time': 27190.02543067932, 'accumulated_logging_time': 1.5034704208374023, 'global_step': 118636, 'preemption_count': 0}), (121007, {'train/accuracy': 0.6760412454605103, 'train/loss': 1.516676664352417, 'train/bleu': 34.55043134398056, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 42881.89913582802, 'total_duration': 70597.18227124214, 'accumulated_submission_time': 42881.89913582802, 'accumulated_eval_time': 27709.88254070282, 'accumulated_logging_time': 1.537132978439331, 'global_step': 121007, 'preemption_count': 0}), (123380, {'train/accuracy': 0.6774241328239441, 'train/loss': 1.5079680681228638, 'train/bleu': 34.12996794621817, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 43722.07191467285, 'total_duration': 71961.45019507408, 'accumulated_submission_time': 43722.07191467285, 'accumulated_eval_time': 28233.87031507492, 'accumulated_logging_time': 1.5666418075561523, 'global_step': 123380, 'preemption_count': 0}), (125752, {'train/accuracy': 0.6760739684104919, 'train/loss': 1.5220497846603394, 'train/bleu': 34.34472898918166, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 44562.15006542206, 'total_duration': 73323.54658699036, 'accumulated_submission_time': 44562.15006542206, 'accumulated_eval_time': 28755.78173518181, 'accumulated_logging_time': 1.5983860492706299, 'global_step': 125752, 'preemption_count': 0}), (128125, {'train/accuracy': 0.6808525323867798, 'train/loss': 1.4908273220062256, 'train/bleu': 34.42436211487829, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 45402.38612151146, 'total_duration': 74677.28153204918, 'accumulated_submission_time': 45402.38612151146, 'accumulated_eval_time': 29269.17542886734, 'accumulated_logging_time': 1.6297407150268555, 'global_step': 128125, 'preemption_count': 0}), (130497, {'train/accuracy': 0.6760064959526062, 'train/loss': 1.5168737173080444, 'train/bleu': 34.720106980678516, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 46242.28758740425, 'total_duration': 76035.48079037666, 'accumulated_submission_time': 46242.28758740425, 'accumulated_eval_time': 29787.36448264122, 'accumulated_logging_time': 1.6617002487182617, 'global_step': 130497, 'preemption_count': 0}), (132869, {'train/accuracy': 0.6761490702629089, 'train/loss': 1.5160126686096191, 'train/bleu': 34.16542832748747, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 47082.251356601715, 'total_duration': 77392.96685242653, 'accumulated_submission_time': 47082.251356601715, 'accumulated_eval_time': 30304.78010368347, 'accumulated_logging_time': 1.691335916519165, 'global_step': 132869, 'preemption_count': 0}), (133333, {'train/accuracy': 0.6787363886833191, 'train/loss': 1.4981261491775513, 'train/bleu': 33.98268396215291, 'validation/accuracy': 0.684963583946228, 'validation/loss': 1.4628443717956543, 'validation/bleu': 29.960543501775675, 'validation/num_examples': 3000, 'test/accuracy': 0.7002033591270447, 'test/loss': 1.3647724390029907, 'test/bleu': 30.12102798861055, 'test/num_examples': 3003, 'score': 47246.126288414, 'total_duration': 78070.21750569344, 'accumulated_submission_time': 47246.126288414, 'accumulated_eval_time': 30818.110419511795, 'accumulated_logging_time': 1.7221424579620361, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0307 11:53:26.222570 140479251105600 submission_runner.py:591] Timing: 47246.126288414
I0307 11:53:26.222637 140479251105600 submission_runner.py:593] Total number of evals: 58
I0307 11:53:26.222679 140479251105600 submission_runner.py:594] ====================
I0307 11:53:26.222843 140479251105600 submission_runner.py:678] Final wmt_post_ln score: 47246.126288414
