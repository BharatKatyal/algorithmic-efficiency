python3 submission_runner.py --framework=jax --workload=imagenet_vit_glu --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --rng_seed=2465481527 --max_global_steps=186666 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=external --tuning_search_space=reference_algorithms/target_setting_algorithms/imagenet_vit_glu/tuning_search_space.json --num_tuning_trials=1 2>&1 | tee -a /logs/imagenet_vit_glu_jax_03-06-2024-11-46-09.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0306 11:46:29.496473 140046356313920 logger_utils.py:61] Removing existing experiment directory /experiment_runs/variants_target_setting/study_0/imagenet_vit_glu_jax because --overwrite was set.
I0306 11:46:29.502264 140046356313920 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/imagenet_vit_glu_jax.
I0306 11:46:30.517055 140046356313920 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0306 11:46:30.517793 140046356313920 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0306 11:46:30.517944 140046356313920 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0306 11:46:30.523987 140046356313920 submission_runner.py:547] Using RNG seed 2465481527
I0306 11:46:31.617535 140046356313920 submission_runner.py:556] --- Tuning run 1/1 ---
I0306 11:46:31.617803 140046356313920 submission_runner.py:561] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/imagenet_vit_glu_jax/trial_1.
I0306 11:46:31.618000 140046356313920 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/imagenet_vit_glu_jax/trial_1/hparams.json.
I0306 11:46:31.801167 140046356313920 submission_runner.py:206] Initializing dataset.
I0306 11:46:31.818132 140046356313920 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0306 11:46:31.828819 140046356313920 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0306 11:46:32.214314 140046356313920 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0306 11:46:40.496008 140046356313920 submission_runner.py:213] Initializing model.
I0306 11:46:52.075357 140046356313920 submission_runner.py:255] Initializing optimizer.
I0306 11:46:53.239111 140046356313920 submission_runner.py:262] Initializing metrics bundle.
I0306 11:46:53.239361 140046356313920 submission_runner.py:280] Initializing checkpoint and logger.
I0306 11:46:53.240922 140046356313920 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/imagenet_vit_glu_jax/trial_1 with prefix checkpoint_
I0306 11:46:53.241106 140046356313920 submission_runner.py:300] Saving meta data to /experiment_runs/variants_target_setting/study_0/imagenet_vit_glu_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0306 11:46:53.578028 140046356313920 logger_utils.py:220] Unable to record git information. Continuing without it.
I0306 11:46:53.908624 140046356313920 submission_runner.py:304] Saving flags to /experiment_runs/variants_target_setting/study_0/imagenet_vit_glu_jax/trial_1/flags_0.json.
I0306 11:46:53.920078 140046356313920 submission_runner.py:314] Starting training loop.
I0306 11:47:33.836397 139883328030464 logging_writer.py:48] [0] global_step=0, grad_norm=0.49873489141464233, loss=6.9077558517456055
I0306 11:47:33.854302 140046356313920 spec.py:321] Evaluating on the training split.
I0306 11:47:33.862059 140046356313920 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0306 11:47:33.871031 140046356313920 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0306 11:47:33.955460 140046356313920 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0306 11:47:57.030372 140046356313920 spec.py:333] Evaluating on the validation split.
I0306 11:47:57.036923 140046356313920 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0306 11:47:57.045817 140046356313920 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0306 11:47:57.085548 140046356313920 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0306 11:48:15.844780 140046356313920 spec.py:349] Evaluating on the test split.
I0306 11:48:15.851235 140046356313920 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0306 11:48:15.856269 140046356313920 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0306 11:48:15.902925 140046356313920 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0306 11:48:21.090684 140046356313920 submission_runner.py:413] Time since start: 87.17s, 	Step: 1, 	{'train/accuracy': 0.0008984374580904841, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 39.93411064147949, 'total_duration': 87.17051649093628, 'accumulated_submission_time': 39.93411064147949, 'accumulated_eval_time': 47.236292600631714, 'accumulated_logging_time': 0}
I0306 11:48:21.108267 139848958600960 logging_writer.py:48] [1] accumulated_eval_time=47.236293, accumulated_logging_time=0, accumulated_submission_time=39.934111, global_step=1, preemption_count=0, score=39.934111, test/accuracy=0.001000, test/loss=6.907757, test/num_examples=10000, total_duration=87.170516, train/accuracy=0.000898, train/loss=6.907756, validation/accuracy=0.001000, validation/loss=6.907756, validation/num_examples=50000
I0306 11:48:49.907399 139884861843200 logging_writer.py:48] [1] global_step=1, grad_norm=0.4766377806663513, loss=6.9077558517456055
I0306 11:48:50.304762 139884870235904 logging_writer.py:48] [2] global_step=2, grad_norm=0.5013837218284607, loss=6.907754898071289
I0306 11:48:50.715850 139884861843200 logging_writer.py:48] [3] global_step=3, grad_norm=0.5026038289070129, loss=6.907752513885498
I0306 11:48:51.115775 139884870235904 logging_writer.py:48] [4] global_step=4, grad_norm=0.5107072591781616, loss=6.907753944396973
I0306 11:48:51.513723 139884861843200 logging_writer.py:48] [5] global_step=5, grad_norm=0.4977814853191376, loss=6.907751560211182
I0306 11:48:51.921853 139884870235904 logging_writer.py:48] [6] global_step=6, grad_norm=0.49421456456184387, loss=6.907751083374023
I0306 11:48:52.327699 139884861843200 logging_writer.py:48] [7] global_step=7, grad_norm=0.4958484172821045, loss=6.907748222351074
I0306 11:48:52.724708 139884870235904 logging_writer.py:48] [8] global_step=8, grad_norm=0.5052130818367004, loss=6.907752990722656
I0306 11:48:53.127455 139884861843200 logging_writer.py:48] [9] global_step=9, grad_norm=0.4997151792049408, loss=6.907739162445068
I0306 11:48:53.526037 139884870235904 logging_writer.py:48] [10] global_step=10, grad_norm=0.4816496670246124, loss=6.9077606201171875
I0306 11:48:53.933822 139884861843200 logging_writer.py:48] [11] global_step=11, grad_norm=0.5116518139839172, loss=6.907742977142334
I0306 11:48:54.344223 139884870235904 logging_writer.py:48] [12] global_step=12, grad_norm=0.5161557793617249, loss=6.907733917236328
I0306 11:48:54.754166 139884861843200 logging_writer.py:48] [13] global_step=13, grad_norm=0.5196222066879272, loss=6.907709121704102
I0306 11:48:55.158865 139884870235904 logging_writer.py:48] [14] global_step=14, grad_norm=0.5148571133613586, loss=6.907742500305176
I0306 11:48:55.560952 139884861843200 logging_writer.py:48] [15] global_step=15, grad_norm=0.5122119784355164, loss=6.907721996307373
I0306 11:48:55.965635 139884870235904 logging_writer.py:48] [16] global_step=16, grad_norm=0.5206007957458496, loss=6.907733917236328
I0306 11:48:56.367511 139884861843200 logging_writer.py:48] [17] global_step=17, grad_norm=0.51217120885849, loss=6.907773971557617
I0306 11:48:56.765735 139884870235904 logging_writer.py:48] [18] global_step=18, grad_norm=0.5196419358253479, loss=6.907712936401367
I0306 11:48:57.169272 139884861843200 logging_writer.py:48] [19] global_step=19, grad_norm=0.5097090005874634, loss=6.907723903656006
I0306 11:48:57.569869 139884870235904 logging_writer.py:48] [20] global_step=20, grad_norm=0.512190580368042, loss=6.907744407653809
I0306 11:48:57.980135 139884861843200 logging_writer.py:48] [21] global_step=21, grad_norm=0.522148609161377, loss=6.907624244689941
I0306 11:48:58.378320 139884870235904 logging_writer.py:48] [22] global_step=22, grad_norm=0.519647479057312, loss=6.9076924324035645
I0306 11:48:58.782598 139884861843200 logging_writer.py:48] [23] global_step=23, grad_norm=0.5049275755882263, loss=6.907718658447266
I0306 11:48:59.184334 139884870235904 logging_writer.py:48] [24] global_step=24, grad_norm=0.49153292179107666, loss=6.907668590545654
I0306 11:48:59.587109 139884861843200 logging_writer.py:48] [25] global_step=25, grad_norm=0.49588266015052795, loss=6.907764434814453
I0306 11:48:59.998018 139884870235904 logging_writer.py:48] [26] global_step=26, grad_norm=0.5278874039649963, loss=6.907654762268066
I0306 11:49:00.405067 139884861843200 logging_writer.py:48] [27] global_step=27, grad_norm=0.5072006583213806, loss=6.9077229499816895
I0306 11:49:00.807779 139884870235904 logging_writer.py:48] [28] global_step=28, grad_norm=0.5225314497947693, loss=6.907733917236328
I0306 11:49:01.216070 139884861843200 logging_writer.py:48] [29] global_step=29, grad_norm=0.5181149244308472, loss=6.90779972076416
I0306 11:49:01.622629 139884870235904 logging_writer.py:48] [30] global_step=30, grad_norm=0.5229712128639221, loss=6.907609462738037
I0306 11:49:02.021460 139884861843200 logging_writer.py:48] [31] global_step=31, grad_norm=0.48280876874923706, loss=6.907702445983887
I0306 11:49:02.431609 139884870235904 logging_writer.py:48] [32] global_step=32, grad_norm=0.4941002428531647, loss=6.907590389251709
I0306 11:49:02.830909 139884861843200 logging_writer.py:48] [33] global_step=33, grad_norm=0.4887225031852722, loss=6.907716274261475
I0306 11:49:03.241281 139884870235904 logging_writer.py:48] [34] global_step=34, grad_norm=0.5040084719657898, loss=6.907665252685547
I0306 11:49:03.653664 139884861843200 logging_writer.py:48] [35] global_step=35, grad_norm=0.5004910826683044, loss=6.907632350921631
I0306 11:49:04.056569 139884870235904 logging_writer.py:48] [36] global_step=36, grad_norm=0.5221587419509888, loss=6.907598972320557
I0306 11:49:04.456574 139884861843200 logging_writer.py:48] [37] global_step=37, grad_norm=0.5012033581733704, loss=6.907758712768555
I0306 11:49:04.859297 139884870235904 logging_writer.py:48] [38] global_step=38, grad_norm=0.4886012077331543, loss=6.90768575668335
I0306 11:49:05.270047 139884861843200 logging_writer.py:48] [39] global_step=39, grad_norm=0.5153070092201233, loss=6.9076056480407715
I0306 11:49:05.678298 139884870235904 logging_writer.py:48] [40] global_step=40, grad_norm=0.5212205052375793, loss=6.907564163208008
I0306 11:49:06.088563 139884861843200 logging_writer.py:48] [41] global_step=41, grad_norm=0.5042503476142883, loss=6.907675266265869
I0306 11:49:06.489146 139884870235904 logging_writer.py:48] [42] global_step=42, grad_norm=0.5246984958648682, loss=6.907480716705322
I0306 11:49:06.896724 139884861843200 logging_writer.py:48] [43] global_step=43, grad_norm=0.5381023287773132, loss=6.907454967498779
I0306 11:49:07.306514 139884870235904 logging_writer.py:48] [44] global_step=44, grad_norm=0.48907172679901123, loss=6.907763481140137
I0306 11:49:07.707361 139884861843200 logging_writer.py:48] [45] global_step=45, grad_norm=0.5259596109390259, loss=6.907639503479004
I0306 11:49:08.115281 139884870235904 logging_writer.py:48] [46] global_step=46, grad_norm=0.4923560917377472, loss=6.907474994659424
I0306 11:49:08.514538 139884861843200 logging_writer.py:48] [47] global_step=47, grad_norm=0.5313974022865295, loss=6.907197952270508
I0306 11:49:08.919152 139884870235904 logging_writer.py:48] [48] global_step=48, grad_norm=0.5365269184112549, loss=6.907539367675781
I0306 11:49:09.327667 139884861843200 logging_writer.py:48] [49] global_step=49, grad_norm=0.5041047930717468, loss=6.907426834106445
I0306 11:49:09.724920 139884870235904 logging_writer.py:48] [50] global_step=50, grad_norm=0.5282918214797974, loss=6.907304286956787
I0306 11:49:10.127880 139884861843200 logging_writer.py:48] [51] global_step=51, grad_norm=0.5348526835441589, loss=6.90751314163208
I0306 11:49:10.531473 139884870235904 logging_writer.py:48] [52] global_step=52, grad_norm=0.5357579588890076, loss=6.907417297363281
I0306 11:49:10.944123 139884861843200 logging_writer.py:48] [53] global_step=53, grad_norm=0.5234463214874268, loss=6.907164096832275
I0306 11:49:11.357619 139884870235904 logging_writer.py:48] [54] global_step=54, grad_norm=0.5453605651855469, loss=6.907403945922852
I0306 11:49:11.759890 139884861843200 logging_writer.py:48] [55] global_step=55, grad_norm=0.5000908970832825, loss=6.907364845275879
I0306 11:49:12.172086 139884870235904 logging_writer.py:48] [56] global_step=56, grad_norm=0.5545632839202881, loss=6.907352924346924
I0306 11:49:12.572829 139884861843200 logging_writer.py:48] [57] global_step=57, grad_norm=0.5243490934371948, loss=6.9071550369262695
I0306 11:49:12.983663 139884870235904 logging_writer.py:48] [58] global_step=58, grad_norm=0.5340942144393921, loss=6.907041549682617
I0306 11:49:13.388560 139884861843200 logging_writer.py:48] [59] global_step=59, grad_norm=0.5563217401504517, loss=6.907160758972168
I0306 11:49:13.788070 139884870235904 logging_writer.py:48] [60] global_step=60, grad_norm=0.5264579057693481, loss=6.906966209411621
I0306 11:49:14.197625 139884861843200 logging_writer.py:48] [61] global_step=61, grad_norm=0.5372955799102783, loss=6.907026290893555
I0306 11:49:14.607796 139884870235904 logging_writer.py:48] [62] global_step=62, grad_norm=0.558904767036438, loss=6.907083988189697
I0306 11:49:15.010442 139884861843200 logging_writer.py:48] [63] global_step=63, grad_norm=0.5386890769004822, loss=6.907053470611572
I0306 11:49:15.413916 139884870235904 logging_writer.py:48] [64] global_step=64, grad_norm=0.5750569105148315, loss=6.906373500823975
I0306 11:49:15.815071 139884861843200 logging_writer.py:48] [65] global_step=65, grad_norm=0.5666248798370361, loss=6.9067559242248535
I0306 11:49:16.214187 139884870235904 logging_writer.py:48] [66] global_step=66, grad_norm=0.5715672969818115, loss=6.9066362380981445
I0306 11:49:16.614110 139884861843200 logging_writer.py:48] [67] global_step=67, grad_norm=0.5703902244567871, loss=6.906872749328613
I0306 11:49:17.008455 139884870235904 logging_writer.py:48] [68] global_step=68, grad_norm=0.5444075465202332, loss=6.90683650970459
I0306 11:49:17.415019 139884861843200 logging_writer.py:48] [69] global_step=69, grad_norm=0.5378837585449219, loss=6.90683650970459
I0306 11:49:17.817517 139884870235904 logging_writer.py:48] [70] global_step=70, grad_norm=0.5314843058586121, loss=6.9069366455078125
I0306 11:49:18.221548 139884861843200 logging_writer.py:48] [71] global_step=71, grad_norm=0.5936073660850525, loss=6.906796455383301
I0306 11:49:18.623585 139884870235904 logging_writer.py:48] [72] global_step=72, grad_norm=0.5429403185844421, loss=6.907275199890137
I0306 11:49:19.025956 139884861843200 logging_writer.py:48] [73] global_step=73, grad_norm=0.5794337391853333, loss=6.906327724456787
I0306 11:49:19.427743 139884870235904 logging_writer.py:48] [74] global_step=74, grad_norm=0.6037243008613586, loss=6.905610084533691
I0306 11:49:19.834306 139884861843200 logging_writer.py:48] [75] global_step=75, grad_norm=0.5646671056747437, loss=6.906254768371582
I0306 11:49:20.244556 139884870235904 logging_writer.py:48] [76] global_step=76, grad_norm=0.6153704524040222, loss=6.905320167541504
I0306 11:49:20.648278 139884861843200 logging_writer.py:48] [77] global_step=77, grad_norm=0.5907476544380188, loss=6.905660629272461
I0306 11:49:21.058698 139884870235904 logging_writer.py:48] [78] global_step=78, grad_norm=0.5854513049125671, loss=6.905831336975098
I0306 11:49:21.464923 139884861843200 logging_writer.py:48] [79] global_step=79, grad_norm=0.5583239793777466, loss=6.905496597290039
I0306 11:49:21.874686 139884870235904 logging_writer.py:48] [80] global_step=80, grad_norm=0.628109335899353, loss=6.90504264831543
I0306 11:49:22.281193 139884861843200 logging_writer.py:48] [81] global_step=81, grad_norm=0.5926212072372437, loss=6.905432224273682
I0306 11:49:22.687166 139884870235904 logging_writer.py:48] [82] global_step=82, grad_norm=0.6279777884483337, loss=6.9051594734191895
I0306 11:49:23.098321 139884861843200 logging_writer.py:48] [83] global_step=83, grad_norm=0.6163225173950195, loss=6.904512882232666
I0306 11:49:23.510061 139884870235904 logging_writer.py:48] [84] global_step=84, grad_norm=0.5427348613739014, loss=6.906772613525391
I0306 11:49:23.918950 139884861843200 logging_writer.py:48] [85] global_step=85, grad_norm=0.6320960521697998, loss=6.9049906730651855
I0306 11:49:24.327119 139884870235904 logging_writer.py:48] [86] global_step=86, grad_norm=0.6262165307998657, loss=6.904393672943115
I0306 11:49:24.725769 139884861843200 logging_writer.py:48] [87] global_step=87, grad_norm=0.6381164789199829, loss=6.903929710388184
I0306 11:49:25.134805 139884870235904 logging_writer.py:48] [88] global_step=88, grad_norm=0.526612401008606, loss=6.9058146476745605
I0306 11:49:25.534590 139884861843200 logging_writer.py:48] [89] global_step=89, grad_norm=0.6347165703773499, loss=6.905588150024414
I0306 11:49:25.945131 139884870235904 logging_writer.py:48] [90] global_step=90, grad_norm=0.6786211133003235, loss=6.903233528137207
I0306 11:49:26.356485 139884861843200 logging_writer.py:48] [91] global_step=91, grad_norm=0.5322975516319275, loss=6.906607151031494
I0306 11:49:26.770852 139884870235904 logging_writer.py:48] [92] global_step=92, grad_norm=0.5465118885040283, loss=6.906466007232666
I0306 11:49:27.185803 139884861843200 logging_writer.py:48] [93] global_step=93, grad_norm=0.6427760124206543, loss=6.905062675476074
I0306 11:49:27.594240 139884870235904 logging_writer.py:48] [94] global_step=94, grad_norm=0.630241334438324, loss=6.903954029083252
I0306 11:49:28.000121 139884861843200 logging_writer.py:48] [95] global_step=95, grad_norm=0.6452612280845642, loss=6.903314590454102
I0306 11:49:28.404825 139884870235904 logging_writer.py:48] [96] global_step=96, grad_norm=0.6467800140380859, loss=6.903492450714111
I0306 11:49:28.815015 139884861843200 logging_writer.py:48] [97] global_step=97, grad_norm=0.601568877696991, loss=6.904916763305664
I0306 11:49:29.226536 139884870235904 logging_writer.py:48] [98] global_step=98, grad_norm=0.642946183681488, loss=6.90352725982666
I0306 11:49:29.634808 139884861843200 logging_writer.py:48] [99] global_step=99, grad_norm=0.648586094379425, loss=6.903270721435547
I0306 11:49:30.039729 139884870235904 logging_writer.py:48] [100] global_step=100, grad_norm=0.5341060757637024, loss=6.905589580535889
I0306 11:52:05.738431 139884861843200 logging_writer.py:48] [500] global_step=500, grad_norm=0.8307052254676819, loss=6.815274238586426
I0306 11:55:20.527477 139884870235904 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.4817265272140503, loss=6.39774751663208
I0306 11:55:21.404052 140046356313920 spec.py:321] Evaluating on the training split.
I0306 11:55:33.327647 140046356313920 spec.py:333] Evaluating on the validation split.
I0306 11:55:45.642013 140046356313920 spec.py:349] Evaluating on the test split.
I0306 11:55:48.360899 140046356313920 submission_runner.py:413] Time since start: 534.44s, 	Step: 1004, 	{'train/accuracy': 0.018183592706918716, 'train/loss': 6.238394260406494, 'validation/accuracy': 0.017479998990893364, 'validation/loss': 6.2476396560668945, 'validation/num_examples': 50000, 'test/accuracy': 0.013000000268220901, 'test/loss': 6.317867279052734, 'test/num_examples': 10000, 'score': 460.1861529350281, 'total_duration': 534.4407677650452, 'accumulated_submission_time': 460.1861529350281, 'accumulated_eval_time': 74.19310164451599, 'accumulated_logging_time': 0.027152538299560547}
I0306 11:55:48.375538 139848966993664 logging_writer.py:48] [1004] accumulated_eval_time=74.193102, accumulated_logging_time=0.027153, accumulated_submission_time=460.186153, global_step=1004, preemption_count=0, score=460.186153, test/accuracy=0.013000, test/loss=6.317867, test/num_examples=10000, total_duration=534.440768, train/accuracy=0.018184, train/loss=6.238394, validation/accuracy=0.017480, validation/loss=6.247640, validation/num_examples=50000
I0306 11:59:02.041676 139849050855168 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.4539158344268799, loss=6.288601875305176
I0306 12:02:17.041213 139848966993664 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.4355202913284302, loss=5.976337432861328
I0306 12:02:48.731300 140046356313920 spec.py:321] Evaluating on the training split.
I0306 12:03:00.663579 140046356313920 spec.py:333] Evaluating on the validation split.
I0306 12:03:13.235379 140046356313920 spec.py:349] Evaluating on the test split.
I0306 12:03:15.925120 140046356313920 submission_runner.py:413] Time since start: 982.00s, 	Step: 2083, 	{'train/accuracy': 0.04937499761581421, 'train/loss': 5.603709697723389, 'validation/accuracy': 0.04901999980211258, 'validation/loss': 5.6366424560546875, 'validation/num_examples': 50000, 'test/accuracy': 0.03710000216960907, 'test/loss': 5.789898872375488, 'test/num_examples': 10000, 'score': 880.4957611560822, 'total_duration': 982.0049915313721, 'accumulated_submission_time': 880.4957611560822, 'accumulated_eval_time': 101.38692164421082, 'accumulated_logging_time': 0.05028724670410156}
I0306 12:03:15.939745 139849050855168 logging_writer.py:48] [2083] accumulated_eval_time=101.386922, accumulated_logging_time=0.050287, accumulated_submission_time=880.495761, global_step=2083, preemption_count=0, score=880.495761, test/accuracy=0.037100, test/loss=5.789899, test/num_examples=10000, total_duration=982.004992, train/accuracy=0.049375, train/loss=5.603710, validation/accuracy=0.049020, validation/loss=5.636642, validation/num_examples=50000
I0306 12:05:58.934934 139848966993664 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.7623378038406372, loss=5.861836910247803
I0306 12:09:14.115720 139849050855168 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.5450390577316284, loss=5.6579084396362305
I0306 12:10:16.298194 140046356313920 spec.py:321] Evaluating on the training split.
I0306 12:10:28.260853 140046356313920 spec.py:333] Evaluating on the validation split.
I0306 12:10:40.817088 140046356313920 spec.py:349] Evaluating on the test split.
I0306 12:10:43.517183 140046356313920 submission_runner.py:413] Time since start: 1429.60s, 	Step: 3161, 	{'train/accuracy': 0.08142577856779099, 'train/loss': 5.167095184326172, 'validation/accuracy': 0.0753600001335144, 'validation/loss': 5.21644926071167, 'validation/num_examples': 50000, 'test/accuracy': 0.060600001364946365, 'test/loss': 5.427204608917236, 'test/num_examples': 10000, 'score': 1300.8083004951477, 'total_duration': 1429.5970304012299, 'accumulated_submission_time': 1300.8083004951477, 'accumulated_eval_time': 128.6058909893036, 'accumulated_logging_time': 0.07343506813049316}
I0306 12:10:43.533459 139848966993664 logging_writer.py:48] [3161] accumulated_eval_time=128.605891, accumulated_logging_time=0.073435, accumulated_submission_time=1300.808300, global_step=3161, preemption_count=0, score=1300.808300, test/accuracy=0.060600, test/loss=5.427205, test/num_examples=10000, total_duration=1429.597030, train/accuracy=0.081426, train/loss=5.167095, validation/accuracy=0.075360, validation/loss=5.216449, validation/num_examples=50000
I0306 12:12:56.284989 139849050855168 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.3965250253677368, loss=6.302175998687744
I0306 12:16:11.593992 139848966993664 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.475455403327942, loss=5.503365516662598
I0306 12:17:43.873384 140046356313920 spec.py:321] Evaluating on the training split.
I0306 12:17:55.828361 140046356313920 spec.py:333] Evaluating on the validation split.
I0306 12:18:08.145049 140046356313920 spec.py:349] Evaluating on the test split.
I0306 12:18:10.847470 140046356313920 submission_runner.py:413] Time since start: 1876.93s, 	Step: 4238, 	{'train/accuracy': 0.1199023425579071, 'train/loss': 4.7676472663879395, 'validation/accuracy': 0.1127999946475029, 'validation/loss': 4.8194355964660645, 'validation/num_examples': 50000, 'test/accuracy': 0.08570000529289246, 'test/loss': 5.088362693786621, 'test/num_examples': 10000, 'score': 1721.1008322238922, 'total_duration': 1876.9273393154144, 'accumulated_submission_time': 1721.1008322238922, 'accumulated_eval_time': 155.5799572467804, 'accumulated_logging_time': 0.09889745712280273}
I0306 12:18:10.863150 139849050855168 logging_writer.py:48] [4238] accumulated_eval_time=155.579957, accumulated_logging_time=0.098897, accumulated_submission_time=1721.100832, global_step=4238, preemption_count=0, score=1721.100832, test/accuracy=0.085700, test/loss=5.088363, test/num_examples=10000, total_duration=1876.927339, train/accuracy=0.119902, train/loss=4.767647, validation/accuracy=0.112800, validation/loss=4.819436, validation/num_examples=50000
I0306 12:19:53.571511 139848966993664 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.5762830972671509, loss=5.296864986419678
I0306 12:23:09.032848 139849050855168 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.5120635032653809, loss=5.060297012329102
I0306 12:25:11.124667 140046356313920 spec.py:321] Evaluating on the training split.
I0306 12:25:23.125268 140046356313920 spec.py:333] Evaluating on the validation split.
I0306 12:25:35.464842 140046356313920 spec.py:349] Evaluating on the test split.
I0306 12:25:38.158549 140046356313920 submission_runner.py:413] Time since start: 2324.24s, 	Step: 5314, 	{'train/accuracy': 0.16050781309604645, 'train/loss': 4.442964553833008, 'validation/accuracy': 0.15065999329090118, 'validation/loss': 4.499810695648193, 'validation/num_examples': 50000, 'test/accuracy': 0.11250000447034836, 'test/loss': 4.832035541534424, 'test/num_examples': 10000, 'score': 2141.316436767578, 'total_duration': 2324.2384185791016, 'accumulated_submission_time': 2141.316436767578, 'accumulated_eval_time': 182.61382508277893, 'accumulated_logging_time': 0.12318015098571777}
I0306 12:25:38.174254 139848966993664 logging_writer.py:48] [5314] accumulated_eval_time=182.613825, accumulated_logging_time=0.123180, accumulated_submission_time=2141.316437, global_step=5314, preemption_count=0, score=2141.316437, test/accuracy=0.112500, test/loss=4.832036, test/num_examples=10000, total_duration=2324.238419, train/accuracy=0.160508, train/loss=4.442965, validation/accuracy=0.150660, validation/loss=4.499811, validation/num_examples=50000
I0306 12:26:51.294372 139849050855168 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.2126576900482178, loss=5.672731399536133
I0306 12:30:06.866482 139848966993664 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.3852869272232056, loss=5.480071544647217
I0306 12:32:38.337208 140046356313920 spec.py:321] Evaluating on the training split.
I0306 12:32:50.319827 140046356313920 spec.py:333] Evaluating on the validation split.
I0306 12:33:02.661554 140046356313920 spec.py:349] Evaluating on the test split.
I0306 12:33:05.366075 140046356313920 submission_runner.py:413] Time since start: 2771.45s, 	Step: 6389, 	{'train/accuracy': 0.19923827052116394, 'train/loss': 4.063134670257568, 'validation/accuracy': 0.18493999540805817, 'validation/loss': 4.153113842010498, 'validation/num_examples': 50000, 'test/accuracy': 0.140500009059906, 'test/loss': 4.567753314971924, 'test/num_examples': 10000, 'score': 2561.4309084415436, 'total_duration': 2771.4459459781647, 'accumulated_submission_time': 2561.4309084415436, 'accumulated_eval_time': 209.64268493652344, 'accumulated_logging_time': 0.1505284309387207}
I0306 12:33:05.381985 139849050855168 logging_writer.py:48] [6389] accumulated_eval_time=209.642685, accumulated_logging_time=0.150528, accumulated_submission_time=2561.430908, global_step=6389, preemption_count=0, score=2561.430908, test/accuracy=0.140500, test/loss=4.567753, test/num_examples=10000, total_duration=2771.445946, train/accuracy=0.199238, train/loss=4.063135, validation/accuracy=0.184940, validation/loss=4.153114, validation/num_examples=50000
I0306 12:33:49.133956 139848966993664 logging_writer.py:48] [6500] global_step=6500, grad_norm=1.8762537240982056, loss=4.888550758361816
I0306 12:37:04.715190 139849050855168 logging_writer.py:48] [7000] global_step=7000, grad_norm=1.392807126045227, loss=4.828146934509277
I0306 12:40:05.529547 140046356313920 spec.py:321] Evaluating on the training split.
I0306 12:40:17.495014 140046356313920 spec.py:333] Evaluating on the validation split.
I0306 12:40:29.860849 140046356313920 spec.py:349] Evaluating on the test split.
I0306 12:40:32.566677 140046356313920 submission_runner.py:413] Time since start: 3218.65s, 	Step: 7464, 	{'train/accuracy': 0.2256445288658142, 'train/loss': 3.8246026039123535, 'validation/accuracy': 0.2128800004720688, 'validation/loss': 3.936664342880249, 'validation/num_examples': 50000, 'test/accuracy': 0.16020001471042633, 'test/loss': 4.363847255706787, 'test/num_examples': 10000, 'score': 2981.532466650009, 'total_duration': 3218.646544933319, 'accumulated_submission_time': 2981.532466650009, 'accumulated_eval_time': 236.6798071861267, 'accumulated_logging_time': 0.17493867874145508}
I0306 12:40:32.581913 139848966993664 logging_writer.py:48] [7464] accumulated_eval_time=236.679807, accumulated_logging_time=0.174939, accumulated_submission_time=2981.532467, global_step=7464, preemption_count=0, score=2981.532467, test/accuracy=0.160200, test/loss=4.363847, test/num_examples=10000, total_duration=3218.646545, train/accuracy=0.225645, train/loss=3.824603, validation/accuracy=0.212880, validation/loss=3.936664, validation/num_examples=50000
I0306 12:40:47.055915 139849050855168 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.4179329872131348, loss=4.68642520904541
I0306 12:44:02.697090 139848966993664 logging_writer.py:48] [8000] global_step=8000, grad_norm=1.3312407732009888, loss=4.513541221618652
I0306 12:47:18.323911 139849050855168 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.4761121273040771, loss=4.411426067352295
I0306 12:47:32.889062 140046356313920 spec.py:321] Evaluating on the training split.
I0306 12:47:44.866724 140046356313920 spec.py:333] Evaluating on the validation split.
I0306 12:47:57.179366 140046356313920 spec.py:349] Evaluating on the test split.
I0306 12:47:59.869820 140046356313920 submission_runner.py:413] Time since start: 3665.95s, 	Step: 8539, 	{'train/accuracy': 0.27769529819488525, 'train/loss': 3.5020840167999268, 'validation/accuracy': 0.24740000069141388, 'validation/loss': 3.6972768306732178, 'validation/num_examples': 50000, 'test/accuracy': 0.1851000040769577, 'test/loss': 4.176933765411377, 'test/num_examples': 10000, 'score': 3401.7890074253082, 'total_duration': 3665.949693918228, 'accumulated_submission_time': 3401.7890074253082, 'accumulated_eval_time': 263.6605386734009, 'accumulated_logging_time': 0.20311331748962402}
I0306 12:47:59.884824 139848966993664 logging_writer.py:48] [8539] accumulated_eval_time=263.660539, accumulated_logging_time=0.203113, accumulated_submission_time=3401.789007, global_step=8539, preemption_count=0, score=3401.789007, test/accuracy=0.185100, test/loss=4.176934, test/num_examples=10000, total_duration=3665.949694, train/accuracy=0.277695, train/loss=3.502084, validation/accuracy=0.247400, validation/loss=3.697277, validation/num_examples=50000
I0306 12:51:00.562832 139849050855168 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.1567140817642212, loss=4.977063179016113
I0306 12:54:16.221250 139848966993664 logging_writer.py:48] [9500] global_step=9500, grad_norm=1.3634059429168701, loss=4.1787896156311035
I0306 12:55:00.140701 140046356313920 spec.py:321] Evaluating on the training split.
I0306 12:55:12.134953 140046356313920 spec.py:333] Evaluating on the validation split.
I0306 12:55:24.524200 140046356313920 spec.py:349] Evaluating on the test split.
I0306 12:55:27.222474 140046356313920 submission_runner.py:413] Time since start: 4113.30s, 	Step: 9614, 	{'train/accuracy': 0.3079492151737213, 'train/loss': 3.29396915435791, 'validation/accuracy': 0.28275999426841736, 'validation/loss': 3.439666509628296, 'validation/num_examples': 50000, 'test/accuracy': 0.21670001745224, 'test/loss': 3.9382307529449463, 'test/num_examples': 10000, 'score': 3821.9996366500854, 'total_duration': 4113.302345752716, 'accumulated_submission_time': 3821.9996366500854, 'accumulated_eval_time': 290.74228501319885, 'accumulated_logging_time': 0.22652745246887207}
I0306 12:55:27.237917 139849050855168 logging_writer.py:48] [9614] accumulated_eval_time=290.742285, accumulated_logging_time=0.226527, accumulated_submission_time=3821.999637, global_step=9614, preemption_count=0, score=3821.999637, test/accuracy=0.216700, test/loss=3.938231, test/num_examples=10000, total_duration=4113.302346, train/accuracy=0.307949, train/loss=3.293969, validation/accuracy=0.282760, validation/loss=3.439667, validation/num_examples=50000
I0306 12:57:58.568510 139848966993664 logging_writer.py:48] [10000] global_step=10000, grad_norm=1.4328737258911133, loss=4.13301944732666
I0306 13:01:14.181204 139849050855168 logging_writer.py:48] [10500] global_step=10500, grad_norm=1.4434903860092163, loss=4.2087202072143555
I0306 13:02:27.451288 140046356313920 spec.py:321] Evaluating on the training split.
I0306 13:02:39.446992 140046356313920 spec.py:333] Evaluating on the validation split.
I0306 13:02:51.798174 140046356313920 spec.py:349] Evaluating on the test split.
I0306 13:02:54.492359 140046356313920 submission_runner.py:413] Time since start: 4560.57s, 	Step: 10689, 	{'train/accuracy': 0.32421875, 'train/loss': 3.2122998237609863, 'validation/accuracy': 0.3041200041770935, 'validation/loss': 3.3279383182525635, 'validation/num_examples': 50000, 'test/accuracy': 0.23280000686645508, 'test/loss': 3.851491689682007, 'test/num_examples': 10000, 'score': 4242.166989564896, 'total_duration': 4560.5722279548645, 'accumulated_submission_time': 4242.166989564896, 'accumulated_eval_time': 317.7833366394043, 'accumulated_logging_time': 0.25070738792419434}
I0306 13:02:54.507689 139848966993664 logging_writer.py:48] [10689] accumulated_eval_time=317.783337, accumulated_logging_time=0.250707, accumulated_submission_time=4242.166990, global_step=10689, preemption_count=0, score=4242.166990, test/accuracy=0.232800, test/loss=3.851492, test/num_examples=10000, total_duration=4560.572228, train/accuracy=0.324219, train/loss=3.212300, validation/accuracy=0.304120, validation/loss=3.327938, validation/num_examples=50000
I0306 13:04:56.515885 139849050855168 logging_writer.py:48] [11000] global_step=11000, grad_norm=1.2422958612442017, loss=4.33397102355957
I0306 13:08:12.078767 139848966993664 logging_writer.py:48] [11500] global_step=11500, grad_norm=1.2334976196289062, loss=3.922358989715576
I0306 13:09:54.697363 140046356313920 spec.py:321] Evaluating on the training split.
I0306 13:10:06.712332 140046356313920 spec.py:333] Evaluating on the validation split.
I0306 13:10:19.060649 140046356313920 spec.py:349] Evaluating on the test split.
I0306 13:10:21.771327 140046356313920 submission_runner.py:413] Time since start: 5007.85s, 	Step: 11764, 	{'train/accuracy': 0.3526367247104645, 'train/loss': 3.0422210693359375, 'validation/accuracy': 0.32580000162124634, 'validation/loss': 3.2038938999176025, 'validation/num_examples': 50000, 'test/accuracy': 0.250900000333786, 'test/loss': 3.7330782413482666, 'test/num_examples': 10000, 'score': 4662.309056758881, 'total_duration': 5007.851195812225, 'accumulated_submission_time': 4662.309056758881, 'accumulated_eval_time': 344.857284784317, 'accumulated_logging_time': 0.27531862258911133}
I0306 13:10:21.786677 139849050855168 logging_writer.py:48] [11764] accumulated_eval_time=344.857285, accumulated_logging_time=0.275319, accumulated_submission_time=4662.309057, global_step=11764, preemption_count=0, score=4662.309057, test/accuracy=0.250900, test/loss=3.733078, test/num_examples=10000, total_duration=5007.851196, train/accuracy=0.352637, train/loss=3.042221, validation/accuracy=0.325800, validation/loss=3.203894, validation/num_examples=50000
I0306 13:11:54.443632 139848966993664 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.0002508163452148, loss=6.046243667602539
I0306 13:15:10.031201 139849050855168 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.9437575936317444, loss=5.794958591461182
I0306 13:17:21.977300 140046356313920 spec.py:321] Evaluating on the training split.
I0306 13:17:34.029358 140046356313920 spec.py:333] Evaluating on the validation split.
I0306 13:17:46.642616 140046356313920 spec.py:349] Evaluating on the test split.
I0306 13:17:49.341180 140046356313920 submission_runner.py:413] Time since start: 5455.42s, 	Step: 12839, 	{'train/accuracy': 0.3786718547344208, 'train/loss': 2.84712553024292, 'validation/accuracy': 0.3472200036048889, 'validation/loss': 3.021505832672119, 'validation/num_examples': 50000, 'test/accuracy': 0.26750001311302185, 'test/loss': 3.576693534851074, 'test/num_examples': 10000, 'score': 5082.453165769577, 'total_duration': 5455.421036720276, 'accumulated_submission_time': 5082.453165769577, 'accumulated_eval_time': 372.2211401462555, 'accumulated_logging_time': 0.2995767593383789}
I0306 13:17:49.357922 139848966993664 logging_writer.py:48] [12839] accumulated_eval_time=372.221140, accumulated_logging_time=0.299577, accumulated_submission_time=5082.453166, global_step=12839, preemption_count=0, score=5082.453166, test/accuracy=0.267500, test/loss=3.576694, test/num_examples=10000, total_duration=5455.421037, train/accuracy=0.378672, train/loss=2.847126, validation/accuracy=0.347220, validation/loss=3.021506, validation/num_examples=50000
I0306 13:18:52.663635 139849050855168 logging_writer.py:48] [13000] global_step=13000, grad_norm=1.2969608306884766, loss=3.8646700382232666
I0306 13:22:08.238349 139848966993664 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.3295962810516357, loss=3.991680860519409
I0306 13:24:49.472890 140046356313920 spec.py:321] Evaluating on the training split.
I0306 13:25:01.469670 140046356313920 spec.py:333] Evaluating on the validation split.
I0306 13:25:14.082333 140046356313920 spec.py:349] Evaluating on the test split.
I0306 13:25:16.772766 140046356313920 submission_runner.py:413] Time since start: 5902.85s, 	Step: 13914, 	{'train/accuracy': 0.42033201456069946, 'train/loss': 2.659414529800415, 'validation/accuracy': 0.36271998286247253, 'validation/loss': 2.9436514377593994, 'validation/num_examples': 50000, 'test/accuracy': 0.2824999988079071, 'test/loss': 3.5125060081481934, 'test/num_examples': 10000, 'score': 5502.520686388016, 'total_duration': 5902.85263133049, 'accumulated_submission_time': 5502.520686388016, 'accumulated_eval_time': 399.5210020542145, 'accumulated_logging_time': 0.32538747787475586}
I0306 13:25:16.787648 139849050855168 logging_writer.py:48] [13914] accumulated_eval_time=399.521002, accumulated_logging_time=0.325387, accumulated_submission_time=5502.520686, global_step=13914, preemption_count=0, score=5502.520686, test/accuracy=0.282500, test/loss=3.512506, test/num_examples=10000, total_duration=5902.852631, train/accuracy=0.420332, train/loss=2.659415, validation/accuracy=0.362720, validation/loss=2.943651, validation/num_examples=50000
I0306 13:25:50.778385 139848966993664 logging_writer.py:48] [14000] global_step=14000, grad_norm=1.3718862533569336, loss=3.7359697818756104
I0306 13:29:06.332889 139849050855168 logging_writer.py:48] [14500] global_step=14500, grad_norm=1.233491063117981, loss=3.6829686164855957
I0306 13:32:16.963185 140046356313920 spec.py:321] Evaluating on the training split.
I0306 13:32:29.096278 140046356313920 spec.py:333] Evaluating on the validation split.
I0306 13:32:41.441644 140046356313920 spec.py:349] Evaluating on the test split.
I0306 13:32:44.138605 140046356313920 submission_runner.py:413] Time since start: 6350.22s, 	Step: 14989, 	{'train/accuracy': 0.42249998450279236, 'train/loss': 2.6097254753112793, 'validation/accuracy': 0.39111998677253723, 'validation/loss': 2.791548252105713, 'validation/num_examples': 50000, 'test/accuracy': 0.2979000210762024, 'test/loss': 3.3838934898376465, 'test/num_examples': 10000, 'score': 5922.650587320328, 'total_duration': 6350.218457937241, 'accumulated_submission_time': 5922.650587320328, 'accumulated_eval_time': 426.6964168548584, 'accumulated_logging_time': 0.34883737564086914}
I0306 13:32:44.154617 139848966993664 logging_writer.py:48] [14989] accumulated_eval_time=426.696417, accumulated_logging_time=0.348837, accumulated_submission_time=5922.650587, global_step=14989, preemption_count=0, score=5922.650587, test/accuracy=0.297900, test/loss=3.383893, test/num_examples=10000, total_duration=6350.218458, train/accuracy=0.422500, train/loss=2.609725, validation/accuracy=0.391120, validation/loss=2.791548, validation/num_examples=50000
I0306 13:32:48.854676 139849050855168 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.8992985486984253, loss=5.477388858795166
I0306 13:36:04.429030 139848966993664 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.2065562009811401, loss=4.002233505249023
I0306 13:39:20.043517 139849050855168 logging_writer.py:48] [16000] global_step=16000, grad_norm=1.1570684909820557, loss=4.099518775939941
I0306 13:39:44.393046 140046356313920 spec.py:321] Evaluating on the training split.
I0306 13:39:56.579971 140046356313920 spec.py:333] Evaluating on the validation split.
I0306 13:40:08.937279 140046356313920 spec.py:349] Evaluating on the test split.
I0306 13:40:11.629356 140046356313920 submission_runner.py:413] Time since start: 6797.71s, 	Step: 16064, 	{'train/accuracy': 0.4336913824081421, 'train/loss': 2.5707640647888184, 'validation/accuracy': 0.4001599848270416, 'validation/loss': 2.7457265853881836, 'validation/num_examples': 50000, 'test/accuracy': 0.31210002303123474, 'test/loss': 3.3176984786987305, 'test/num_examples': 10000, 'score': 6342.840968847275, 'total_duration': 6797.709228992462, 'accumulated_submission_time': 6342.840968847275, 'accumulated_eval_time': 453.9327321052551, 'accumulated_logging_time': 0.3738982677459717}
I0306 13:40:11.645012 139848966993664 logging_writer.py:48] [16064] accumulated_eval_time=453.932732, accumulated_logging_time=0.373898, accumulated_submission_time=6342.840969, global_step=16064, preemption_count=0, score=6342.840969, test/accuracy=0.312100, test/loss=3.317698, test/num_examples=10000, total_duration=6797.709229, train/accuracy=0.433691, train/loss=2.570764, validation/accuracy=0.400160, validation/loss=2.745727, validation/num_examples=50000
I0306 13:43:02.541543 139849050855168 logging_writer.py:48] [16500] global_step=16500, grad_norm=1.4157943725585938, loss=3.5521702766418457
I0306 13:46:18.149388 139848966993664 logging_writer.py:48] [17000] global_step=17000, grad_norm=1.3378942012786865, loss=3.3930811882019043
I0306 13:47:11.834854 140046356313920 spec.py:321] Evaluating on the training split.
I0306 13:47:24.013074 140046356313920 spec.py:333] Evaluating on the validation split.
I0306 13:47:36.376394 140046356313920 spec.py:349] Evaluating on the test split.
I0306 13:47:39.074058 140046356313920 submission_runner.py:413] Time since start: 7245.15s, 	Step: 17139, 	{'train/accuracy': 0.4525195360183716, 'train/loss': 2.465885877609253, 'validation/accuracy': 0.41086000204086304, 'validation/loss': 2.6918418407440186, 'validation/num_examples': 50000, 'test/accuracy': 0.31950002908706665, 'test/loss': 3.2830100059509277, 'test/num_examples': 10000, 'score': 6762.984875917435, 'total_duration': 7245.153917789459, 'accumulated_submission_time': 6762.984875917435, 'accumulated_eval_time': 481.17189621925354, 'accumulated_logging_time': 0.39795756340026855}
I0306 13:47:39.091209 139849050855168 logging_writer.py:48] [17139] accumulated_eval_time=481.171896, accumulated_logging_time=0.397958, accumulated_submission_time=6762.984876, global_step=17139, preemption_count=0, score=6762.984876, test/accuracy=0.319500, test/loss=3.283010, test/num_examples=10000, total_duration=7245.153918, train/accuracy=0.452520, train/loss=2.465886, validation/accuracy=0.410860, validation/loss=2.691842, validation/num_examples=50000
I0306 13:50:00.663857 139848966993664 logging_writer.py:48] [17500] global_step=17500, grad_norm=1.2835009098052979, loss=3.8969669342041016
I0306 13:53:16.221997 139849050855168 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.0732444524765015, loss=5.220379829406738
I0306 13:54:39.252473 140046356313920 spec.py:321] Evaluating on the training split.
I0306 13:54:51.517815 140046356313920 spec.py:333] Evaluating on the validation split.
I0306 13:55:03.869408 140046356313920 spec.py:349] Evaluating on the test split.
I0306 13:55:06.577133 140046356313920 submission_runner.py:413] Time since start: 7692.66s, 	Step: 18214, 	{'train/accuracy': 0.4608398377895355, 'train/loss': 2.4279236793518066, 'validation/accuracy': 0.42508000135421753, 'validation/loss': 2.6118505001068115, 'validation/num_examples': 50000, 'test/accuracy': 0.32610002160072327, 'test/loss': 3.2200052738189697, 'test/num_examples': 10000, 'score': 7183.099342107773, 'total_duration': 7692.657005786896, 'accumulated_submission_time': 7183.099342107773, 'accumulated_eval_time': 508.4965364933014, 'accumulated_logging_time': 0.42426609992980957}
I0306 13:55:06.593300 139848966993664 logging_writer.py:48] [18214] accumulated_eval_time=508.496536, accumulated_logging_time=0.424266, accumulated_submission_time=7183.099342, global_step=18214, preemption_count=0, score=7183.099342, test/accuracy=0.326100, test/loss=3.220005, test/num_examples=10000, total_duration=7692.657006, train/accuracy=0.460840, train/loss=2.427924, validation/accuracy=0.425080, validation/loss=2.611851, validation/num_examples=50000
I0306 13:56:58.788932 139849050855168 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.4653162956237793, loss=3.3970947265625
I0306 14:00:14.416309 139848966993664 logging_writer.py:48] [19000] global_step=19000, grad_norm=1.4619970321655273, loss=4.102080821990967
I0306 14:02:06.781499 140046356313920 spec.py:321] Evaluating on the training split.
I0306 14:02:19.170026 140046356313920 spec.py:333] Evaluating on the validation split.
I0306 14:02:31.540427 140046356313920 spec.py:349] Evaluating on the test split.
I0306 14:02:34.242471 140046356313920 submission_runner.py:413] Time since start: 8140.32s, 	Step: 19289, 	{'train/accuracy': 0.473457008600235, 'train/loss': 2.3687477111816406, 'validation/accuracy': 0.4354199767112732, 'validation/loss': 2.550684690475464, 'validation/num_examples': 50000, 'test/accuracy': 0.3393000066280365, 'test/loss': 3.161468029022217, 'test/num_examples': 10000, 'score': 7603.240678787231, 'total_duration': 8140.322327852249, 'accumulated_submission_time': 7603.240678787231, 'accumulated_eval_time': 535.9574823379517, 'accumulated_logging_time': 0.44896745681762695}
I0306 14:02:34.260924 139849050855168 logging_writer.py:48] [19289] accumulated_eval_time=535.957482, accumulated_logging_time=0.448967, accumulated_submission_time=7603.240679, global_step=19289, preemption_count=0, score=7603.240679, test/accuracy=0.339300, test/loss=3.161468, test/num_examples=10000, total_duration=8140.322328, train/accuracy=0.473457, train/loss=2.368748, validation/accuracy=0.435420, validation/loss=2.550685, validation/num_examples=50000
I0306 14:03:57.128831 139848966993664 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.3733793497085571, loss=3.224992275238037
I0306 14:07:12.718347 139849050855168 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.9657780528068542, loss=5.247488975524902
I0306 14:09:34.419414 140046356313920 spec.py:321] Evaluating on the training split.
I0306 14:09:46.890351 140046356313920 spec.py:333] Evaluating on the validation split.
I0306 14:09:59.468634 140046356313920 spec.py:349] Evaluating on the test split.
I0306 14:10:02.165798 140046356313920 submission_runner.py:413] Time since start: 8588.25s, 	Step: 20364, 	{'train/accuracy': 0.48640623688697815, 'train/loss': 2.279480457305908, 'validation/accuracy': 0.44711998105049133, 'validation/loss': 2.477479934692383, 'validation/num_examples': 50000, 'test/accuracy': 0.3477000296115875, 'test/loss': 3.099433422088623, 'test/num_examples': 10000, 'score': 8023.35152554512, 'total_duration': 8588.245665550232, 'accumulated_submission_time': 8023.35152554512, 'accumulated_eval_time': 563.7038655281067, 'accumulated_logging_time': 0.47674012184143066}
I0306 14:10:02.184244 139848966993664 logging_writer.py:48] [20364] accumulated_eval_time=563.703866, accumulated_logging_time=0.476740, accumulated_submission_time=8023.351526, global_step=20364, preemption_count=0, score=8023.351526, test/accuracy=0.347700, test/loss=3.099433, test/num_examples=10000, total_duration=8588.245666, train/accuracy=0.486406, train/loss=2.279480, validation/accuracy=0.447120, validation/loss=2.477480, validation/num_examples=50000
I0306 14:10:02.199829 139849050855168 logging_writer.py:48] [20364] global_step=20364, preemption_count=0, score=8023.351526
I0306 14:10:02.709113 140046356313920 checkpoints.py:490] Saving checkpoint at step: 20364
I0306 14:10:04.222163 140046356313920 checkpoints.py:422] Saved checkpoint at /experiment_runs/variants_target_setting/study_0/imagenet_vit_glu_jax/trial_1/checkpoint_20364
I0306 14:10:04.246528 140046356313920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/variants_target_setting/study_0/imagenet_vit_glu_jax/trial_1/checkpoint_20364.
I0306 14:10:04.364423 140046356313920 submission_runner.py:588] Tuning trial 1/1
I0306 14:10:04.364609 140046356313920 submission_runner.py:589] Hyperparameters: Hyperparameters(learning_rate=0.0008445074561975979, beta1=0.8895758153482813, beta2=0.9978504782314613, warmup_steps=6999, weight_decay=0.08135402759553023)
I0306 14:10:04.369829 140046356313920 submission_runner.py:590] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0008984374580904841, 'train/loss': 6.907756328582764, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 6.907756805419922, 'test/num_examples': 10000, 'score': 39.93411064147949, 'total_duration': 87.17051649093628, 'accumulated_submission_time': 39.93411064147949, 'accumulated_eval_time': 47.236292600631714, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1004, {'train/accuracy': 0.018183592706918716, 'train/loss': 6.238394260406494, 'validation/accuracy': 0.017479998990893364, 'validation/loss': 6.2476396560668945, 'validation/num_examples': 50000, 'test/accuracy': 0.013000000268220901, 'test/loss': 6.317867279052734, 'test/num_examples': 10000, 'score': 460.1861529350281, 'total_duration': 534.4407677650452, 'accumulated_submission_time': 460.1861529350281, 'accumulated_eval_time': 74.19310164451599, 'accumulated_logging_time': 0.027152538299560547, 'global_step': 1004, 'preemption_count': 0}), (2083, {'train/accuracy': 0.04937499761581421, 'train/loss': 5.603709697723389, 'validation/accuracy': 0.04901999980211258, 'validation/loss': 5.6366424560546875, 'validation/num_examples': 50000, 'test/accuracy': 0.03710000216960907, 'test/loss': 5.789898872375488, 'test/num_examples': 10000, 'score': 880.4957611560822, 'total_duration': 982.0049915313721, 'accumulated_submission_time': 880.4957611560822, 'accumulated_eval_time': 101.38692164421082, 'accumulated_logging_time': 0.05028724670410156, 'global_step': 2083, 'preemption_count': 0}), (3161, {'train/accuracy': 0.08142577856779099, 'train/loss': 5.167095184326172, 'validation/accuracy': 0.0753600001335144, 'validation/loss': 5.21644926071167, 'validation/num_examples': 50000, 'test/accuracy': 0.060600001364946365, 'test/loss': 5.427204608917236, 'test/num_examples': 10000, 'score': 1300.8083004951477, 'total_duration': 1429.5970304012299, 'accumulated_submission_time': 1300.8083004951477, 'accumulated_eval_time': 128.6058909893036, 'accumulated_logging_time': 0.07343506813049316, 'global_step': 3161, 'preemption_count': 0}), (4238, {'train/accuracy': 0.1199023425579071, 'train/loss': 4.7676472663879395, 'validation/accuracy': 0.1127999946475029, 'validation/loss': 4.8194355964660645, 'validation/num_examples': 50000, 'test/accuracy': 0.08570000529289246, 'test/loss': 5.088362693786621, 'test/num_examples': 10000, 'score': 1721.1008322238922, 'total_duration': 1876.9273393154144, 'accumulated_submission_time': 1721.1008322238922, 'accumulated_eval_time': 155.5799572467804, 'accumulated_logging_time': 0.09889745712280273, 'global_step': 4238, 'preemption_count': 0}), (5314, {'train/accuracy': 0.16050781309604645, 'train/loss': 4.442964553833008, 'validation/accuracy': 0.15065999329090118, 'validation/loss': 4.499810695648193, 'validation/num_examples': 50000, 'test/accuracy': 0.11250000447034836, 'test/loss': 4.832035541534424, 'test/num_examples': 10000, 'score': 2141.316436767578, 'total_duration': 2324.2384185791016, 'accumulated_submission_time': 2141.316436767578, 'accumulated_eval_time': 182.61382508277893, 'accumulated_logging_time': 0.12318015098571777, 'global_step': 5314, 'preemption_count': 0}), (6389, {'train/accuracy': 0.19923827052116394, 'train/loss': 4.063134670257568, 'validation/accuracy': 0.18493999540805817, 'validation/loss': 4.153113842010498, 'validation/num_examples': 50000, 'test/accuracy': 0.140500009059906, 'test/loss': 4.567753314971924, 'test/num_examples': 10000, 'score': 2561.4309084415436, 'total_duration': 2771.4459459781647, 'accumulated_submission_time': 2561.4309084415436, 'accumulated_eval_time': 209.64268493652344, 'accumulated_logging_time': 0.1505284309387207, 'global_step': 6389, 'preemption_count': 0}), (7464, {'train/accuracy': 0.2256445288658142, 'train/loss': 3.8246026039123535, 'validation/accuracy': 0.2128800004720688, 'validation/loss': 3.936664342880249, 'validation/num_examples': 50000, 'test/accuracy': 0.16020001471042633, 'test/loss': 4.363847255706787, 'test/num_examples': 10000, 'score': 2981.532466650009, 'total_duration': 3218.646544933319, 'accumulated_submission_time': 2981.532466650009, 'accumulated_eval_time': 236.6798071861267, 'accumulated_logging_time': 0.17493867874145508, 'global_step': 7464, 'preemption_count': 0}), (8539, {'train/accuracy': 0.27769529819488525, 'train/loss': 3.5020840167999268, 'validation/accuracy': 0.24740000069141388, 'validation/loss': 3.6972768306732178, 'validation/num_examples': 50000, 'test/accuracy': 0.1851000040769577, 'test/loss': 4.176933765411377, 'test/num_examples': 10000, 'score': 3401.7890074253082, 'total_duration': 3665.949693918228, 'accumulated_submission_time': 3401.7890074253082, 'accumulated_eval_time': 263.6605386734009, 'accumulated_logging_time': 0.20311331748962402, 'global_step': 8539, 'preemption_count': 0}), (9614, {'train/accuracy': 0.3079492151737213, 'train/loss': 3.29396915435791, 'validation/accuracy': 0.28275999426841736, 'validation/loss': 3.439666509628296, 'validation/num_examples': 50000, 'test/accuracy': 0.21670001745224, 'test/loss': 3.9382307529449463, 'test/num_examples': 10000, 'score': 3821.9996366500854, 'total_duration': 4113.302345752716, 'accumulated_submission_time': 3821.9996366500854, 'accumulated_eval_time': 290.74228501319885, 'accumulated_logging_time': 0.22652745246887207, 'global_step': 9614, 'preemption_count': 0}), (10689, {'train/accuracy': 0.32421875, 'train/loss': 3.2122998237609863, 'validation/accuracy': 0.3041200041770935, 'validation/loss': 3.3279383182525635, 'validation/num_examples': 50000, 'test/accuracy': 0.23280000686645508, 'test/loss': 3.851491689682007, 'test/num_examples': 10000, 'score': 4242.166989564896, 'total_duration': 4560.5722279548645, 'accumulated_submission_time': 4242.166989564896, 'accumulated_eval_time': 317.7833366394043, 'accumulated_logging_time': 0.25070738792419434, 'global_step': 10689, 'preemption_count': 0}), (11764, {'train/accuracy': 0.3526367247104645, 'train/loss': 3.0422210693359375, 'validation/accuracy': 0.32580000162124634, 'validation/loss': 3.2038938999176025, 'validation/num_examples': 50000, 'test/accuracy': 0.250900000333786, 'test/loss': 3.7330782413482666, 'test/num_examples': 10000, 'score': 4662.309056758881, 'total_duration': 5007.851195812225, 'accumulated_submission_time': 4662.309056758881, 'accumulated_eval_time': 344.857284784317, 'accumulated_logging_time': 0.27531862258911133, 'global_step': 11764, 'preemption_count': 0}), (12839, {'train/accuracy': 0.3786718547344208, 'train/loss': 2.84712553024292, 'validation/accuracy': 0.3472200036048889, 'validation/loss': 3.021505832672119, 'validation/num_examples': 50000, 'test/accuracy': 0.26750001311302185, 'test/loss': 3.576693534851074, 'test/num_examples': 10000, 'score': 5082.453165769577, 'total_duration': 5455.421036720276, 'accumulated_submission_time': 5082.453165769577, 'accumulated_eval_time': 372.2211401462555, 'accumulated_logging_time': 0.2995767593383789, 'global_step': 12839, 'preemption_count': 0}), (13914, {'train/accuracy': 0.42033201456069946, 'train/loss': 2.659414529800415, 'validation/accuracy': 0.36271998286247253, 'validation/loss': 2.9436514377593994, 'validation/num_examples': 50000, 'test/accuracy': 0.2824999988079071, 'test/loss': 3.5125060081481934, 'test/num_examples': 10000, 'score': 5502.520686388016, 'total_duration': 5902.85263133049, 'accumulated_submission_time': 5502.520686388016, 'accumulated_eval_time': 399.5210020542145, 'accumulated_logging_time': 0.32538747787475586, 'global_step': 13914, 'preemption_count': 0}), (14989, {'train/accuracy': 0.42249998450279236, 'train/loss': 2.6097254753112793, 'validation/accuracy': 0.39111998677253723, 'validation/loss': 2.791548252105713, 'validation/num_examples': 50000, 'test/accuracy': 0.2979000210762024, 'test/loss': 3.3838934898376465, 'test/num_examples': 10000, 'score': 5922.650587320328, 'total_duration': 6350.218457937241, 'accumulated_submission_time': 5922.650587320328, 'accumulated_eval_time': 426.6964168548584, 'accumulated_logging_time': 0.34883737564086914, 'global_step': 14989, 'preemption_count': 0}), (16064, {'train/accuracy': 0.4336913824081421, 'train/loss': 2.5707640647888184, 'validation/accuracy': 0.4001599848270416, 'validation/loss': 2.7457265853881836, 'validation/num_examples': 50000, 'test/accuracy': 0.31210002303123474, 'test/loss': 3.3176984786987305, 'test/num_examples': 10000, 'score': 6342.840968847275, 'total_duration': 6797.709228992462, 'accumulated_submission_time': 6342.840968847275, 'accumulated_eval_time': 453.9327321052551, 'accumulated_logging_time': 0.3738982677459717, 'global_step': 16064, 'preemption_count': 0}), (17139, {'train/accuracy': 0.4525195360183716, 'train/loss': 2.465885877609253, 'validation/accuracy': 0.41086000204086304, 'validation/loss': 2.6918418407440186, 'validation/num_examples': 50000, 'test/accuracy': 0.31950002908706665, 'test/loss': 3.2830100059509277, 'test/num_examples': 10000, 'score': 6762.984875917435, 'total_duration': 7245.153917789459, 'accumulated_submission_time': 6762.984875917435, 'accumulated_eval_time': 481.17189621925354, 'accumulated_logging_time': 0.39795756340026855, 'global_step': 17139, 'preemption_count': 0}), (18214, {'train/accuracy': 0.4608398377895355, 'train/loss': 2.4279236793518066, 'validation/accuracy': 0.42508000135421753, 'validation/loss': 2.6118505001068115, 'validation/num_examples': 50000, 'test/accuracy': 0.32610002160072327, 'test/loss': 3.2200052738189697, 'test/num_examples': 10000, 'score': 7183.099342107773, 'total_duration': 7692.657005786896, 'accumulated_submission_time': 7183.099342107773, 'accumulated_eval_time': 508.4965364933014, 'accumulated_logging_time': 0.42426609992980957, 'global_step': 18214, 'preemption_count': 0}), (19289, {'train/accuracy': 0.473457008600235, 'train/loss': 2.3687477111816406, 'validation/accuracy': 0.4354199767112732, 'validation/loss': 2.550684690475464, 'validation/num_examples': 50000, 'test/accuracy': 0.3393000066280365, 'test/loss': 3.161468029022217, 'test/num_examples': 10000, 'score': 7603.240678787231, 'total_duration': 8140.322327852249, 'accumulated_submission_time': 7603.240678787231, 'accumulated_eval_time': 535.9574823379517, 'accumulated_logging_time': 0.44896745681762695, 'global_step': 19289, 'preemption_count': 0}), (20364, {'train/accuracy': 0.48640623688697815, 'train/loss': 2.279480457305908, 'validation/accuracy': 0.44711998105049133, 'validation/loss': 2.477479934692383, 'validation/num_examples': 50000, 'test/accuracy': 0.3477000296115875, 'test/loss': 3.099433422088623, 'test/num_examples': 10000, 'score': 8023.35152554512, 'total_duration': 8588.245665550232, 'accumulated_submission_time': 8023.35152554512, 'accumulated_eval_time': 563.7038655281067, 'accumulated_logging_time': 0.47674012184143066, 'global_step': 20364, 'preemption_count': 0})], 'global_step': 20364}
I0306 14:10:04.369989 140046356313920 submission_runner.py:591] Timing: 8023.35152554512
I0306 14:10:04.370039 140046356313920 submission_runner.py:593] Total number of evals: 20
I0306 14:10:04.370082 140046356313920 submission_runner.py:594] ====================
I0306 14:10:04.370218 140046356313920 submission_runner.py:678] Final imagenet_vit_glu score: 8023.35152554512
