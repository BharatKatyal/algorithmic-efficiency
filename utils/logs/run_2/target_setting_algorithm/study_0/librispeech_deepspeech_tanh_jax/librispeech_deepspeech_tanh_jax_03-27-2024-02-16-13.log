python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech_tanh --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --rng_seed=4106826979 --max_global_steps=48000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab --tuning_ruleset=external --tuning_search_space=reference_algorithms/target_setting_algorithms/imagenet_resnet/tuning_search_space.json --num_tuning_trials=1 2>&1 | tee -a /logs/librispeech_deepspeech_tanh_jax_03-27-2024-02-16-13.log
I0327 02:16:35.036179 140502730008384 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_tanh_jax.
I0327 02:16:36.075139 140502730008384 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0327 02:16:36.076266 140502730008384 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0327 02:16:36.076386 140502730008384 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0327 02:16:36.083045 140502730008384 submission_runner.py:557] Using RNG seed 4106826979
I0327 02:16:37.236869 140502730008384 submission_runner.py:566] --- Tuning run 1/1 ---
I0327 02:16:37.237057 140502730008384 submission_runner.py:571] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_tanh_jax/trial_1.
I0327 02:16:37.237486 140502730008384 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_tanh_jax/trial_1/hparams.json.
I0327 02:16:37.415035 140502730008384 submission_runner.py:211] Initializing dataset.
I0327 02:16:37.415226 140502730008384 submission_runner.py:222] Initializing model.
I0327 02:16:39.997179 140502730008384 submission_runner.py:264] Initializing optimizer.
I0327 02:16:40.663047 140502730008384 submission_runner.py:271] Initializing metrics bundle.
I0327 02:16:40.663217 140502730008384 submission_runner.py:289] Initializing checkpoint and logger.
I0327 02:16:40.664172 140502730008384 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_tanh_jax/trial_1 with prefix checkpoint_
I0327 02:16:40.664302 140502730008384 submission_runner.py:309] Saving meta data to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_tanh_jax/trial_1/meta_data_0.json.
I0327 02:16:40.664511 140502730008384 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0327 02:16:40.664572 140502730008384 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0327 02:16:40.938388 140502730008384 logger_utils.py:220] Unable to record git information. Continuing without it.
I0327 02:16:41.192794 140502730008384 submission_runner.py:313] Saving flags to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_tanh_jax/trial_1/flags_0.json.
I0327 02:16:41.208128 140502730008384 submission_runner.py:323] Starting training loop.
I0327 02:16:41.492188 140502730008384 input_pipeline.py:20] Loading split = train-clean-100
I0327 02:16:41.556441 140502730008384 input_pipeline.py:20] Loading split = train-clean-360
I0327 02:16:41.707833 140502730008384 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0327 02:17:23.134717 140339071403776 logging_writer.py:48] [0] global_step=0, grad_norm=23.499956130981445, loss=33.001861572265625
I0327 02:17:23.168056 140502730008384 spec.py:321] Evaluating on the training split.
I0327 02:17:23.419637 140502730008384 input_pipeline.py:20] Loading split = train-clean-100
I0327 02:17:23.454718 140502730008384 input_pipeline.py:20] Loading split = train-clean-360
I0327 02:17:23.779682 140502730008384 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0327 02:19:39.775810 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 02:19:39.969243 140502730008384 input_pipeline.py:20] Loading split = dev-clean
I0327 02:19:39.974890 140502730008384 input_pipeline.py:20] Loading split = dev-other
I0327 02:21:01.449944 140502730008384 spec.py:349] Evaluating on the test split.
I0327 02:21:01.644989 140502730008384 input_pipeline.py:20] Loading split = test-clean
I0327 02:21:46.210495 140502730008384 submission_runner.py:422] Time since start: 305.00s, 	Step: 1, 	{'train/ctc_loss': Array(32.055508, dtype=float32), 'train/wer': 4.979707640089124, 'validation/ctc_loss': Array(30.983448, dtype=float32), 'validation/wer': 4.536721472913871, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.985773, dtype=float32), 'test/wer': 4.794020270956472, 'test/num_examples': 2472, 'score': 41.9598753452301, 'total_duration': 305.00030064582825, 'accumulated_submission_time': 41.9598753452301, 'accumulated_eval_time': 263.0403838157654, 'accumulated_logging_time': 0}
I0327 02:21:46.237636 140333996308224 logging_writer.py:48] [1] accumulated_eval_time=263.040384, accumulated_logging_time=0, accumulated_submission_time=41.959875, global_step=1, preemption_count=0, score=41.959875, test/ctc_loss=30.98577308654785, test/num_examples=2472, test/wer=4.794020, total_duration=305.000301, train/ctc_loss=32.05550765991211, train/wer=4.979708, validation/ctc_loss=30.983448028564453, validation/num_examples=5348, validation/wer=4.536721
I0327 02:21:55.139405 140346197935872 logging_writer.py:48] [1] global_step=1, grad_norm=25.161029815673828, loss=33.60686111450195
I0327 02:21:56.018562 140346206328576 logging_writer.py:48] [2] global_step=2, grad_norm=62.716880798339844, loss=24.863887786865234
I0327 02:21:56.903630 140346197935872 logging_writer.py:48] [3] global_step=3, grad_norm=17.548673629760742, loss=15.752157211303711
I0327 02:21:57.720963 140346206328576 logging_writer.py:48] [4] global_step=4, grad_norm=15.157703399658203, loss=11.841824531555176
I0327 02:21:58.613259 140346197935872 logging_writer.py:48] [5] global_step=5, grad_norm=7.844407558441162, loss=11.047361373901367
I0327 02:21:59.501045 140346206328576 logging_writer.py:48] [6] global_step=6, grad_norm=5.145331859588623, loss=9.473228454589844
I0327 02:22:00.406641 140346197935872 logging_writer.py:48] [7] global_step=7, grad_norm=3.667415142059326, loss=8.764813423156738
I0327 02:22:01.319287 140346206328576 logging_writer.py:48] [8] global_step=8, grad_norm=12.089398384094238, loss=8.622017860412598
I0327 02:22:02.242350 140346197935872 logging_writer.py:48] [9] global_step=9, grad_norm=5.127384185791016, loss=7.975786209106445
I0327 02:22:03.146477 140346206328576 logging_writer.py:48] [10] global_step=10, grad_norm=8.583948135375977, loss=8.376331329345703
I0327 02:22:04.068840 140346197935872 logging_writer.py:48] [11] global_step=11, grad_norm=3.657048463821411, loss=7.98080587387085
I0327 02:22:04.984093 140346206328576 logging_writer.py:48] [12] global_step=12, grad_norm=2.602867841720581, loss=7.909412384033203
I0327 02:22:05.891903 140346197935872 logging_writer.py:48] [13] global_step=13, grad_norm=2.7939484119415283, loss=7.784536838531494
I0327 02:22:06.814227 140346206328576 logging_writer.py:48] [14] global_step=14, grad_norm=1.815486192703247, loss=7.3599724769592285
I0327 02:22:07.736558 140346197935872 logging_writer.py:48] [15] global_step=15, grad_norm=2.476771354675293, loss=7.40231466293335
I0327 02:22:08.657677 140346206328576 logging_writer.py:48] [16] global_step=16, grad_norm=1.9041461944580078, loss=7.079452037811279
I0327 02:22:09.560145 140346197935872 logging_writer.py:48] [17] global_step=17, grad_norm=3.19974422454834, loss=6.954498291015625
I0327 02:22:10.482839 140346206328576 logging_writer.py:48] [18] global_step=18, grad_norm=1.1838324069976807, loss=6.599829196929932
I0327 02:22:11.394611 140346197935872 logging_writer.py:48] [19] global_step=19, grad_norm=3.206939697265625, loss=6.457324028015137
I0327 02:22:12.312436 140346206328576 logging_writer.py:48] [20] global_step=20, grad_norm=6.229804515838623, loss=6.407444000244141
I0327 02:22:13.222402 140346197935872 logging_writer.py:48] [21] global_step=21, grad_norm=29.91022491455078, loss=7.471304893493652
I0327 02:22:14.148049 140346206328576 logging_writer.py:48] [22] global_step=22, grad_norm=18.42483139038086, loss=9.691553115844727
I0327 02:22:15.077884 140346197935872 logging_writer.py:48] [23] global_step=23, grad_norm=9.509669303894043, loss=7.897865295410156
I0327 02:22:16.010958 140346206328576 logging_writer.py:48] [24] global_step=24, grad_norm=3.8549187183380127, loss=7.254210472106934
I0327 02:22:16.934454 140346197935872 logging_writer.py:48] [25] global_step=25, grad_norm=2.7349095344543457, loss=7.074655532836914
I0327 02:22:17.855897 140346206328576 logging_writer.py:48] [26] global_step=26, grad_norm=1.900574803352356, loss=6.784155368804932
I0327 02:22:18.769261 140346197935872 logging_writer.py:48] [27] global_step=27, grad_norm=1.4566339254379272, loss=6.6000447273254395
I0327 02:22:19.694437 140346206328576 logging_writer.py:48] [28] global_step=28, grad_norm=2.235698699951172, loss=6.554972171783447
I0327 02:22:20.613172 140346197935872 logging_writer.py:48] [29] global_step=29, grad_norm=1.591605305671692, loss=6.385023593902588
I0327 02:22:21.535983 140346206328576 logging_writer.py:48] [30] global_step=30, grad_norm=1.1643850803375244, loss=6.405972480773926
I0327 02:22:22.461414 140346197935872 logging_writer.py:48] [31] global_step=31, grad_norm=2.2163140773773193, loss=6.349649429321289
I0327 02:22:23.383547 140346206328576 logging_writer.py:48] [32] global_step=32, grad_norm=4.316435813903809, loss=6.330005168914795
I0327 02:22:24.297153 140346197935872 logging_writer.py:48] [33] global_step=33, grad_norm=5.532530307769775, loss=6.321599960327148
I0327 02:22:25.209442 140346206328576 logging_writer.py:48] [34] global_step=34, grad_norm=8.162510871887207, loss=6.322555065155029
I0327 02:22:26.142333 140346197935872 logging_writer.py:48] [35] global_step=35, grad_norm=10.13010025024414, loss=6.600416660308838
I0327 02:22:27.059280 140346206328576 logging_writer.py:48] [36] global_step=36, grad_norm=16.537738800048828, loss=6.746868133544922
I0327 02:22:27.984665 140346197935872 logging_writer.py:48] [37] global_step=37, grad_norm=12.605850219726562, loss=7.498598098754883
I0327 02:22:28.908372 140346206328576 logging_writer.py:48] [38] global_step=38, grad_norm=1.5010572671890259, loss=6.427146911621094
I0327 02:22:29.830533 140346197935872 logging_writer.py:48] [39] global_step=39, grad_norm=6.374935150146484, loss=6.437252521514893
I0327 02:22:30.744755 140346206328576 logging_writer.py:48] [40] global_step=40, grad_norm=5.94914436340332, loss=6.429315090179443
I0327 02:22:31.650185 140346197935872 logging_writer.py:48] [41] global_step=41, grad_norm=8.45734691619873, loss=6.50080680847168
I0327 02:22:32.569156 140346206328576 logging_writer.py:48] [42] global_step=42, grad_norm=8.029031753540039, loss=6.683388710021973
I0327 02:22:33.488037 140346197935872 logging_writer.py:48] [43] global_step=43, grad_norm=5.992147445678711, loss=6.463388919830322
I0327 02:22:34.415418 140346206328576 logging_writer.py:48] [44] global_step=44, grad_norm=6.311943054199219, loss=6.468791961669922
I0327 02:22:35.334497 140346197935872 logging_writer.py:48] [45] global_step=45, grad_norm=8.203131675720215, loss=6.4648942947387695
I0327 02:22:36.254625 140346206328576 logging_writer.py:48] [46] global_step=46, grad_norm=8.17447280883789, loss=6.639329433441162
I0327 02:22:37.175102 140346197935872 logging_writer.py:48] [47] global_step=47, grad_norm=7.365802764892578, loss=6.490288734436035
I0327 02:22:38.100024 140346206328576 logging_writer.py:48] [48] global_step=48, grad_norm=7.193552017211914, loss=6.549288749694824
I0327 02:22:39.017580 140346197935872 logging_writer.py:48] [49] global_step=49, grad_norm=7.990109443664551, loss=6.471354961395264
I0327 02:22:39.944518 140346206328576 logging_writer.py:48] [50] global_step=50, grad_norm=8.79444408416748, loss=6.727444648742676
I0327 02:22:40.862358 140346197935872 logging_writer.py:48] [51] global_step=51, grad_norm=11.274232864379883, loss=6.678290843963623
I0327 02:22:41.789258 140346206328576 logging_writer.py:48] [52] global_step=52, grad_norm=11.00308609008789, loss=7.191030979156494
I0327 02:22:42.697663 140346197935872 logging_writer.py:48] [53] global_step=53, grad_norm=10.163606643676758, loss=6.783849239349365
I0327 02:22:43.617265 140346206328576 logging_writer.py:48] [54] global_step=54, grad_norm=8.885749816894531, loss=7.00455904006958
I0327 02:22:44.537803 140346197935872 logging_writer.py:48] [55] global_step=55, grad_norm=8.190896987915039, loss=6.884571552276611
I0327 02:22:45.450387 140346206328576 logging_writer.py:48] [56] global_step=56, grad_norm=7.6903815269470215, loss=6.9680609703063965
I0327 02:22:46.360538 140346197935872 logging_writer.py:48] [57] global_step=57, grad_norm=8.579052925109863, loss=6.896981716156006
I0327 02:22:47.281196 140346206328576 logging_writer.py:48] [58] global_step=58, grad_norm=7.987725734710693, loss=7.126723289489746
I0327 02:22:48.196866 140346197935872 logging_writer.py:48] [59] global_step=59, grad_norm=7.114780902862549, loss=6.981008529663086
I0327 02:22:49.108095 140346206328576 logging_writer.py:48] [60] global_step=60, grad_norm=6.307547092437744, loss=6.908003807067871
I0327 02:22:50.020002 140346197935872 logging_writer.py:48] [61] global_step=61, grad_norm=7.293615341186523, loss=6.9306488037109375
I0327 02:22:50.935474 140346206328576 logging_writer.py:48] [62] global_step=62, grad_norm=7.504027843475342, loss=7.127601623535156
I0327 02:22:51.839762 140346197935872 logging_writer.py:48] [63] global_step=63, grad_norm=7.1646199226379395, loss=6.9316606521606445
I0327 02:22:52.747847 140346206328576 logging_writer.py:48] [64] global_step=64, grad_norm=6.730869770050049, loss=7.078503131866455
I0327 02:22:53.649129 140346197935872 logging_writer.py:48] [65] global_step=65, grad_norm=6.4267683029174805, loss=6.949072360992432
I0327 02:22:54.558428 140346206328576 logging_writer.py:48] [66] global_step=66, grad_norm=6.049197673797607, loss=6.99960994720459
I0327 02:22:55.472539 140346197935872 logging_writer.py:48] [67] global_step=67, grad_norm=5.432431221008301, loss=6.824140548706055
I0327 02:22:56.372356 140346206328576 logging_writer.py:48] [68] global_step=68, grad_norm=5.638271331787109, loss=6.852856159210205
I0327 02:22:57.281034 140346197935872 logging_writer.py:48] [69] global_step=69, grad_norm=8.205431938171387, loss=6.974885940551758
I0327 02:22:58.187666 140346206328576 logging_writer.py:48] [70] global_step=70, grad_norm=8.413544654846191, loss=7.231553554534912
I0327 02:22:59.153822 140346197935872 logging_writer.py:48] [71] global_step=71, grad_norm=6.761009693145752, loss=6.899796009063721
I0327 02:23:00.064425 140346206328576 logging_writer.py:48] [72] global_step=72, grad_norm=5.976245403289795, loss=6.910114288330078
I0327 02:23:00.975098 140346197935872 logging_writer.py:48] [73] global_step=73, grad_norm=6.488383769989014, loss=6.809106826782227
I0327 02:23:01.946216 140346206328576 logging_writer.py:48] [74] global_step=74, grad_norm=5.9402313232421875, loss=6.878101348876953
I0327 02:23:02.832831 140346197935872 logging_writer.py:48] [75] global_step=75, grad_norm=7.3550004959106445, loss=6.845937252044678
I0327 02:23:03.722787 140346206328576 logging_writer.py:48] [76] global_step=76, grad_norm=6.268299102783203, loss=7.026861667633057
I0327 02:23:04.614131 140346197935872 logging_writer.py:48] [77] global_step=77, grad_norm=6.427439212799072, loss=6.887489318847656
I0327 02:23:05.513814 140346206328576 logging_writer.py:48] [78] global_step=78, grad_norm=4.668721675872803, loss=6.75266170501709
I0327 02:23:06.389018 140346197935872 logging_writer.py:48] [79] global_step=79, grad_norm=3.4952285289764404, loss=6.584677219390869
I0327 02:23:07.285156 140346206328576 logging_writer.py:48] [80] global_step=80, grad_norm=3.270259141921997, loss=6.5167012214660645
I0327 02:23:08.182975 140346197935872 logging_writer.py:48] [81] global_step=81, grad_norm=4.139656066894531, loss=6.43505859375
I0327 02:23:09.070549 140346206328576 logging_writer.py:48] [82] global_step=82, grad_norm=3.8957619667053223, loss=6.491988182067871
I0327 02:23:09.972748 140346197935872 logging_writer.py:48] [83] global_step=83, grad_norm=4.164219379425049, loss=6.411826133728027
I0327 02:23:10.871542 140346206328576 logging_writer.py:48] [84] global_step=84, grad_norm=4.164990425109863, loss=6.398880481719971
I0327 02:23:11.766930 140346197935872 logging_writer.py:48] [85] global_step=85, grad_norm=4.547989368438721, loss=6.3613786697387695
I0327 02:23:12.654905 140346206328576 logging_writer.py:48] [86] global_step=86, grad_norm=4.2333598136901855, loss=6.366849899291992
I0327 02:23:13.538644 140346197935872 logging_writer.py:48] [87] global_step=87, grad_norm=3.618391990661621, loss=6.2792840003967285
I0327 02:23:14.432036 140346206328576 logging_writer.py:48] [88] global_step=88, grad_norm=3.205557346343994, loss=6.223860263824463
I0327 02:23:15.322029 140346197935872 logging_writer.py:48] [89] global_step=89, grad_norm=3.445667028427124, loss=6.221258640289307
I0327 02:23:16.214337 140346206328576 logging_writer.py:48] [90] global_step=90, grad_norm=3.21724009513855, loss=6.231112957000732
I0327 02:23:17.109215 140346197935872 logging_writer.py:48] [91] global_step=91, grad_norm=2.8975141048431396, loss=6.174412250518799
I0327 02:23:18.011414 140346206328576 logging_writer.py:48] [92] global_step=92, grad_norm=2.3573157787323, loss=6.124541282653809
I0327 02:23:18.899758 140346197935872 logging_writer.py:48] [93] global_step=93, grad_norm=2.297870397567749, loss=6.106947898864746
I0327 02:23:19.804440 140346206328576 logging_writer.py:48] [94] global_step=94, grad_norm=2.191486120223999, loss=6.096131324768066
I0327 02:23:20.703879 140346197935872 logging_writer.py:48] [95] global_step=95, grad_norm=2.5402894020080566, loss=6.118707180023193
I0327 02:23:21.594264 140346206328576 logging_writer.py:48] [96] global_step=96, grad_norm=2.6404457092285156, loss=6.119946002960205
I0327 02:23:22.500599 140346197935872 logging_writer.py:48] [97] global_step=97, grad_norm=2.7243380546569824, loss=6.131630897521973
I0327 02:23:23.386546 140346206328576 logging_writer.py:48] [98] global_step=98, grad_norm=2.666165351867676, loss=6.083791732788086
I0327 02:23:24.284784 140346197935872 logging_writer.py:48] [99] global_step=99, grad_norm=2.6406970024108887, loss=6.093435764312744
I0327 02:23:25.171175 140346206328576 logging_writer.py:48] [100] global_step=100, grad_norm=2.340644359588623, loss=6.090449333190918
I0327 02:28:44.756296 140346197935872 logging_writer.py:48] [500] global_step=500, grad_norm=141.09475708007812, loss=488.8664855957031
I0327 02:36:05.776725 140346206328576 logging_writer.py:48] [1000] global_step=1000, grad_norm=80.74978637695312, loss=534.4360961914062
I0327 02:42:37.150238 140338942125824 logging_writer.py:48] [1500] global_step=1500, grad_norm=72.34074401855469, loss=1556.9486083984375
I0327 02:45:46.586860 140502730008384 spec.py:321] Evaluating on the training split.
I0327 02:46:22.029797 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 02:47:07.415657 140502730008384 spec.py:349] Evaluating on the test split.
I0327 02:47:29.727746 140502730008384 submission_runner.py:422] Time since start: 1848.51s, 	Step: 1714, 	{'train/ctc_loss': Array(1678.4785, dtype=float32), 'train/wer': 0.9446046425408333, 'validation/ctc_loss': Array(2283.8062, dtype=float32), 'validation/wer': 0.8965600471147069, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2205.3608, dtype=float32), 'test/wer': 0.8994779924034693, 'test/num_examples': 2472, 'score': 1482.2365572452545, 'total_duration': 1848.5136897563934, 'accumulated_submission_time': 1482.2365572452545, 'accumulated_eval_time': 366.17541241645813, 'accumulated_logging_time': 0.04054617881774902}
I0327 02:47:29.760195 140346248292096 logging_writer.py:48] [1714] accumulated_eval_time=366.175412, accumulated_logging_time=0.040546, accumulated_submission_time=1482.236557, global_step=1714, preemption_count=0, score=1482.236557, test/ctc_loss=2205.36083984375, test/num_examples=2472, test/wer=0.899478, total_duration=1848.513690, train/ctc_loss=1678.478515625, train/wer=0.944605, validation/ctc_loss=2283.80615234375, validation/num_examples=5348, validation/wer=0.896560
I0327 02:51:14.520893 140346239899392 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.137983560562134, loss=1759.9542236328125
I0327 02:57:52.068796 140346903652096 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0, loss=1812.5050048828125
I0327 03:05:19.396444 140346895259392 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.5630125403404236, loss=1777.9544677734375
I0327 03:11:29.808618 140502730008384 spec.py:321] Evaluating on the training split.
I0327 03:12:04.536651 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 03:12:48.256276 140502730008384 spec.py:349] Evaluating on the test split.
I0327 03:13:10.314823 140502730008384 submission_runner.py:422] Time since start: 3389.10s, 	Step: 3464, 	{'train/ctc_loss': Array(1761.2479, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': Array(3345.958, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3181.6133, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2922.2044739723206, 'total_duration': 3389.101282119751, 'accumulated_submission_time': 2922.2044739723206, 'accumulated_eval_time': 466.6764621734619, 'accumulated_logging_time': 0.08878040313720703}
I0327 03:13:10.345852 140346248292096 logging_writer.py:48] [3464] accumulated_eval_time=466.676462, accumulated_logging_time=0.088780, accumulated_submission_time=2922.204474, global_step=3464, preemption_count=0, score=2922.204474, test/ctc_loss=3181.61328125, test/num_examples=2472, test/wer=0.899580, total_duration=3389.101282, train/ctc_loss=1761.2479248046875, train/wer=0.942722, validation/ctc_loss=3345.9580078125, validation/num_examples=5348, validation/wer=0.896618
I0327 03:13:38.680524 140346239899392 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.17638808488845825, loss=1808.1236572265625
I0327 03:20:42.800812 140346248292096 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0, loss=1867.5096435546875
I0327 03:27:36.127853 140346248292096 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0, loss=1786.6988525390625
I0327 03:35:17.808538 140346239899392 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1809.430419921875
I0327 03:37:10.835344 140502730008384 spec.py:321] Evaluating on the training split.
I0327 03:37:47.044514 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 03:38:32.464844 140502730008384 spec.py:349] Evaluating on the test split.
I0327 03:38:55.813813 140502730008384 submission_runner.py:422] Time since start: 4934.60s, 	Step: 5123, 	{'train/ctc_loss': Array(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': Array(3357.8315, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4362.615800142288, 'total_duration': 4934.600059747696, 'accumulated_submission_time': 4362.615800142288, 'accumulated_eval_time': 571.6493661403656, 'accumulated_logging_time': 0.13571786880493164}
I0327 03:38:55.849173 140346248292096 logging_writer.py:48] [5123] accumulated_eval_time=571.649366, accumulated_logging_time=0.135718, accumulated_submission_time=4362.615800, global_step=5123, preemption_count=0, score=4362.615800, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=4934.600060, train/ctc_loss=1741.2979736328125, train/wer=0.943324, validation/ctc_loss=3357.83154296875, validation/num_examples=5348, validation/wer=0.896618
I0327 03:43:59.015723 140346248292096 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0, loss=1783.3385009765625
I0327 03:51:39.760357 140346239899392 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1763.3160400390625
I0327 03:58:36.259833 140346248292096 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1791.0740966796875
I0327 04:02:56.088761 140502730008384 spec.py:321] Evaluating on the training split.
I0327 04:03:32.376191 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 04:04:18.182695 140502730008384 spec.py:349] Evaluating on the test split.
I0327 04:04:41.226714 140502730008384 submission_runner.py:422] Time since start: 6480.01s, 	Step: 6792, 	{'train/ctc_loss': Array(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': Array(3357.716, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 5802.779655694962, 'total_duration': 6480.01359128952, 'accumulated_submission_time': 5802.779655694962, 'accumulated_eval_time': 676.7824006080627, 'accumulated_logging_time': 0.1844029426574707}
I0327 04:04:41.263562 140346903652096 logging_writer.py:48] [6792] accumulated_eval_time=676.782401, accumulated_logging_time=0.184403, accumulated_submission_time=5802.779656, global_step=6792, preemption_count=0, score=5802.779656, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=6480.013591, train/ctc_loss=1724.861328125, train/wer=0.943700, validation/ctc_loss=3357.716064453125, validation/num_examples=5348, validation/wer=0.896618
I0327 04:07:22.365521 140346895259392 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0, loss=1768.55419921875
I0327 04:14:21.862257 140346903652096 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1811.4791259765625
I0327 04:21:45.234886 140346895259392 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.0, loss=1851.3033447265625
I0327 04:28:39.967343 140346903652096 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0, loss=1841.1839599609375
I0327 04:28:41.371566 140502730008384 spec.py:321] Evaluating on the training split.
I0327 04:29:17.600131 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 04:30:01.467235 140502730008384 spec.py:349] Evaluating on the test split.
I0327 04:30:24.419981 140502730008384 submission_runner.py:422] Time since start: 8023.21s, 	Step: 8503, 	{'train/ctc_loss': Array(1832.9288, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': Array(3357.587, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7242.806547164917, 'total_duration': 8023.206823348999, 'accumulated_submission_time': 7242.806547164917, 'accumulated_eval_time': 779.8258347511292, 'accumulated_logging_time': 0.23680782318115234}
I0327 04:30:24.454226 140346903652096 logging_writer.py:48] [8503] accumulated_eval_time=779.825835, accumulated_logging_time=0.236808, accumulated_submission_time=7242.806547, global_step=8503, preemption_count=0, score=7242.806547, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=8023.206823, train/ctc_loss=1832.9288330078125, train/wer=0.941551, validation/ctc_loss=3357.5869140625, validation/num_examples=5348, validation/wer=0.896618
I0327 04:37:23.238713 140346895259392 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0, loss=1819.3321533203125
I0327 04:44:28.517021 140346903652096 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.0, loss=1822.5703125
I0327 04:51:56.653189 140346895259392 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.0, loss=1788.9462890625
I0327 04:54:24.951716 140502730008384 spec.py:321] Evaluating on the training split.
I0327 04:55:01.608811 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 04:55:46.840878 140502730008384 spec.py:349] Evaluating on the test split.
I0327 04:56:09.271371 140502730008384 submission_runner.py:422] Time since start: 9568.06s, 	Step: 10162, 	{'train/ctc_loss': Array(1752.8004, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': Array(3357.4324, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 8683.226366519928, 'total_duration': 9568.057299137115, 'accumulated_submission_time': 8683.226366519928, 'accumulated_eval_time': 884.1396083831787, 'accumulated_logging_time': 0.2855856418609619}
I0327 04:56:09.301480 140346903652096 logging_writer.py:48] [10162] accumulated_eval_time=884.139608, accumulated_logging_time=0.285586, accumulated_submission_time=8683.226367, global_step=10162, preemption_count=0, score=8683.226367, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=9568.057299, train/ctc_loss=1752.8004150390625, train/wer=0.942641, validation/ctc_loss=3357.432373046875, validation/num_examples=5348, validation/wer=0.896618
I0327 05:00:31.937638 140346903652096 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.0, loss=1865.8760986328125
I0327 05:07:51.870155 140346895259392 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.0, loss=1812.49755859375
I0327 05:15:12.691081 140346903652096 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.0, loss=1822.0438232421875
I0327 05:20:09.487856 140502730008384 spec.py:321] Evaluating on the training split.
I0327 05:20:46.338948 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 05:21:32.433704 140502730008384 spec.py:349] Evaluating on the test split.
I0327 05:21:55.839056 140502730008384 submission_runner.py:422] Time since start: 11114.63s, 	Step: 11849, 	{'train/ctc_loss': Array(1746.1039, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 10123.33424949646, 'total_duration': 11114.625593423843, 'accumulated_submission_time': 10123.33424949646, 'accumulated_eval_time': 990.4855401515961, 'accumulated_logging_time': 0.3280649185180664}
I0327 05:21:55.880301 140346903652096 logging_writer.py:48] [11849] accumulated_eval_time=990.485540, accumulated_logging_time=0.328065, accumulated_submission_time=10123.334249, global_step=11849, preemption_count=0, score=10123.334249, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=11114.625593, train/ctc_loss=1746.1038818359375, train/wer=0.942824, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 05:23:50.512289 140346895259392 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.0, loss=1791.442626953125
I0327 05:30:59.442281 140346903652096 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.0, loss=1853.97705078125
I0327 05:38:03.289765 140346895259392 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.0, loss=1845.556640625
I0327 05:45:22.689873 140346903652096 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.0, loss=1826.4638671875
I0327 05:45:56.535535 140502730008384 spec.py:321] Evaluating on the training split.
I0327 05:46:32.704743 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 05:47:17.476751 140502730008384 spec.py:349] Evaluating on the test split.
I0327 05:47:39.664119 140502730008384 submission_runner.py:422] Time since start: 12658.45s, 	Step: 13545, 	{'train/ctc_loss': Array(1733.7323, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 11563.896816015244, 'total_duration': 12658.4535343647, 'accumulated_submission_time': 11563.896816015244, 'accumulated_eval_time': 1093.6117329597473, 'accumulated_logging_time': 0.39888429641723633}
I0327 05:47:39.688251 140346903652096 logging_writer.py:48] [13545] accumulated_eval_time=1093.611733, accumulated_logging_time=0.398884, accumulated_submission_time=11563.896816, global_step=13545, preemption_count=0, score=11563.896816, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=12658.453534, train/ctc_loss=1733.7322998046875, train/wer=0.944086, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 05:53:52.106955 140346895259392 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.0, loss=1820.3594970703125
I0327 06:01:24.468101 140346903652096 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.0, loss=1812.49755859375
I0327 06:08:23.805058 140346895259392 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.0, loss=1803.68603515625
I0327 06:11:39.721506 140502730008384 spec.py:321] Evaluating on the training split.
I0327 06:12:18.092952 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 06:13:03.460251 140502730008384 spec.py:349] Evaluating on the test split.
I0327 06:13:26.887137 140502730008384 submission_runner.py:422] Time since start: 14205.67s, 	Step: 15212, 	{'train/ctc_loss': Array(1786.8575, dtype=float32), 'train/wer': 0.9427990785714666, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 13003.853729963303, 'total_duration': 14205.672280311584, 'accumulated_submission_time': 13003.853729963303, 'accumulated_eval_time': 1200.770696401596, 'accumulated_logging_time': 0.43520283699035645}
I0327 06:13:26.922034 140346903652096 logging_writer.py:48] [15212] accumulated_eval_time=1200.770696, accumulated_logging_time=0.435203, accumulated_submission_time=13003.853730, global_step=15212, preemption_count=0, score=13003.853730, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=14205.672280, train/ctc_loss=1786.8575439453125, train/wer=0.942799, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 06:17:16.053198 140346903652096 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.0, loss=1800.2613525390625
I0327 06:24:19.546854 140346895259392 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.0, loss=1826.7244873046875
I0327 06:32:07.246262 140346903652096 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.0, loss=1820.618408203125
I0327 06:37:27.604875 140502730008384 spec.py:321] Evaluating on the training split.
I0327 06:38:04.131989 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 06:38:49.747787 140502730008384 spec.py:349] Evaluating on the test split.
I0327 06:39:12.475469 140502730008384 submission_runner.py:422] Time since start: 15751.26s, 	Step: 16897, 	{'train/ctc_loss': Array(1755.9307, dtype=float32), 'train/wer': 0.9423383225986367, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14444.457047462463, 'total_duration': 15751.261131763458, 'accumulated_submission_time': 14444.457047462463, 'accumulated_eval_time': 1305.6351425647736, 'accumulated_logging_time': 0.48407936096191406}
I0327 06:39:12.508748 140346181732096 logging_writer.py:48] [16897] accumulated_eval_time=1305.635143, accumulated_logging_time=0.484079, accumulated_submission_time=14444.457047, global_step=16897, preemption_count=0, score=14444.457047, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=15751.261132, train/ctc_loss=1755.9306640625, train/wer=0.942338, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 06:40:30.243095 140346173339392 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.0, loss=1872.6932373046875
I0327 06:48:03.014938 140346181732096 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.0, loss=1784.2010498046875
I0327 06:54:50.686568 140346181732096 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.0, loss=1854.24560546875
I0327 07:02:30.692946 140346173339392 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.0, loss=1790.8160400390625
I0327 07:03:12.990309 140502730008384 spec.py:321] Evaluating on the training split.
I0327 07:03:50.020306 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 07:04:35.366137 140502730008384 spec.py:349] Evaluating on the test split.
I0327 07:04:58.225296 140502730008384 submission_runner.py:422] Time since start: 17297.01s, 	Step: 18543, 	{'train/ctc_loss': Array(1731.2422, dtype=float32), 'train/wer': 0.9431396916893625, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15884.854880809784, 'total_duration': 17297.012016057968, 'accumulated_submission_time': 15884.854880809784, 'accumulated_eval_time': 1410.865191936493, 'accumulated_logging_time': 0.5360684394836426}
I0327 07:04:58.257235 140345889892096 logging_writer.py:48] [18543] accumulated_eval_time=1410.865192, accumulated_logging_time=0.536068, accumulated_submission_time=15884.854881, global_step=18543, preemption_count=0, score=15884.854881, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=17297.012016, train/ctc_loss=1731.2421875, train/wer=0.943140, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 07:10:55.352359 140345881499392 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.0, loss=1930.7425537109375
I0327 07:18:43.529479 140345889892096 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.0, loss=1870.50390625
I0327 07:25:40.675804 140345889892096 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.0, loss=1844.8916015625
I0327 07:28:58.781864 140502730008384 spec.py:321] Evaluating on the training split.
I0327 07:29:36.096973 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 07:30:20.963813 140502730008384 spec.py:349] Evaluating on the test split.
I0327 07:30:43.552521 140502730008384 submission_runner.py:422] Time since start: 18842.34s, 	Step: 20213, 	{'train/ctc_loss': Array(1763.6094, dtype=float32), 'train/wer': 0.9432716912443612, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 17325.298138141632, 'total_duration': 18842.339307785034, 'accumulated_submission_time': 17325.298138141632, 'accumulated_eval_time': 1515.630853652954, 'accumulated_logging_time': 0.5837047100067139}
I0327 07:30:43.583904 140346611812096 logging_writer.py:48] [20213] accumulated_eval_time=1515.630854, accumulated_logging_time=0.583705, accumulated_submission_time=17325.298138, global_step=20213, preemption_count=0, score=17325.298138, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=18842.339308, train/ctc_loss=1763.609375, train/wer=0.943272, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 07:34:42.335225 140346603419392 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.0, loss=1797.101806640625
I0327 07:41:54.337493 140346284132096 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.0, loss=1747.77880859375
I0327 07:49:51.019439 140346275739392 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.0, loss=1800.2613525390625
I0327 07:54:43.667580 140502730008384 spec.py:321] Evaluating on the training split.
I0327 07:55:20.374253 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 07:56:06.478633 140502730008384 spec.py:349] Evaluating on the test split.
I0327 07:56:30.354231 140502730008384 submission_runner.py:422] Time since start: 20389.14s, 	Step: 21843, 	{'train/ctc_loss': Array(1739.3417, dtype=float32), 'train/wer': 0.944685667249717, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 18765.29754781723, 'total_duration': 20389.143488883972, 'accumulated_submission_time': 18765.29754781723, 'accumulated_eval_time': 1622.3151769638062, 'accumulated_logging_time': 0.6342389583587646}
I0327 07:56:30.382678 140346023008000 logging_writer.py:48] [21843] accumulated_eval_time=1622.315177, accumulated_logging_time=0.634239, accumulated_submission_time=18765.297548, global_step=21843, preemption_count=0, score=18765.297548, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=20389.143489, train/ctc_loss=1739.3416748046875, train/wer=0.944686, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 07:58:28.310103 140346014615296 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.0, loss=1786.69140625
I0327 08:06:09.589775 140346023008000 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.0, loss=1759.0679931640625
I0327 08:13:18.797107 140346023008000 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.0, loss=1787.6895751953125
I0327 08:20:30.411644 140502730008384 spec.py:321] Evaluating on the training split.
I0327 08:21:07.826306 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 08:21:54.811433 140502730008384 spec.py:349] Evaluating on the test split.
I0327 08:22:18.121021 140502730008384 submission_runner.py:422] Time since start: 21936.91s, 	Step: 23472, 	{'train/ctc_loss': Array(1769.4664, dtype=float32), 'train/wer': 0.9432456399645285, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 20205.250517606735, 'total_duration': 21936.90806865692, 'accumulated_submission_time': 20205.250517606735, 'accumulated_eval_time': 1730.0197913646698, 'accumulated_logging_time': 0.6753904819488525}
I0327 08:22:18.153668 140345726043904 logging_writer.py:48] [23472] accumulated_eval_time=1730.019791, accumulated_logging_time=0.675390, accumulated_submission_time=20205.250518, global_step=23472, preemption_count=0, score=20205.250518, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=21936.908069, train/ctc_loss=1769.4664306640625, train/wer=0.943246, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 08:22:39.923836 140345717651200 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.0, loss=1768.1805419921875
I0327 08:29:16.556914 140345726043904 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.0, loss=1818.936767578125
I0327 08:36:52.864500 140345717651200 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.0, loss=1880.1201171875
I0327 08:44:01.951608 140345726043904 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.0, loss=1808.272705078125
I0327 08:46:18.484050 140502730008384 spec.py:321] Evaluating on the training split.
I0327 08:46:55.287102 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 08:47:41.668482 140502730008384 spec.py:349] Evaluating on the test split.
I0327 08:48:04.697295 140502730008384 submission_runner.py:422] Time since start: 23483.48s, 	Step: 25159, 	{'train/ctc_loss': Array(1736.9141, dtype=float32), 'train/wer': 0.9439109001278072, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 21645.498234033585, 'total_duration': 23483.48348093033, 'accumulated_submission_time': 21645.498234033585, 'accumulated_eval_time': 1836.2274386882782, 'accumulated_logging_time': 0.7238397598266602}
I0327 08:48:04.732631 140346903652096 logging_writer.py:48] [25159] accumulated_eval_time=1836.227439, accumulated_logging_time=0.723840, accumulated_submission_time=21645.498234, global_step=25159, preemption_count=0, score=21645.498234, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=23483.483481, train/ctc_loss=1736.9140625, train/wer=0.943911, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 08:52:54.226412 140346895259392 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.0, loss=1913.99755859375
I0327 09:00:15.130211 140345664612096 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.0, loss=1852.099365234375
I0327 09:07:48.222725 140345656219392 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.0, loss=1847.421142578125
I0327 09:12:05.424676 140502730008384 spec.py:321] Evaluating on the training split.
I0327 09:12:42.228422 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 09:13:26.905315 140502730008384 spec.py:349] Evaluating on the test split.
I0327 09:13:50.184472 140502730008384 submission_runner.py:422] Time since start: 25028.97s, 	Step: 26769, 	{'train/ctc_loss': Array(1715.2401, dtype=float32), 'train/wer': 0.9450143703143059, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 23086.111219644547, 'total_duration': 25028.971300125122, 'accumulated_submission_time': 23086.111219644547, 'accumulated_eval_time': 1940.9822540283203, 'accumulated_logging_time': 0.7747857570648193}
I0327 09:13:50.219965 140345818212096 logging_writer.py:48] [26769] accumulated_eval_time=1940.982254, accumulated_logging_time=0.774786, accumulated_submission_time=23086.111220, global_step=26769, preemption_count=0, score=23086.111220, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=25028.971300, train/ctc_loss=1715.2401123046875, train/wer=0.945014, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 09:17:00.636014 140345818212096 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.0, loss=1821.7845458984375
I0327 09:24:37.311406 140345809819392 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.0, loss=1825.1614990234375
I0327 09:32:06.634791 140345818212096 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.0, loss=1855.993408203125
I0327 09:37:50.529812 140502730008384 spec.py:321] Evaluating on the training split.
I0327 09:38:28.094199 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 09:39:13.792979 140502730008384 spec.py:349] Evaluating on the test split.
I0327 09:39:37.514087 140502730008384 submission_runner.py:422] Time since start: 26576.30s, 	Step: 28398, 	{'train/ctc_loss': Array(1783.4905, dtype=float32), 'train/wer': 0.9417576703068122, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 24526.3410012722, 'total_duration': 26576.300320625305, 'accumulated_submission_time': 24526.3410012722, 'accumulated_eval_time': 2047.9609479904175, 'accumulated_logging_time': 0.8246512413024902}
I0327 09:39:37.559184 140346903652096 logging_writer.py:48] [28398] accumulated_eval_time=2047.960948, accumulated_logging_time=0.824651, accumulated_submission_time=24526.341001, global_step=28398, preemption_count=0, score=24526.341001, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=26576.300321, train/ctc_loss=1783.490478515625, train/wer=0.941758, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 09:40:54.916610 140346895259392 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.0, loss=1807.7618408203125
I0327 09:48:14.254609 140346903652096 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.0, loss=1862.068359375
I0327 09:55:41.552620 140346895259392 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.0, loss=1779.24072265625
I0327 10:03:16.245494 140346903652096 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.0, loss=1838.7958984375
I0327 10:03:38.377508 140502730008384 spec.py:321] Evaluating on the training split.
I0327 10:04:16.536749 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 10:05:01.914297 140502730008384 spec.py:349] Evaluating on the test split.
I0327 10:05:24.940054 140502730008384 submission_runner.py:422] Time since start: 28123.73s, 	Step: 30029, 	{'train/ctc_loss': Array(1824.6843, dtype=float32), 'train/wer': 0.9416600198590335, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 25967.05706334114, 'total_duration': 28123.726251602173, 'accumulated_submission_time': 25967.05706334114, 'accumulated_eval_time': 2154.51789522171, 'accumulated_logging_time': 0.9046425819396973}
I0327 10:05:24.971821 140345889892096 logging_writer.py:48] [30029] accumulated_eval_time=2154.517895, accumulated_logging_time=0.904643, accumulated_submission_time=25967.057063, global_step=30029, preemption_count=0, score=25967.057063, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=28123.726252, train/ctc_loss=1824.684326171875, train/wer=0.941660, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 10:12:01.357494 140345881499392 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.0, loss=1805.84912109375
I0327 10:19:49.999653 140345889892096 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.0, loss=1818.5491943359375
I0327 10:27:04.482530 140345881499392 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.0, loss=1789.6893310546875
I0327 10:29:26.428462 140502730008384 spec.py:321] Evaluating on the training split.
I0327 10:30:03.938071 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 10:30:50.905276 140502730008384 spec.py:349] Evaluating on the test split.
I0327 10:31:14.317114 140502730008384 submission_runner.py:422] Time since start: 29673.10s, 	Step: 31650, 	{'train/ctc_loss': Array(1692.767, dtype=float32), 'train/wer': 0.9447677853176417, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 27408.433246850967, 'total_duration': 29673.102831363678, 'accumulated_submission_time': 27408.433246850967, 'accumulated_eval_time': 2262.4004530906677, 'accumulated_logging_time': 0.9510495662689209}
I0327 10:31:14.352861 140346319972096 logging_writer.py:48] [31650] accumulated_eval_time=2262.400453, accumulated_logging_time=0.951050, accumulated_submission_time=27408.433247, global_step=31650, preemption_count=0, score=27408.433247, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=29673.102831, train/ctc_loss=1692.7669677734375, train/wer=0.944768, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 10:36:07.675356 140345664612096 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.0, loss=1816.0980224609375
I0327 10:43:20.891760 140345656219392 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.0, loss=1791.3172607421875
I0327 10:51:15.691238 140346319972096 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.0, loss=1880.948974609375
I0327 10:55:14.466868 140502730008384 spec.py:321] Evaluating on the training split.
I0327 10:55:51.420935 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 10:56:37.595960 140502730008384 spec.py:349] Evaluating on the test split.
I0327 10:57:01.386382 140502730008384 submission_runner.py:422] Time since start: 31220.18s, 	Step: 33294, 	{'train/ctc_loss': Array(1787.1725, dtype=float32), 'train/wer': 0.9427091658940503, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 28848.465141296387, 'total_duration': 31220.17554306984, 'accumulated_submission_time': 28848.465141296387, 'accumulated_eval_time': 2369.317343711853, 'accumulated_logging_time': 1.001239538192749}
I0327 10:57:01.416196 140346319972096 logging_writer.py:48] [33294] accumulated_eval_time=2369.317344, accumulated_logging_time=1.001240, accumulated_submission_time=28848.465141, global_step=33294, preemption_count=0, score=28848.465141, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=31220.175543, train/ctc_loss=1787.1724853515625, train/wer=0.942709, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 10:59:42.164780 140346311579392 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.0, loss=1803.4320068359375
I0327 11:07:42.844434 140346319972096 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.0, loss=1834.316162109375
I0327 11:14:49.059168 140346311579392 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.0, loss=1900.216552734375
I0327 11:21:02.511598 140502730008384 spec.py:321] Evaluating on the training split.
I0327 11:21:40.621346 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 11:22:26.124682 140502730008384 spec.py:349] Evaluating on the test split.
I0327 11:22:50.003767 140502730008384 submission_runner.py:422] Time since start: 32768.79s, 	Step: 34887, 	{'train/ctc_loss': Array(1714.3213, dtype=float32), 'train/wer': 0.9448971433842748, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 30289.483254909515, 'total_duration': 32768.7904548645, 'accumulated_submission_time': 30289.483254909515, 'accumulated_eval_time': 2476.804412126541, 'accumulated_logging_time': 1.042654037475586}
I0327 11:22:50.039257 140346319972096 logging_writer.py:48] [34887] accumulated_eval_time=2476.804412, accumulated_logging_time=1.042654, accumulated_submission_time=30289.483255, global_step=34887, preemption_count=0, score=30289.483255, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=32768.790455, train/ctc_loss=1714.3212890625, train/wer=0.944897, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 11:24:15.488996 140346311579392 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.0, loss=1836.1580810546875
I0327 11:31:20.873479 140345664612096 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.0, loss=1818.032470703125
I0327 11:39:16.810178 140345656219392 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.0, loss=1806.6136474609375
I0327 11:46:21.166830 140345664612096 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.0, loss=1764.2808837890625
I0327 11:46:50.475263 140502730008384 spec.py:321] Evaluating on the training split.
I0327 11:47:27.378732 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 11:48:12.388372 140502730008384 spec.py:349] Evaluating on the test split.
I0327 11:48:35.816131 140502730008384 submission_runner.py:422] Time since start: 34314.60s, 	Step: 36532, 	{'train/ctc_loss': Array(1760.6809, dtype=float32), 'train/wer': 0.9432324554919642, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 31729.835213899612, 'total_duration': 34314.602588415146, 'accumulated_submission_time': 31729.835213899612, 'accumulated_eval_time': 2582.139939069748, 'accumulated_logging_time': 1.0938012599945068}
I0327 11:48:35.853566 140345664612096 logging_writer.py:48] [36532] accumulated_eval_time=2582.139939, accumulated_logging_time=1.093801, accumulated_submission_time=31729.835214, global_step=36532, preemption_count=0, score=31729.835214, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=34314.602588, train/ctc_loss=1760.680908203125, train/wer=0.943232, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 11:55:26.637310 140345656219392 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.0, loss=1835.6314697265625
I0327 12:02:35.280564 140345664612096 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.0, loss=1825.9427490234375
I0327 12:10:30.112276 140345656219392 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.0, loss=1916.57666015625
I0327 12:12:36.339268 140502730008384 spec.py:321] Evaluating on the training split.
I0327 12:13:13.937456 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 12:13:59.718666 140502730008384 spec.py:349] Evaluating on the test split.
I0327 12:14:23.699678 140502730008384 submission_runner.py:422] Time since start: 35862.49s, 	Step: 38139, 	{'train/ctc_loss': Array(1852.6161, dtype=float32), 'train/wer': 0.941680272071945, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 33170.24009466171, 'total_duration': 35862.488893032074, 'accumulated_submission_time': 33170.24009466171, 'accumulated_eval_time': 2689.497986793518, 'accumulated_logging_time': 1.1452248096466064}
I0327 12:14:23.729274 140346473572096 logging_writer.py:48] [38139] accumulated_eval_time=2689.497987, accumulated_logging_time=1.145225, accumulated_submission_time=33170.240095, global_step=38139, preemption_count=0, score=33170.240095, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=35862.488893, train/ctc_loss=1852.6160888671875, train/wer=0.941680, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 12:19:00.181164 140346465179392 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.0, loss=1802.669921875
I0327 12:26:48.502780 140346473572096 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.0, loss=1818.16162109375
I0327 12:33:59.414027 140346473572096 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.0, loss=1789.439208984375
I0327 12:38:24.233015 140502730008384 spec.py:321] Evaluating on the training split.
I0327 12:39:01.813034 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 12:39:46.730605 140502730008384 spec.py:349] Evaluating on the test split.
I0327 12:40:09.348688 140502730008384 submission_runner.py:422] Time since start: 37408.14s, 	Step: 39790, 	{'train/ctc_loss': Array(1901.3655, dtype=float32), 'train/wer': 0.940312095793757, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 34610.66215443611, 'total_duration': 37408.135347127914, 'accumulated_submission_time': 34610.66215443611, 'accumulated_eval_time': 2794.6085135936737, 'accumulated_logging_time': 1.188056468963623}
I0327 12:40:09.384311 140346473572096 logging_writer.py:48] [39790] accumulated_eval_time=2794.608514, accumulated_logging_time=1.188056, accumulated_submission_time=34610.662154, global_step=39790, preemption_count=0, score=34610.662154, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=37408.135347, train/ctc_loss=1901.365478515625, train/wer=0.940312, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 12:42:53.532902 140346465179392 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.0, loss=1805.339599609375
I0327 12:50:08.463808 140345818212096 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.0, loss=1813.2679443359375
I0327 12:57:55.964908 140345809819392 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.0, loss=1792.57177734375
I0327 13:04:09.782807 140502730008384 spec.py:321] Evaluating on the training split.
I0327 13:04:46.604337 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 13:05:31.333342 140502730008384 spec.py:349] Evaluating on the test split.
I0327 13:05:55.620055 140502730008384 submission_runner.py:422] Time since start: 38954.41s, 	Step: 41423, 	{'train/ctc_loss': Array(1959.8617, dtype=float32), 'train/wer': 0.9371047844119075, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 36050.97627019882, 'total_duration': 38954.408056259155, 'accumulated_submission_time': 36050.97627019882, 'accumulated_eval_time': 2900.4421646595, 'accumulated_logging_time': 1.2397139072418213}
I0327 13:05:55.654296 140346903652096 logging_writer.py:48] [41423] accumulated_eval_time=2900.442165, accumulated_logging_time=1.239714, accumulated_submission_time=36050.976270, global_step=41423, preemption_count=0, score=36050.976270, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=38954.408056, train/ctc_loss=1959.8616943359375, train/wer=0.937105, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 13:06:54.538993 140346895259392 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.0, loss=1868.0469970703125
I0327 13:14:20.244689 140346903652096 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.0, loss=1813.396484375
I0327 13:21:42.258468 140346903652096 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.0, loss=1934.389892578125
I0327 13:29:20.782800 140346895259392 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.0, loss=1722.0311279296875
I0327 13:29:56.015163 140502730008384 spec.py:321] Evaluating on the training split.
I0327 13:30:33.275781 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 13:31:18.755759 140502730008384 spec.py:349] Evaluating on the test split.
I0327 13:31:41.969420 140502730008384 submission_runner.py:422] Time since start: 40500.76s, 	Step: 43038, 	{'train/ctc_loss': Array(2009.269, dtype=float32), 'train/wer': 0.9366967129626904, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 37491.25642633438, 'total_duration': 40500.75623226166, 'accumulated_submission_time': 37491.25642633438, 'accumulated_eval_time': 3006.3914148807526, 'accumulated_logging_time': 1.2869277000427246}
I0327 13:31:42.005319 140346903652096 logging_writer.py:48] [43038] accumulated_eval_time=3006.391415, accumulated_logging_time=1.286928, accumulated_submission_time=37491.256426, global_step=43038, preemption_count=0, score=37491.256426, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=40500.756232, train/ctc_loss=2009.26904296875, train/wer=0.936697, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 13:37:54.018858 140346903652096 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.0, loss=1838.927978515625
I0327 13:45:28.756192 140346895259392 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.0, loss=1846.4884033203125
I0327 13:52:50.754979 140345920612096 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.0, loss=1807.7618408203125
I0327 13:55:42.405142 140502730008384 spec.py:321] Evaluating on the training split.
I0327 13:56:19.052594 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 13:57:04.719996 140502730008384 spec.py:349] Evaluating on the test split.
I0327 13:57:28.161234 140502730008384 submission_runner.py:422] Time since start: 42046.95s, 	Step: 44705, 	{'train/ctc_loss': Array(1873.3391, dtype=float32), 'train/wer': 0.9401422956587576, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 38931.57281589508, 'total_duration': 42046.94741487503, 'accumulated_submission_time': 38931.57281589508, 'accumulated_eval_time': 3112.141885995865, 'accumulated_logging_time': 1.3365168571472168}
I0327 13:57:28.201768 140345920612096 logging_writer.py:48] [44705] accumulated_eval_time=3112.141886, accumulated_logging_time=1.336517, accumulated_submission_time=38931.572816, global_step=44705, preemption_count=0, score=38931.572816, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=42046.947415, train/ctc_loss=1873.339111328125, train/wer=0.940142, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 14:01:34.917658 140345912219392 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.0, loss=1769.402587890625
I0327 14:09:08.274273 140338942125824 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.0, loss=1804.957763671875
I0327 14:16:38.555632 140334600288000 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.0, loss=1849.423095703125
I0327 14:21:29.092845 140502730008384 spec.py:321] Evaluating on the training split.
I0327 14:22:07.330413 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 14:22:52.526895 140502730008384 spec.py:349] Evaluating on the test split.
I0327 14:23:16.045096 140502730008384 submission_runner.py:422] Time since start: 43594.83s, 	Step: 46303, 	{'train/ctc_loss': Array(1866.555, dtype=float32), 'train/wer': 0.9415328062295403, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 40372.381477832794, 'total_duration': 43594.83157324791, 'accumulated_submission_time': 40372.381477832794, 'accumulated_eval_time': 3219.0888180732727, 'accumulated_logging_time': 1.3925731182098389}
I0327 14:23:16.084165 140338942125824 logging_writer.py:48] [46303] accumulated_eval_time=3219.088818, accumulated_logging_time=1.392573, accumulated_submission_time=40372.381478, global_step=46303, preemption_count=0, score=40372.381478, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=43594.831573, train/ctc_loss=1866.5550537109375, train/wer=0.941533, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 14:25:56.363525 140338942125824 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.0, loss=1807.7618408203125
I0327 14:33:15.581255 140334600288000 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.0, loss=1749.809814453125
I0327 14:40:45.700983 140346248292096 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.0, loss=1783.206787109375
I0327 14:47:16.189773 140502730008384 spec.py:321] Evaluating on the training split.
I0327 14:47:53.568722 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 14:48:39.908350 140502730008384 spec.py:349] Evaluating on the test split.
I0327 14:49:02.968230 140502730008384 submission_runner.py:422] Time since start: 45141.75s, 	Step: 47951, 	{'train/ctc_loss': Array(1827.016, dtype=float32), 'train/wer': 0.941605522275386, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 41812.4059650898, 'total_duration': 45141.75443768501, 'accumulated_submission_time': 41812.4059650898, 'accumulated_eval_time': 3325.8616778850555, 'accumulated_logging_time': 1.445969581604004}
I0327 14:49:03.014110 140346248292096 logging_writer.py:48] [47951] accumulated_eval_time=3325.861678, accumulated_logging_time=1.445970, accumulated_submission_time=41812.405965, global_step=47951, preemption_count=0, score=41812.405965, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=45141.754438, train/ctc_loss=1827.0159912109375, train/wer=0.941606, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 14:49:40.279489 140502730008384 spec.py:321] Evaluating on the training split.
I0327 14:50:16.477586 140502730008384 spec.py:333] Evaluating on the validation split.
I0327 14:50:56.682495 140502730008384 spec.py:349] Evaluating on the test split.
I0327 14:51:16.387716 140502730008384 submission_runner.py:422] Time since start: 45275.18s, 	Step: 48000, 	{'train/ctc_loss': Array(1805.6487, dtype=float32), 'train/wer': 0.9422779591135544, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 41849.637591362, 'total_duration': 45275.1767513752, 'accumulated_submission_time': 41849.637591362, 'accumulated_eval_time': 3421.967125892639, 'accumulated_logging_time': 1.5229506492614746}
I0327 14:51:16.413298 140345664612096 logging_writer.py:48] [48000] accumulated_eval_time=3421.967126, accumulated_logging_time=1.522951, accumulated_submission_time=41849.637591, global_step=48000, preemption_count=0, score=41849.637591, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=45275.176751, train/ctc_loss=1805.648681640625, train/wer=0.942278, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 14:51:16.431771 140345656219392 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=41849.637591
I0327 14:51:17.874313 140502730008384 submission_runner.py:596] Tuning trial 1/1
I0327 14:51:17.874564 140502730008384 submission_runner.py:597] Hyperparameters: Hyperparameters(learning_rate=4.131896390902391, beta1=0.9274758113254791, beta2=0.9978504782314613, warmup_steps=6999, decay_steps_factor=0.9007765761611038, end_factor=0.001, weight_decay=5.6687777311501786e-06, label_smoothing=0.2)
I0327 14:51:17.888661 140502730008384 submission_runner.py:598] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(32.055508, dtype=float32), 'train/wer': 4.979707640089124, 'validation/ctc_loss': Array(30.983448, dtype=float32), 'validation/wer': 4.536721472913871, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.985773, dtype=float32), 'test/wer': 4.794020270956472, 'test/num_examples': 2472, 'score': 41.9598753452301, 'total_duration': 305.00030064582825, 'accumulated_submission_time': 41.9598753452301, 'accumulated_eval_time': 263.0403838157654, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1714, {'train/ctc_loss': Array(1678.4785, dtype=float32), 'train/wer': 0.9446046425408333, 'validation/ctc_loss': Array(2283.8062, dtype=float32), 'validation/wer': 0.8965600471147069, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2205.3608, dtype=float32), 'test/wer': 0.8994779924034693, 'test/num_examples': 2472, 'score': 1482.2365572452545, 'total_duration': 1848.5136897563934, 'accumulated_submission_time': 1482.2365572452545, 'accumulated_eval_time': 366.17541241645813, 'accumulated_logging_time': 0.04054617881774902, 'global_step': 1714, 'preemption_count': 0}), (3464, {'train/ctc_loss': Array(1761.2479, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': Array(3345.958, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3181.6133, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2922.2044739723206, 'total_duration': 3389.101282119751, 'accumulated_submission_time': 2922.2044739723206, 'accumulated_eval_time': 466.6764621734619, 'accumulated_logging_time': 0.08878040313720703, 'global_step': 3464, 'preemption_count': 0}), (5123, {'train/ctc_loss': Array(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': Array(3357.8315, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4362.615800142288, 'total_duration': 4934.600059747696, 'accumulated_submission_time': 4362.615800142288, 'accumulated_eval_time': 571.6493661403656, 'accumulated_logging_time': 0.13571786880493164, 'global_step': 5123, 'preemption_count': 0}), (6792, {'train/ctc_loss': Array(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': Array(3357.716, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 5802.779655694962, 'total_duration': 6480.01359128952, 'accumulated_submission_time': 5802.779655694962, 'accumulated_eval_time': 676.7824006080627, 'accumulated_logging_time': 0.1844029426574707, 'global_step': 6792, 'preemption_count': 0}), (8503, {'train/ctc_loss': Array(1832.9288, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': Array(3357.587, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7242.806547164917, 'total_duration': 8023.206823348999, 'accumulated_submission_time': 7242.806547164917, 'accumulated_eval_time': 779.8258347511292, 'accumulated_logging_time': 0.23680782318115234, 'global_step': 8503, 'preemption_count': 0}), (10162, {'train/ctc_loss': Array(1752.8004, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': Array(3357.4324, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 8683.226366519928, 'total_duration': 9568.057299137115, 'accumulated_submission_time': 8683.226366519928, 'accumulated_eval_time': 884.1396083831787, 'accumulated_logging_time': 0.2855856418609619, 'global_step': 10162, 'preemption_count': 0}), (11849, {'train/ctc_loss': Array(1746.1039, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 10123.33424949646, 'total_duration': 11114.625593423843, 'accumulated_submission_time': 10123.33424949646, 'accumulated_eval_time': 990.4855401515961, 'accumulated_logging_time': 0.3280649185180664, 'global_step': 11849, 'preemption_count': 0}), (13545, {'train/ctc_loss': Array(1733.7323, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 11563.896816015244, 'total_duration': 12658.4535343647, 'accumulated_submission_time': 11563.896816015244, 'accumulated_eval_time': 1093.6117329597473, 'accumulated_logging_time': 0.39888429641723633, 'global_step': 13545, 'preemption_count': 0}), (15212, {'train/ctc_loss': Array(1786.8575, dtype=float32), 'train/wer': 0.9427990785714666, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 13003.853729963303, 'total_duration': 14205.672280311584, 'accumulated_submission_time': 13003.853729963303, 'accumulated_eval_time': 1200.770696401596, 'accumulated_logging_time': 0.43520283699035645, 'global_step': 15212, 'preemption_count': 0}), (16897, {'train/ctc_loss': Array(1755.9307, dtype=float32), 'train/wer': 0.9423383225986367, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14444.457047462463, 'total_duration': 15751.261131763458, 'accumulated_submission_time': 14444.457047462463, 'accumulated_eval_time': 1305.6351425647736, 'accumulated_logging_time': 0.48407936096191406, 'global_step': 16897, 'preemption_count': 0}), (18543, {'train/ctc_loss': Array(1731.2422, dtype=float32), 'train/wer': 0.9431396916893625, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15884.854880809784, 'total_duration': 17297.012016057968, 'accumulated_submission_time': 15884.854880809784, 'accumulated_eval_time': 1410.865191936493, 'accumulated_logging_time': 0.5360684394836426, 'global_step': 18543, 'preemption_count': 0}), (20213, {'train/ctc_loss': Array(1763.6094, dtype=float32), 'train/wer': 0.9432716912443612, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 17325.298138141632, 'total_duration': 18842.339307785034, 'accumulated_submission_time': 17325.298138141632, 'accumulated_eval_time': 1515.630853652954, 'accumulated_logging_time': 0.5837047100067139, 'global_step': 20213, 'preemption_count': 0}), (21843, {'train/ctc_loss': Array(1739.3417, dtype=float32), 'train/wer': 0.944685667249717, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 18765.29754781723, 'total_duration': 20389.143488883972, 'accumulated_submission_time': 18765.29754781723, 'accumulated_eval_time': 1622.3151769638062, 'accumulated_logging_time': 0.6342389583587646, 'global_step': 21843, 'preemption_count': 0}), (23472, {'train/ctc_loss': Array(1769.4664, dtype=float32), 'train/wer': 0.9432456399645285, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 20205.250517606735, 'total_duration': 21936.90806865692, 'accumulated_submission_time': 20205.250517606735, 'accumulated_eval_time': 1730.0197913646698, 'accumulated_logging_time': 0.6753904819488525, 'global_step': 23472, 'preemption_count': 0}), (25159, {'train/ctc_loss': Array(1736.9141, dtype=float32), 'train/wer': 0.9439109001278072, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 21645.498234033585, 'total_duration': 23483.48348093033, 'accumulated_submission_time': 21645.498234033585, 'accumulated_eval_time': 1836.2274386882782, 'accumulated_logging_time': 0.7238397598266602, 'global_step': 25159, 'preemption_count': 0}), (26769, {'train/ctc_loss': Array(1715.2401, dtype=float32), 'train/wer': 0.9450143703143059, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 23086.111219644547, 'total_duration': 25028.971300125122, 'accumulated_submission_time': 23086.111219644547, 'accumulated_eval_time': 1940.9822540283203, 'accumulated_logging_time': 0.7747857570648193, 'global_step': 26769, 'preemption_count': 0}), (28398, {'train/ctc_loss': Array(1783.4905, dtype=float32), 'train/wer': 0.9417576703068122, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 24526.3410012722, 'total_duration': 26576.300320625305, 'accumulated_submission_time': 24526.3410012722, 'accumulated_eval_time': 2047.9609479904175, 'accumulated_logging_time': 0.8246512413024902, 'global_step': 28398, 'preemption_count': 0}), (30029, {'train/ctc_loss': Array(1824.6843, dtype=float32), 'train/wer': 0.9416600198590335, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 25967.05706334114, 'total_duration': 28123.726251602173, 'accumulated_submission_time': 25967.05706334114, 'accumulated_eval_time': 2154.51789522171, 'accumulated_logging_time': 0.9046425819396973, 'global_step': 30029, 'preemption_count': 0}), (31650, {'train/ctc_loss': Array(1692.767, dtype=float32), 'train/wer': 0.9447677853176417, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 27408.433246850967, 'total_duration': 29673.102831363678, 'accumulated_submission_time': 27408.433246850967, 'accumulated_eval_time': 2262.4004530906677, 'accumulated_logging_time': 0.9510495662689209, 'global_step': 31650, 'preemption_count': 0}), (33294, {'train/ctc_loss': Array(1787.1725, dtype=float32), 'train/wer': 0.9427091658940503, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 28848.465141296387, 'total_duration': 31220.17554306984, 'accumulated_submission_time': 28848.465141296387, 'accumulated_eval_time': 2369.317343711853, 'accumulated_logging_time': 1.001239538192749, 'global_step': 33294, 'preemption_count': 0}), (34887, {'train/ctc_loss': Array(1714.3213, dtype=float32), 'train/wer': 0.9448971433842748, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 30289.483254909515, 'total_duration': 32768.7904548645, 'accumulated_submission_time': 30289.483254909515, 'accumulated_eval_time': 2476.804412126541, 'accumulated_logging_time': 1.042654037475586, 'global_step': 34887, 'preemption_count': 0}), (36532, {'train/ctc_loss': Array(1760.6809, dtype=float32), 'train/wer': 0.9432324554919642, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 31729.835213899612, 'total_duration': 34314.602588415146, 'accumulated_submission_time': 31729.835213899612, 'accumulated_eval_time': 2582.139939069748, 'accumulated_logging_time': 1.0938012599945068, 'global_step': 36532, 'preemption_count': 0}), (38139, {'train/ctc_loss': Array(1852.6161, dtype=float32), 'train/wer': 0.941680272071945, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 33170.24009466171, 'total_duration': 35862.488893032074, 'accumulated_submission_time': 33170.24009466171, 'accumulated_eval_time': 2689.497986793518, 'accumulated_logging_time': 1.1452248096466064, 'global_step': 38139, 'preemption_count': 0}), (39790, {'train/ctc_loss': Array(1901.3655, dtype=float32), 'train/wer': 0.940312095793757, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 34610.66215443611, 'total_duration': 37408.135347127914, 'accumulated_submission_time': 34610.66215443611, 'accumulated_eval_time': 2794.6085135936737, 'accumulated_logging_time': 1.188056468963623, 'global_step': 39790, 'preemption_count': 0}), (41423, {'train/ctc_loss': Array(1959.8617, dtype=float32), 'train/wer': 0.9371047844119075, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 36050.97627019882, 'total_duration': 38954.408056259155, 'accumulated_submission_time': 36050.97627019882, 'accumulated_eval_time': 2900.4421646595, 'accumulated_logging_time': 1.2397139072418213, 'global_step': 41423, 'preemption_count': 0}), (43038, {'train/ctc_loss': Array(2009.269, dtype=float32), 'train/wer': 0.9366967129626904, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 37491.25642633438, 'total_duration': 40500.75623226166, 'accumulated_submission_time': 37491.25642633438, 'accumulated_eval_time': 3006.3914148807526, 'accumulated_logging_time': 1.2869277000427246, 'global_step': 43038, 'preemption_count': 0}), (44705, {'train/ctc_loss': Array(1873.3391, dtype=float32), 'train/wer': 0.9401422956587576, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 38931.57281589508, 'total_duration': 42046.94741487503, 'accumulated_submission_time': 38931.57281589508, 'accumulated_eval_time': 3112.141885995865, 'accumulated_logging_time': 1.3365168571472168, 'global_step': 44705, 'preemption_count': 0}), (46303, {'train/ctc_loss': Array(1866.555, dtype=float32), 'train/wer': 0.9415328062295403, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 40372.381477832794, 'total_duration': 43594.83157324791, 'accumulated_submission_time': 40372.381477832794, 'accumulated_eval_time': 3219.0888180732727, 'accumulated_logging_time': 1.3925731182098389, 'global_step': 46303, 'preemption_count': 0}), (47951, {'train/ctc_loss': Array(1827.016, dtype=float32), 'train/wer': 0.941605522275386, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 41812.4059650898, 'total_duration': 45141.75443768501, 'accumulated_submission_time': 41812.4059650898, 'accumulated_eval_time': 3325.8616778850555, 'accumulated_logging_time': 1.445969581604004, 'global_step': 47951, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(1805.6487, dtype=float32), 'train/wer': 0.9422779591135544, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 41849.637591362, 'total_duration': 45275.1767513752, 'accumulated_submission_time': 41849.637591362, 'accumulated_eval_time': 3421.967125892639, 'accumulated_logging_time': 1.5229506492614746, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0327 14:51:17.888841 140502730008384 submission_runner.py:599] Timing: 41849.637591362
I0327 14:51:17.888901 140502730008384 submission_runner.py:601] Total number of evals: 31
I0327 14:51:17.888956 140502730008384 submission_runner.py:602] ====================
I0327 14:51:17.892888 140502730008384 submission_runner.py:686] Final librispeech_deepspeech_tanh score: 0
