python3 submission_runner.py --framework=jax --workload=librispeech_conformer_layernorm --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --rng_seed=2060265407 --max_global_steps=80000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab --tuning_ruleset=external --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_conformer_layernorm/tuning_search_space.json --num_tuning_trials=1 2>&1 | tee -a /logs/librispeech_conformer_layernorm_jax_03-06-2024-00-11-59.log
I0306 00:12:22.320387 140368653526848 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/librispeech_conformer_layernorm_jax.
I0306 00:12:23.446742 140368653526848 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0306 00:12:23.448381 140368653526848 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0306 00:12:23.448599 140368653526848 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0306 00:12:23.456427 140368653526848 submission_runner.py:547] Using RNG seed 2060265407
I0306 00:12:24.696138 140368653526848 submission_runner.py:556] --- Tuning run 1/1 ---
I0306 00:12:24.696380 140368653526848 submission_runner.py:561] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/librispeech_conformer_layernorm_jax/trial_1.
I0306 00:12:24.696585 140368653526848 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_layernorm_jax/trial_1/hparams.json.
I0306 00:12:24.881583 140368653526848 submission_runner.py:206] Initializing dataset.
I0306 00:12:24.881823 140368653526848 submission_runner.py:213] Initializing model.
I0306 00:12:29.844942 140368653526848 submission_runner.py:255] Initializing optimizer.
I0306 00:12:31.111761 140368653526848 submission_runner.py:262] Initializing metrics bundle.
I0306 00:12:31.112006 140368653526848 submission_runner.py:280] Initializing checkpoint and logger.
I0306 00:12:31.113520 140368653526848 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/librispeech_conformer_layernorm_jax/trial_1 with prefix checkpoint_
I0306 00:12:31.113686 140368653526848 submission_runner.py:300] Saving meta data to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_layernorm_jax/trial_1/meta_data_0.json.
I0306 00:12:31.113904 140368653526848 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0306 00:12:31.113975 140368653526848 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0306 00:12:31.406385 140368653526848 logger_utils.py:220] Unable to record git information. Continuing without it.
I0306 00:12:31.676843 140368653526848 submission_runner.py:304] Saving flags to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_layernorm_jax/trial_1/flags_0.json.
I0306 00:12:31.694057 140368653526848 submission_runner.py:314] Starting training loop.
I0306 00:12:31.997039 140368653526848 input_pipeline.py:20] Loading split = train-clean-100
I0306 00:12:32.044994 140368653526848 input_pipeline.py:20] Loading split = train-clean-360
I0306 00:12:32.532274 140368653526848 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0306 00:13:31.096031 140195433273088 logging_writer.py:48] [0] global_step=0, grad_norm=29.797077178955078, loss=32.84510040283203
I0306 00:13:31.132822 140368653526848 spec.py:321] Evaluating on the training split.
I0306 00:13:31.296355 140368653526848 input_pipeline.py:20] Loading split = train-clean-100
I0306 00:13:31.331224 140368653526848 input_pipeline.py:20] Loading split = train-clean-360
I0306 00:13:31.718334 140368653526848 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0306 00:14:47.034567 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 00:14:47.148546 140368653526848 input_pipeline.py:20] Loading split = dev-clean
I0306 00:14:47.154786 140368653526848 input_pipeline.py:20] Loading split = dev-other
I0306 00:15:50.930471 140368653526848 spec.py:349] Evaluating on the test split.
I0306 00:15:51.047956 140368653526848 input_pipeline.py:20] Loading split = test-clean
I0306 00:16:27.979370 140368653526848 submission_runner.py:413] Time since start: 236.28s, 	Step: 1, 	{'train/ctc_loss': Array(32.04605, dtype=float32), 'train/wer': 1.335516315099015, 'validation/ctc_loss': Array(31.162418, dtype=float32), 'validation/wer': 1.3074331173909266, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.273266, dtype=float32), 'test/wer': 1.3451343610992628, 'test/num_examples': 2472, 'score': 59.43868660926819, 'total_duration': 236.28323650360107, 'accumulated_submission_time': 59.43868660926819, 'accumulated_eval_time': 176.84447622299194, 'accumulated_logging_time': 0}
I0306 00:16:28.006805 140185937368832 logging_writer.py:48] [1] accumulated_eval_time=176.844476, accumulated_logging_time=0, accumulated_submission_time=59.438687, global_step=1, preemption_count=0, score=59.438687, test/ctc_loss=31.273265838623047, test/num_examples=2472, test/wer=1.345134, total_duration=236.283237, train/ctc_loss=32.046051025390625, train/wer=1.335516, validation/ctc_loss=31.162418365478516, validation/num_examples=5348, validation/wer=1.307433
I0306 00:16:51.272937 140197624751872 logging_writer.py:48] [1] global_step=1, grad_norm=27.739713668823242, loss=32.91159439086914
I0306 00:16:52.167100 140197633144576 logging_writer.py:48] [2] global_step=2, grad_norm=28.951396942138672, loss=32.39813995361328
I0306 00:16:52.996592 140197624751872 logging_writer.py:48] [3] global_step=3, grad_norm=29.710731506347656, loss=32.59090805053711
I0306 00:16:53.891560 140197633144576 logging_writer.py:48] [4] global_step=4, grad_norm=28.974218368530273, loss=32.67429733276367
I0306 00:16:54.803930 140197624751872 logging_writer.py:48] [5] global_step=5, grad_norm=29.220212936401367, loss=33.42625427246094
I0306 00:16:55.704063 140197633144576 logging_writer.py:48] [6] global_step=6, grad_norm=31.069679260253906, loss=32.883033752441406
I0306 00:16:56.595892 140197624751872 logging_writer.py:48] [7] global_step=7, grad_norm=31.498449325561523, loss=32.19734191894531
I0306 00:16:57.518594 140197633144576 logging_writer.py:48] [8] global_step=8, grad_norm=34.64455032348633, loss=32.53892135620117
I0306 00:16:58.428655 140197624751872 logging_writer.py:48] [9] global_step=9, grad_norm=37.075565338134766, loss=32.37123107910156
I0306 00:16:59.336295 140197633144576 logging_writer.py:48] [10] global_step=10, grad_norm=40.42967224121094, loss=31.989521026611328
I0306 00:17:00.236578 140197624751872 logging_writer.py:48] [11] global_step=11, grad_norm=45.0001335144043, loss=32.02588653564453
I0306 00:17:01.145578 140197633144576 logging_writer.py:48] [12] global_step=12, grad_norm=39.17539596557617, loss=31.743499755859375
I0306 00:17:02.056595 140197624751872 logging_writer.py:48] [13] global_step=13, grad_norm=45.843509674072266, loss=31.875404357910156
I0306 00:17:02.952520 140197633144576 logging_writer.py:48] [14] global_step=14, grad_norm=50.070796966552734, loss=31.99599838256836
I0306 00:17:03.849381 140197624751872 logging_writer.py:48] [15] global_step=15, grad_norm=60.45774459838867, loss=32.48859405517578
I0306 00:17:04.749461 140197633144576 logging_writer.py:48] [16] global_step=16, grad_norm=54.64936065673828, loss=31.8890380859375
I0306 00:17:05.651998 140197624751872 logging_writer.py:48] [17] global_step=17, grad_norm=65.11579895019531, loss=31.553665161132812
I0306 00:17:06.554178 140197633144576 logging_writer.py:48] [18] global_step=18, grad_norm=63.203372955322266, loss=30.748620986938477
I0306 00:17:07.456365 140197624751872 logging_writer.py:48] [19] global_step=19, grad_norm=64.7912826538086, loss=30.510576248168945
I0306 00:17:08.402188 140197633144576 logging_writer.py:48] [20] global_step=20, grad_norm=63.494075775146484, loss=30.76947593688965
I0306 00:17:09.305317 140197624751872 logging_writer.py:48] [21] global_step=21, grad_norm=67.29352569580078, loss=29.996870040893555
I0306 00:17:10.198924 140197633144576 logging_writer.py:48] [22] global_step=22, grad_norm=71.38676452636719, loss=29.2812442779541
I0306 00:17:11.103295 140197624751872 logging_writer.py:48] [23] global_step=23, grad_norm=69.34581756591797, loss=29.643579483032227
I0306 00:17:12.014459 140197633144576 logging_writer.py:48] [24] global_step=24, grad_norm=60.97818374633789, loss=29.010848999023438
I0306 00:17:12.922336 140197624751872 logging_writer.py:48] [25] global_step=25, grad_norm=64.403564453125, loss=28.92452049255371
I0306 00:17:13.818460 140197633144576 logging_writer.py:48] [26] global_step=26, grad_norm=66.07318878173828, loss=27.49187469482422
I0306 00:17:14.722343 140197624751872 logging_writer.py:48] [27] global_step=27, grad_norm=69.107177734375, loss=27.214357376098633
I0306 00:17:15.646569 140197633144576 logging_writer.py:48] [28] global_step=28, grad_norm=73.78704833984375, loss=26.414676666259766
I0306 00:17:16.558321 140197624751872 logging_writer.py:48] [29] global_step=29, grad_norm=67.85669708251953, loss=26.885574340820312
I0306 00:17:17.462787 140197633144576 logging_writer.py:48] [30] global_step=30, grad_norm=58.79157638549805, loss=26.845420837402344
I0306 00:17:18.385862 140197624751872 logging_writer.py:48] [31] global_step=31, grad_norm=56.68141174316406, loss=25.696290969848633
I0306 00:17:19.297586 140197633144576 logging_writer.py:48] [32] global_step=32, grad_norm=56.13970184326172, loss=25.510499954223633
I0306 00:17:20.213601 140197624751872 logging_writer.py:48] [33] global_step=33, grad_norm=52.642005920410156, loss=25.035259246826172
I0306 00:17:21.116939 140197633144576 logging_writer.py:48] [34] global_step=34, grad_norm=45.47507095336914, loss=25.519845962524414
I0306 00:17:22.016608 140197624751872 logging_writer.py:48] [35] global_step=35, grad_norm=46.17290496826172, loss=24.54812240600586
I0306 00:17:22.923673 140197633144576 logging_writer.py:48] [36] global_step=36, grad_norm=41.00753402709961, loss=24.351886749267578
I0306 00:17:23.849598 140197624751872 logging_writer.py:48] [37] global_step=37, grad_norm=40.93434143066406, loss=23.79977798461914
I0306 00:17:24.752771 140197633144576 logging_writer.py:48] [38] global_step=38, grad_norm=32.73464584350586, loss=24.226940155029297
I0306 00:17:25.646905 140197624751872 logging_writer.py:48] [39] global_step=39, grad_norm=34.35213851928711, loss=23.113561630249023
I0306 00:17:26.553751 140197633144576 logging_writer.py:48] [40] global_step=40, grad_norm=32.24427795410156, loss=22.008007049560547
I0306 00:17:27.454419 140197624751872 logging_writer.py:48] [41] global_step=41, grad_norm=26.515609741210938, loss=23.139379501342773
I0306 00:17:28.367676 140197633144576 logging_writer.py:48] [42] global_step=42, grad_norm=28.822338104248047, loss=23.203083038330078
I0306 00:17:29.307811 140197624751872 logging_writer.py:48] [43] global_step=43, grad_norm=26.100526809692383, loss=21.85009002685547
I0306 00:17:30.224906 140197633144576 logging_writer.py:48] [44] global_step=44, grad_norm=29.899085998535156, loss=22.329160690307617
I0306 00:17:31.152489 140197624751872 logging_writer.py:48] [45] global_step=45, grad_norm=30.522024154663086, loss=21.617313385009766
I0306 00:17:32.070781 140197633144576 logging_writer.py:48] [46] global_step=46, grad_norm=48.09651184082031, loss=24.012088775634766
I0306 00:17:32.985572 140197624751872 logging_writer.py:48] [47] global_step=47, grad_norm=39.359405517578125, loss=21.79850959777832
I0306 00:17:33.902112 140197633144576 logging_writer.py:48] [48] global_step=48, grad_norm=48.40169906616211, loss=22.155702590942383
I0306 00:17:34.820227 140197624751872 logging_writer.py:48] [49] global_step=49, grad_norm=55.1515007019043, loss=22.43305015563965
I0306 00:17:35.731055 140197633144576 logging_writer.py:48] [50] global_step=50, grad_norm=50.242374420166016, loss=20.251733779907227
I0306 00:17:36.639703 140197624751872 logging_writer.py:48] [51] global_step=51, grad_norm=51.26905822753906, loss=20.28009033203125
I0306 00:17:37.547074 140197633144576 logging_writer.py:48] [52] global_step=52, grad_norm=42.880104064941406, loss=18.570688247680664
I0306 00:17:38.457931 140197624751872 logging_writer.py:48] [53] global_step=53, grad_norm=65.28028106689453, loss=20.48951530456543
I0306 00:17:39.367315 140197633144576 logging_writer.py:48] [54] global_step=54, grad_norm=70.74110412597656, loss=20.56432342529297
I0306 00:17:40.275471 140197624751872 logging_writer.py:48] [55] global_step=55, grad_norm=75.46235656738281, loss=19.92597198486328
I0306 00:17:41.214712 140197633144576 logging_writer.py:48] [56] global_step=56, grad_norm=82.46778106689453, loss=19.512245178222656
I0306 00:17:42.132434 140197624751872 logging_writer.py:48] [57] global_step=57, grad_norm=79.27022552490234, loss=18.580692291259766
I0306 00:17:43.044449 140197633144576 logging_writer.py:48] [58] global_step=58, grad_norm=93.34049987792969, loss=18.939577102661133
I0306 00:17:43.954090 140197624751872 logging_writer.py:48] [59] global_step=59, grad_norm=84.43389892578125, loss=17.500696182250977
I0306 00:17:44.866552 140197633144576 logging_writer.py:48] [60] global_step=60, grad_norm=74.48936462402344, loss=16.34104347229004
I0306 00:17:45.782305 140197624751872 logging_writer.py:48] [61] global_step=61, grad_norm=73.09320831298828, loss=15.761775970458984
I0306 00:17:46.697934 140197633144576 logging_writer.py:48] [62] global_step=62, grad_norm=75.18191528320312, loss=15.449485778808594
I0306 00:17:47.609266 140197624751872 logging_writer.py:48] [63] global_step=63, grad_norm=77.14340209960938, loss=14.789715766906738
I0306 00:17:48.514153 140197633144576 logging_writer.py:48] [64] global_step=64, grad_norm=72.80628204345703, loss=14.160245895385742
I0306 00:17:49.425291 140197624751872 logging_writer.py:48] [65] global_step=65, grad_norm=60.2521858215332, loss=13.075541496276855
I0306 00:17:50.338813 140197633144576 logging_writer.py:48] [66] global_step=66, grad_norm=63.222904205322266, loss=12.663515090942383
I0306 00:17:51.245605 140197624751872 logging_writer.py:48] [67] global_step=67, grad_norm=64.92564392089844, loss=11.970026969909668
I0306 00:17:52.155431 140197633144576 logging_writer.py:48] [68] global_step=68, grad_norm=83.83110809326172, loss=11.306044578552246
I0306 00:17:53.062252 140197624751872 logging_writer.py:48] [69] global_step=69, grad_norm=89.07621765136719, loss=10.27940845489502
I0306 00:17:53.968161 140197633144576 logging_writer.py:48] [70] global_step=70, grad_norm=72.70067596435547, loss=9.326480865478516
I0306 00:17:54.877272 140197624751872 logging_writer.py:48] [71] global_step=71, grad_norm=59.49699401855469, loss=8.697126388549805
I0306 00:17:55.784043 140197633144576 logging_writer.py:48] [72] global_step=72, grad_norm=41.4983024597168, loss=8.034476280212402
I0306 00:17:56.697724 140197624751872 logging_writer.py:48] [73] global_step=73, grad_norm=27.80832290649414, loss=7.682812690734863
I0306 00:17:57.593120 140197633144576 logging_writer.py:48] [74] global_step=74, grad_norm=17.124086380004883, loss=7.436310768127441
I0306 00:17:58.489940 140197624751872 logging_writer.py:48] [75] global_step=75, grad_norm=8.344138145446777, loss=7.300528049468994
I0306 00:17:59.394593 140197633144576 logging_writer.py:48] [76] global_step=76, grad_norm=4.645171642303467, loss=7.269991874694824
I0306 00:18:00.292724 140197624751872 logging_writer.py:48] [77] global_step=77, grad_norm=7.189801216125488, loss=7.291865348815918
I0306 00:18:01.217469 140197633144576 logging_writer.py:48] [78] global_step=78, grad_norm=10.912614822387695, loss=7.3425750732421875
I0306 00:18:02.114586 140197624751872 logging_writer.py:48] [79] global_step=79, grad_norm=13.198782920837402, loss=7.376825332641602
I0306 00:18:03.010246 140197633144576 logging_writer.py:48] [80] global_step=80, grad_norm=15.579193115234375, loss=7.51837158203125
I0306 00:18:03.915642 140197624751872 logging_writer.py:48] [81] global_step=81, grad_norm=17.175588607788086, loss=7.590259552001953
I0306 00:18:04.817600 140197633144576 logging_writer.py:48] [82] global_step=82, grad_norm=18.65079116821289, loss=7.704566478729248
I0306 00:18:05.724606 140197624751872 logging_writer.py:48] [83] global_step=83, grad_norm=19.74595069885254, loss=7.829900741577148
I0306 00:18:06.620512 140197633144576 logging_writer.py:48] [84] global_step=84, grad_norm=20.31237030029297, loss=7.892021179199219
I0306 00:18:07.523180 140197624751872 logging_writer.py:48] [85] global_step=85, grad_norm=20.90896224975586, loss=8.016267776489258
I0306 00:18:08.432852 140197633144576 logging_writer.py:48] [86] global_step=86, grad_norm=21.485393524169922, loss=8.094894409179688
I0306 00:18:09.339860 140197624751872 logging_writer.py:48] [87] global_step=87, grad_norm=21.867544174194336, loss=8.171817779541016
I0306 00:18:10.248023 140197633144576 logging_writer.py:48] [88] global_step=88, grad_norm=21.9309024810791, loss=8.214167594909668
I0306 00:18:11.143625 140197624751872 logging_writer.py:48] [89] global_step=89, grad_norm=22.18886947631836, loss=8.269801139831543
I0306 00:18:12.046185 140197633144576 logging_writer.py:48] [90] global_step=90, grad_norm=22.315528869628906, loss=8.303452491760254
I0306 00:18:12.950985 140197624751872 logging_writer.py:48] [91] global_step=91, grad_norm=22.386756896972656, loss=8.336480140686035
I0306 00:18:13.887621 140197633144576 logging_writer.py:48] [92] global_step=92, grad_norm=22.518239974975586, loss=8.367853164672852
I0306 00:18:14.783266 140197624751872 logging_writer.py:48] [93] global_step=93, grad_norm=22.487445831298828, loss=8.385306358337402
I0306 00:18:15.687335 140197633144576 logging_writer.py:48] [94] global_step=94, grad_norm=22.636688232421875, loss=8.429574966430664
I0306 00:18:16.588919 140197624751872 logging_writer.py:48] [95] global_step=95, grad_norm=22.52263641357422, loss=8.393107414245605
I0306 00:18:17.504607 140197633144576 logging_writer.py:48] [96] global_step=96, grad_norm=22.467403411865234, loss=8.372322082519531
I0306 00:18:18.409643 140197624751872 logging_writer.py:48] [97] global_step=97, grad_norm=22.56238555908203, loss=8.38834285736084
I0306 00:18:19.311956 140197633144576 logging_writer.py:48] [98] global_step=98, grad_norm=22.427364349365234, loss=8.350062370300293
I0306 00:18:20.232499 140197624751872 logging_writer.py:48] [99] global_step=99, grad_norm=22.30473518371582, loss=8.31018352508545
I0306 00:18:21.143517 140197633144576 logging_writer.py:48] [100] global_step=100, grad_norm=22.25484275817871, loss=8.282084465026855
I0306 00:23:49.201974 140197624751872 logging_writer.py:48] [500] global_step=500, grad_norm=0.4483058750629425, loss=5.796970844268799
I0306 00:31:28.046741 140197633144576 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.2642224431037903, loss=5.781431674957275
I0306 00:38:10.715419 140197700286208 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.7655763030052185, loss=5.58915376663208
I0306 00:40:28.545480 140368653526848 spec.py:321] Evaluating on the training split.
I0306 00:41:05.439774 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 00:41:49.929136 140368653526848 spec.py:349] Evaluating on the test split.
I0306 00:42:12.008342 140368653526848 submission_runner.py:413] Time since start: 1780.31s, 	Step: 1653, 	{'train/ctc_loss': Array(5.8413258, dtype=float32), 'train/wer': 0.9438116619637082, 'validation/ctc_loss': Array(5.9000506, dtype=float32), 'validation/wer': 0.8963959180126863, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.8528247, dtype=float32), 'test/wer': 0.899396746084943, 'test/num_examples': 2472, 'score': 1499.9074122905731, 'total_duration': 1780.3051240444183, 'accumulated_submission_time': 1499.9074122905731, 'accumulated_eval_time': 280.29821395874023, 'accumulated_logging_time': 0.04264330863952637}
I0306 00:42:12.048359 140197700286208 logging_writer.py:48] [1653] accumulated_eval_time=280.298214, accumulated_logging_time=0.042643, accumulated_submission_time=1499.907412, global_step=1653, preemption_count=0, score=1499.907412, test/ctc_loss=5.852824687957764, test/num_examples=2472, test/wer=0.899397, total_duration=1780.305124, train/ctc_loss=5.841325759887695, train/wer=0.943812, validation/ctc_loss=5.900050640106201, validation/num_examples=5348, validation/wer=0.896396
I0306 00:46:59.531449 140197691893504 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.3868350982666016, loss=4.236348628997803
I0306 00:53:47.051858 140197700286208 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.7223957777023315, loss=3.252750873565674
I0306 01:01:32.754107 140197691893504 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.8974704742431641, loss=2.934234142303467
I0306 01:06:12.197527 140368653526848 spec.py:321] Evaluating on the training split.
I0306 01:07:00.562685 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 01:07:47.661955 140368653526848 spec.py:349] Evaluating on the test split.
I0306 01:08:12.446129 140368653526848 submission_runner.py:413] Time since start: 3340.74s, 	Step: 3342, 	{'train/ctc_loss': Array(2.6792448, dtype=float32), 'train/wer': 0.6031004448232693, 'validation/ctc_loss': Array(3.106749, dtype=float32), 'validation/wer': 0.6445929115537233, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.741641, dtype=float32), 'test/wer': 0.594580870554303, 'test/num_examples': 2472, 'score': 2939.9757294654846, 'total_duration': 3340.7420020103455, 'accumulated_submission_time': 2939.9757294654846, 'accumulated_eval_time': 400.53701853752136, 'accumulated_logging_time': 0.10327672958374023}
I0306 01:08:12.478789 140197925566208 logging_writer.py:48] [3342] accumulated_eval_time=400.537019, accumulated_logging_time=0.103277, accumulated_submission_time=2939.975729, global_step=3342, preemption_count=0, score=2939.975729, test/ctc_loss=2.741641044616699, test/num_examples=2472, test/wer=0.594581, total_duration=3340.742002, train/ctc_loss=2.6792447566986084, train/wer=0.603100, validation/ctc_loss=3.1067490577697754, validation/num_examples=5348, validation/wer=0.644593
I0306 01:10:14.355180 140197917173504 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.8867403864860535, loss=2.5879101753234863
I0306 01:17:43.469330 140197925566208 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8333742618560791, loss=2.3504459857940674
I0306 01:24:40.576879 140197236307712 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.0437690019607544, loss=2.2453222274780273
I0306 01:32:13.031892 140368653526848 spec.py:321] Evaluating on the training split.
I0306 01:33:06.157769 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 01:33:55.531519 140368653526848 spec.py:349] Evaluating on the test split.
I0306 01:34:20.055775 140368653526848 submission_runner.py:413] Time since start: 4908.36s, 	Step: 4996, 	{'train/ctc_loss': Array(0.8614964, dtype=float32), 'train/wer': 0.28743193594467376, 'validation/ctc_loss': Array(1.2200164, dtype=float32), 'validation/wer': 0.3474419996717418, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.9223739, dtype=float32), 'test/wer': 0.28805882233461294, 'test/num_examples': 2472, 'score': 4380.456327915192, 'total_duration': 4908.355667829514, 'accumulated_submission_time': 4380.456327915192, 'accumulated_eval_time': 527.5549194812775, 'accumulated_logging_time': 0.14971232414245605}
I0306 01:34:20.089295 140198140606208 logging_writer.py:48] [4996] accumulated_eval_time=527.554919, accumulated_logging_time=0.149712, accumulated_submission_time=4380.456328, global_step=4996, preemption_count=0, score=4380.456328, test/ctc_loss=0.92237389087677, test/num_examples=2472, test/wer=0.288059, total_duration=4908.355668, train/ctc_loss=0.8614963889122009, train/wer=0.287432, validation/ctc_loss=1.220016360282898, validation/num_examples=5348, validation/wer=0.347442
I0306 01:34:23.989878 140198132213504 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.7835532426834106, loss=2.1375949382781982
I0306 01:40:57.652230 140198140606208 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.9587447643280029, loss=2.0068936347961426
I0306 01:48:37.067973 140198132213504 logging_writer.py:48] [6000] global_step=6000, grad_norm=1.008760690689087, loss=1.9461373090744019
I0306 01:55:40.287236 140197812926208 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.9499858021736145, loss=1.8836932182312012
I0306 01:58:20.233573 140368653526848 spec.py:321] Evaluating on the training split.
I0306 01:59:13.832120 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 02:00:02.991858 140368653526848 spec.py:349] Evaluating on the test split.
I0306 02:00:28.170753 140368653526848 submission_runner.py:413] Time since start: 6476.47s, 	Step: 6687, 	{'train/ctc_loss': Array(0.5708487, dtype=float32), 'train/wer': 0.19616395605776582, 'validation/ctc_loss': Array(0.8928377, dtype=float32), 'validation/wer': 0.26462438572269903, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.61350936, dtype=float32), 'test/wer': 0.20236426786911219, 'test/num_examples': 2472, 'score': 5820.5239317417145, 'total_duration': 6476.470409393311, 'accumulated_submission_time': 5820.5239317417145, 'accumulated_eval_time': 655.4858725070953, 'accumulated_logging_time': 0.1988532543182373}
I0306 02:00:28.203639 140197633726208 logging_writer.py:48] [6687] accumulated_eval_time=655.485873, accumulated_logging_time=0.198853, accumulated_submission_time=5820.523932, global_step=6687, preemption_count=0, score=5820.523932, test/ctc_loss=0.6135093569755554, test/num_examples=2472, test/wer=0.202364, total_duration=6476.470409, train/ctc_loss=0.5708487033843994, train/wer=0.196164, validation/ctc_loss=0.8928377032279968, validation/num_examples=5348, validation/wer=0.264624
I0306 02:04:48.882367 140197625333504 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.8803375959396362, loss=1.7959613800048828
I0306 02:11:55.250129 140197633726208 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.7054188251495361, loss=1.8061833381652832
I0306 02:19:28.378950 140197625333504 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.772388219833374, loss=1.7711817026138306
I0306 02:24:28.850021 140368653526848 spec.py:321] Evaluating on the training split.
I0306 02:25:21.512343 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 02:26:10.392664 140368653526848 spec.py:349] Evaluating on the test split.
I0306 02:26:35.550849 140368653526848 submission_runner.py:413] Time since start: 8043.85s, 	Step: 8337, 	{'train/ctc_loss': Array(0.48657462, dtype=float32), 'train/wer': 0.170698427884929, 'validation/ctc_loss': Array(0.8043178, dtype=float32), 'validation/wer': 0.2403139693175126, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.53787524, dtype=float32), 'test/wer': 0.17654825015741474, 'test/num_examples': 2472, 'score': 7261.095873594284, 'total_duration': 8043.8543112277985, 'accumulated_submission_time': 7261.095873594284, 'accumulated_eval_time': 782.1844766139984, 'accumulated_logging_time': 0.24726557731628418}
I0306 02:26:35.578321 140197633726208 logging_writer.py:48] [8337] accumulated_eval_time=782.184477, accumulated_logging_time=0.247266, accumulated_submission_time=7261.095874, global_step=8337, preemption_count=0, score=7261.095874, test/ctc_loss=0.537875235080719, test/num_examples=2472, test/wer=0.176548, total_duration=8043.854311, train/ctc_loss=0.48657462000846863, train/wer=0.170698, validation/ctc_loss=0.8043177723884583, validation/num_examples=5348, validation/wer=0.240314
I0306 02:28:40.605476 140197625333504 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.6642415523529053, loss=1.7034595012664795
I0306 02:36:03.012635 140197633726208 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.800682544708252, loss=1.7192318439483643
I0306 02:43:20.469619 140197633726208 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.8919713497161865, loss=1.7136235237121582
I0306 02:50:35.802324 140368653526848 spec.py:321] Evaluating on the training split.
I0306 02:51:28.917546 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 02:52:18.608341 140368653526848 spec.py:349] Evaluating on the test split.
I0306 02:52:43.827789 140368653526848 submission_runner.py:413] Time since start: 9612.13s, 	Step: 9988, 	{'train/ctc_loss': Array(0.45870402, dtype=float32), 'train/wer': 0.1585854436626105, 'validation/ctc_loss': Array(0.735139, dtype=float32), 'validation/wer': 0.22115913764638867, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48113072, dtype=float32), 'test/wer': 0.15983182012065078, 'test/num_examples': 2472, 'score': 8701.249643325806, 'total_duration': 9612.125791549683, 'accumulated_submission_time': 8701.249643325806, 'accumulated_eval_time': 910.2020516395569, 'accumulated_logging_time': 0.2858603000640869}
I0306 02:52:43.863053 140197633726208 logging_writer.py:48] [9988] accumulated_eval_time=910.202052, accumulated_logging_time=0.285860, accumulated_submission_time=8701.249643, global_step=9988, preemption_count=0, score=8701.249643, test/ctc_loss=0.4811307191848755, test/num_examples=2472, test/wer=0.159832, total_duration=9612.125792, train/ctc_loss=0.45870402455329895, train/wer=0.158585, validation/ctc_loss=0.735139012336731, validation/num_examples=5348, validation/wer=0.221159
I0306 02:52:53.817738 140197625333504 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.9662894010543823, loss=1.652398705482483
I0306 02:59:46.937684 140197633726208 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.7069675326347351, loss=1.6635990142822266
I0306 03:07:10.881569 140197625333504 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.8364183306694031, loss=1.6562484502792358
I0306 03:14:41.531294 140197633726208 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.7064129710197449, loss=1.587822675704956
I0306 03:16:44.818454 140368653526848 spec.py:321] Evaluating on the training split.
I0306 03:17:37.557876 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 03:18:26.751375 140368653526848 spec.py:349] Evaluating on the test split.
I0306 03:18:51.944475 140368653526848 submission_runner.py:413] Time since start: 11180.25s, 	Step: 11653, 	{'train/ctc_loss': Array(0.39806068, dtype=float32), 'train/wer': 0.14108331738029906, 'validation/ctc_loss': Array(0.68826026, dtype=float32), 'validation/wer': 0.20677370458692568, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4464232, dtype=float32), 'test/wer': 0.15107752929945362, 'test/num_examples': 2472, 'score': 10142.129729270935, 'total_duration': 11180.247917890549, 'accumulated_submission_time': 10142.129729270935, 'accumulated_eval_time': 1037.3256344795227, 'accumulated_logging_time': 0.33499813079833984}
I0306 03:18:51.968967 140197633726208 logging_writer.py:48] [11653] accumulated_eval_time=1037.325634, accumulated_logging_time=0.334998, accumulated_submission_time=10142.129729, global_step=11653, preemption_count=0, score=10142.129729, test/ctc_loss=0.446423202753067, test/num_examples=2472, test/wer=0.151078, total_duration=11180.247918, train/ctc_loss=0.39806067943573, train/wer=0.141083, validation/ctc_loss=0.6882602572441101, validation/num_examples=5348, validation/wer=0.206774
I0306 03:23:42.145378 140197625333504 logging_writer.py:48] [12000] global_step=12000, grad_norm=1.13613760471344, loss=1.6346511840820312
I0306 03:31:14.673440 140197633726208 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.7393649220466614, loss=1.5419741868972778
I0306 03:38:31.716678 140197625333504 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.938694953918457, loss=1.6034222841262817
I0306 03:42:52.916419 140368653526848 spec.py:321] Evaluating on the training split.
I0306 03:43:46.715842 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 03:44:36.341936 140368653526848 spec.py:349] Evaluating on the test split.
I0306 03:45:01.797206 140368653526848 submission_runner.py:413] Time since start: 12750.09s, 	Step: 13278, 	{'train/ctc_loss': Array(0.3296816, dtype=float32), 'train/wer': 0.12123560154970228, 'validation/ctc_loss': Array(0.64749724, dtype=float32), 'validation/wer': 0.19373026830280854, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40584028, dtype=float32), 'test/wer': 0.1378546909593159, 'test/num_examples': 2472, 'score': 11583.005396842957, 'total_duration': 12750.09497499466, 'accumulated_submission_time': 11583.005396842957, 'accumulated_eval_time': 1166.1983239650726, 'accumulated_logging_time': 0.3727586269378662}
I0306 03:45:01.833684 140197633726208 logging_writer.py:48] [13278] accumulated_eval_time=1166.198324, accumulated_logging_time=0.372759, accumulated_submission_time=11583.005397, global_step=13278, preemption_count=0, score=11583.005397, test/ctc_loss=0.40584027767181396, test/num_examples=2472, test/wer=0.137855, total_duration=12750.094975, train/ctc_loss=0.3296816051006317, train/wer=0.121236, validation/ctc_loss=0.6474972367286682, validation/num_examples=5348, validation/wer=0.193730
I0306 03:47:57.921753 140197633726208 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.8176186084747314, loss=1.5196260213851929
I0306 03:55:16.003806 140197625333504 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.9177827835083008, loss=1.6063854694366455
I0306 04:03:04.543040 140197633726208 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.7844929099082947, loss=1.4871314764022827
I0306 04:09:02.460340 140368653526848 spec.py:321] Evaluating on the training split.
I0306 04:09:57.696080 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 04:10:47.396370 140368653526848 spec.py:349] Evaluating on the test split.
I0306 04:11:13.346953 140368653526848 submission_runner.py:413] Time since start: 14321.65s, 	Step: 14930, 	{'train/ctc_loss': Array(0.2971048, dtype=float32), 'train/wer': 0.10709744691943884, 'validation/ctc_loss': Array(0.6080465, dtype=float32), 'validation/wer': 0.1855913957731929, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38348365, dtype=float32), 'test/wer': 0.1286535453862247, 'test/num_examples': 2472, 'score': 13023.560427188873, 'total_duration': 14321.646738529205, 'accumulated_submission_time': 13023.560427188873, 'accumulated_eval_time': 1297.0788695812225, 'accumulated_logging_time': 0.42177796363830566}
I0306 04:11:13.392259 140197633726208 logging_writer.py:48] [14930] accumulated_eval_time=1297.078870, accumulated_logging_time=0.421778, accumulated_submission_time=13023.560427, global_step=14930, preemption_count=0, score=13023.560427, test/ctc_loss=0.3834836483001709, test/num_examples=2472, test/wer=0.128654, total_duration=14321.646739, train/ctc_loss=0.2971048057079315, train/wer=0.107097, validation/ctc_loss=0.6080464720726013, validation/num_examples=5348, validation/wer=0.185591
I0306 04:12:07.714300 140197625333504 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.7813771367073059, loss=1.4907996654510498
I0306 04:19:30.588920 140197633726208 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.9375016689300537, loss=1.5421255826950073
I0306 04:26:30.939864 140197625333504 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6856816411018372, loss=1.5247383117675781
I0306 04:34:21.162879 140197633726208 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.804739773273468, loss=1.5177427530288696
I0306 04:35:13.595963 140368653526848 spec.py:321] Evaluating on the training split.
I0306 04:36:07.543190 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 04:36:58.801119 140368653526848 spec.py:349] Evaluating on the test split.
I0306 04:37:24.080517 140368653526848 submission_runner.py:413] Time since start: 15892.38s, 	Step: 16570, 	{'train/ctc_loss': Array(0.29405665, dtype=float32), 'train/wer': 0.10955825963843469, 'validation/ctc_loss': Array(0.5923404, dtype=float32), 'validation/wer': 0.17953792830454637, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36786732, dtype=float32), 'test/wer': 0.12507870737107224, 'test/num_examples': 2472, 'score': 14463.678126335144, 'total_duration': 15892.383562803268, 'accumulated_submission_time': 14463.678126335144, 'accumulated_eval_time': 1427.5605869293213, 'accumulated_logging_time': 0.4926631450653076}
I0306 04:37:24.108007 140197633726208 logging_writer.py:48] [16570] accumulated_eval_time=1427.560587, accumulated_logging_time=0.492663, accumulated_submission_time=14463.678126, global_step=16570, preemption_count=0, score=14463.678126, test/ctc_loss=0.3678673207759857, test/num_examples=2472, test/wer=0.125079, total_duration=15892.383563, train/ctc_loss=0.29405665397644043, train/wer=0.109558, validation/ctc_loss=0.5923404097557068, validation/num_examples=5348, validation/wer=0.179538
I0306 04:43:11.013719 140197625333504 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.7904871106147766, loss=1.4794343709945679
I0306 04:51:06.459886 140197633726208 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.782556414604187, loss=1.4922858476638794
I0306 04:58:01.569705 140197633726208 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.8614926934242249, loss=1.4829870462417603
I0306 05:01:24.510775 140368653526848 spec.py:321] Evaluating on the training split.
I0306 05:02:18.572091 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 05:03:09.553767 140368653526848 spec.py:349] Evaluating on the test split.
I0306 05:03:34.971124 140368653526848 submission_runner.py:413] Time since start: 17463.27s, 	Step: 18219, 	{'train/ctc_loss': Array(0.2975938, dtype=float32), 'train/wer': 0.10742748354001956, 'validation/ctc_loss': Array(0.58038473, dtype=float32), 'validation/wer': 0.17330102242775905, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36038235, dtype=float32), 'test/wer': 0.12052891353360551, 'test/num_examples': 2472, 'score': 15904.00504398346, 'total_duration': 17463.268262147903, 'accumulated_submission_time': 15904.00504398346, 'accumulated_eval_time': 1558.0121684074402, 'accumulated_logging_time': 0.5351059436798096}
I0306 05:03:35.005527 140197633726208 logging_writer.py:48] [18219] accumulated_eval_time=1558.012168, accumulated_logging_time=0.535106, accumulated_submission_time=15904.005044, global_step=18219, preemption_count=0, score=15904.005044, test/ctc_loss=0.3603823482990265, test/num_examples=2472, test/wer=0.120529, total_duration=17463.268262, train/ctc_loss=0.2975938022136688, train/wer=0.107427, validation/ctc_loss=0.5803847312927246, validation/num_examples=5348, validation/wer=0.173301
I0306 05:07:29.520725 140197625333504 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.6848519444465637, loss=1.4768519401550293
I0306 05:14:34.150972 140197633726208 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.7164268493652344, loss=1.38287353515625
I0306 05:22:32.526987 140197625333504 logging_writer.py:48] [19500] global_step=19500, grad_norm=1.1491776704788208, loss=1.4600528478622437
I0306 05:27:35.275616 140368653526848 spec.py:321] Evaluating on the training split.
I0306 05:28:29.113850 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 05:29:19.905275 140368653526848 spec.py:349] Evaluating on the test split.
I0306 05:29:44.908221 140368653526848 submission_runner.py:413] Time since start: 19033.21s, 	Step: 19863, 	{'train/ctc_loss': Array(0.2913368, dtype=float32), 'train/wer': 0.10476165352045796, 'validation/ctc_loss': Array(0.5629047, dtype=float32), 'validation/wer': 0.16939088793844193, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3433145, dtype=float32), 'test/wer': 0.11601974285540187, 'test/num_examples': 2472, 'score': 17344.200548171997, 'total_duration': 19033.211532354355, 'accumulated_submission_time': 17344.200548171997, 'accumulated_eval_time': 1687.6422917842865, 'accumulated_logging_time': 0.5837585926055908}
I0306 05:29:44.934736 140197633726208 logging_writer.py:48] [19863] accumulated_eval_time=1687.642292, accumulated_logging_time=0.583759, accumulated_submission_time=17344.200548, global_step=19863, preemption_count=0, score=17344.200548, test/ctc_loss=0.3433144986629486, test/num_examples=2472, test/wer=0.116020, total_duration=19033.211532, train/ctc_loss=0.2913368046283722, train/wer=0.104762, validation/ctc_loss=0.5629047155380249, validation/num_examples=5348, validation/wer=0.169391
I0306 05:31:30.110399 140197625333504 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.858001708984375, loss=1.5256314277648926
I0306 05:39:12.628459 140197633726208 logging_writer.py:48] [20500] global_step=20500, grad_norm=1.0529613494873047, loss=1.4365332126617432
I0306 05:46:17.221517 140197633726208 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.8307763934135437, loss=1.4161885976791382
I0306 05:53:45.312894 140368653526848 spec.py:321] Evaluating on the training split.
I0306 05:54:40.368660 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 05:55:31.769035 140368653526848 spec.py:349] Evaluating on the test split.
I0306 05:55:57.696334 140368653526848 submission_runner.py:413] Time since start: 20606.00s, 	Step: 21473, 	{'train/ctc_loss': Array(0.27229375, dtype=float32), 'train/wer': 0.10090029838460747, 'validation/ctc_loss': Array(0.54559654, dtype=float32), 'validation/wer': 0.16386842638809773, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33318508, dtype=float32), 'test/wer': 0.11402920805151016, 'test/num_examples': 2472, 'score': 18784.50802588463, 'total_duration': 20605.99622106552, 'accumulated_submission_time': 18784.50802588463, 'accumulated_eval_time': 1820.0197281837463, 'accumulated_logging_time': 0.6220605373382568}
I0306 05:55:57.735111 140197633726208 logging_writer.py:48] [21473] accumulated_eval_time=1820.019728, accumulated_logging_time=0.622061, accumulated_submission_time=18784.508026, global_step=21473, preemption_count=0, score=18784.508026, test/ctc_loss=0.333185076713562, test/num_examples=2472, test/wer=0.114029, total_duration=20605.996221, train/ctc_loss=0.27229374647140503, train/wer=0.100900, validation/ctc_loss=0.5455965399742126, validation/num_examples=5348, validation/wer=0.163868
I0306 05:56:19.366618 140197625333504 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.8662906289100647, loss=1.459130048751831
I0306 06:03:07.194547 140197633726208 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.8636831045150757, loss=1.4665007591247559
I0306 06:11:00.207359 140197625333504 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.8515315651893616, loss=1.4775564670562744
I0306 06:18:10.910892 140197633726208 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.8373427391052246, loss=1.3401634693145752
I0306 06:19:57.744064 140368653526848 spec.py:321] Evaluating on the training split.
I0306 06:20:51.973443 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 06:21:42.969140 140368653526848 spec.py:349] Evaluating on the test split.
I0306 06:22:08.669866 140368653526848 submission_runner.py:413] Time since start: 22176.97s, 	Step: 23122, 	{'train/ctc_loss': Array(0.24628168, dtype=float32), 'train/wer': 0.08880537139478907, 'validation/ctc_loss': Array(0.525093, dtype=float32), 'validation/wer': 0.156849493613447, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32100928, dtype=float32), 'test/wer': 0.10610769199520646, 'test/num_examples': 2472, 'score': 20224.44011592865, 'total_duration': 22176.966822862625, 'accumulated_submission_time': 20224.44011592865, 'accumulated_eval_time': 1950.936583995819, 'accumulated_logging_time': 0.6775879859924316}
I0306 06:22:08.702483 140197633726208 logging_writer.py:48] [23122] accumulated_eval_time=1950.936584, accumulated_logging_time=0.677588, accumulated_submission_time=20224.440116, global_step=23122, preemption_count=0, score=20224.440116, test/ctc_loss=0.3210092782974243, test/num_examples=2472, test/wer=0.106108, total_duration=22176.966823, train/ctc_loss=0.2462816834449768, train/wer=0.088805, validation/ctc_loss=0.5250930190086365, validation/num_examples=5348, validation/wer=0.156849
I0306 06:27:36.067848 140197625333504 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.8006777167320251, loss=1.3542619943618774
I0306 06:34:50.997080 140197633726208 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.7617939710617065, loss=1.3762035369873047
I0306 06:42:38.925603 140197625333504 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.7613263726234436, loss=1.3973240852355957
I0306 06:46:12.731429 140368653526848 spec.py:321] Evaluating on the training split.
I0306 06:47:07.168837 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 06:47:57.466511 140368653526848 spec.py:349] Evaluating on the test split.
I0306 06:48:22.628729 140368653526848 submission_runner.py:413] Time since start: 23750.93s, 	Step: 24721, 	{'train/ctc_loss': Array(0.23662172, dtype=float32), 'train/wer': 0.08667936044236939, 'validation/ctc_loss': Array(0.5193285, dtype=float32), 'validation/wer': 0.1571970611236085, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3122066, dtype=float32), 'test/wer': 0.10537647512847075, 'test/num_examples': 2472, 'score': 21668.39456653595, 'total_duration': 23750.926522254944, 'accumulated_submission_time': 21668.39456653595, 'accumulated_eval_time': 2080.825926065445, 'accumulated_logging_time': 0.7248210906982422}
I0306 06:48:22.664203 140197633726208 logging_writer.py:48] [24721] accumulated_eval_time=2080.825926, accumulated_logging_time=0.724821, accumulated_submission_time=21668.394567, global_step=24721, preemption_count=0, score=21668.394567, test/ctc_loss=0.31220659613609314, test/num_examples=2472, test/wer=0.105376, total_duration=23750.926522, train/ctc_loss=0.23662172257900238, train/wer=0.086679, validation/ctc_loss=0.5193284749984741, validation/num_examples=5348, validation/wer=0.157197
I0306 06:51:56.023904 140197625333504 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.8265722990036011, loss=1.36077082157135
I0306 06:59:28.199362 140197633726208 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.8884899616241455, loss=1.3785313367843628
I0306 07:06:49.842517 140197633726208 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.7433770298957825, loss=1.3931645154953003
I0306 07:12:23.274800 140368653526848 spec.py:321] Evaluating on the training split.
I0306 07:13:18.281064 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 07:14:09.196526 140368653526848 spec.py:349] Evaluating on the test split.
I0306 07:14:34.512666 140368653526848 submission_runner.py:413] Time since start: 25322.81s, 	Step: 26370, 	{'train/ctc_loss': Array(0.23301694, dtype=float32), 'train/wer': 0.08400241380368412, 'validation/ctc_loss': Array(0.5139927, dtype=float32), 'validation/wer': 0.15359587553221274, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30361816, dtype=float32), 'test/wer': 0.10165945605589746, 'test/num_examples': 2472, 'score': 23108.929773569107, 'total_duration': 25322.810742616653, 'accumulated_submission_time': 23108.929773569107, 'accumulated_eval_time': 2212.0559780597687, 'accumulated_logging_time': 0.7746150493621826}
I0306 07:14:34.553615 140197633726208 logging_writer.py:48] [26370] accumulated_eval_time=2212.055978, accumulated_logging_time=0.774615, accumulated_submission_time=23108.929774, global_step=26370, preemption_count=0, score=23108.929774, test/ctc_loss=0.3036181628704071, test/num_examples=2472, test/wer=0.101659, total_duration=25322.810743, train/ctc_loss=0.2330169379711151, train/wer=0.084002, validation/ctc_loss=0.5139927268028259, validation/num_examples=5348, validation/wer=0.153596
I0306 07:16:14.545392 140197625333504 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.7416338324546814, loss=1.4829431772232056
I0306 07:23:28.685905 140197633726208 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.7104581594467163, loss=1.3238662481307983
I0306 07:31:06.973053 140197625333504 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.7078086137771606, loss=1.3187109231948853
I0306 07:38:35.303233 140368653526848 spec.py:321] Evaluating on the training split.
I0306 07:39:28.698244 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 07:40:19.960480 140368653526848 spec.py:349] Evaluating on the test split.
I0306 07:40:45.388141 140368653526848 submission_runner.py:413] Time since start: 26893.69s, 	Step: 27995, 	{'train/ctc_loss': Array(0.23725732, dtype=float32), 'train/wer': 0.08590093603744149, 'validation/ctc_loss': Array(0.49488494, dtype=float32), 'validation/wer': 0.14768722785946686, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2944605, dtype=float32), 'test/wer': 0.10017671074279447, 'test/num_examples': 2472, 'score': 24549.60434961319, 'total_duration': 26893.68610572815, 'accumulated_submission_time': 24549.60434961319, 'accumulated_eval_time': 2342.133118391037, 'accumulated_logging_time': 0.8296201229095459}
I0306 07:40:45.423112 140197633726208 logging_writer.py:48] [27995] accumulated_eval_time=2342.133118, accumulated_logging_time=0.829620, accumulated_submission_time=24549.604350, global_step=27995, preemption_count=0, score=24549.604350, test/ctc_loss=0.2944605052471161, test/num_examples=2472, test/wer=0.100177, total_duration=26893.686106, train/ctc_loss=0.23725731670856476, train/wer=0.085901, validation/ctc_loss=0.4948849380016327, validation/num_examples=5348, validation/wer=0.147687
I0306 07:40:50.120954 140197625333504 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.7749430537223816, loss=1.3014112710952759
I0306 07:48:00.995076 140197633726208 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.8751118779182434, loss=1.3786451816558838
I0306 07:55:37.920483 140197633726208 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.8996725678443909, loss=1.2432746887207031
I0306 08:03:06.004470 140197625333504 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.8488414883613586, loss=1.3331300020217896
I0306 08:04:45.908465 140368653526848 spec.py:321] Evaluating on the training split.
I0306 08:05:39.563902 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 08:06:30.960862 140368653526848 spec.py:349] Evaluating on the test split.
I0306 08:06:56.750384 140368653526848 submission_runner.py:413] Time since start: 28465.05s, 	Step: 29605, 	{'train/ctc_loss': Array(0.22392258, dtype=float32), 'train/wer': 0.08040195551793552, 'validation/ctc_loss': Array(0.48500586, dtype=float32), 'validation/wer': 0.1463259217780009, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28525266, dtype=float32), 'test/wer': 0.09690654642211524, 'test/num_examples': 2472, 'score': 25990.009491443634, 'total_duration': 28465.048188209534, 'accumulated_submission_time': 25990.009491443634, 'accumulated_eval_time': 2472.966958999634, 'accumulated_logging_time': 0.8828892707824707}
I0306 08:06:56.782346 140197633726208 logging_writer.py:48] [29605] accumulated_eval_time=2472.966959, accumulated_logging_time=0.882889, accumulated_submission_time=25990.009491, global_step=29605, preemption_count=0, score=25990.009491, test/ctc_loss=0.2852526605129242, test/num_examples=2472, test/wer=0.096907, total_duration=28465.048188, train/ctc_loss=0.22392258048057556, train/wer=0.080402, validation/ctc_loss=0.48500585556030273, validation/num_examples=5348, validation/wer=0.146326
I0306 08:12:20.756540 140197633726208 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.9102540016174316, loss=1.3272037506103516
I0306 08:19:47.005031 140197625333504 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.906282365322113, loss=1.3150978088378906
I0306 08:27:32.772740 140197633726208 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.786133348941803, loss=1.3310421705245972
I0306 08:30:56.744281 140368653526848 spec.py:321] Evaluating on the training split.
I0306 08:31:51.365151 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 08:32:42.519718 140368653526848 spec.py:349] Evaluating on the test split.
I0306 08:33:08.635572 140368653526848 submission_runner.py:413] Time since start: 30036.94s, 	Step: 31248, 	{'train/ctc_loss': Array(0.22642238, dtype=float32), 'train/wer': 0.08093224909975394, 'validation/ctc_loss': Array(0.4764266, dtype=float32), 'validation/wer': 0.14229027679890324, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2792893, dtype=float32), 'test/wer': 0.09245831048280624, 'test/num_examples': 2472, 'score': 27429.89633321762, 'total_duration': 30036.93883419037, 'accumulated_submission_time': 27429.89633321762, 'accumulated_eval_time': 2604.855622768402, 'accumulated_logging_time': 0.9278955459594727}
I0306 08:33:08.665369 140197633726208 logging_writer.py:48] [31248] accumulated_eval_time=2604.855623, accumulated_logging_time=0.927896, accumulated_submission_time=27429.896333, global_step=31248, preemption_count=0, score=27429.896333, test/ctc_loss=0.2792893052101135, test/num_examples=2472, test/wer=0.092458, total_duration=30036.938834, train/ctc_loss=0.22642238438129425, train/wer=0.080932, validation/ctc_loss=0.4764266014099121, validation/num_examples=5348, validation/wer=0.142290
I0306 08:36:35.835441 140197625333504 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.0454347133636475, loss=1.3057454824447632
I0306 08:44:26.861875 140197633726208 logging_writer.py:48] [32000] global_step=32000, grad_norm=1.0242162942886353, loss=1.2786914110183716
I0306 08:51:35.395027 140197625333504 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.850385308265686, loss=1.2927082777023315
I0306 08:57:09.216608 140368653526848 spec.py:321] Evaluating on the training split.
I0306 08:58:03.194114 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 08:58:53.541671 140368653526848 spec.py:349] Evaluating on the test split.
I0306 08:59:19.029523 140368653526848 submission_runner.py:413] Time since start: 31607.33s, 	Step: 32848, 	{'train/ctc_loss': Array(0.21477792, dtype=float32), 'train/wer': 0.07501305455204237, 'validation/ctc_loss': Array(0.46601358, dtype=float32), 'validation/wer': 0.13709607345260047, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27015576, dtype=float32), 'test/wer': 0.09091463043080861, 'test/num_examples': 2472, 'score': 28870.375794649124, 'total_duration': 31607.326843976974, 'accumulated_submission_time': 28870.375794649124, 'accumulated_eval_time': 2734.659970998764, 'accumulated_logging_time': 0.9689691066741943}
I0306 08:59:19.065971 140197633726208 logging_writer.py:48] [32848] accumulated_eval_time=2734.659971, accumulated_logging_time=0.968969, accumulated_submission_time=28870.375795, global_step=32848, preemption_count=0, score=28870.375795, test/ctc_loss=0.27015575766563416, test/num_examples=2472, test/wer=0.090915, total_duration=31607.326844, train/ctc_loss=0.21477791666984558, train/wer=0.075013, validation/ctc_loss=0.4660135805606842, validation/num_examples=5348, validation/wer=0.137096
I0306 09:01:20.709900 140197633726208 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.8177382946014404, loss=1.2270188331604004
I0306 09:08:34.118036 140197625333504 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.0285900831222534, loss=1.275200605392456
I0306 09:16:32.892179 140197633726208 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.8657715916633606, loss=1.2122814655303955
I0306 09:23:19.800759 140368653526848 spec.py:321] Evaluating on the training split.
I0306 09:24:14.617423 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 09:25:05.263396 140368653526848 spec.py:349] Evaluating on the test split.
I0306 09:25:30.768294 140368653526848 submission_runner.py:413] Time since start: 33179.07s, 	Step: 34483, 	{'train/ctc_loss': Array(0.1691217, dtype=float32), 'train/wer': 0.0641124196672919, 'validation/ctc_loss': Array(0.45554063, dtype=float32), 'validation/wer': 0.13636231982003727, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2657611, dtype=float32), 'test/wer': 0.08985842828996811, 'test/num_examples': 2472, 'score': 30311.0317735672, 'total_duration': 33179.065029382706, 'accumulated_submission_time': 30311.0317735672, 'accumulated_eval_time': 2865.6183681488037, 'accumulated_logging_time': 1.0220990180969238}
I0306 09:25:30.804158 140197633726208 logging_writer.py:48] [34483] accumulated_eval_time=2865.618368, accumulated_logging_time=1.022099, accumulated_submission_time=30311.031774, global_step=34483, preemption_count=0, score=30311.031774, test/ctc_loss=0.2657611072063446, test/num_examples=2472, test/wer=0.089858, total_duration=33179.065029, train/ctc_loss=0.16912169754505157, train/wer=0.064112, validation/ctc_loss=0.45554062724113464, validation/num_examples=5348, validation/wer=0.136362
I0306 09:25:44.872724 140197625333504 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.7904938459396362, loss=1.2351109981536865
I0306 09:33:12.516437 140197633726208 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.9191157817840576, loss=1.2544212341308594
I0306 09:40:17.425460 140197633726208 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.8662474155426025, loss=1.20461106300354
I0306 09:48:14.005425 140197625333504 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.9374988675117493, loss=1.223697543144226
I0306 09:49:31.109109 140368653526848 spec.py:321] Evaluating on the training split.
I0306 09:50:24.566743 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 09:51:15.226364 140368653526848 spec.py:349] Evaluating on the test split.
I0306 09:51:40.860406 140368653526848 submission_runner.py:413] Time since start: 34749.16s, 	Step: 36086, 	{'train/ctc_loss': Array(0.18087982, dtype=float32), 'train/wer': 0.06586978099723342, 'validation/ctc_loss': Array(0.43970668, dtype=float32), 'validation/wer': 0.12929511378008632, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24949893, dtype=float32), 'test/wer': 0.08327747648934658, 'test/num_examples': 2472, 'score': 31751.257553577423, 'total_duration': 34749.1638610363, 'accumulated_submission_time': 31751.257553577423, 'accumulated_eval_time': 2995.367404460907, 'accumulated_logging_time': 1.0751361846923828}
I0306 09:51:40.887098 140197633726208 logging_writer.py:48] [36086] accumulated_eval_time=2995.367404, accumulated_logging_time=1.075136, accumulated_submission_time=31751.257554, global_step=36086, preemption_count=0, score=31751.257554, test/ctc_loss=0.24949893355369568, test/num_examples=2472, test/wer=0.083277, total_duration=34749.163861, train/ctc_loss=0.18087981641292572, train/wer=0.065870, validation/ctc_loss=0.4397066831588745, validation/num_examples=5348, validation/wer=0.129295
I0306 09:57:10.323359 140197625333504 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.8510245084762573, loss=1.2747926712036133
I0306 10:05:10.816002 140197633726208 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.7753262519836426, loss=1.2405669689178467
I0306 10:12:13.315346 140197633726208 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.9181503057479858, loss=1.2248142957687378
I0306 10:15:42.161581 140368653526848 spec.py:321] Evaluating on the training split.
I0306 10:16:36.065745 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 10:17:26.436630 140368653526848 spec.py:349] Evaluating on the test split.
I0306 10:17:52.197234 140368653526848 submission_runner.py:413] Time since start: 36320.49s, 	Step: 37722, 	{'train/ctc_loss': Array(0.21984127, dtype=float32), 'train/wer': 0.07977739568134604, 'validation/ctc_loss': Array(0.43781972, dtype=float32), 'validation/wer': 0.13055987333095184, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24441521, dtype=float32), 'test/wer': 0.08287124489671562, 'test/num_examples': 2472, 'score': 33192.45899128914, 'total_duration': 36320.49487328529, 'accumulated_submission_time': 33192.45899128914, 'accumulated_eval_time': 3125.3948628902435, 'accumulated_logging_time': 1.113063097000122}
I0306 10:17:52.236290 140197633726208 logging_writer.py:48] [37722] accumulated_eval_time=3125.394863, accumulated_logging_time=1.113063, accumulated_submission_time=33192.458991, global_step=37722, preemption_count=0, score=33192.458991, test/ctc_loss=0.24441520869731903, test/num_examples=2472, test/wer=0.082871, total_duration=36320.494873, train/ctc_loss=0.21984127163887024, train/wer=0.079777, validation/ctc_loss=0.4378197193145752, validation/num_examples=5348, validation/wer=0.130560
I0306 10:21:44.032217 140197625333504 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.9062004685401917, loss=1.219929575920105
I0306 10:28:56.206722 140197633726208 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.908507227897644, loss=1.1995548009872437
I0306 10:36:47.838318 140197625333504 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.7955454587936401, loss=1.1905781030654907
I0306 10:41:52.606459 140368653526848 spec.py:321] Evaluating on the training split.
I0306 10:42:45.029813 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 10:43:36.790261 140368653526848 spec.py:349] Evaluating on the test split.
I0306 10:44:03.287802 140368653526848 submission_runner.py:413] Time since start: 37891.59s, 	Step: 39351, 	{'train/ctc_loss': Array(0.21354522, dtype=float32), 'train/wer': 0.0769969523196234, 'validation/ctc_loss': Array(0.42240757, dtype=float32), 'validation/wer': 0.12242100080133621, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23700641, dtype=float32), 'test/wer': 0.07819958158145959, 'test/num_examples': 2472, 'score': 34632.75153207779, 'total_duration': 37891.58566737175, 'accumulated_submission_time': 34632.75153207779, 'accumulated_eval_time': 3256.068284034729, 'accumulated_logging_time': 1.1671512126922607}
I0306 10:44:03.325170 140197633726208 logging_writer.py:48] [39351] accumulated_eval_time=3256.068284, accumulated_logging_time=1.167151, accumulated_submission_time=34632.751532, global_step=39351, preemption_count=0, score=34632.751532, test/ctc_loss=0.23700641095638275, test/num_examples=2472, test/wer=0.078200, total_duration=37891.585667, train/ctc_loss=0.21354521811008453, train/wer=0.076997, validation/ctc_loss=0.4224075675010681, validation/num_examples=5348, validation/wer=0.122421
I0306 10:45:58.165271 140197625333504 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.9235931634902954, loss=1.2180134057998657
I0306 10:53:37.290014 140197633726208 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.9805817008018494, loss=1.1877784729003906
I0306 11:00:48.744570 140197633726208 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.797613263130188, loss=1.1838254928588867
I0306 11:08:03.474479 140368653526848 spec.py:321] Evaluating on the training split.
I0306 11:08:56.608142 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 11:09:46.981858 140368653526848 spec.py:349] Evaluating on the test split.
I0306 11:10:13.268669 140368653526848 submission_runner.py:413] Time since start: 39461.57s, 	Step: 40964, 	{'train/ctc_loss': Array(0.23763691, dtype=float32), 'train/wer': 0.08760989991284052, 'validation/ctc_loss': Array(0.40826672, dtype=float32), 'validation/wer': 0.1209341842300897, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23022155, dtype=float32), 'test/wer': 0.07596530782198932, 'test/num_examples': 2472, 'score': 36072.824429273605, 'total_duration': 39461.56752991676, 'accumulated_submission_time': 36072.824429273605, 'accumulated_eval_time': 3385.8554463386536, 'accumulated_logging_time': 1.2193019390106201}
I0306 11:10:13.314135 140197633726208 logging_writer.py:48] [40964] accumulated_eval_time=3385.855446, accumulated_logging_time=1.219302, accumulated_submission_time=36072.824429, global_step=40964, preemption_count=0, score=36072.824429, test/ctc_loss=0.23022155463695526, test/num_examples=2472, test/wer=0.075965, total_duration=39461.567530, train/ctc_loss=0.23763690888881683, train/wer=0.087610, validation/ctc_loss=0.40826672315597534, validation/num_examples=5348, validation/wer=0.120934
I0306 11:10:41.565402 140197625333504 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.0654293298721313, loss=1.205310344696045
I0306 11:17:32.441555 140197633726208 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.8620880842208862, loss=1.1942365169525146
I0306 11:25:21.606609 140197625333504 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.9748373031616211, loss=1.1478337049484253
I0306 11:32:38.214589 140197633726208 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.0760430097579956, loss=1.1034091711044312
I0306 11:34:13.671320 140368653526848 spec.py:321] Evaluating on the training split.
I0306 11:35:04.819319 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 11:35:55.545332 140368653526848 spec.py:349] Evaluating on the test split.
I0306 11:36:21.105595 140368653526848 submission_runner.py:413] Time since start: 41029.40s, 	Step: 42613, 	{'train/ctc_loss': Array(0.20402001, dtype=float32), 'train/wer': 0.07291182167690125, 'validation/ctc_loss': Array(0.3942063, dtype=float32), 'validation/wer': 0.11646407986329012, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22189872, dtype=float32), 'test/wer': 0.07383259196067678, 'test/num_examples': 2472, 'score': 37513.10461354256, 'total_duration': 41029.40463256836, 'accumulated_submission_time': 37513.10461354256, 'accumulated_eval_time': 3513.2828528881073, 'accumulated_logging_time': 1.2802455425262451}
I0306 11:36:21.140999 140197633726208 logging_writer.py:48] [42613] accumulated_eval_time=3513.282853, accumulated_logging_time=1.280246, accumulated_submission_time=37513.104614, global_step=42613, preemption_count=0, score=37513.104614, test/ctc_loss=0.22189871966838837, test/num_examples=2472, test/wer=0.073833, total_duration=41029.404633, train/ctc_loss=0.20402000844478607, train/wer=0.072912, validation/ctc_loss=0.39420628547668457, validation/num_examples=5348, validation/wer=0.116464
I0306 11:41:59.506032 140197625333504 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.0130654573440552, loss=1.117771029472351
I0306 11:49:20.665990 140197633726208 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.174303650856018, loss=1.1000863313674927
I0306 11:57:01.635634 140197625333504 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.9856799244880676, loss=1.1243921518325806
I0306 12:00:21.469492 140368653526848 spec.py:321] Evaluating on the training split.
I0306 12:01:14.437181 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 12:02:05.331139 140368653526848 spec.py:349] Evaluating on the test split.
I0306 12:02:31.577996 140368653526848 submission_runner.py:413] Time since start: 42599.88s, 	Step: 44208, 	{'train/ctc_loss': Array(0.18054171, dtype=float32), 'train/wer': 0.06762940776947658, 'validation/ctc_loss': Array(0.3837639, dtype=float32), 'validation/wer': 0.11307529663921527, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21255401, dtype=float32), 'test/wer': 0.06991245709178803, 'test/num_examples': 2472, 'score': 38953.35588002205, 'total_duration': 42599.876101493835, 'accumulated_submission_time': 38953.35588002205, 'accumulated_eval_time': 3643.3835871219635, 'accumulated_logging_time': 1.3324365615844727}
I0306 12:02:31.616978 140197633726208 logging_writer.py:48] [44208] accumulated_eval_time=3643.383587, accumulated_logging_time=1.332437, accumulated_submission_time=38953.355880, global_step=44208, preemption_count=0, score=38953.355880, test/ctc_loss=0.21255400776863098, test/num_examples=2472, test/wer=0.069912, total_duration=42599.876101, train/ctc_loss=0.18054170906543732, train/wer=0.067629, validation/ctc_loss=0.3837639093399048, validation/num_examples=5348, validation/wer=0.113075
I0306 12:06:27.836208 140197633726208 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.9354760050773621, loss=1.1535977125167847
I0306 12:14:10.642879 140197625333504 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.1165342330932617, loss=1.124974012374878
I0306 12:21:45.359921 140197633726208 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.9704746007919312, loss=1.0493935346603394
I0306 12:26:31.982096 140368653526848 spec.py:321] Evaluating on the training split.
I0306 12:27:25.350769 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 12:28:15.657579 140368653526848 spec.py:349] Evaluating on the test split.
I0306 12:28:41.468141 140368653526848 submission_runner.py:413] Time since start: 44169.77s, 	Step: 45827, 	{'train/ctc_loss': Array(0.1470921, dtype=float32), 'train/wer': 0.05540571403712928, 'validation/ctc_loss': Array(0.37341017, dtype=float32), 'validation/wer': 0.10961893084372012, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20728362, dtype=float32), 'test/wer': 0.0687953202120529, 'test/num_examples': 2472, 'score': 40393.64510297775, 'total_duration': 44169.76618552208, 'accumulated_submission_time': 40393.64510297775, 'accumulated_eval_time': 3772.8617856502533, 'accumulated_logging_time': 1.3857619762420654}
I0306 12:28:41.518617 140197633726208 logging_writer.py:48] [45827] accumulated_eval_time=3772.861786, accumulated_logging_time=1.385762, accumulated_submission_time=40393.645103, global_step=45827, preemption_count=0, score=40393.645103, test/ctc_loss=0.207283616065979, test/num_examples=2472, test/wer=0.068795, total_duration=44169.766186, train/ctc_loss=0.14709210395812988, train/wer=0.055406, validation/ctc_loss=0.373410165309906, validation/num_examples=5348, validation/wer=0.109619
I0306 12:30:54.616146 140197625333504 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.0038437843322754, loss=1.127238154411316
I0306 12:38:30.000239 140197633726208 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.0337436199188232, loss=1.0188188552856445
I0306 12:46:01.987553 140197625333504 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.9221831560134888, loss=1.1190354824066162
I0306 12:52:41.515818 140368653526848 spec.py:321] Evaluating on the training split.
I0306 12:53:33.840319 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 12:54:22.550000 140368653526848 spec.py:349] Evaluating on the test split.
I0306 12:54:47.340155 140368653526848 submission_runner.py:413] Time since start: 45735.64s, 	Step: 47421, 	{'train/ctc_loss': Array(0.1583361, dtype=float32), 'train/wer': 0.05976309829628463, 'validation/ctc_loss': Array(0.371881, dtype=float32), 'validation/wer': 0.10798729447657299, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20136462, dtype=float32), 'test/wer': 0.06589076432474153, 'test/num_examples': 2472, 'score': 41833.555126428604, 'total_duration': 45735.643411397934, 'accumulated_submission_time': 41833.555126428604, 'accumulated_eval_time': 3898.6836307048798, 'accumulated_logging_time': 1.4615850448608398}
I0306 12:54:47.372142 140197633726208 logging_writer.py:48] [47421] accumulated_eval_time=3898.683631, accumulated_logging_time=1.461585, accumulated_submission_time=41833.555126, global_step=47421, preemption_count=0, score=41833.555126, test/ctc_loss=0.20136462152004242, test/num_examples=2472, test/wer=0.065891, total_duration=45735.643411, train/ctc_loss=0.1583361029624939, train/wer=0.059763, validation/ctc_loss=0.37188100814819336, validation/num_examples=5348, validation/wer=0.107987
I0306 12:55:48.532233 140197625333504 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.1919903755187988, loss=1.0059131383895874
I0306 13:02:22.541170 140197633726208 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.0863471031188965, loss=1.0234076976776123
I0306 13:09:43.503662 140197633726208 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.9408738017082214, loss=1.0189107656478882
I0306 13:16:26.100347 140197625333504 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.485582709312439, loss=1.0967336893081665
I0306 13:18:48.257094 140368653526848 spec.py:321] Evaluating on the training split.
I0306 13:19:42.117604 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 13:20:31.104151 140368653526848 spec.py:349] Evaluating on the test split.
I0306 13:20:55.934647 140368653526848 submission_runner.py:413] Time since start: 47304.23s, 	Step: 49159, 	{'train/ctc_loss': Array(0.13405813, dtype=float32), 'train/wer': 0.05007730630475863, 'validation/ctc_loss': Array(0.3562352, dtype=float32), 'validation/wer': 0.1024648329262288, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19233635, dtype=float32), 'test/wer': 0.06221436841143136, 'test/num_examples': 2472, 'score': 43274.36798667908, 'total_duration': 47304.23278069496, 'accumulated_submission_time': 43274.36798667908, 'accumulated_eval_time': 4026.3534049987793, 'accumulated_logging_time': 1.509256362915039}
I0306 13:20:55.979393 140197633726208 logging_writer.py:48] [49159] accumulated_eval_time=4026.353405, accumulated_logging_time=1.509256, accumulated_submission_time=43274.367987, global_step=49159, preemption_count=0, score=43274.367987, test/ctc_loss=0.19233635067939758, test/num_examples=2472, test/wer=0.062214, total_duration=47304.232781, train/ctc_loss=0.1340581327676773, train/wer=0.050077, validation/ctc_loss=0.35623520612716675, validation/num_examples=5348, validation/wer=0.102465
I0306 13:25:25.938003 140197633726208 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.1270701885223389, loss=1.0520901679992676
I0306 13:32:05.629740 140197625333504 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.1818560361862183, loss=1.0324496030807495
I0306 13:39:35.531620 140197633726208 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.227312445640564, loss=1.0347217321395874
I0306 13:44:56.328099 140368653526848 spec.py:321] Evaluating on the training split.
I0306 13:45:47.539982 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 13:46:35.889382 140368653526848 spec.py:349] Evaluating on the test split.
I0306 13:47:00.291245 140368653526848 submission_runner.py:413] Time since start: 48868.59s, 	Step: 50921, 	{'train/ctc_loss': Array(0.12810867, dtype=float32), 'train/wer': 0.04891569663372352, 'validation/ctc_loss': Array(0.3469465, dtype=float32), 'validation/wer': 0.09910501366133408, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18913877, dtype=float32), 'test/wer': 0.06018321044827656, 'test/num_examples': 2472, 'score': 44714.6427052021, 'total_duration': 48868.591153383255, 'accumulated_submission_time': 44714.6427052021, 'accumulated_eval_time': 4150.310555458069, 'accumulated_logging_time': 1.569610357284546}
I0306 13:47:00.331930 140197633726208 logging_writer.py:48] [50921] accumulated_eval_time=4150.310555, accumulated_logging_time=1.569610, accumulated_submission_time=44714.642705, global_step=50921, preemption_count=0, score=44714.642705, test/ctc_loss=0.1891387701034546, test/num_examples=2472, test/wer=0.060183, total_duration=48868.591153, train/ctc_loss=0.1281086653470993, train/wer=0.048916, validation/ctc_loss=0.34694650769233704, validation/num_examples=5348, validation/wer=0.099105
I0306 13:48:01.407151 140197625333504 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.1262669563293457, loss=0.9989595413208008
I0306 13:54:45.723186 140197633726208 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.2294520139694214, loss=1.0282533168792725
I0306 14:01:07.676968 140197625333504 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.2580190896987915, loss=1.0283331871032715
I0306 14:08:11.578708 140197633726208 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.240034580230713, loss=1.02621591091156
I0306 14:11:00.871176 140368653526848 spec.py:321] Evaluating on the training split.
I0306 14:11:51.363663 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 14:12:39.264150 140368653526848 spec.py:349] Evaluating on the test split.
I0306 14:13:03.235288 140368653526848 submission_runner.py:413] Time since start: 50431.54s, 	Step: 52716, 	{'train/ctc_loss': Array(0.1176779, dtype=float32), 'train/wer': 0.0445879917184265, 'validation/ctc_loss': Array(0.34046492, dtype=float32), 'validation/wer': 0.09754095986560724, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18197228, dtype=float32), 'test/wer': 0.05878171145369975, 'test/num_examples': 2472, 'score': 46155.10847115517, 'total_duration': 50431.535818099976, 'accumulated_submission_time': 46155.10847115517, 'accumulated_eval_time': 4272.6694712638855, 'accumulated_logging_time': 1.626260757446289}
I0306 14:13:03.269846 140197633726208 logging_writer.py:48] [52716] accumulated_eval_time=4272.669471, accumulated_logging_time=1.626261, accumulated_submission_time=46155.108471, global_step=52716, preemption_count=0, score=46155.108471, test/ctc_loss=0.18197228014469147, test/num_examples=2472, test/wer=0.058782, total_duration=50431.535818, train/ctc_loss=0.11767789721488953, train/wer=0.044588, validation/ctc_loss=0.34046491980552673, validation/num_examples=5348, validation/wer=0.097541
I0306 14:16:40.769014 140197625333504 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.070088267326355, loss=0.994559645652771
I0306 14:23:33.431408 140197633726208 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.0027811527252197, loss=0.980419397354126
I0306 14:30:02.571324 140197633726208 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.2469849586486816, loss=0.9980049133300781
I0306 14:37:03.493398 140368653526848 spec.py:321] Evaluating on the training split.
I0306 14:37:53.315165 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 14:38:41.548342 140368653526848 spec.py:349] Evaluating on the test split.
I0306 14:39:06.035606 140368653526848 submission_runner.py:413] Time since start: 51994.34s, 	Step: 54500, 	{'train/ctc_loss': Array(0.11792959, dtype=float32), 'train/wer': 0.044051829721964676, 'validation/ctc_loss': Array(0.3320718, dtype=float32), 'validation/wer': 0.09512729660059666, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17826898, dtype=float32), 'test/wer': 0.05713647350354437, 'test/num_examples': 2472, 'score': 47595.26068067551, 'total_duration': 51994.33613705635, 'accumulated_submission_time': 47595.26068067551, 'accumulated_eval_time': 4395.206291437149, 'accumulated_logging_time': 1.6741979122161865}
I0306 14:39:06.072584 140197633726208 logging_writer.py:48] [54500] accumulated_eval_time=4395.206291, accumulated_logging_time=1.674198, accumulated_submission_time=47595.260681, global_step=54500, preemption_count=0, score=47595.260681, test/ctc_loss=0.17826898396015167, test/num_examples=2472, test/wer=0.057136, total_duration=51994.336137, train/ctc_loss=0.11792958527803421, train/wer=0.044052, validation/ctc_loss=0.33207181096076965, validation/num_examples=5348, validation/wer=0.095127
I0306 14:39:06.940082 140197625333504 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.2746045589447021, loss=0.9849367141723633
I0306 14:45:31.812429 140197633726208 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.3197208642959595, loss=0.961979866027832
I0306 14:52:34.008364 140197625333504 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.1117209196090698, loss=0.9569240212440491
I0306 14:59:11.786211 140197633726208 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.0321215391159058, loss=0.934877872467041
I0306 15:03:06.230846 140368653526848 spec.py:321] Evaluating on the training split.
I0306 15:03:57.797299 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 15:04:46.159283 140368653526848 spec.py:349] Evaluating on the test split.
I0306 15:05:10.308611 140368653526848 submission_runner.py:413] Time since start: 53558.61s, 	Step: 56291, 	{'train/ctc_loss': Array(0.12536079, dtype=float32), 'train/wer': 0.04607978740783498, 'validation/ctc_loss': Array(0.33076006, dtype=float32), 'validation/wer': 0.0936308253762901, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17600378, dtype=float32), 'test/wer': 0.056608372433124124, 'test/num_examples': 2472, 'score': 49035.34192085266, 'total_duration': 53558.60917925835, 'accumulated_submission_time': 49035.34192085266, 'accumulated_eval_time': 4519.2787165641785, 'accumulated_logging_time': 1.7297980785369873}
I0306 15:05:10.343378 140197633726208 logging_writer.py:48] [56291] accumulated_eval_time=4519.278717, accumulated_logging_time=1.729798, accumulated_submission_time=49035.341921, global_step=56291, preemption_count=0, score=49035.341921, test/ctc_loss=0.17600378394126892, test/num_examples=2472, test/wer=0.056608, total_duration=53558.609179, train/ctc_loss=0.12536078691482544, train/wer=0.046080, validation/ctc_loss=0.33076006174087524, validation/num_examples=5348, validation/wer=0.093631
I0306 15:07:50.421487 140197625333504 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.040345311164856, loss=0.9103348255157471
I0306 15:14:23.875710 140197633726208 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.1709567308425903, loss=0.9330524206161499
I0306 15:21:22.777284 140197625333504 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.2573926448822021, loss=0.9447735548019409
I0306 15:28:06.920956 140197633726208 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.1028003692626953, loss=0.9410209059715271
I0306 15:29:10.751318 140368653526848 spec.py:321] Evaluating on the training split.
I0306 15:30:01.570341 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 15:30:49.791495 140368653526848 spec.py:349] Evaluating on the test split.
I0306 15:31:14.113149 140368653526848 submission_runner.py:413] Time since start: 55122.41s, 	Step: 58085, 	{'train/ctc_loss': Array(0.10389668, dtype=float32), 'train/wer': 0.03928195126325464, 'validation/ctc_loss': Array(0.32660738, dtype=float32), 'validation/wer': 0.09245295770296494, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1746935, dtype=float32), 'test/wer': 0.05569435134970447, 'test/num_examples': 2472, 'score': 50475.675526857376, 'total_duration': 55122.41311168671, 'accumulated_submission_time': 50475.675526857376, 'accumulated_eval_time': 4642.6346299648285, 'accumulated_logging_time': 1.779306173324585}
I0306 15:31:14.147974 140197633726208 logging_writer.py:48] [58085] accumulated_eval_time=4642.634630, accumulated_logging_time=1.779306, accumulated_submission_time=50475.675527, global_step=58085, preemption_count=0, score=50475.675527, test/ctc_loss=0.1746934950351715, test/num_examples=2472, test/wer=0.055694, total_duration=55122.413112, train/ctc_loss=0.10389667749404907, train/wer=0.039282, validation/ctc_loss=0.3266073763370514, validation/num_examples=5348, validation/wer=0.092453
I0306 15:36:47.212480 140197625333504 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.261629343032837, loss=0.9444986581802368
I0306 15:43:35.718627 140197633726208 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.9912802577018738, loss=1.027512550354004
I0306 15:50:27.704904 140197625333504 logging_writer.py:48] [59500] global_step=59500, grad_norm=1.177157998085022, loss=0.9504156708717346
I0306 15:55:14.156799 140368653526848 spec.py:321] Evaluating on the training split.
I0306 15:56:03.897145 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 15:56:52.524179 140368653526848 spec.py:349] Evaluating on the test split.
I0306 15:57:16.929552 140368653526848 submission_runner.py:413] Time since start: 56685.23s, 	Step: 59839, 	{'train/ctc_loss': Array(0.1103173, dtype=float32), 'train/wer': 0.04063027702980238, 'validation/ctc_loss': Array(0.326486, dtype=float32), 'validation/wer': 0.09247226700908504, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17408077, dtype=float32), 'test/wer': 0.0558771555663884, 'test/num_examples': 2472, 'score': 51915.60684299469, 'total_duration': 56685.22946357727, 'accumulated_submission_time': 51915.60684299469, 'accumulated_eval_time': 4765.401458740234, 'accumulated_logging_time': 1.8323278427124023}
I0306 15:57:16.970272 140197633726208 logging_writer.py:48] [59839] accumulated_eval_time=4765.401459, accumulated_logging_time=1.832328, accumulated_submission_time=51915.606843, global_step=59839, preemption_count=0, score=51915.606843, test/ctc_loss=0.1740807741880417, test/num_examples=2472, test/wer=0.055877, total_duration=56685.229464, train/ctc_loss=0.11031729727983475, train/wer=0.040630, validation/ctc_loss=0.3264859914779663, validation/num_examples=5348, validation/wer=0.092472
I0306 15:59:20.458903 140197625333504 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.1567354202270508, loss=1.0129578113555908
I0306 16:05:59.212602 140197633726208 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.5012919902801514, loss=0.9900726079940796
I0306 16:12:52.306283 140197633726208 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.361257553100586, loss=0.995542049407959
I0306 16:19:31.611219 140197625333504 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.0019840002059937, loss=0.9557827115058899
I0306 16:21:17.082413 140368653526848 spec.py:321] Evaluating on the training split.
I0306 16:22:07.946906 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 16:22:56.099992 140368653526848 spec.py:349] Evaluating on the test split.
I0306 16:23:20.299683 140368653526848 submission_runner.py:413] Time since start: 58248.60s, 	Step: 61625, 	{'train/ctc_loss': Array(0.10717867, dtype=float32), 'train/wer': 0.039855153327679325, 'validation/ctc_loss': Array(0.32647872, dtype=float32), 'validation/wer': 0.09252054027438524, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17407557, dtype=float32), 'test/wer': 0.05579590924786221, 'test/num_examples': 2472, 'score': 53355.64107942581, 'total_duration': 58248.59918427467, 'accumulated_submission_time': 53355.64107942581, 'accumulated_eval_time': 4888.612338542938, 'accumulated_logging_time': 1.8906810283660889}
I0306 16:23:20.335628 140197633726208 logging_writer.py:48] [61625] accumulated_eval_time=4888.612339, accumulated_logging_time=1.890681, accumulated_submission_time=53355.641079, global_step=61625, preemption_count=0, score=53355.641079, test/ctc_loss=0.17407557368278503, test/num_examples=2472, test/wer=0.055796, total_duration=58248.599184, train/ctc_loss=0.10717867314815521, train/wer=0.039855, validation/ctc_loss=0.3264787197113037, validation/num_examples=5348, validation/wer=0.092521
I0306 16:28:10.638568 140197633726208 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.331028699874878, loss=0.9720261693000793
I0306 16:34:46.545423 140197625333504 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.1004165410995483, loss=1.0037895441055298
I0306 16:41:41.858214 140197633726208 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.1288281679153442, loss=0.9449970126152039
I0306 16:47:20.760201 140368653526848 spec.py:321] Evaluating on the training split.
I0306 16:48:11.499765 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 16:49:00.229249 140368653526848 spec.py:349] Evaluating on the test split.
I0306 16:49:25.119996 140368653526848 submission_runner.py:413] Time since start: 59813.42s, 	Step: 63437, 	{'train/ctc_loss': Array(0.11267566, dtype=float32), 'train/wer': 0.04155160928505953, 'validation/ctc_loss': Array(0.32646677, dtype=float32), 'validation/wer': 0.0925108856213252, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1740723, dtype=float32), 'test/wer': 0.055816220827493754, 'test/num_examples': 2472, 'score': 54795.98932790756, 'total_duration': 59813.418239593506, 'accumulated_submission_time': 54795.98932790756, 'accumulated_eval_time': 5012.964464902878, 'accumulated_logging_time': 1.9420464038848877}
I0306 16:49:25.164343 140197633726208 logging_writer.py:48] [63437] accumulated_eval_time=5012.964465, accumulated_logging_time=1.942046, accumulated_submission_time=54795.989328, global_step=63437, preemption_count=0, score=54795.989328, test/ctc_loss=0.1740722954273224, test/num_examples=2472, test/wer=0.055816, total_duration=59813.418240, train/ctc_loss=0.11267565935850143, train/wer=0.041552, validation/ctc_loss=0.32646676898002625, validation/num_examples=5348, validation/wer=0.092511
I0306 16:50:14.123199 140197625333504 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.3079556226730347, loss=0.9594475030899048
I0306 16:56:42.345400 140197633726208 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.1144365072250366, loss=0.9747958183288574
I0306 17:03:13.207222 140197625333504 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.607924222946167, loss=0.9375156760215759
I0306 17:10:13.200026 140197633726208 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.6477434635162354, loss=0.9919612407684326
I0306 17:13:25.475041 140368653526848 spec.py:321] Evaluating on the training split.
I0306 17:14:16.089684 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 17:15:04.708560 140368653526848 spec.py:349] Evaluating on the test split.
I0306 17:15:29.309123 140368653526848 submission_runner.py:413] Time since start: 61377.61s, 	Step: 65253, 	{'train/ctc_loss': Array(0.11117318, dtype=float32), 'train/wer': 0.041401131459493305, 'validation/ctc_loss': Array(0.32646716, dtype=float32), 'validation/wer': 0.0925108856213252, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17407396, dtype=float32), 'test/wer': 0.055816220827493754, 'test/num_examples': 2472, 'score': 56236.21738529205, 'total_duration': 61377.60923695564, 'accumulated_submission_time': 56236.21738529205, 'accumulated_eval_time': 5136.792751073837, 'accumulated_logging_time': 2.0066773891448975}
I0306 17:15:29.346864 140197633726208 logging_writer.py:48] [65253] accumulated_eval_time=5136.792751, accumulated_logging_time=2.006677, accumulated_submission_time=56236.217385, global_step=65253, preemption_count=0, score=56236.217385, test/ctc_loss=0.1740739643573761, test/num_examples=2472, test/wer=0.055816, total_duration=61377.609237, train/ctc_loss=0.11117317527532578, train/wer=0.041401, validation/ctc_loss=0.3264671564102173, validation/num_examples=5348, validation/wer=0.092511
I0306 17:18:38.657096 140197625333504 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.122876763343811, loss=0.9736979603767395
I0306 17:25:33.139338 140197633726208 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.9951307773590088, loss=0.9823903441429138
I0306 17:31:59.356397 140197625333504 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.9381797909736633, loss=0.9235357046127319
I0306 17:39:04.653959 140197633726208 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.040683627128601, loss=0.9113644957542419
I0306 17:39:29.509313 140368653526848 spec.py:321] Evaluating on the training split.
I0306 17:40:20.169261 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 17:41:08.318317 140368653526848 spec.py:349] Evaluating on the test split.
I0306 17:41:32.489090 140368653526848 submission_runner.py:413] Time since start: 62940.79s, 	Step: 67034, 	{'train/ctc_loss': Array(0.11271247, dtype=float32), 'train/wer': 0.04184054104911283, 'validation/ctc_loss': Array(0.3264626, dtype=float32), 'validation/wer': 0.09253019492744528, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17407136, dtype=float32), 'test/wer': 0.055816220827493754, 'test/num_examples': 2472, 'score': 57676.30484843254, 'total_duration': 62940.789618730545, 'accumulated_submission_time': 57676.30484843254, 'accumulated_eval_time': 5259.7671530246735, 'accumulated_logging_time': 2.0593419075012207}
I0306 17:41:32.525240 140197633726208 logging_writer.py:48] [67034] accumulated_eval_time=5259.767153, accumulated_logging_time=2.059342, accumulated_submission_time=57676.304848, global_step=67034, preemption_count=0, score=57676.304848, test/ctc_loss=0.17407135665416718, test/num_examples=2472, test/wer=0.055816, total_duration=62940.789619, train/ctc_loss=0.11271246522665024, train/wer=0.041841, validation/ctc_loss=0.32646259665489197, validation/num_examples=5348, validation/wer=0.092530
I0306 17:47:28.839270 140197625333504 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.5549101829528809, loss=0.9145714044570923
I0306 17:54:26.218208 140197633726208 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.3197667598724365, loss=0.9562872052192688
I0306 18:00:47.895891 140197625333504 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.023168683052063, loss=0.9716007709503174
I0306 18:05:33.004549 140368653526848 spec.py:321] Evaluating on the training split.
I0306 18:06:23.503036 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 18:07:11.863294 140368653526848 spec.py:349] Evaluating on the test split.
I0306 18:07:36.289195 140368653526848 submission_runner.py:413] Time since start: 64504.59s, 	Step: 68840, 	{'train/ctc_loss': Array(0.09781582, dtype=float32), 'train/wer': 0.03746756147065974, 'validation/ctc_loss': Array(0.32647124, dtype=float32), 'validation/wer': 0.09250123096826515, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17407419, dtype=float32), 'test/wer': 0.055816220827493754, 'test/num_examples': 2472, 'score': 59116.708715200424, 'total_duration': 64504.5895178318, 'accumulated_submission_time': 59116.708715200424, 'accumulated_eval_time': 5383.0462102890015, 'accumulated_logging_time': 2.1086673736572266}
I0306 18:07:36.324705 140197633726208 logging_writer.py:48] [68840] accumulated_eval_time=5383.046210, accumulated_logging_time=2.108667, accumulated_submission_time=59116.708715, global_step=68840, preemption_count=0, score=59116.708715, test/ctc_loss=0.174074187874794, test/num_examples=2472, test/wer=0.055816, total_duration=64504.589518, train/ctc_loss=0.09781581908464432, train/wer=0.037468, validation/ctc_loss=0.3264712393283844, validation/num_examples=5348, validation/wer=0.092501
I0306 18:09:39.231045 140197625333504 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.360188603401184, loss=1.004062294960022
I0306 18:16:04.325510 140197633726208 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.4589719772338867, loss=0.9270358681678772
I0306 18:23:04.608103 140197625333504 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.9335079789161682, loss=0.946979820728302
I0306 18:29:32.670141 140197633726208 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.0421189069747925, loss=0.9468826651573181
I0306 18:31:36.382686 140368653526848 spec.py:321] Evaluating on the training split.
I0306 18:32:27.471203 140368653526848 spec.py:333] Evaluating on the validation split.
I0306 18:33:16.230680 140368653526848 spec.py:349] Evaluating on the test split.
I0306 18:33:40.430928 140368653526848 submission_runner.py:413] Time since start: 66068.73s, 	Step: 70652, 	{'train/ctc_loss': Array(0.09789545, dtype=float32), 'train/wer': 0.0358192723997893, 'validation/ctc_loss': Array(0.32647496, dtype=float32), 'validation/wer': 0.09254950423356537, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17407486, dtype=float32), 'test/wer': 0.0558365324071253, 'test/num_examples': 2472, 'score': 60556.68738603592, 'total_duration': 66068.72992968559, 'accumulated_submission_time': 60556.68738603592, 'accumulated_eval_time': 5507.087564945221, 'accumulated_logging_time': 2.1622884273529053}
I0306 18:33:40.466850 140197633726208 logging_writer.py:48] [70652] accumulated_eval_time=5507.087565, accumulated_logging_time=2.162288, accumulated_submission_time=60556.687386, global_step=70652, preemption_count=0, score=60556.687386, test/ctc_loss=0.17407485842704773, test/num_examples=2472, test/wer=0.055837, total_duration=66068.729930, train/ctc_loss=0.09789545089006424, train/wer=0.035819, validation/ctc_loss=0.32647496461868286, validation/num_examples=5348, validation/wer=0.092550
I0306 18:38:07.053986 140197625333504 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.0450332164764404, loss=0.9757700562477112
I0306 18:42:12.284095 140197633726208 logging_writer.py:48] [71312] global_step=71312, preemption_count=0, score=61068.449815
I0306 18:42:13.165439 140368653526848 checkpoints.py:490] Saving checkpoint at step: 71312
I0306 18:42:14.620646 140368653526848 checkpoints.py:422] Saved checkpoint at /experiment_runs/variants_target_setting/study_0/librispeech_conformer_layernorm_jax/trial_1/checkpoint_71312
I0306 18:42:14.657350 140368653526848 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_layernorm_jax/trial_1/checkpoint_71312.
I0306 18:42:18.170701 140368653526848 submission_runner.py:588] Tuning trial 1/1
I0306 18:42:18.170954 140368653526848 submission_runner.py:589] Hyperparameters: Hyperparameters(learning_rate=0.001308209823469072, beta1=0.9731333693827139, beta2=0.9981232922116359, warmup_steps=9999, weight_decay=0.16375311233774334)
I0306 18:42:18.193957 140368653526848 submission_runner.py:590] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(32.04605, dtype=float32), 'train/wer': 1.335516315099015, 'validation/ctc_loss': Array(31.162418, dtype=float32), 'validation/wer': 1.3074331173909266, 'validation/num_examples': 5348, 'test/ctc_loss': Array(31.273266, dtype=float32), 'test/wer': 1.3451343610992628, 'test/num_examples': 2472, 'score': 59.43868660926819, 'total_duration': 236.28323650360107, 'accumulated_submission_time': 59.43868660926819, 'accumulated_eval_time': 176.84447622299194, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1653, {'train/ctc_loss': Array(5.8413258, dtype=float32), 'train/wer': 0.9438116619637082, 'validation/ctc_loss': Array(5.9000506, dtype=float32), 'validation/wer': 0.8963959180126863, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.8528247, dtype=float32), 'test/wer': 0.899396746084943, 'test/num_examples': 2472, 'score': 1499.9074122905731, 'total_duration': 1780.3051240444183, 'accumulated_submission_time': 1499.9074122905731, 'accumulated_eval_time': 280.29821395874023, 'accumulated_logging_time': 0.04264330863952637, 'global_step': 1653, 'preemption_count': 0}), (3342, {'train/ctc_loss': Array(2.6792448, dtype=float32), 'train/wer': 0.6031004448232693, 'validation/ctc_loss': Array(3.106749, dtype=float32), 'validation/wer': 0.6445929115537233, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.741641, dtype=float32), 'test/wer': 0.594580870554303, 'test/num_examples': 2472, 'score': 2939.9757294654846, 'total_duration': 3340.7420020103455, 'accumulated_submission_time': 2939.9757294654846, 'accumulated_eval_time': 400.53701853752136, 'accumulated_logging_time': 0.10327672958374023, 'global_step': 3342, 'preemption_count': 0}), (4996, {'train/ctc_loss': Array(0.8614964, dtype=float32), 'train/wer': 0.28743193594467376, 'validation/ctc_loss': Array(1.2200164, dtype=float32), 'validation/wer': 0.3474419996717418, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.9223739, dtype=float32), 'test/wer': 0.28805882233461294, 'test/num_examples': 2472, 'score': 4380.456327915192, 'total_duration': 4908.355667829514, 'accumulated_submission_time': 4380.456327915192, 'accumulated_eval_time': 527.5549194812775, 'accumulated_logging_time': 0.14971232414245605, 'global_step': 4996, 'preemption_count': 0}), (6687, {'train/ctc_loss': Array(0.5708487, dtype=float32), 'train/wer': 0.19616395605776582, 'validation/ctc_loss': Array(0.8928377, dtype=float32), 'validation/wer': 0.26462438572269903, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.61350936, dtype=float32), 'test/wer': 0.20236426786911219, 'test/num_examples': 2472, 'score': 5820.5239317417145, 'total_duration': 6476.470409393311, 'accumulated_submission_time': 5820.5239317417145, 'accumulated_eval_time': 655.4858725070953, 'accumulated_logging_time': 0.1988532543182373, 'global_step': 6687, 'preemption_count': 0}), (8337, {'train/ctc_loss': Array(0.48657462, dtype=float32), 'train/wer': 0.170698427884929, 'validation/ctc_loss': Array(0.8043178, dtype=float32), 'validation/wer': 0.2403139693175126, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.53787524, dtype=float32), 'test/wer': 0.17654825015741474, 'test/num_examples': 2472, 'score': 7261.095873594284, 'total_duration': 8043.8543112277985, 'accumulated_submission_time': 7261.095873594284, 'accumulated_eval_time': 782.1844766139984, 'accumulated_logging_time': 0.24726557731628418, 'global_step': 8337, 'preemption_count': 0}), (9988, {'train/ctc_loss': Array(0.45870402, dtype=float32), 'train/wer': 0.1585854436626105, 'validation/ctc_loss': Array(0.735139, dtype=float32), 'validation/wer': 0.22115913764638867, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48113072, dtype=float32), 'test/wer': 0.15983182012065078, 'test/num_examples': 2472, 'score': 8701.249643325806, 'total_duration': 9612.125791549683, 'accumulated_submission_time': 8701.249643325806, 'accumulated_eval_time': 910.2020516395569, 'accumulated_logging_time': 0.2858603000640869, 'global_step': 9988, 'preemption_count': 0}), (11653, {'train/ctc_loss': Array(0.39806068, dtype=float32), 'train/wer': 0.14108331738029906, 'validation/ctc_loss': Array(0.68826026, dtype=float32), 'validation/wer': 0.20677370458692568, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4464232, dtype=float32), 'test/wer': 0.15107752929945362, 'test/num_examples': 2472, 'score': 10142.129729270935, 'total_duration': 11180.247917890549, 'accumulated_submission_time': 10142.129729270935, 'accumulated_eval_time': 1037.3256344795227, 'accumulated_logging_time': 0.33499813079833984, 'global_step': 11653, 'preemption_count': 0}), (13278, {'train/ctc_loss': Array(0.3296816, dtype=float32), 'train/wer': 0.12123560154970228, 'validation/ctc_loss': Array(0.64749724, dtype=float32), 'validation/wer': 0.19373026830280854, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40584028, dtype=float32), 'test/wer': 0.1378546909593159, 'test/num_examples': 2472, 'score': 11583.005396842957, 'total_duration': 12750.09497499466, 'accumulated_submission_time': 11583.005396842957, 'accumulated_eval_time': 1166.1983239650726, 'accumulated_logging_time': 0.3727586269378662, 'global_step': 13278, 'preemption_count': 0}), (14930, {'train/ctc_loss': Array(0.2971048, dtype=float32), 'train/wer': 0.10709744691943884, 'validation/ctc_loss': Array(0.6080465, dtype=float32), 'validation/wer': 0.1855913957731929, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38348365, dtype=float32), 'test/wer': 0.1286535453862247, 'test/num_examples': 2472, 'score': 13023.560427188873, 'total_duration': 14321.646738529205, 'accumulated_submission_time': 13023.560427188873, 'accumulated_eval_time': 1297.0788695812225, 'accumulated_logging_time': 0.42177796363830566, 'global_step': 14930, 'preemption_count': 0}), (16570, {'train/ctc_loss': Array(0.29405665, dtype=float32), 'train/wer': 0.10955825963843469, 'validation/ctc_loss': Array(0.5923404, dtype=float32), 'validation/wer': 0.17953792830454637, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36786732, dtype=float32), 'test/wer': 0.12507870737107224, 'test/num_examples': 2472, 'score': 14463.678126335144, 'total_duration': 15892.383562803268, 'accumulated_submission_time': 14463.678126335144, 'accumulated_eval_time': 1427.5605869293213, 'accumulated_logging_time': 0.4926631450653076, 'global_step': 16570, 'preemption_count': 0}), (18219, {'train/ctc_loss': Array(0.2975938, dtype=float32), 'train/wer': 0.10742748354001956, 'validation/ctc_loss': Array(0.58038473, dtype=float32), 'validation/wer': 0.17330102242775905, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36038235, dtype=float32), 'test/wer': 0.12052891353360551, 'test/num_examples': 2472, 'score': 15904.00504398346, 'total_duration': 17463.268262147903, 'accumulated_submission_time': 15904.00504398346, 'accumulated_eval_time': 1558.0121684074402, 'accumulated_logging_time': 0.5351059436798096, 'global_step': 18219, 'preemption_count': 0}), (19863, {'train/ctc_loss': Array(0.2913368, dtype=float32), 'train/wer': 0.10476165352045796, 'validation/ctc_loss': Array(0.5629047, dtype=float32), 'validation/wer': 0.16939088793844193, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3433145, dtype=float32), 'test/wer': 0.11601974285540187, 'test/num_examples': 2472, 'score': 17344.200548171997, 'total_duration': 19033.211532354355, 'accumulated_submission_time': 17344.200548171997, 'accumulated_eval_time': 1687.6422917842865, 'accumulated_logging_time': 0.5837585926055908, 'global_step': 19863, 'preemption_count': 0}), (21473, {'train/ctc_loss': Array(0.27229375, dtype=float32), 'train/wer': 0.10090029838460747, 'validation/ctc_loss': Array(0.54559654, dtype=float32), 'validation/wer': 0.16386842638809773, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33318508, dtype=float32), 'test/wer': 0.11402920805151016, 'test/num_examples': 2472, 'score': 18784.50802588463, 'total_duration': 20605.99622106552, 'accumulated_submission_time': 18784.50802588463, 'accumulated_eval_time': 1820.0197281837463, 'accumulated_logging_time': 0.6220605373382568, 'global_step': 21473, 'preemption_count': 0}), (23122, {'train/ctc_loss': Array(0.24628168, dtype=float32), 'train/wer': 0.08880537139478907, 'validation/ctc_loss': Array(0.525093, dtype=float32), 'validation/wer': 0.156849493613447, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32100928, dtype=float32), 'test/wer': 0.10610769199520646, 'test/num_examples': 2472, 'score': 20224.44011592865, 'total_duration': 22176.966822862625, 'accumulated_submission_time': 20224.44011592865, 'accumulated_eval_time': 1950.936583995819, 'accumulated_logging_time': 0.6775879859924316, 'global_step': 23122, 'preemption_count': 0}), (24721, {'train/ctc_loss': Array(0.23662172, dtype=float32), 'train/wer': 0.08667936044236939, 'validation/ctc_loss': Array(0.5193285, dtype=float32), 'validation/wer': 0.1571970611236085, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3122066, dtype=float32), 'test/wer': 0.10537647512847075, 'test/num_examples': 2472, 'score': 21668.39456653595, 'total_duration': 23750.926522254944, 'accumulated_submission_time': 21668.39456653595, 'accumulated_eval_time': 2080.825926065445, 'accumulated_logging_time': 0.7248210906982422, 'global_step': 24721, 'preemption_count': 0}), (26370, {'train/ctc_loss': Array(0.23301694, dtype=float32), 'train/wer': 0.08400241380368412, 'validation/ctc_loss': Array(0.5139927, dtype=float32), 'validation/wer': 0.15359587553221274, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30361816, dtype=float32), 'test/wer': 0.10165945605589746, 'test/num_examples': 2472, 'score': 23108.929773569107, 'total_duration': 25322.810742616653, 'accumulated_submission_time': 23108.929773569107, 'accumulated_eval_time': 2212.0559780597687, 'accumulated_logging_time': 0.7746150493621826, 'global_step': 26370, 'preemption_count': 0}), (27995, {'train/ctc_loss': Array(0.23725732, dtype=float32), 'train/wer': 0.08590093603744149, 'validation/ctc_loss': Array(0.49488494, dtype=float32), 'validation/wer': 0.14768722785946686, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2944605, dtype=float32), 'test/wer': 0.10017671074279447, 'test/num_examples': 2472, 'score': 24549.60434961319, 'total_duration': 26893.68610572815, 'accumulated_submission_time': 24549.60434961319, 'accumulated_eval_time': 2342.133118391037, 'accumulated_logging_time': 0.8296201229095459, 'global_step': 27995, 'preemption_count': 0}), (29605, {'train/ctc_loss': Array(0.22392258, dtype=float32), 'train/wer': 0.08040195551793552, 'validation/ctc_loss': Array(0.48500586, dtype=float32), 'validation/wer': 0.1463259217780009, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28525266, dtype=float32), 'test/wer': 0.09690654642211524, 'test/num_examples': 2472, 'score': 25990.009491443634, 'total_duration': 28465.048188209534, 'accumulated_submission_time': 25990.009491443634, 'accumulated_eval_time': 2472.966958999634, 'accumulated_logging_time': 0.8828892707824707, 'global_step': 29605, 'preemption_count': 0}), (31248, {'train/ctc_loss': Array(0.22642238, dtype=float32), 'train/wer': 0.08093224909975394, 'validation/ctc_loss': Array(0.4764266, dtype=float32), 'validation/wer': 0.14229027679890324, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2792893, dtype=float32), 'test/wer': 0.09245831048280624, 'test/num_examples': 2472, 'score': 27429.89633321762, 'total_duration': 30036.93883419037, 'accumulated_submission_time': 27429.89633321762, 'accumulated_eval_time': 2604.855622768402, 'accumulated_logging_time': 0.9278955459594727, 'global_step': 31248, 'preemption_count': 0}), (32848, {'train/ctc_loss': Array(0.21477792, dtype=float32), 'train/wer': 0.07501305455204237, 'validation/ctc_loss': Array(0.46601358, dtype=float32), 'validation/wer': 0.13709607345260047, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27015576, dtype=float32), 'test/wer': 0.09091463043080861, 'test/num_examples': 2472, 'score': 28870.375794649124, 'total_duration': 31607.326843976974, 'accumulated_submission_time': 28870.375794649124, 'accumulated_eval_time': 2734.659970998764, 'accumulated_logging_time': 0.9689691066741943, 'global_step': 32848, 'preemption_count': 0}), (34483, {'train/ctc_loss': Array(0.1691217, dtype=float32), 'train/wer': 0.0641124196672919, 'validation/ctc_loss': Array(0.45554063, dtype=float32), 'validation/wer': 0.13636231982003727, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2657611, dtype=float32), 'test/wer': 0.08985842828996811, 'test/num_examples': 2472, 'score': 30311.0317735672, 'total_duration': 33179.065029382706, 'accumulated_submission_time': 30311.0317735672, 'accumulated_eval_time': 2865.6183681488037, 'accumulated_logging_time': 1.0220990180969238, 'global_step': 34483, 'preemption_count': 0}), (36086, {'train/ctc_loss': Array(0.18087982, dtype=float32), 'train/wer': 0.06586978099723342, 'validation/ctc_loss': Array(0.43970668, dtype=float32), 'validation/wer': 0.12929511378008632, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24949893, dtype=float32), 'test/wer': 0.08327747648934658, 'test/num_examples': 2472, 'score': 31751.257553577423, 'total_duration': 34749.1638610363, 'accumulated_submission_time': 31751.257553577423, 'accumulated_eval_time': 2995.367404460907, 'accumulated_logging_time': 1.0751361846923828, 'global_step': 36086, 'preemption_count': 0}), (37722, {'train/ctc_loss': Array(0.21984127, dtype=float32), 'train/wer': 0.07977739568134604, 'validation/ctc_loss': Array(0.43781972, dtype=float32), 'validation/wer': 0.13055987333095184, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24441521, dtype=float32), 'test/wer': 0.08287124489671562, 'test/num_examples': 2472, 'score': 33192.45899128914, 'total_duration': 36320.49487328529, 'accumulated_submission_time': 33192.45899128914, 'accumulated_eval_time': 3125.3948628902435, 'accumulated_logging_time': 1.113063097000122, 'global_step': 37722, 'preemption_count': 0}), (39351, {'train/ctc_loss': Array(0.21354522, dtype=float32), 'train/wer': 0.0769969523196234, 'validation/ctc_loss': Array(0.42240757, dtype=float32), 'validation/wer': 0.12242100080133621, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23700641, dtype=float32), 'test/wer': 0.07819958158145959, 'test/num_examples': 2472, 'score': 34632.75153207779, 'total_duration': 37891.58566737175, 'accumulated_submission_time': 34632.75153207779, 'accumulated_eval_time': 3256.068284034729, 'accumulated_logging_time': 1.1671512126922607, 'global_step': 39351, 'preemption_count': 0}), (40964, {'train/ctc_loss': Array(0.23763691, dtype=float32), 'train/wer': 0.08760989991284052, 'validation/ctc_loss': Array(0.40826672, dtype=float32), 'validation/wer': 0.1209341842300897, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23022155, dtype=float32), 'test/wer': 0.07596530782198932, 'test/num_examples': 2472, 'score': 36072.824429273605, 'total_duration': 39461.56752991676, 'accumulated_submission_time': 36072.824429273605, 'accumulated_eval_time': 3385.8554463386536, 'accumulated_logging_time': 1.2193019390106201, 'global_step': 40964, 'preemption_count': 0}), (42613, {'train/ctc_loss': Array(0.20402001, dtype=float32), 'train/wer': 0.07291182167690125, 'validation/ctc_loss': Array(0.3942063, dtype=float32), 'validation/wer': 0.11646407986329012, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22189872, dtype=float32), 'test/wer': 0.07383259196067678, 'test/num_examples': 2472, 'score': 37513.10461354256, 'total_duration': 41029.40463256836, 'accumulated_submission_time': 37513.10461354256, 'accumulated_eval_time': 3513.2828528881073, 'accumulated_logging_time': 1.2802455425262451, 'global_step': 42613, 'preemption_count': 0}), (44208, {'train/ctc_loss': Array(0.18054171, dtype=float32), 'train/wer': 0.06762940776947658, 'validation/ctc_loss': Array(0.3837639, dtype=float32), 'validation/wer': 0.11307529663921527, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21255401, dtype=float32), 'test/wer': 0.06991245709178803, 'test/num_examples': 2472, 'score': 38953.35588002205, 'total_duration': 42599.876101493835, 'accumulated_submission_time': 38953.35588002205, 'accumulated_eval_time': 3643.3835871219635, 'accumulated_logging_time': 1.3324365615844727, 'global_step': 44208, 'preemption_count': 0}), (45827, {'train/ctc_loss': Array(0.1470921, dtype=float32), 'train/wer': 0.05540571403712928, 'validation/ctc_loss': Array(0.37341017, dtype=float32), 'validation/wer': 0.10961893084372012, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20728362, dtype=float32), 'test/wer': 0.0687953202120529, 'test/num_examples': 2472, 'score': 40393.64510297775, 'total_duration': 44169.76618552208, 'accumulated_submission_time': 40393.64510297775, 'accumulated_eval_time': 3772.8617856502533, 'accumulated_logging_time': 1.3857619762420654, 'global_step': 45827, 'preemption_count': 0}), (47421, {'train/ctc_loss': Array(0.1583361, dtype=float32), 'train/wer': 0.05976309829628463, 'validation/ctc_loss': Array(0.371881, dtype=float32), 'validation/wer': 0.10798729447657299, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20136462, dtype=float32), 'test/wer': 0.06589076432474153, 'test/num_examples': 2472, 'score': 41833.555126428604, 'total_duration': 45735.643411397934, 'accumulated_submission_time': 41833.555126428604, 'accumulated_eval_time': 3898.6836307048798, 'accumulated_logging_time': 1.4615850448608398, 'global_step': 47421, 'preemption_count': 0}), (49159, {'train/ctc_loss': Array(0.13405813, dtype=float32), 'train/wer': 0.05007730630475863, 'validation/ctc_loss': Array(0.3562352, dtype=float32), 'validation/wer': 0.1024648329262288, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19233635, dtype=float32), 'test/wer': 0.06221436841143136, 'test/num_examples': 2472, 'score': 43274.36798667908, 'total_duration': 47304.23278069496, 'accumulated_submission_time': 43274.36798667908, 'accumulated_eval_time': 4026.3534049987793, 'accumulated_logging_time': 1.509256362915039, 'global_step': 49159, 'preemption_count': 0}), (50921, {'train/ctc_loss': Array(0.12810867, dtype=float32), 'train/wer': 0.04891569663372352, 'validation/ctc_loss': Array(0.3469465, dtype=float32), 'validation/wer': 0.09910501366133408, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18913877, dtype=float32), 'test/wer': 0.06018321044827656, 'test/num_examples': 2472, 'score': 44714.6427052021, 'total_duration': 48868.591153383255, 'accumulated_submission_time': 44714.6427052021, 'accumulated_eval_time': 4150.310555458069, 'accumulated_logging_time': 1.569610357284546, 'global_step': 50921, 'preemption_count': 0}), (52716, {'train/ctc_loss': Array(0.1176779, dtype=float32), 'train/wer': 0.0445879917184265, 'validation/ctc_loss': Array(0.34046492, dtype=float32), 'validation/wer': 0.09754095986560724, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18197228, dtype=float32), 'test/wer': 0.05878171145369975, 'test/num_examples': 2472, 'score': 46155.10847115517, 'total_duration': 50431.535818099976, 'accumulated_submission_time': 46155.10847115517, 'accumulated_eval_time': 4272.6694712638855, 'accumulated_logging_time': 1.626260757446289, 'global_step': 52716, 'preemption_count': 0}), (54500, {'train/ctc_loss': Array(0.11792959, dtype=float32), 'train/wer': 0.044051829721964676, 'validation/ctc_loss': Array(0.3320718, dtype=float32), 'validation/wer': 0.09512729660059666, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17826898, dtype=float32), 'test/wer': 0.05713647350354437, 'test/num_examples': 2472, 'score': 47595.26068067551, 'total_duration': 51994.33613705635, 'accumulated_submission_time': 47595.26068067551, 'accumulated_eval_time': 4395.206291437149, 'accumulated_logging_time': 1.6741979122161865, 'global_step': 54500, 'preemption_count': 0}), (56291, {'train/ctc_loss': Array(0.12536079, dtype=float32), 'train/wer': 0.04607978740783498, 'validation/ctc_loss': Array(0.33076006, dtype=float32), 'validation/wer': 0.0936308253762901, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17600378, dtype=float32), 'test/wer': 0.056608372433124124, 'test/num_examples': 2472, 'score': 49035.34192085266, 'total_duration': 53558.60917925835, 'accumulated_submission_time': 49035.34192085266, 'accumulated_eval_time': 4519.2787165641785, 'accumulated_logging_time': 1.7297980785369873, 'global_step': 56291, 'preemption_count': 0}), (58085, {'train/ctc_loss': Array(0.10389668, dtype=float32), 'train/wer': 0.03928195126325464, 'validation/ctc_loss': Array(0.32660738, dtype=float32), 'validation/wer': 0.09245295770296494, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1746935, dtype=float32), 'test/wer': 0.05569435134970447, 'test/num_examples': 2472, 'score': 50475.675526857376, 'total_duration': 55122.41311168671, 'accumulated_submission_time': 50475.675526857376, 'accumulated_eval_time': 4642.6346299648285, 'accumulated_logging_time': 1.779306173324585, 'global_step': 58085, 'preemption_count': 0}), (59839, {'train/ctc_loss': Array(0.1103173, dtype=float32), 'train/wer': 0.04063027702980238, 'validation/ctc_loss': Array(0.326486, dtype=float32), 'validation/wer': 0.09247226700908504, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17408077, dtype=float32), 'test/wer': 0.0558771555663884, 'test/num_examples': 2472, 'score': 51915.60684299469, 'total_duration': 56685.22946357727, 'accumulated_submission_time': 51915.60684299469, 'accumulated_eval_time': 4765.401458740234, 'accumulated_logging_time': 1.8323278427124023, 'global_step': 59839, 'preemption_count': 0}), (61625, {'train/ctc_loss': Array(0.10717867, dtype=float32), 'train/wer': 0.039855153327679325, 'validation/ctc_loss': Array(0.32647872, dtype=float32), 'validation/wer': 0.09252054027438524, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17407557, dtype=float32), 'test/wer': 0.05579590924786221, 'test/num_examples': 2472, 'score': 53355.64107942581, 'total_duration': 58248.59918427467, 'accumulated_submission_time': 53355.64107942581, 'accumulated_eval_time': 4888.612338542938, 'accumulated_logging_time': 1.8906810283660889, 'global_step': 61625, 'preemption_count': 0}), (63437, {'train/ctc_loss': Array(0.11267566, dtype=float32), 'train/wer': 0.04155160928505953, 'validation/ctc_loss': Array(0.32646677, dtype=float32), 'validation/wer': 0.0925108856213252, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1740723, dtype=float32), 'test/wer': 0.055816220827493754, 'test/num_examples': 2472, 'score': 54795.98932790756, 'total_duration': 59813.418239593506, 'accumulated_submission_time': 54795.98932790756, 'accumulated_eval_time': 5012.964464902878, 'accumulated_logging_time': 1.9420464038848877, 'global_step': 63437, 'preemption_count': 0}), (65253, {'train/ctc_loss': Array(0.11117318, dtype=float32), 'train/wer': 0.041401131459493305, 'validation/ctc_loss': Array(0.32646716, dtype=float32), 'validation/wer': 0.0925108856213252, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17407396, dtype=float32), 'test/wer': 0.055816220827493754, 'test/num_examples': 2472, 'score': 56236.21738529205, 'total_duration': 61377.60923695564, 'accumulated_submission_time': 56236.21738529205, 'accumulated_eval_time': 5136.792751073837, 'accumulated_logging_time': 2.0066773891448975, 'global_step': 65253, 'preemption_count': 0}), (67034, {'train/ctc_loss': Array(0.11271247, dtype=float32), 'train/wer': 0.04184054104911283, 'validation/ctc_loss': Array(0.3264626, dtype=float32), 'validation/wer': 0.09253019492744528, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17407136, dtype=float32), 'test/wer': 0.055816220827493754, 'test/num_examples': 2472, 'score': 57676.30484843254, 'total_duration': 62940.789618730545, 'accumulated_submission_time': 57676.30484843254, 'accumulated_eval_time': 5259.7671530246735, 'accumulated_logging_time': 2.0593419075012207, 'global_step': 67034, 'preemption_count': 0}), (68840, {'train/ctc_loss': Array(0.09781582, dtype=float32), 'train/wer': 0.03746756147065974, 'validation/ctc_loss': Array(0.32647124, dtype=float32), 'validation/wer': 0.09250123096826515, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17407419, dtype=float32), 'test/wer': 0.055816220827493754, 'test/num_examples': 2472, 'score': 59116.708715200424, 'total_duration': 64504.5895178318, 'accumulated_submission_time': 59116.708715200424, 'accumulated_eval_time': 5383.0462102890015, 'accumulated_logging_time': 2.1086673736572266, 'global_step': 68840, 'preemption_count': 0}), (70652, {'train/ctc_loss': Array(0.09789545, dtype=float32), 'train/wer': 0.0358192723997893, 'validation/ctc_loss': Array(0.32647496, dtype=float32), 'validation/wer': 0.09254950423356537, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17407486, dtype=float32), 'test/wer': 0.0558365324071253, 'test/num_examples': 2472, 'score': 60556.68738603592, 'total_duration': 66068.72992968559, 'accumulated_submission_time': 60556.68738603592, 'accumulated_eval_time': 5507.087564945221, 'accumulated_logging_time': 2.1622884273529053, 'global_step': 70652, 'preemption_count': 0})], 'global_step': 71312}
I0306 18:42:18.194223 140368653526848 submission_runner.py:591] Timing: 61068.449815273285
I0306 18:42:18.194282 140368653526848 submission_runner.py:593] Total number of evals: 43
I0306 18:42:18.194348 140368653526848 submission_runner.py:594] ====================
I0306 18:42:18.198890 140368653526848 submission_runner.py:678] Final librispeech_conformer_layernorm score: 61068.449815273285
