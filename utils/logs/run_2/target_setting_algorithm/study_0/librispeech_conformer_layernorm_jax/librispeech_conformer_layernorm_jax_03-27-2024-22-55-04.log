python3 submission_runner.py --framework=jax --workload=librispeech_conformer_layernorm --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --rng_seed=852930489 --max_global_steps=80000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab --tuning_ruleset=external --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_conformer_layernorm/tuning_search_space.json --num_tuning_trials=1 2>&1 | tee -a /logs/librispeech_conformer_layernorm_jax_03-27-2024-22-55-04.log
I0327 22:55:25.962850 140002444732224 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/librispeech_conformer_layernorm_jax.
I0327 22:55:27.025054 140002444732224 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0327 22:55:27.025906 140002444732224 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0327 22:55:27.026062 140002444732224 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0327 22:55:27.032424 140002444732224 submission_runner.py:557] Using RNG seed 852930489
I0327 22:55:28.256307 140002444732224 submission_runner.py:566] --- Tuning run 1/1 ---
I0327 22:55:28.256531 140002444732224 submission_runner.py:571] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/librispeech_conformer_layernorm_jax/trial_1.
I0327 22:55:28.256873 140002444732224 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_layernorm_jax/trial_1/hparams.json.
I0327 22:55:28.442701 140002444732224 submission_runner.py:211] Initializing dataset.
I0327 22:55:28.442975 140002444732224 submission_runner.py:222] Initializing model.
I0327 22:55:33.291160 140002444732224 submission_runner.py:264] Initializing optimizer.
I0327 22:55:34.504625 140002444732224 submission_runner.py:271] Initializing metrics bundle.
I0327 22:55:34.504831 140002444732224 submission_runner.py:289] Initializing checkpoint and logger.
I0327 22:55:34.505918 140002444732224 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/librispeech_conformer_layernorm_jax/trial_1 with prefix checkpoint_
I0327 22:55:34.506057 140002444732224 submission_runner.py:309] Saving meta data to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_layernorm_jax/trial_1/meta_data_0.json.
I0327 22:55:34.506252 140002444732224 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0327 22:55:34.506311 140002444732224 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0327 22:55:34.789547 140002444732224 logger_utils.py:220] Unable to record git information. Continuing without it.
I0327 22:55:35.039434 140002444732224 submission_runner.py:313] Saving flags to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_layernorm_jax/trial_1/flags_0.json.
I0327 22:55:35.054898 140002444732224 submission_runner.py:323] Starting training loop.
I0327 22:55:35.362795 140002444732224 input_pipeline.py:20] Loading split = train-clean-100
I0327 22:55:35.401921 140002444732224 input_pipeline.py:20] Loading split = train-clean-360
I0327 22:55:35.799917 140002444732224 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0327 22:56:34.611259 139828146460416 logging_writer.py:48] [0] global_step=0, grad_norm=69.69271087646484, loss=31.55531120300293
I0327 22:56:34.650583 140002444732224 spec.py:321] Evaluating on the training split.
I0327 22:56:34.838617 140002444732224 input_pipeline.py:20] Loading split = train-clean-100
I0327 22:56:34.878391 140002444732224 input_pipeline.py:20] Loading split = train-clean-360
I0327 22:56:35.385743 140002444732224 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0327 22:57:56.029006 140002444732224 spec.py:333] Evaluating on the validation split.
I0327 22:57:56.144740 140002444732224 input_pipeline.py:20] Loading split = dev-clean
I0327 22:57:56.150991 140002444732224 input_pipeline.py:20] Loading split = dev-other
I0327 22:59:02.601832 140002444732224 spec.py:349] Evaluating on the test split.
I0327 22:59:02.721294 140002444732224 input_pipeline.py:20] Loading split = test-clean
I0327 22:59:40.574827 140002444732224 submission_runner.py:422] Time since start: 245.52s, 	Step: 1, 	{'train/ctc_loss': Array(31.144068, dtype=float32), 'train/wer': 1.5954490556608292, 'validation/ctc_loss': Array(30.784756, dtype=float32), 'validation/wer': 1.5437114417293414, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.686705, dtype=float32), 'test/wer': 1.5518656185891577, 'test/num_examples': 2472, 'score': 59.595560789108276, 'total_duration': 245.51749229431152, 'accumulated_submission_time': 59.595560789108276, 'accumulated_eval_time': 185.92183756828308, 'accumulated_logging_time': 0}
I0327 22:59:40.602116 139818264688384 logging_writer.py:48] [1] accumulated_eval_time=185.921838, accumulated_logging_time=0, accumulated_submission_time=59.595561, global_step=1, preemption_count=0, score=59.595561, test/ctc_loss=30.686704635620117, test/num_examples=2472, test/wer=1.551866, total_duration=245.517492, train/ctc_loss=31.144067764282227, train/wer=1.595449, validation/ctc_loss=30.78475570678711, validation/num_examples=5348, validation/wer=1.543711
I0327 23:00:03.216515 139830184855296 logging_writer.py:48] [1] global_step=1, grad_norm=66.2665786743164, loss=31.807159423828125
I0327 23:00:04.065301 139830193248000 logging_writer.py:48] [2] global_step=2, grad_norm=73.21293640136719, loss=32.49951171875
I0327 23:00:04.901904 139830184855296 logging_writer.py:48] [3] global_step=3, grad_norm=83.96888732910156, loss=31.743051528930664
I0327 23:00:05.744565 139830193248000 logging_writer.py:48] [4] global_step=4, grad_norm=94.58753204345703, loss=31.098506927490234
I0327 23:00:06.638939 139830184855296 logging_writer.py:48] [5] global_step=5, grad_norm=94.93644714355469, loss=30.576231002807617
I0327 23:00:07.533970 139830193248000 logging_writer.py:48] [6] global_step=6, grad_norm=98.8062744140625, loss=30.903261184692383
I0327 23:00:08.431686 139830184855296 logging_writer.py:48] [7] global_step=7, grad_norm=112.72425079345703, loss=29.52086067199707
I0327 23:00:09.332443 139830193248000 logging_writer.py:48] [8] global_step=8, grad_norm=112.21054077148438, loss=29.465744018554688
I0327 23:00:10.237245 139830184855296 logging_writer.py:48] [9] global_step=9, grad_norm=95.47245788574219, loss=28.61150550842285
I0327 23:00:11.133585 139830193248000 logging_writer.py:48] [10] global_step=10, grad_norm=105.9131088256836, loss=26.334545135498047
I0327 23:00:12.030764 139830184855296 logging_writer.py:48] [11] global_step=11, grad_norm=109.74503326416016, loss=26.257701873779297
I0327 23:00:12.940020 139830193248000 logging_writer.py:48] [12] global_step=12, grad_norm=96.47804260253906, loss=24.234315872192383
I0327 23:00:13.837203 139830184855296 logging_writer.py:48] [13] global_step=13, grad_norm=88.87198638916016, loss=23.792064666748047
I0327 23:00:14.734798 139830193248000 logging_writer.py:48] [14] global_step=14, grad_norm=87.22779083251953, loss=22.392492294311523
I0327 23:00:15.636323 139830184855296 logging_writer.py:48] [15] global_step=15, grad_norm=67.38036346435547, loss=21.867923736572266
I0327 23:00:16.537200 139830193248000 logging_writer.py:48] [16] global_step=16, grad_norm=48.35993576049805, loss=21.423120498657227
I0327 23:00:17.443231 139830184855296 logging_writer.py:48] [17] global_step=17, grad_norm=30.602258682250977, loss=21.940837860107422
I0327 23:00:18.343935 139830193248000 logging_writer.py:48] [18] global_step=18, grad_norm=24.360960006713867, loss=20.726015090942383
I0327 23:00:19.233502 139830184855296 logging_writer.py:48] [19] global_step=19, grad_norm=17.161901473999023, loss=20.2121524810791
I0327 23:00:20.140433 139830193248000 logging_writer.py:48] [20] global_step=20, grad_norm=14.713422775268555, loss=19.416566848754883
I0327 23:00:21.042984 139830184855296 logging_writer.py:48] [21] global_step=21, grad_norm=16.097230911254883, loss=19.52886962890625
I0327 23:00:21.941424 139830193248000 logging_writer.py:48] [22] global_step=22, grad_norm=26.740259170532227, loss=20.852615356445312
I0327 23:00:22.844588 139830184855296 logging_writer.py:48] [23] global_step=23, grad_norm=20.281352996826172, loss=19.148086547851562
I0327 23:00:23.748673 139830193248000 logging_writer.py:48] [24] global_step=24, grad_norm=14.537927627563477, loss=18.766359329223633
I0327 23:00:24.656720 139830184855296 logging_writer.py:48] [25] global_step=25, grad_norm=17.09040641784668, loss=18.88262367248535
I0327 23:00:25.557196 139830193248000 logging_writer.py:48] [26] global_step=26, grad_norm=15.948432922363281, loss=19.16650390625
I0327 23:00:26.450977 139830184855296 logging_writer.py:48] [27] global_step=27, grad_norm=16.706012725830078, loss=18.830242156982422
I0327 23:00:27.357598 139830193248000 logging_writer.py:48] [28] global_step=28, grad_norm=18.745237350463867, loss=18.49217414855957
I0327 23:00:28.266143 139830184855296 logging_writer.py:48] [29] global_step=29, grad_norm=15.481447219848633, loss=18.998247146606445
I0327 23:00:29.162802 139830193248000 logging_writer.py:48] [30] global_step=30, grad_norm=24.32942008972168, loss=17.138898849487305
I0327 23:00:30.065279 139830184855296 logging_writer.py:48] [31] global_step=31, grad_norm=19.1087589263916, loss=18.660919189453125
I0327 23:00:30.969807 139830193248000 logging_writer.py:48] [32] global_step=32, grad_norm=16.613759994506836, loss=17.096193313598633
I0327 23:00:31.882990 139830184855296 logging_writer.py:48] [33] global_step=33, grad_norm=18.447298049926758, loss=17.061203002929688
I0327 23:00:32.779874 139830193248000 logging_writer.py:48] [34] global_step=34, grad_norm=25.36440086364746, loss=19.062442779541016
I0327 23:00:33.676635 139830184855296 logging_writer.py:48] [35] global_step=35, grad_norm=20.290102005004883, loss=16.670455932617188
I0327 23:00:34.581765 139830193248000 logging_writer.py:48] [36] global_step=36, grad_norm=22.36478614807129, loss=16.605573654174805
I0327 23:00:35.488913 139830184855296 logging_writer.py:48] [37] global_step=37, grad_norm=20.92293357849121, loss=15.821731567382812
I0327 23:00:36.381492 139830193248000 logging_writer.py:48] [38] global_step=38, grad_norm=26.354764938354492, loss=16.650390625
I0327 23:00:37.288931 139830184855296 logging_writer.py:48] [39] global_step=39, grad_norm=18.85599708557129, loss=15.41911792755127
I0327 23:00:38.181570 139830193248000 logging_writer.py:48] [40] global_step=40, grad_norm=23.92792320251465, loss=15.1040620803833
I0327 23:00:39.080103 139830184855296 logging_writer.py:48] [41] global_step=41, grad_norm=24.501482009887695, loss=14.865703582763672
I0327 23:00:39.986298 139830193248000 logging_writer.py:48] [42] global_step=42, grad_norm=25.106861114501953, loss=13.727795600891113
I0327 23:00:40.881826 139830184855296 logging_writer.py:48] [43] global_step=43, grad_norm=16.99090003967285, loss=13.28454875946045
I0327 23:00:41.791229 139830193248000 logging_writer.py:48] [44] global_step=44, grad_norm=8.504040718078613, loss=13.151768684387207
I0327 23:00:42.696678 139830184855296 logging_writer.py:48] [45] global_step=45, grad_norm=14.85273265838623, loss=13.851398468017578
I0327 23:00:43.591703 139830193248000 logging_writer.py:48] [46] global_step=46, grad_norm=11.497505187988281, loss=12.992671012878418
I0327 23:00:44.497117 139830184855296 logging_writer.py:48] [47] global_step=47, grad_norm=11.69864559173584, loss=12.723993301391602
I0327 23:00:45.395073 139830193248000 logging_writer.py:48] [48] global_step=48, grad_norm=17.10811996459961, loss=12.190776824951172
I0327 23:00:46.299952 139830184855296 logging_writer.py:48] [49] global_step=49, grad_norm=15.764779090881348, loss=11.924592971801758
I0327 23:00:47.203989 139830193248000 logging_writer.py:48] [50] global_step=50, grad_norm=19.60544776916504, loss=11.301527976989746
I0327 23:00:48.100169 139830184855296 logging_writer.py:48] [51] global_step=51, grad_norm=46.790164947509766, loss=10.239753723144531
I0327 23:00:49.012319 139830193248000 logging_writer.py:48] [52] global_step=52, grad_norm=31.031538009643555, loss=7.512914657592773
I0327 23:00:49.927198 139830184855296 logging_writer.py:48] [53] global_step=53, grad_norm=9.647826194763184, loss=7.056478023529053
I0327 23:00:50.825294 139830193248000 logging_writer.py:48] [54] global_step=54, grad_norm=13.177210807800293, loss=7.120807647705078
I0327 23:00:51.727349 139830184855296 logging_writer.py:48] [55] global_step=55, grad_norm=12.498493194580078, loss=7.049080848693848
I0327 23:00:52.632800 139830193248000 logging_writer.py:48] [56] global_step=56, grad_norm=9.296428680419922, loss=6.926537990570068
I0327 23:00:53.537387 139830184855296 logging_writer.py:48] [57] global_step=57, grad_norm=4.831500053405762, loss=6.818483829498291
I0327 23:00:54.443216 139830193248000 logging_writer.py:48] [58] global_step=58, grad_norm=2.8133068084716797, loss=6.793334007263184
I0327 23:00:55.348796 139830184855296 logging_writer.py:48] [59] global_step=59, grad_norm=5.230678081512451, loss=6.805751800537109
I0327 23:00:56.240597 139830193248000 logging_writer.py:48] [60] global_step=60, grad_norm=4.069845676422119, loss=6.756172180175781
I0327 23:00:57.141442 139830184855296 logging_writer.py:48] [61] global_step=61, grad_norm=2.848919630050659, loss=6.718788146972656
I0327 23:00:58.047087 139830193248000 logging_writer.py:48] [62] global_step=62, grad_norm=2.2146503925323486, loss=6.703195571899414
I0327 23:00:58.955379 139830184855296 logging_writer.py:48] [63] global_step=63, grad_norm=2.226874828338623, loss=6.6579670906066895
I0327 23:00:59.861781 139830193248000 logging_writer.py:48] [64] global_step=64, grad_norm=1.9832110404968262, loss=6.625685691833496
I0327 23:01:00.759632 139830184855296 logging_writer.py:48] [65] global_step=65, grad_norm=1.9354132413864136, loss=6.605374336242676
I0327 23:01:01.660463 139830193248000 logging_writer.py:48] [66] global_step=66, grad_norm=1.7521497011184692, loss=6.557249546051025
I0327 23:01:02.645611 139830184855296 logging_writer.py:48] [67] global_step=67, grad_norm=2.012460947036743, loss=6.534592628479004
I0327 23:01:03.543559 139830193248000 logging_writer.py:48] [68] global_step=68, grad_norm=1.6260418891906738, loss=6.518732070922852
I0327 23:01:04.442091 139830184855296 logging_writer.py:48] [69] global_step=69, grad_norm=1.6546639204025269, loss=6.484195709228516
I0327 23:01:05.339438 139830193248000 logging_writer.py:48] [70] global_step=70, grad_norm=1.6868265867233276, loss=6.4718241691589355
I0327 23:01:06.251458 139830184855296 logging_writer.py:48] [71] global_step=71, grad_norm=1.3934756517410278, loss=6.443295001983643
I0327 23:01:07.148697 139830193248000 logging_writer.py:48] [72] global_step=72, grad_norm=1.5985181331634521, loss=6.422828674316406
I0327 23:01:08.053587 139830184855296 logging_writer.py:48] [73] global_step=73, grad_norm=1.3706482648849487, loss=6.417243003845215
I0327 23:01:08.954909 139830193248000 logging_writer.py:48] [74] global_step=74, grad_norm=1.8817448616027832, loss=6.391418933868408
I0327 23:01:09.857914 139830184855296 logging_writer.py:48] [75] global_step=75, grad_norm=1.7153217792510986, loss=6.368225574493408
I0327 23:01:10.759670 139830193248000 logging_writer.py:48] [76] global_step=76, grad_norm=2.6892318725585938, loss=6.346279144287109
I0327 23:01:11.660462 139830184855296 logging_writer.py:48] [77] global_step=77, grad_norm=2.3918447494506836, loss=6.3278398513793945
I0327 23:01:12.556773 139830193248000 logging_writer.py:48] [78] global_step=78, grad_norm=3.7907516956329346, loss=6.341037750244141
I0327 23:01:13.457766 139830184855296 logging_writer.py:48] [79] global_step=79, grad_norm=4.0109992027282715, loss=6.296314716339111
I0327 23:01:14.351589 139830193248000 logging_writer.py:48] [80] global_step=80, grad_norm=5.41093111038208, loss=6.283410549163818
I0327 23:01:15.255732 139830184855296 logging_writer.py:48] [81] global_step=81, grad_norm=5.132919788360596, loss=6.297010898590088
I0327 23:01:16.160311 139830193248000 logging_writer.py:48] [82] global_step=82, grad_norm=5.168135166168213, loss=6.255181312561035
I0327 23:01:17.057737 139830184855296 logging_writer.py:48] [83] global_step=83, grad_norm=4.827764987945557, loss=6.286078453063965
I0327 23:01:17.957535 139830193248000 logging_writer.py:48] [84] global_step=84, grad_norm=6.221900939941406, loss=6.23970890045166
I0327 23:01:18.856790 139830184855296 logging_writer.py:48] [85] global_step=85, grad_norm=6.5603718757629395, loss=6.262400150299072
I0327 23:01:19.752993 139830193248000 logging_writer.py:48] [86] global_step=86, grad_norm=7.721577167510986, loss=6.220232009887695
I0327 23:01:20.658535 139830184855296 logging_writer.py:48] [87] global_step=87, grad_norm=7.875493049621582, loss=6.229465961456299
I0327 23:01:21.556338 139830193248000 logging_writer.py:48] [88] global_step=88, grad_norm=7.550817966461182, loss=6.20481538772583
I0327 23:01:22.460250 139830184855296 logging_writer.py:48] [89] global_step=89, grad_norm=6.764349937438965, loss=6.193621635437012
I0327 23:01:23.369686 139830193248000 logging_writer.py:48] [90] global_step=90, grad_norm=6.454153537750244, loss=6.16093111038208
I0327 23:01:24.285965 139830184855296 logging_writer.py:48] [91] global_step=91, grad_norm=5.2499260902404785, loss=6.1561174392700195
I0327 23:01:25.189815 139830193248000 logging_writer.py:48] [92] global_step=92, grad_norm=5.341551780700684, loss=6.13692045211792
I0327 23:01:26.086347 139830184855296 logging_writer.py:48] [93] global_step=93, grad_norm=5.162524223327637, loss=6.131098747253418
I0327 23:01:26.983968 139830193248000 logging_writer.py:48] [94] global_step=94, grad_norm=5.703703880310059, loss=6.117974281311035
I0327 23:01:27.888630 139830184855296 logging_writer.py:48] [95] global_step=95, grad_norm=5.160260200500488, loss=6.129302978515625
I0327 23:01:28.783195 139830193248000 logging_writer.py:48] [96] global_step=96, grad_norm=4.874695777893066, loss=6.0995378494262695
I0327 23:01:29.682038 139830184855296 logging_writer.py:48] [97] global_step=97, grad_norm=4.681457996368408, loss=6.069565296173096
I0327 23:01:30.589132 139830193248000 logging_writer.py:48] [98] global_step=98, grad_norm=6.097553253173828, loss=6.067508697509766
I0327 23:01:31.481826 139830184855296 logging_writer.py:48] [99] global_step=99, grad_norm=6.334469318389893, loss=6.07076358795166
I0327 23:01:32.381969 139830193248000 logging_writer.py:48] [100] global_step=100, grad_norm=7.938417911529541, loss=6.093895435333252
I0327 23:06:42.448297 139830184855296 logging_writer.py:48] [500] global_step=500, grad_norm=2.863858699798584, loss=5.547236919403076
I0327 23:13:16.123812 139830193248000 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.3705445528030396, loss=3.4338438510894775
I0327 23:19:47.628262 139832209684224 logging_writer.py:48] [1500] global_step=1500, grad_norm=2.0287725925445557, loss=2.8107223510742188
I0327 23:23:41.254297 140002444732224 spec.py:321] Evaluating on the training split.
I0327 23:24:24.377460 140002444732224 spec.py:333] Evaluating on the validation split.
I0327 23:25:11.890598 140002444732224 spec.py:349] Evaluating on the test split.
I0327 23:25:35.458661 140002444732224 submission_runner.py:422] Time since start: 1800.40s, 	Step: 1803, 	{'train/ctc_loss': Array(4.095391, dtype=float32), 'train/wer': 0.840152623014974, 'validation/ctc_loss': Array(4.45944, dtype=float32), 'validation/wer': 0.8330613939388088, 'validation/num_examples': 5348, 'test/ctc_loss': Array(4.2168097, dtype=float32), 'test/wer': 0.8155708569455447, 'test/num_examples': 2472, 'score': 1500.1719553470612, 'total_duration': 1800.3977222442627, 'accumulated_submission_time': 1500.1719553470612, 'accumulated_eval_time': 300.12022733688354, 'accumulated_logging_time': 0.04186701774597168}
I0327 23:25:35.493865 139831994644224 logging_writer.py:48] [1803] accumulated_eval_time=300.120227, accumulated_logging_time=0.041867, accumulated_submission_time=1500.171955, global_step=1803, preemption_count=0, score=1500.171955, test/ctc_loss=4.2168097496032715, test/num_examples=2472, test/wer=0.815571, total_duration=1800.397722, train/ctc_loss=4.095390796661377, train/wer=0.840153, validation/ctc_loss=4.459440231323242, validation/num_examples=5348, validation/wer=0.833061
I0327 23:28:08.453744 139831986251520 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.1356593370437622, loss=2.576056957244873
I0327 23:34:37.495876 139831339284224 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.1866141557693481, loss=2.3323135375976562
I0327 23:41:24.928010 139831330891520 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.4988735914230347, loss=2.245692253112793
I0327 23:47:59.818316 139831994644224 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.406667709350586, loss=2.2249724864959717
I0327 23:49:35.826769 140002444732224 spec.py:321] Evaluating on the training split.
I0327 23:50:28.411406 140002444732224 spec.py:333] Evaluating on the validation split.
I0327 23:51:19.236665 140002444732224 spec.py:349] Evaluating on the test split.
I0327 23:51:44.617371 140002444732224 submission_runner.py:422] Time since start: 3369.56s, 	Step: 3626, 	{'train/ctc_loss': Array(0.79150915, dtype=float32), 'train/wer': 0.267293503449378, 'validation/ctc_loss': Array(1.1359301, dtype=float32), 'validation/wer': 0.32738928526603395, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8338466, dtype=float32), 'test/wer': 0.2679909816586436, 'test/num_examples': 2472, 'score': 2940.422672510147, 'total_duration': 3369.5566635131836, 'accumulated_submission_time': 2940.422672510147, 'accumulated_eval_time': 428.9050886631012, 'accumulated_logging_time': 0.09521222114562988}
I0327 23:51:44.654426 139831994644224 logging_writer.py:48] [3626] accumulated_eval_time=428.905089, accumulated_logging_time=0.095212, accumulated_submission_time=2940.422673, global_step=3626, preemption_count=0, score=2940.422673, test/ctc_loss=0.8338466286659241, test/num_examples=2472, test/wer=0.267991, total_duration=3369.556664, train/ctc_loss=0.7915091514587402, train/wer=0.267294, validation/ctc_loss=1.135930061340332, validation/num_examples=5348, validation/wer=0.327389
I0327 23:56:33.408038 139831986251520 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8108907341957092, loss=1.9524117708206177
I0328 00:03:07.174787 139831994644224 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.8502591848373413, loss=1.9412225484848022
I0328 00:10:00.482634 139831986251520 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.745354413986206, loss=1.7920080423355103
I0328 00:15:44.630867 140002444732224 spec.py:321] Evaluating on the training split.
I0328 00:16:37.705290 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 00:17:28.559017 140002444732224 spec.py:349] Evaluating on the test split.
I0328 00:17:54.291034 140002444732224 submission_runner.py:422] Time since start: 4939.23s, 	Step: 5428, 	{'train/ctc_loss': Array(0.48875508, dtype=float32), 'train/wer': 0.1729999841864731, 'validation/ctc_loss': Array(0.8318542, dtype=float32), 'validation/wer': 0.2478542533574056, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.55740714, dtype=float32), 'test/wer': 0.1852619178193488, 'test/num_examples': 2472, 'score': 4380.319248199463, 'total_duration': 4939.229757785797, 'accumulated_submission_time': 4380.319248199463, 'accumulated_eval_time': 558.5591344833374, 'accumulated_logging_time': 0.14757418632507324}
I0328 00:17:54.332946 139831779604224 logging_writer.py:48] [5428] accumulated_eval_time=558.559134, accumulated_logging_time=0.147574, accumulated_submission_time=4380.319248, global_step=5428, preemption_count=0, score=4380.319248, test/ctc_loss=0.5574071407318115, test/num_examples=2472, test/wer=0.185262, total_duration=4939.229758, train/ctc_loss=0.48875507712364197, train/wer=0.173000, validation/ctc_loss=0.8318542242050171, validation/num_examples=5348, validation/wer=0.247854
I0328 00:18:50.430232 139831771211520 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8841682076454163, loss=1.7653826475143433
I0328 00:25:35.862680 139831779604224 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.8371074795722961, loss=1.694322109222412
I0328 00:32:20.686643 139831124244224 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.7407850623130798, loss=1.6010808944702148
I0328 00:39:14.539303 139831115851520 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.7141370177268982, loss=1.637459635734558
I0328 00:41:54.866828 140002444732224 spec.py:321] Evaluating on the training split.
I0328 00:42:48.945502 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 00:43:40.539332 140002444732224 spec.py:349] Evaluating on the test split.
I0328 00:44:06.708461 140002444732224 submission_runner.py:422] Time since start: 6511.65s, 	Step: 7189, 	{'train/ctc_loss': Array(0.40930986, dtype=float32), 'train/wer': 0.1436186367016096, 'validation/ctc_loss': Array(0.7136427, dtype=float32), 'validation/wer': 0.21319404887185378, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46206, dtype=float32), 'test/wer': 0.1526008977718197, 'test/num_examples': 2472, 'score': 5820.772198915482, 'total_duration': 6511.6455771923065, 'accumulated_submission_time': 5820.772198915482, 'accumulated_eval_time': 690.3928747177124, 'accumulated_logging_time': 0.20771169662475586}
I0328 00:44:06.748090 139832209684224 logging_writer.py:48] [7189] accumulated_eval_time=690.392875, accumulated_logging_time=0.207712, accumulated_submission_time=5820.772199, global_step=7189, preemption_count=0, score=5820.772199, test/ctc_loss=0.46206000447273254, test/num_examples=2472, test/wer=0.152601, total_duration=6511.645577, train/ctc_loss=0.40930986404418945, train/wer=0.143619, validation/ctc_loss=0.7136427164077759, validation/num_examples=5348, validation/wer=0.213194
I0328 00:48:09.864355 139831554324224 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.720621645450592, loss=1.7264772653579712
I0328 00:55:04.449006 139831545931520 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.6840469241142273, loss=1.5939505100250244
I0328 01:01:56.227150 139832209684224 logging_writer.py:48] [8500] global_step=8500, grad_norm=1.1360459327697754, loss=1.6563719511032104
I0328 01:08:07.329068 140002444732224 spec.py:321] Evaluating on the training split.
I0328 01:09:00.510942 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 01:09:51.916213 140002444732224 spec.py:349] Evaluating on the test split.
I0328 01:10:17.883716 140002444732224 submission_runner.py:422] Time since start: 8082.82s, 	Step: 8949, 	{'train/ctc_loss': Array(0.35587066, dtype=float32), 'train/wer': 0.12813933766770313, 'validation/ctc_loss': Array(0.6459704, dtype=float32), 'validation/wer': 0.19563223495563686, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41139576, dtype=float32), 'test/wer': 0.14155139845225764, 'test/num_examples': 2472, 'score': 7261.275091409683, 'total_duration': 8082.823434591293, 'accumulated_submission_time': 7261.275091409683, 'accumulated_eval_time': 820.9422154426575, 'accumulated_logging_time': 0.26285457611083984}
I0328 01:10:17.920320 139832209684224 logging_writer.py:48] [8949] accumulated_eval_time=820.942215, accumulated_logging_time=0.262855, accumulated_submission_time=7261.275091, global_step=8949, preemption_count=0, score=7261.275091, test/ctc_loss=0.41139575839042664, test/num_examples=2472, test/wer=0.141551, total_duration=8082.823435, train/ctc_loss=0.35587066411972046, train/wer=0.128139, validation/ctc_loss=0.6459704041481018, validation/num_examples=5348, validation/wer=0.195632
I0328 01:10:57.861817 139832201291520 logging_writer.py:48] [9000] global_step=9000, grad_norm=1.017083764076233, loss=1.6268062591552734
I0328 01:17:31.187353 139831554324224 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.9256991147994995, loss=1.5676416158676147
I0328 01:24:32.198771 139831545931520 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.5925813317298889, loss=1.5338425636291504
I0328 01:31:31.880029 139831554324224 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.6822331547737122, loss=1.5360591411590576
I0328 01:34:18.549038 140002444732224 spec.py:321] Evaluating on the training split.
I0328 01:35:11.556273 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 01:36:02.745392 140002444732224 spec.py:349] Evaluating on the test split.
I0328 01:36:28.539175 140002444732224 submission_runner.py:422] Time since start: 9653.48s, 	Step: 10718, 	{'train/ctc_loss': Array(0.3555755, dtype=float32), 'train/wer': 0.12436316690761852, 'validation/ctc_loss': Array(0.63100934, dtype=float32), 'validation/wer': 0.1882464253647045, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38889992, dtype=float32), 'test/wer': 0.13074563808827414, 'test/num_examples': 2472, 'score': 8701.822609901428, 'total_duration': 9653.478703975677, 'accumulated_submission_time': 8701.822609901428, 'accumulated_eval_time': 950.9268486499786, 'accumulated_logging_time': 0.31924915313720703}
I0328 01:36:28.575449 139831626004224 logging_writer.py:48] [10718] accumulated_eval_time=950.926849, accumulated_logging_time=0.319249, accumulated_submission_time=8701.822610, global_step=10718, preemption_count=0, score=8701.822610, test/ctc_loss=0.38889992237091064, test/num_examples=2472, test/wer=0.130746, total_duration=9653.478704, train/ctc_loss=0.3555755019187927, train/wer=0.124363, validation/ctc_loss=0.6310093402862549, validation/num_examples=5348, validation/wer=0.188246
I0328 01:40:06.191463 139831617611520 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.7210559844970703, loss=1.5174447298049927
I0328 01:47:08.183427 139831626004224 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6174716353416443, loss=1.541218638420105
I0328 01:54:04.363684 139831617611520 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.684281587600708, loss=1.4883339405059814
I0328 02:00:29.089854 140002444732224 spec.py:321] Evaluating on the training split.
I0328 02:01:22.849034 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 02:02:14.208576 140002444732224 spec.py:349] Evaluating on the test split.
I0328 02:02:39.969546 140002444732224 submission_runner.py:422] Time since start: 11224.91s, 	Step: 12441, 	{'train/ctc_loss': Array(0.30882823, dtype=float32), 'train/wer': 0.11118733116372068, 'validation/ctc_loss': Array(0.6040676, dtype=float32), 'validation/wer': 0.18083165181459204, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36773804, dtype=float32), 'test/wer': 0.12410375154875794, 'test/num_examples': 2472, 'score': 10142.258544921875, 'total_duration': 11224.908006191254, 'accumulated_submission_time': 10142.258544921875, 'accumulated_eval_time': 1081.8001081943512, 'accumulated_logging_time': 0.3719062805175781}
I0328 02:02:40.007145 139831626004224 logging_writer.py:48] [12441] accumulated_eval_time=1081.800108, accumulated_logging_time=0.371906, accumulated_submission_time=10142.258545, global_step=12441, preemption_count=0, score=10142.258545, test/ctc_loss=0.3677380383014679, test/num_examples=2472, test/wer=0.124104, total_duration=11224.908006, train/ctc_loss=0.3088282346725464, train/wer=0.111187, validation/ctc_loss=0.6040676236152649, validation/num_examples=5348, validation/wer=0.180832
I0328 02:03:26.096388 139831617611520 logging_writer.py:48] [12500] global_step=12500, grad_norm=1.1303433179855347, loss=1.5640947818756104
I0328 02:10:01.192765 139831626004224 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.7857480049133301, loss=1.495874047279358
I0328 02:17:17.864794 139831626004224 logging_writer.py:48] [13500] global_step=13500, grad_norm=2.249678134918213, loss=1.487253189086914
I0328 02:23:56.655039 139831617611520 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.9008256793022156, loss=1.4824272394180298
I0328 02:26:40.219288 140002444732224 spec.py:321] Evaluating on the training split.
I0328 02:27:35.182455 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 02:28:26.666374 140002444732224 spec.py:349] Evaluating on the test split.
I0328 02:28:52.454288 140002444732224 submission_runner.py:422] Time since start: 12797.39s, 	Step: 14190, 	{'train/ctc_loss': Array(0.2673932, dtype=float32), 'train/wer': 0.10080345302789984, 'validation/ctc_loss': Array(0.5752747, dtype=float32), 'validation/wer': 0.1739285748766618, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34668887, dtype=float32), 'test/wer': 0.11894461032234477, 'test/num_examples': 2472, 'score': 11582.391997814178, 'total_duration': 12797.393310070038, 'accumulated_submission_time': 11582.391997814178, 'accumulated_eval_time': 1214.0290806293488, 'accumulated_logging_time': 0.4271676540374756}
I0328 02:28:52.487253 139831626004224 logging_writer.py:48] [14190] accumulated_eval_time=1214.029081, accumulated_logging_time=0.427168, accumulated_submission_time=11582.391998, global_step=14190, preemption_count=0, score=11582.391998, test/ctc_loss=0.3466888666152954, test/num_examples=2472, test/wer=0.118945, total_duration=12797.393310, train/ctc_loss=0.26739320158958435, train/wer=0.100803, validation/ctc_loss=0.5752747058868408, validation/num_examples=5348, validation/wer=0.173929
I0328 02:32:55.250303 139831626004224 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.7122266888618469, loss=1.4637722969055176
I0328 02:39:34.963993 139831617611520 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.8950492143630981, loss=1.4489434957504272
I0328 02:46:53.164831 139831626004224 logging_writer.py:48] [15500] global_step=15500, grad_norm=1.5426453351974487, loss=1.408524990081787
I0328 02:52:53.229427 140002444732224 spec.py:321] Evaluating on the training split.
I0328 02:53:46.993505 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 02:54:37.717658 140002444732224 spec.py:349] Evaluating on the test split.
I0328 02:55:03.693150 140002444732224 submission_runner.py:422] Time since start: 14368.63s, 	Step: 15963, 	{'train/ctc_loss': Array(0.2514518, dtype=float32), 'train/wer': 0.09180237166766861, 'validation/ctc_loss': Array(0.5608643, dtype=float32), 'validation/wer': 0.16682275022447068, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33763376, dtype=float32), 'test/wer': 0.11276989011435419, 'test/num_examples': 2472, 'score': 13023.056012392044, 'total_duration': 14368.632350683212, 'accumulated_submission_time': 13023.056012392044, 'accumulated_eval_time': 1344.4869689941406, 'accumulated_logging_time': 0.47449159622192383}
I0328 02:55:03.741902 139831626004224 logging_writer.py:48] [15963] accumulated_eval_time=1344.486969, accumulated_logging_time=0.474492, accumulated_submission_time=13023.056012, global_step=15963, preemption_count=0, score=13023.056012, test/ctc_loss=0.33763375878334045, test/num_examples=2472, test/wer=0.112770, total_duration=14368.632351, train/ctc_loss=0.2514517903327942, train/wer=0.091802, validation/ctc_loss=0.5608643293380737, validation/num_examples=5348, validation/wer=0.166823
I0328 02:55:32.934784 139831617611520 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.9293757081031799, loss=1.4406872987747192
I0328 03:02:24.388171 139831626004224 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.9387122392654419, loss=1.4480376243591309
I0328 03:08:56.602119 139831617611520 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.7420834898948669, loss=1.4250460863113403
I0328 03:16:22.141768 139831626004224 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.9672583341598511, loss=1.349947214126587
I0328 03:19:03.915448 140002444732224 spec.py:321] Evaluating on the training split.
I0328 03:19:56.820076 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 03:20:47.342834 140002444732224 spec.py:349] Evaluating on the test split.
I0328 03:21:13.453541 140002444732224 submission_runner.py:422] Time since start: 15938.39s, 	Step: 17707, 	{'train/ctc_loss': Array(0.23556301, dtype=float32), 'train/wer': 0.08935093072768908, 'validation/ctc_loss': Array(0.5287409, dtype=float32), 'validation/wer': 0.1579211601031117, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32146436, dtype=float32), 'test/wer': 0.10712327097678387, 'test/num_examples': 2472, 'score': 14463.13798880577, 'total_duration': 15938.392670154572, 'accumulated_submission_time': 14463.13798880577, 'accumulated_eval_time': 1474.01935172081, 'accumulated_logging_time': 0.551727294921875}
I0328 03:21:13.484972 139832209684224 logging_writer.py:48] [17707] accumulated_eval_time=1474.019352, accumulated_logging_time=0.551727, accumulated_submission_time=14463.137989, global_step=17707, preemption_count=0, score=14463.137989, test/ctc_loss=0.3214643597602844, test/num_examples=2472, test/wer=0.107123, total_duration=15938.392670, train/ctc_loss=0.2355630099773407, train/wer=0.089351, validation/ctc_loss=0.5287408828735352, validation/num_examples=5348, validation/wer=0.157921
I0328 03:24:59.319310 139832201291520 logging_writer.py:48] [18000] global_step=18000, grad_norm=1.424534797668457, loss=1.362218976020813
I0328 03:32:09.300632 139832209684224 logging_writer.py:48] [18500] global_step=18500, grad_norm=1.0245392322540283, loss=1.3786782026290894
I0328 03:38:42.557827 139830177044224 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.6816341280937195, loss=1.3689405918121338
I0328 03:45:14.015214 140002444732224 spec.py:321] Evaluating on the training split.
I0328 03:46:08.548903 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 03:46:59.709874 140002444732224 spec.py:349] Evaluating on the test split.
I0328 03:47:25.661885 140002444732224 submission_runner.py:422] Time since start: 17510.60s, 	Step: 19447, 	{'train/ctc_loss': Array(0.25388935, dtype=float32), 'train/wer': 0.09018747190565589, 'validation/ctc_loss': Array(0.54082644, dtype=float32), 'validation/wer': 0.15938866736823812, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32557067, dtype=float32), 'test/wer': 0.10604675725631182, 'test/num_examples': 2472, 'score': 15903.59057211876, 'total_duration': 17510.60142850876, 'accumulated_submission_time': 15903.59057211876, 'accumulated_eval_time': 1605.6605203151703, 'accumulated_logging_time': 0.5993692874908447}
I0328 03:47:25.699269 139831626004224 logging_writer.py:48] [19447] accumulated_eval_time=1605.660520, accumulated_logging_time=0.599369, accumulated_submission_time=15903.590572, global_step=19447, preemption_count=0, score=15903.590572, test/ctc_loss=0.325570672750473, test/num_examples=2472, test/wer=0.106047, total_duration=17510.601429, train/ctc_loss=0.2538893520832062, train/wer=0.090187, validation/ctc_loss=0.5408264398574829, validation/num_examples=5348, validation/wer=0.159389
I0328 03:48:07.184015 139831617611520 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.7504755258560181, loss=1.3792444467544556
I0328 03:54:35.481206 139831626004224 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.72818922996521, loss=1.3781534433364868
I0328 04:01:49.973551 139831617611520 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.9615168571472168, loss=1.362154245376587
I0328 04:08:30.184679 139831626004224 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.7771939039230347, loss=1.3351094722747803
I0328 04:11:25.970227 140002444732224 spec.py:321] Evaluating on the training split.
I0328 04:12:20.299450 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 04:13:11.323179 140002444732224 spec.py:349] Evaluating on the test split.
I0328 04:13:37.090518 140002444732224 submission_runner.py:422] Time since start: 19082.03s, 	Step: 21204, 	{'train/ctc_loss': Array(0.23959489, dtype=float32), 'train/wer': 0.08956657081805472, 'validation/ctc_loss': Array(0.5109227, dtype=float32), 'validation/wer': 0.153200034756751, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30394173, dtype=float32), 'test/wer': 0.10257347713931712, 'test/num_examples': 2472, 'score': 17343.781029462814, 'total_duration': 19082.02796602249, 'accumulated_submission_time': 17343.781029462814, 'accumulated_eval_time': 1736.7732248306274, 'accumulated_logging_time': 0.6557137966156006}
I0328 04:13:37.135065 139831917844224 logging_writer.py:48] [21204] accumulated_eval_time=1736.773225, accumulated_logging_time=0.655714, accumulated_submission_time=17343.781029, global_step=21204, preemption_count=0, score=17343.781029, test/ctc_loss=0.3039417266845703, test/num_examples=2472, test/wer=0.102573, total_duration=19082.027966, train/ctc_loss=0.23959489166736603, train/wer=0.089567, validation/ctc_loss=0.5109226703643799, validation/num_examples=5348, validation/wer=0.153200
I0328 04:17:25.675631 139831909451520 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.8575328588485718, loss=1.388685941696167
I0328 04:24:06.662014 139831917844224 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.9876412153244019, loss=1.3890551328659058
I0328 04:31:12.208059 139831909451520 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.6726641058921814, loss=1.364639401435852
I0328 04:37:37.634523 140002444732224 spec.py:321] Evaluating on the training split.
I0328 04:38:32.174337 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 04:39:23.012432 140002444732224 spec.py:349] Evaluating on the test split.
I0328 04:39:48.695686 140002444732224 submission_runner.py:422] Time since start: 20653.63s, 	Step: 22977, 	{'train/ctc_loss': Array(0.21597603, dtype=float32), 'train/wer': 0.08100113180368351, 'validation/ctc_loss': Array(0.5011605, dtype=float32), 'validation/wer': 0.14936713749191421, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29222873, dtype=float32), 'test/wer': 0.09871427700932302, 'test/num_examples': 2472, 'score': 18784.199924230576, 'total_duration': 20653.634942293167, 'accumulated_submission_time': 18784.199924230576, 'accumulated_eval_time': 1867.8287971019745, 'accumulated_logging_time': 0.7179415225982666}
I0328 04:39:48.734364 139831375124224 logging_writer.py:48] [22977] accumulated_eval_time=1867.828797, accumulated_logging_time=0.717942, accumulated_submission_time=18784.199924, global_step=22977, preemption_count=0, score=18784.199924, test/ctc_loss=0.29222872853279114, test/num_examples=2472, test/wer=0.098714, total_duration=20653.634942, train/ctc_loss=0.2159760296344757, train/wer=0.081001, validation/ctc_loss=0.5011605024337769, validation/num_examples=5348, validation/wer=0.149367
I0328 04:40:07.239131 139831366731520 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.7879232168197632, loss=1.3401647806167603
I0328 04:46:53.192914 139831375124224 logging_writer.py:48] [23500] global_step=23500, grad_norm=1.235954761505127, loss=1.2627482414245605
I0328 04:53:39.842347 139831917844224 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.7139050960540771, loss=1.305980920791626
I0328 05:00:50.464466 139831909451520 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.9506497979164124, loss=1.2909306287765503
I0328 05:03:49.381988 140002444732224 spec.py:321] Evaluating on the training split.
I0328 05:04:43.759238 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 05:05:35.573667 140002444732224 spec.py:349] Evaluating on the test split.
I0328 05:06:01.427922 140002444732224 submission_runner.py:422] Time since start: 22226.37s, 	Step: 24700, 	{'train/ctc_loss': Array(0.19944268, dtype=float32), 'train/wer': 0.07526603606266627, 'validation/ctc_loss': Array(0.48467597, dtype=float32), 'validation/wer': 0.1470500207575041, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28198373, dtype=float32), 'test/wer': 0.09668311904616822, 'test/num_examples': 2472, 'score': 20224.769287347794, 'total_duration': 22226.36704683304, 'accumulated_submission_time': 20224.769287347794, 'accumulated_eval_time': 1999.8688099384308, 'accumulated_logging_time': 0.7733640670776367}
I0328 05:06:01.476045 139832209684224 logging_writer.py:48] [24700] accumulated_eval_time=1999.868810, accumulated_logging_time=0.773364, accumulated_submission_time=20224.769287, global_step=24700, preemption_count=0, score=20224.769287, test/ctc_loss=0.28198373317718506, test/num_examples=2472, test/wer=0.096683, total_duration=22226.367047, train/ctc_loss=0.19944268465042114, train/wer=0.075266, validation/ctc_loss=0.48467597365379333, validation/num_examples=5348, validation/wer=0.147050
I0328 05:09:56.235374 139830177044224 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.8564397692680359, loss=1.364748239517212
I0328 05:17:07.592941 139830168651520 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.6346674561500549, loss=1.29249107837677
I0328 05:24:01.869010 139832209684224 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.8428468108177185, loss=1.2715591192245483
I0328 05:30:01.886529 140002444732224 spec.py:321] Evaluating on the training split.
I0328 05:30:56.469657 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 05:31:47.729505 140002444732224 spec.py:349] Evaluating on the test split.
I0328 05:32:13.552055 140002444732224 submission_runner.py:422] Time since start: 23798.49s, 	Step: 26447, 	{'train/ctc_loss': Array(0.19055541, dtype=float32), 'train/wer': 0.07278228435796448, 'validation/ctc_loss': Array(0.474879, dtype=float32), 'validation/wer': 0.14236751402338357, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2783278, dtype=float32), 'test/wer': 0.09357544736254138, 'test/num_examples': 2472, 'score': 21665.098954916, 'total_duration': 23798.49133992195, 'accumulated_submission_time': 21665.098954916, 'accumulated_eval_time': 2131.528559446335, 'accumulated_logging_time': 0.8407466411590576}
I0328 05:32:13.593060 139831410964224 logging_writer.py:48] [26447] accumulated_eval_time=2131.528559, accumulated_logging_time=0.840747, accumulated_submission_time=21665.098955, global_step=26447, preemption_count=0, score=21665.098955, test/ctc_loss=0.2783277928829193, test/num_examples=2472, test/wer=0.093575, total_duration=23798.491340, train/ctc_loss=0.1905554085969925, train/wer=0.072782, validation/ctc_loss=0.4748789966106415, validation/num_examples=5348, validation/wer=0.142368
I0328 05:32:55.021577 139831402571520 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.861873209476471, loss=1.2591853141784668
I0328 05:39:22.556005 139831410964224 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.9679946899414062, loss=1.349602460861206
I0328 05:46:12.208312 139831402571520 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.2740917205810547, loss=1.3642139434814453
I0328 05:53:15.431338 139831410964224 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.6939437389373779, loss=1.2484735250473022
I0328 05:56:14.296879 140002444732224 spec.py:321] Evaluating on the training split.
I0328 05:57:09.236220 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 05:58:00.203044 140002444732224 spec.py:349] Evaluating on the test split.
I0328 05:58:25.763006 140002444732224 submission_runner.py:422] Time since start: 25370.70s, 	Step: 28234, 	{'train/ctc_loss': Array(0.18396299, dtype=float32), 'train/wer': 0.0676376430638942, 'validation/ctc_loss': Array(0.46728623, dtype=float32), 'validation/wer': 0.1378105177790436, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27716658, dtype=float32), 'test/wer': 0.09225519468649077, 'test/num_examples': 2472, 'score': 23105.721066474915, 'total_duration': 25370.701897144318, 'accumulated_submission_time': 23105.721066474915, 'accumulated_eval_time': 2262.988550186157, 'accumulated_logging_time': 0.8999254703521729}
I0328 05:58:25.804514 139831114000128 logging_writer.py:48] [28234] accumulated_eval_time=2262.988550, accumulated_logging_time=0.899925, accumulated_submission_time=23105.721066, global_step=28234, preemption_count=0, score=23105.721066, test/ctc_loss=0.27716657519340515, test/num_examples=2472, test/wer=0.092255, total_duration=25370.701897, train/ctc_loss=0.18396298587322235, train/wer=0.067638, validation/ctc_loss=0.46728622913360596, validation/num_examples=5348, validation/wer=0.137811
I0328 06:01:50.988523 139831105607424 logging_writer.py:48] [28500] global_step=28500, grad_norm=1.3381532430648804, loss=1.2674529552459717
I0328 06:08:49.604098 139831114000128 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.7577853798866272, loss=1.2451151609420776
I0328 06:15:29.705911 139831105607424 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.6205768585205078, loss=1.2838904857635498
I0328 06:22:26.188032 140002444732224 spec.py:321] Evaluating on the training split.
I0328 06:23:19.302326 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 06:24:11.216731 140002444732224 spec.py:349] Evaluating on the test split.
I0328 06:24:37.131084 140002444732224 submission_runner.py:422] Time since start: 26942.07s, 	Step: 29992, 	{'train/ctc_loss': Array(0.20105429, dtype=float32), 'train/wer': 0.07587428497139885, 'validation/ctc_loss': Array(0.46363172, dtype=float32), 'validation/wer': 0.1387663284319878, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27467206, dtype=float32), 'test/wer': 0.09264111469949018, 'test/num_examples': 2472, 'score': 24546.02538204193, 'total_duration': 26942.068878173828, 'accumulated_submission_time': 24546.02538204193, 'accumulated_eval_time': 2393.9246048927307, 'accumulated_logging_time': 0.957648515701294}
I0328 06:24:37.175282 139830571280128 logging_writer.py:48] [29992] accumulated_eval_time=2393.924605, accumulated_logging_time=0.957649, accumulated_submission_time=24546.025382, global_step=29992, preemption_count=0, score=24546.025382, test/ctc_loss=0.2746720612049103, test/num_examples=2472, test/wer=0.092641, total_duration=26942.068878, train/ctc_loss=0.20105428993701935, train/wer=0.075874, validation/ctc_loss=0.4636317193508148, validation/num_examples=5348, validation/wer=0.138766
I0328 06:24:44.158242 139830562887424 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.7508001923561096, loss=1.2795823812484741
I0328 06:31:17.622230 139830571280128 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.0141545534133911, loss=1.267956018447876
I0328 06:38:32.359479 139830571280128 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.9612778425216675, loss=1.2621591091156006
I0328 06:45:18.331993 139830562887424 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.7362949252128601, loss=1.2320892810821533
I0328 06:48:37.634767 140002444732224 spec.py:321] Evaluating on the training split.
I0328 06:49:31.339875 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 06:50:22.712802 140002444732224 spec.py:349] Evaluating on the test split.
I0328 06:50:48.844580 140002444732224 submission_runner.py:422] Time since start: 28513.78s, 	Step: 31728, 	{'train/ctc_loss': Array(0.17956269, dtype=float32), 'train/wer': 0.06554566221194906, 'validation/ctc_loss': Array(0.45461053, dtype=float32), 'validation/wer': 0.13447966247332901, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25810274, dtype=float32), 'test/wer': 0.08628359027481568, 'test/num_examples': 2472, 'score': 25986.407766342163, 'total_duration': 28513.783675193787, 'accumulated_submission_time': 25986.407766342163, 'accumulated_eval_time': 2525.128483772278, 'accumulated_logging_time': 1.0193750858306885}
I0328 06:50:48.879056 139830817036032 logging_writer.py:48] [31728] accumulated_eval_time=2525.128484, accumulated_logging_time=1.019375, accumulated_submission_time=25986.407766, global_step=31728, preemption_count=0, score=25986.407766, test/ctc_loss=0.25810274481773376, test/num_examples=2472, test/wer=0.086284, total_duration=28513.783675, train/ctc_loss=0.17956268787384033, train/wer=0.065546, validation/ctc_loss=0.45461052656173706, validation/num_examples=5348, validation/wer=0.134480
I0328 06:54:22.212457 139830817036032 logging_writer.py:48] [32000] global_step=32000, grad_norm=3.0436391830444336, loss=1.2160663604736328
I0328 07:01:03.145977 139830808643328 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.665556788444519, loss=1.2422977685928345
I0328 07:08:27.871406 139830161676032 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.9429477453231812, loss=1.3002506494522095
I0328 07:14:49.179760 140002444732224 spec.py:321] Evaluating on the training split.
I0328 07:15:44.385632 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 07:16:35.428031 140002444732224 spec.py:349] Evaluating on the test split.
I0328 07:17:00.927368 140002444732224 submission_runner.py:422] Time since start: 30085.87s, 	Step: 33488, 	{'train/ctc_loss': Array(0.18582933, dtype=float32), 'train/wer': 0.06800260955776215, 'validation/ctc_loss': Array(0.4406355, dtype=float32), 'validation/wer': 0.1310715699431341, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25114754, dtype=float32), 'test/wer': 0.08478053338208112, 'test/num_examples': 2472, 'score': 27426.630671024323, 'total_duration': 30085.866382598877, 'accumulated_submission_time': 27426.630671024323, 'accumulated_eval_time': 2656.8701095581055, 'accumulated_logging_time': 1.0686450004577637}
I0328 07:17:00.970307 139830817036032 logging_writer.py:48] [33488] accumulated_eval_time=2656.870110, accumulated_logging_time=1.068645, accumulated_submission_time=27426.630671, global_step=33488, preemption_count=0, score=27426.630671, test/ctc_loss=0.2511475384235382, test/num_examples=2472, test/wer=0.084781, total_duration=30085.866383, train/ctc_loss=0.18582932651042938, train/wer=0.068003, validation/ctc_loss=0.4406355023384094, validation/num_examples=5348, validation/wer=0.131072
I0328 07:17:11.031301 139830808643328 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.6642632484436035, loss=1.228548526763916
I0328 07:24:01.947767 139830817036032 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.6740873456001282, loss=1.2556771039962769
I0328 07:30:35.330738 139830808643328 logging_writer.py:48] [34500] global_step=34500, grad_norm=1.1659412384033203, loss=1.2027171850204468
I0328 07:38:00.899328 139830817036032 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.6767598986625671, loss=1.208470106124878
I0328 07:41:01.664587 140002444732224 spec.py:321] Evaluating on the training split.
I0328 07:41:55.328599 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 07:42:46.316753 140002444732224 spec.py:349] Evaluating on the test split.
I0328 07:43:12.127247 140002444732224 submission_runner.py:422] Time since start: 31657.07s, 	Step: 35229, 	{'train/ctc_loss': Array(0.17369977, dtype=float32), 'train/wer': 0.06156420175410552, 'validation/ctc_loss': Array(0.42272478, dtype=float32), 'validation/wer': 0.123058207903299, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2449416, dtype=float32), 'test/wer': 0.08065728271687689, 'test/num_examples': 2472, 'score': 28867.240092277527, 'total_duration': 31657.06634402275, 'accumulated_submission_time': 28867.240092277527, 'accumulated_eval_time': 2787.3269016742706, 'accumulated_logging_time': 1.1331031322479248}
I0328 07:43:12.161665 139831114000128 logging_writer.py:48] [35229] accumulated_eval_time=2787.326902, accumulated_logging_time=1.133103, accumulated_submission_time=28867.240092, global_step=35229, preemption_count=0, score=28867.240092, test/ctc_loss=0.2449416071176529, test/num_examples=2472, test/wer=0.080657, total_duration=31657.066344, train/ctc_loss=0.1736997663974762, train/wer=0.061564, validation/ctc_loss=0.42272478342056274, validation/num_examples=5348, validation/wer=0.123058
I0328 07:46:41.311355 139831105607424 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.7486905455589294, loss=1.1987357139587402
I0328 07:53:53.243387 139831114000128 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.820453405380249, loss=1.186084508895874
I0328 08:00:27.331902 139830243600128 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.4573129415512085, loss=1.2052788734436035
I0328 08:07:12.723253 140002444732224 spec.py:321] Evaluating on the training split.
I0328 08:08:07.650790 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 08:08:58.964471 140002444732224 spec.py:349] Evaluating on the test split.
I0328 08:09:24.960361 140002444732224 submission_runner.py:422] Time since start: 33229.90s, 	Step: 36958, 	{'train/ctc_loss': Array(0.1301626, dtype=float32), 'train/wer': 0.04947572337873989, 'validation/ctc_loss': Array(0.41261932, dtype=float32), 'validation/wer': 0.12281684157679794, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23866884, dtype=float32), 'test/wer': 0.07974326163345724, 'test/num_examples': 2472, 'score': 30307.72377705574, 'total_duration': 33229.89807128906, 'accumulated_submission_time': 30307.72377705574, 'accumulated_eval_time': 2919.556672334671, 'accumulated_logging_time': 1.184589147567749}
I0328 08:09:25.006453 139830243600128 logging_writer.py:48] [36958] accumulated_eval_time=2919.556672, accumulated_logging_time=1.184589, accumulated_submission_time=30307.723777, global_step=36958, preemption_count=0, score=30307.723777, test/ctc_loss=0.23866884410381317, test/num_examples=2472, test/wer=0.079743, total_duration=33229.898071, train/ctc_loss=0.13016259670257568, train/wer=0.049476, validation/ctc_loss=0.41261932253837585, validation/num_examples=5348, validation/wer=0.122817
I0328 08:09:58.047679 139830235207424 logging_writer.py:48] [37000] global_step=37000, grad_norm=1.0341691970825195, loss=1.1592347621917725
I0328 08:16:32.685324 139831114000128 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.6044324040412903, loss=1.197161316871643
I0328 08:23:54.306016 139831105607424 logging_writer.py:48] [38000] global_step=38000, grad_norm=1.801181674003601, loss=1.150858759880066
I0328 08:30:35.992309 139831114000128 logging_writer.py:48] [38500] global_step=38500, grad_norm=1.303269624710083, loss=1.1671332120895386
I0328 08:33:25.329245 140002444732224 spec.py:321] Evaluating on the training split.
I0328 08:34:19.464480 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 08:35:10.141032 140002444732224 spec.py:349] Evaluating on the test split.
I0328 08:35:35.663187 140002444732224 submission_runner.py:422] Time since start: 34800.60s, 	Step: 38702, 	{'train/ctc_loss': Array(0.14963634, dtype=float32), 'train/wer': 0.05615509704124517, 'validation/ctc_loss': Array(0.41541314, dtype=float32), 'validation/wer': 0.12419745696438399, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23655531, dtype=float32), 'test/wer': 0.08000731216866735, 'test/num_examples': 2472, 'score': 31747.961825847626, 'total_duration': 34800.60106897354, 'accumulated_submission_time': 31747.961825847626, 'accumulated_eval_time': 3049.883457660675, 'accumulated_logging_time': 1.2530457973480225}
I0328 08:35:35.703437 139830601996032 logging_writer.py:48] [38702] accumulated_eval_time=3049.883458, accumulated_logging_time=1.253046, accumulated_submission_time=31747.961826, global_step=38702, preemption_count=0, score=31747.961826, test/ctc_loss=0.2365553081035614, test/num_examples=2472, test/wer=0.080007, total_duration=34800.601069, train/ctc_loss=0.14963634312152863, train/wer=0.056155, validation/ctc_loss=0.41541314125061035, validation/num_examples=5348, validation/wer=0.124197
I0328 08:39:25.548043 139830593603328 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.7020601630210876, loss=1.1573073863983154
I0328 08:46:02.781045 139830601996032 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.5068045854568481, loss=1.137589931488037
I0328 08:53:16.756569 139830593603328 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.4350024461746216, loss=1.1171702146530151
I0328 08:59:35.824392 140002444732224 spec.py:321] Evaluating on the training split.
I0328 09:00:29.215354 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 09:01:20.837280 140002444732224 spec.py:349] Evaluating on the test split.
I0328 09:01:46.388659 140002444732224 submission_runner.py:422] Time since start: 36371.33s, 	Step: 40465, 	{'train/ctc_loss': Array(0.17602299, dtype=float32), 'train/wer': 0.06591344250202047, 'validation/ctc_loss': Array(0.4037802, dtype=float32), 'validation/wer': 0.11797020574065671, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22502472, dtype=float32), 'test/wer': 0.07482785936262264, 'test/num_examples': 2472, 'score': 33188.00236105919, 'total_duration': 36371.32774734497, 'accumulated_submission_time': 33188.00236105919, 'accumulated_eval_time': 3180.441960334778, 'accumulated_logging_time': 1.3102242946624756}
I0328 09:01:46.434176 139830817036032 logging_writer.py:48] [40465] accumulated_eval_time=3180.441960, accumulated_logging_time=1.310224, accumulated_submission_time=33188.002361, global_step=40465, preemption_count=0, score=33188.002361, test/ctc_loss=0.22502471506595612, test/num_examples=2472, test/wer=0.074828, total_duration=36371.327747, train/ctc_loss=0.1760229915380478, train/wer=0.065913, validation/ctc_loss=0.4037801921367645, validation/num_examples=5348, validation/wer=0.117970
I0328 09:02:14.276037 139830808643328 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.6693512201309204, loss=1.1763991117477417
I0328 09:09:03.711678 139830817036032 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.7361025810241699, loss=1.1876343488693237
I0328 09:15:55.967980 139830817036032 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.9222940802574158, loss=1.1326630115509033
I0328 09:23:05.670898 139830808643328 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.9560531973838806, loss=1.1566290855407715
I0328 09:25:47.030507 140002444732224 spec.py:321] Evaluating on the training split.
I0328 09:26:39.734831 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 09:27:31.247348 140002444732224 spec.py:349] Evaluating on the test split.
I0328 09:27:57.268974 140002444732224 submission_runner.py:422] Time since start: 37942.21s, 	Step: 42185, 	{'train/ctc_loss': Array(0.17704529, dtype=float32), 'train/wer': 0.06581102161132915, 'validation/ctc_loss': Array(0.39160708, dtype=float32), 'validation/wer': 0.11502553655734381, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22013047, dtype=float32), 'test/wer': 0.07287794771799402, 'test/num_examples': 2472, 'score': 34628.517916202545, 'total_duration': 37942.206201314926, 'accumulated_submission_time': 34628.517916202545, 'accumulated_eval_time': 3310.6726155281067, 'accumulated_logging_time': 1.3750653266906738}
I0328 09:27:57.311676 139830817036032 logging_writer.py:48] [42185] accumulated_eval_time=3310.672616, accumulated_logging_time=1.375065, accumulated_submission_time=34628.517916, global_step=42185, preemption_count=0, score=34628.517916, test/ctc_loss=0.22013047337532043, test/num_examples=2472, test/wer=0.072878, total_duration=37942.206201, train/ctc_loss=0.1770452857017517, train/wer=0.065811, validation/ctc_loss=0.3916070759296417, validation/num_examples=5348, validation/wer=0.115026
I0328 09:32:03.475195 139830817036032 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.1550482511520386, loss=1.1192559003829956
I0328 09:39:10.265808 139830808643328 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.727622389793396, loss=1.1037629842758179
I0328 09:46:07.046894 139830817036032 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.5342845916748047, loss=1.0483944416046143
I0328 09:51:57.311717 140002444732224 spec.py:321] Evaluating on the training split.
I0328 09:52:48.751274 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 09:53:40.296933 140002444732224 spec.py:349] Evaluating on the test split.
I0328 09:54:06.422098 140002444732224 submission_runner.py:422] Time since start: 39511.36s, 	Step: 43922, 	{'train/ctc_loss': Array(0.19677651, dtype=float32), 'train/wer': 0.07444238407496885, 'validation/ctc_loss': Array(0.38611034, dtype=float32), 'validation/wer': 0.11226430578217171, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21493606, dtype=float32), 'test/wer': 0.07033900026405053, 'test/num_examples': 2472, 'score': 36068.43892288208, 'total_duration': 39511.36161732674, 'accumulated_submission_time': 36068.43892288208, 'accumulated_eval_time': 3439.7774698734283, 'accumulated_logging_time': 1.4349782466888428}
I0328 09:54:06.460754 139830898960128 logging_writer.py:48] [43922] accumulated_eval_time=3439.777470, accumulated_logging_time=1.434978, accumulated_submission_time=36068.438923, global_step=43922, preemption_count=0, score=36068.438923, test/ctc_loss=0.2149360626935959, test/num_examples=2472, test/wer=0.070339, total_duration=39511.361617, train/ctc_loss=0.19677650928497314, train/wer=0.074442, validation/ctc_loss=0.3861103355884552, validation/num_examples=5348, validation/wer=0.112264
I0328 09:55:07.333913 139830890567424 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.86810302734375, loss=1.133246898651123
I0328 10:01:45.483474 139830898960128 logging_writer.py:48] [44500] global_step=44500, grad_norm=1.699091911315918, loss=1.0613335371017456
I0328 10:08:48.091389 139830890567424 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.0964356660842896, loss=1.1238023042678833
I0328 10:15:57.024518 139830898960128 logging_writer.py:48] [45500] global_step=45500, grad_norm=1.9803599119186401, loss=1.0736134052276611
I0328 10:18:06.622411 140002444732224 spec.py:321] Evaluating on the training split.
I0328 10:18:57.872554 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 10:19:48.907162 140002444732224 spec.py:349] Evaluating on the test split.
I0328 10:20:14.874395 140002444732224 submission_runner.py:422] Time since start: 41079.81s, 	Step: 45670, 	{'train/ctc_loss': Array(0.16540317, dtype=float32), 'train/wer': 0.06011221732244523, 'validation/ctc_loss': Array(0.36965913, dtype=float32), 'validation/wer': 0.10758179904805121, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20512938, dtype=float32), 'test/wer': 0.06749537911563382, 'test/num_examples': 2472, 'score': 37508.522341012955, 'total_duration': 41079.81269240379, 'accumulated_submission_time': 37508.522341012955, 'accumulated_eval_time': 3568.022716522217, 'accumulated_logging_time': 1.490485429763794}
I0328 10:20:14.915662 139830601996032 logging_writer.py:48] [45670] accumulated_eval_time=3568.022717, accumulated_logging_time=1.490485, accumulated_submission_time=37508.522341, global_step=45670, preemption_count=0, score=37508.522341, test/ctc_loss=0.20512938499450684, test/num_examples=2472, test/wer=0.067495, total_duration=41079.812692, train/ctc_loss=0.16540317237377167, train/wer=0.060112, validation/ctc_loss=0.3696591258049011, validation/num_examples=5348, validation/wer=0.107582
I0328 10:24:37.231107 139830593603328 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.7213670015335083, loss=1.0818297863006592
I0328 10:31:42.673038 139830274316032 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.8537087440490723, loss=1.0814900398254395
I0328 10:38:28.085201 139830265923328 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.1419336795806885, loss=1.0146057605743408
I0328 10:44:15.426809 140002444732224 spec.py:321] Evaluating on the training split.
I0328 10:45:08.208452 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 10:45:59.782421 140002444732224 spec.py:349] Evaluating on the test split.
I0328 10:46:25.819651 140002444732224 submission_runner.py:422] Time since start: 42650.76s, 	Step: 47396, 	{'train/ctc_loss': Array(0.14902788, dtype=float32), 'train/wer': 0.056166701925111064, 'validation/ctc_loss': Array(0.368786, dtype=float32), 'validation/wer': 0.10731146876237002, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20425586, dtype=float32), 'test/wer': 0.06607356854142547, 'test/num_examples': 2472, 'score': 38948.955758333206, 'total_duration': 42650.75806093216, 'accumulated_submission_time': 38948.955758333206, 'accumulated_eval_time': 3698.4091176986694, 'accumulated_logging_time': 1.5476596355438232}
I0328 10:46:25.864932 139830601996032 logging_writer.py:48] [47396] accumulated_eval_time=3698.409118, accumulated_logging_time=1.547660, accumulated_submission_time=38948.955758, global_step=47396, preemption_count=0, score=38948.955758, test/ctc_loss=0.2042558640241623, test/num_examples=2472, test/wer=0.066074, total_duration=42650.758061, train/ctc_loss=0.14902788400650024, train/wer=0.056167, validation/ctc_loss=0.3687860071659088, validation/num_examples=5348, validation/wer=0.107311
I0328 10:47:46.546941 139830593603328 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.9642901420593262, loss=1.0477545261383057
I0328 10:54:18.175175 139830601996032 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.4038583040237427, loss=1.05958890914917
I0328 11:01:34.156882 139830601996032 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.7957111597061157, loss=1.026029109954834
I0328 11:08:19.034437 139830593603328 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.1352766752243042, loss=1.0245519876480103
I0328 11:10:26.825026 140002444732224 spec.py:321] Evaluating on the training split.
I0328 11:11:20.948725 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 11:12:12.720213 140002444732224 spec.py:349] Evaluating on the test split.
I0328 11:12:38.376202 140002444732224 submission_runner.py:422] Time since start: 44223.32s, 	Step: 49143, 	{'train/ctc_loss': Array(0.12587143, dtype=float32), 'train/wer': 0.04756441070593481, 'validation/ctc_loss': Array(0.36090443, dtype=float32), 'validation/wer': 0.10338202496693281, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20120494, dtype=float32), 'test/wer': 0.06664229277110881, 'test/num_examples': 2472, 'score': 40389.83613157272, 'total_duration': 44223.31549620628, 'accumulated_submission_time': 40389.83613157272, 'accumulated_eval_time': 3829.954564809799, 'accumulated_logging_time': 1.6105077266693115}
I0328 11:12:38.415542 139832209684224 logging_writer.py:48] [49143] accumulated_eval_time=3829.954565, accumulated_logging_time=1.610508, accumulated_submission_time=40389.836132, global_step=49143, preemption_count=0, score=40389.836132, test/ctc_loss=0.20120494067668915, test/num_examples=2472, test/wer=0.066642, total_duration=44223.315496, train/ctc_loss=0.1258714348077774, train/wer=0.047564, validation/ctc_loss=0.36090442538261414, validation/num_examples=5348, validation/wer=0.103382
I0328 11:17:19.229894 139831451924224 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.7176867723464966, loss=1.0138574838638306
I0328 11:24:02.049246 139831443531520 logging_writer.py:48] [50000] global_step=50000, grad_norm=2.4462504386901855, loss=0.9921932220458984
I0328 11:31:21.790907 139831451924224 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.7625987529754639, loss=1.0663882493972778
I0328 11:36:38.889565 140002444732224 spec.py:321] Evaluating on the training split.
I0328 11:37:31.853508 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 11:38:22.581786 140002444732224 spec.py:349] Evaluating on the test split.
I0328 11:38:48.318996 140002444732224 submission_runner.py:422] Time since start: 45793.26s, 	Step: 50914, 	{'train/ctc_loss': Array(0.13706723, dtype=float32), 'train/wer': 0.051920685179251275, 'validation/ctc_loss': Array(0.3541431, dtype=float32), 'validation/wer': 0.1030151481506512, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19621822, dtype=float32), 'test/wer': 0.06396116425974448, 'test/num_examples': 2472, 'score': 41830.231493234634, 'total_duration': 45793.258259534836, 'accumulated_submission_time': 41830.231493234634, 'accumulated_eval_time': 3959.3782358169556, 'accumulated_logging_time': 1.6653227806091309}
I0328 11:38:48.358134 139831882004224 logging_writer.py:48] [50914] accumulated_eval_time=3959.378236, accumulated_logging_time=1.665323, accumulated_submission_time=41830.231493, global_step=50914, preemption_count=0, score=41830.231493, test/ctc_loss=0.19621822237968445, test/num_examples=2472, test/wer=0.063961, total_duration=45793.258260, train/ctc_loss=0.13706722855567932, train/wer=0.051921, validation/ctc_loss=0.3541431128978729, validation/num_examples=5348, validation/wer=0.103015
I0328 11:39:55.235320 139831873611520 logging_writer.py:48] [51000] global_step=51000, grad_norm=1.961382269859314, loss=1.0299956798553467
I0328 11:46:53.941270 139831124244224 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.8584114909172058, loss=1.063395619392395
I0328 11:53:29.599268 139831115851520 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.8234687447547913, loss=1.0145360231399536
I0328 12:00:56.698582 139831124244224 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.4461950063705444, loss=0.9843364357948303
I0328 12:02:48.942105 140002444732224 spec.py:321] Evaluating on the training split.
I0328 12:03:42.069880 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 12:04:34.912588 140002444732224 spec.py:349] Evaluating on the test split.
I0328 12:05:00.826866 140002444732224 submission_runner.py:422] Time since start: 47365.77s, 	Step: 52639, 	{'train/ctc_loss': Array(0.1219428, dtype=float32), 'train/wer': 0.04634620340147741, 'validation/ctc_loss': Array(0.348766, dtype=float32), 'validation/wer': 0.10078492329378144, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19256645, dtype=float32), 'test/wer': 0.06274246948185161, 'test/num_examples': 2472, 'score': 43270.73646521568, 'total_duration': 47365.765083789825, 'accumulated_submission_time': 43270.73646521568, 'accumulated_eval_time': 4091.2563354969025, 'accumulated_logging_time': 1.7211670875549316}
I0328 12:05:00.868390 139831554324224 logging_writer.py:48] [52639] accumulated_eval_time=4091.256335, accumulated_logging_time=1.721167, accumulated_submission_time=43270.736465, global_step=52639, preemption_count=0, score=43270.736465, test/ctc_loss=0.19256645441055298, test/num_examples=2472, test/wer=0.062742, total_duration=47365.765084, train/ctc_loss=0.12194279581308365, train/wer=0.046346, validation/ctc_loss=0.3487659990787506, validation/num_examples=5348, validation/wer=0.100785
I0328 12:09:39.135549 139831545931520 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.7318806052207947, loss=1.0191446542739868
I0328 12:16:57.724695 139831554324224 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.8196479678153992, loss=1.0551989078521729
I0328 12:23:36.112282 139831226644224 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.6777312755584717, loss=0.9555473923683167
I0328 12:29:01.376825 140002444732224 spec.py:321] Evaluating on the training split.
I0328 12:29:54.992724 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 12:30:45.902472 140002444732224 spec.py:349] Evaluating on the test split.
I0328 12:31:11.990985 140002444732224 submission_runner.py:422] Time since start: 48936.93s, 	Step: 54370, 	{'train/ctc_loss': Array(0.12319872, dtype=float32), 'train/wer': 0.04637173325811736, 'validation/ctc_loss': Array(0.3454306, dtype=float32), 'validation/wer': 0.09896984851849348, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1905708, dtype=float32), 'test/wer': 0.061239412589117054, 'test/num_examples': 2472, 'score': 44711.165451049805, 'total_duration': 48936.92869734764, 'accumulated_submission_time': 44711.165451049805, 'accumulated_eval_time': 4221.863181114197, 'accumulated_logging_time': 1.7806179523468018}
I0328 12:31:12.039587 139830642964224 logging_writer.py:48] [54370] accumulated_eval_time=4221.863181, accumulated_logging_time=1.780618, accumulated_submission_time=44711.165451, global_step=54370, preemption_count=0, score=44711.165451, test/ctc_loss=0.19057080149650574, test/num_examples=2472, test/wer=0.061239, total_duration=48936.928697, train/ctc_loss=0.12319871783256531, train/wer=0.046372, validation/ctc_loss=0.3454306125640869, validation/num_examples=5348, validation/wer=0.098970
I0328 12:32:52.737373 139830634571520 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.9535242319107056, loss=1.042961597442627
I0328 12:39:20.832350 139830315284224 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.5817952752113342, loss=1.0396242141723633
I0328 12:46:40.041982 139830306891520 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.3013843297958374, loss=1.0201538801193237
I0328 12:53:23.004384 139831554324224 logging_writer.py:48] [56000] global_step=56000, grad_norm=5.022093772888184, loss=1.024786114692688
I0328 12:55:12.337923 140002444732224 spec.py:321] Evaluating on the training split.
I0328 12:56:05.911683 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 12:56:57.031739 140002444732224 spec.py:349] Evaluating on the test split.
I0328 12:57:22.797188 140002444732224 submission_runner.py:422] Time since start: 50507.74s, 	Step: 56132, 	{'train/ctc_loss': Array(0.11897294, dtype=float32), 'train/wer': 0.04465424430641822, 'validation/ctc_loss': Array(0.342649, dtype=float32), 'validation/wer': 0.09799472855942921, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18938068, dtype=float32), 'test/wer': 0.06042694940385514, 'test/num_examples': 2472, 'score': 46151.37986826897, 'total_duration': 50507.73611855507, 'accumulated_submission_time': 46151.37986826897, 'accumulated_eval_time': 4352.316321372986, 'accumulated_logging_time': 1.8505048751831055}
I0328 12:57:22.841540 139831779604224 logging_writer.py:48] [56132] accumulated_eval_time=4352.316321, accumulated_logging_time=1.850505, accumulated_submission_time=46151.379868, global_step=56132, preemption_count=0, score=46151.379868, test/ctc_loss=0.1893806755542755, test/num_examples=2472, test/wer=0.060427, total_duration=50507.736119, train/ctc_loss=0.11897294223308563, train/wer=0.044654, validation/ctc_loss=0.34264901280403137, validation/num_examples=5348, validation/wer=0.097995
I0328 13:02:17.789399 139831771211520 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.2005482912063599, loss=0.9594621658325195
I0328 13:09:02.509925 139831779604224 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.9648008346557617, loss=1.0274593830108643
I0328 13:16:20.506714 139831771211520 logging_writer.py:48] [57500] global_step=57500, grad_norm=2.2265777587890625, loss=1.015425682067871
I0328 13:21:23.294195 140002444732224 spec.py:321] Evaluating on the training split.
I0328 13:22:14.904733 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 13:23:06.271126 140002444732224 spec.py:349] Evaluating on the test split.
I0328 13:23:31.851363 140002444732224 submission_runner.py:422] Time since start: 52076.79s, 	Step: 57861, 	{'train/ctc_loss': Array(0.12764521, dtype=float32), 'train/wer': 0.04722250838455639, 'validation/ctc_loss': Array(0.34064272, dtype=float32), 'validation/wer': 0.09756026917172732, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18807088, dtype=float32), 'test/wer': 0.059959783072329534, 'test/num_examples': 2472, 'score': 47591.754549741745, 'total_duration': 52076.789647340775, 'accumulated_submission_time': 47591.754549741745, 'accumulated_eval_time': 4480.866909980774, 'accumulated_logging_time': 1.910968542098999}
I0328 13:23:31.893196 139832209684224 logging_writer.py:48] [57861] accumulated_eval_time=4480.866910, accumulated_logging_time=1.910969, accumulated_submission_time=47591.754550, global_step=57861, preemption_count=0, score=47591.754550, test/ctc_loss=0.1880708783864975, test/num_examples=2472, test/wer=0.059960, total_duration=52076.789647, train/ctc_loss=0.12764520943164825, train/wer=0.047223, validation/ctc_loss=0.3406427204608917, validation/num_examples=5348, validation/wer=0.097560
I0328 13:25:19.522465 139832201291520 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.7324754595756531, loss=0.992775559425354
I0328 13:32:13.938316 139832209684224 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.94803386926651, loss=1.013379454612732
I0328 13:39:07.840391 139832209684224 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.924850344657898, loss=1.0209214687347412
I0328 13:46:14.689418 139832201291520 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.9894908666610718, loss=1.0509384870529175
I0328 13:47:32.177441 140002444732224 spec.py:321] Evaluating on the training split.
I0328 13:48:25.660067 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 13:49:17.100433 140002444732224 spec.py:349] Evaluating on the test split.
I0328 13:49:42.942517 140002444732224 submission_runner.py:422] Time since start: 53647.88s, 	Step: 59587, 	{'train/ctc_loss': Array(0.1374247, dtype=float32), 'train/wer': 0.050577767128807766, 'validation/ctc_loss': Array(0.34034002, dtype=float32), 'validation/wer': 0.09730924819216621, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1877432, dtype=float32), 'test/wer': 0.060081652550118825, 'test/num_examples': 2472, 'score': 49031.96312832832, 'total_duration': 53647.88099217415, 'accumulated_submission_time': 49031.96312832832, 'accumulated_eval_time': 4611.625438451767, 'accumulated_logging_time': 1.9681634902954102}
I0328 13:49:42.985324 139832209684224 logging_writer.py:48] [59587] accumulated_eval_time=4611.625438, accumulated_logging_time=1.968163, accumulated_submission_time=49031.963128, global_step=59587, preemption_count=0, score=49031.963128, test/ctc_loss=0.1877432018518448, test/num_examples=2472, test/wer=0.060082, total_duration=53647.880992, train/ctc_loss=0.13742470741271973, train/wer=0.050578, validation/ctc_loss=0.3403400182723999, validation/num_examples=5348, validation/wer=0.097309
I0328 13:55:04.364428 139832209684224 logging_writer.py:48] [60000] global_step=60000, grad_norm=2.569488048553467, loss=0.9675111174583435
I0328 14:02:07.180953 139832201291520 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.2253702878952026, loss=0.9956276416778564
I0328 14:09:08.683059 139832209684224 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.0320481061935425, loss=1.0354371070861816
I0328 14:13:43.605136 140002444732224 spec.py:321] Evaluating on the training split.
I0328 14:14:36.625777 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 14:15:28.372926 140002444732224 spec.py:349] Evaluating on the test split.
I0328 14:15:54.615308 140002444732224 submission_runner.py:422] Time since start: 55219.55s, 	Step: 61341, 	{'train/ctc_loss': Array(0.11774658, dtype=float32), 'train/wer': 0.045093819355706506, 'validation/ctc_loss': Array(0.3403337, dtype=float32), 'validation/wer': 0.09740579472276664, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18774082, dtype=float32), 'test/wer': 0.060081652550118825, 'test/num_examples': 2472, 'score': 50472.50412297249, 'total_duration': 55219.55290055275, 'accumulated_submission_time': 50472.50412297249, 'accumulated_eval_time': 4742.62815785408, 'accumulated_logging_time': 2.026418447494507}
I0328 14:15:54.666754 139832209684224 logging_writer.py:48] [61341] accumulated_eval_time=4742.628158, accumulated_logging_time=2.026418, accumulated_submission_time=50472.504123, global_step=61341, preemption_count=0, score=50472.504123, test/ctc_loss=0.18774081766605377, test/num_examples=2472, test/wer=0.060082, total_duration=55219.552901, train/ctc_loss=0.11774657666683197, train/wer=0.045094, validation/ctc_loss=0.3403337001800537, validation/num_examples=5348, validation/wer=0.097406
I0328 14:17:57.840096 139832201291520 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.164008617401123, loss=1.0365879535675049
I0328 14:24:44.917784 139831882004224 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.2123562097549438, loss=1.0128214359283447
I0328 14:31:42.245878 139831873611520 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.937863826751709, loss=0.9987699389457703
I0328 14:38:44.490593 139831882004224 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.9066290259361267, loss=1.0359864234924316
I0328 14:39:55.158049 140002444732224 spec.py:321] Evaluating on the training split.
I0328 14:40:47.251299 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 14:41:38.507335 140002444732224 spec.py:349] Evaluating on the test split.
I0328 14:42:04.336310 140002444732224 submission_runner.py:422] Time since start: 56789.28s, 	Step: 63093, 	{'train/ctc_loss': Array(0.12637022, dtype=float32), 'train/wer': 0.04678586098306787, 'validation/ctc_loss': Array(0.34032816, dtype=float32), 'validation/wer': 0.09743475868194676, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18774717, dtype=float32), 'test/wer': 0.06014258728901346, 'test/num_examples': 2472, 'score': 51912.91632771492, 'total_duration': 56789.27572798729, 'accumulated_submission_time': 51912.91632771492, 'accumulated_eval_time': 4871.80082154274, 'accumulated_logging_time': 2.093965530395508}
I0328 14:42:04.384379 139830898960128 logging_writer.py:48] [63093] accumulated_eval_time=4871.800822, accumulated_logging_time=2.093966, accumulated_submission_time=51912.916328, global_step=63093, preemption_count=0, score=51912.916328, test/ctc_loss=0.18774716556072235, test/num_examples=2472, test/wer=0.060143, total_duration=56789.275728, train/ctc_loss=0.12637022137641907, train/wer=0.046786, validation/ctc_loss=0.3403281569480896, validation/num_examples=5348, validation/wer=0.097435
I0328 14:47:23.965182 139830890567424 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.9748256802558899, loss=0.9756448864936829
I0328 14:54:26.589658 139830898960128 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.213922381401062, loss=0.9729172587394714
I0328 15:01:03.932306 139830890567424 logging_writer.py:48] [64500] global_step=64500, grad_norm=3.635587692260742, loss=0.9605394601821899
I0328 15:06:04.792125 140002444732224 spec.py:321] Evaluating on the training split.
I0328 15:06:57.332263 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 15:07:48.551260 140002444732224 spec.py:349] Evaluating on the test split.
I0328 15:08:14.302301 140002444732224 submission_runner.py:422] Time since start: 58359.24s, 	Step: 64848, 	{'train/ctc_loss': Array(0.12142679, dtype=float32), 'train/wer': 0.045630049546935676, 'validation/ctc_loss': Array(0.34032327, dtype=float32), 'validation/wer': 0.09743475868194676, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18774422, dtype=float32), 'test/wer': 0.06010196412975037, 'test/num_examples': 2472, 'score': 53353.246735572815, 'total_duration': 58359.24052262306, 'accumulated_submission_time': 53353.246735572815, 'accumulated_eval_time': 5001.304198741913, 'accumulated_logging_time': 2.156931161880493}
I0328 15:08:14.347992 139831114000128 logging_writer.py:48] [64848] accumulated_eval_time=5001.304199, accumulated_logging_time=2.156931, accumulated_submission_time=53353.246736, global_step=64848, preemption_count=0, score=53353.246736, test/ctc_loss=0.18774421513080597, test/num_examples=2472, test/wer=0.060102, total_duration=58359.240523, train/ctc_loss=0.1214267909526825, train/wer=0.045630, validation/ctc_loss=0.340323269367218, validation/num_examples=5348, validation/wer=0.097435
I0328 15:10:15.334476 139830786320128 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.9663717746734619, loss=0.9535185694694519
I0328 15:16:56.430365 139830777927424 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.3065872192382812, loss=0.9908905625343323
I0328 15:24:08.247192 139831114000128 logging_writer.py:48] [66000] global_step=66000, grad_norm=2.7745094299316406, loss=0.9422376155853271
I0328 15:30:44.131719 139831105607424 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.7801911234855652, loss=1.0165576934814453
I0328 15:32:14.986622 140002444732224 spec.py:321] Evaluating on the training split.
I0328 15:33:06.818116 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 15:33:57.612541 140002444732224 spec.py:349] Evaluating on the test split.
I0328 15:34:23.489828 140002444732224 submission_runner.py:422] Time since start: 59928.43s, 	Step: 66604, 	{'train/ctc_loss': Array(0.12866996, dtype=float32), 'train/wer': 0.04746721932166913, 'validation/ctc_loss': Array(0.3403258, dtype=float32), 'validation/wer': 0.09742510402888672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18774408, dtype=float32), 'test/wer': 0.06010196412975037, 'test/num_examples': 2472, 'score': 54793.808423519135, 'total_duration': 59928.429037332535, 'accumulated_submission_time': 54793.808423519135, 'accumulated_eval_time': 5129.801570892334, 'accumulated_logging_time': 2.217597007751465}
I0328 15:34:23.531474 139832209684224 logging_writer.py:48] [66604] accumulated_eval_time=5129.801571, accumulated_logging_time=2.217597, accumulated_submission_time=54793.808424, global_step=66604, preemption_count=0, score=54793.808424, test/ctc_loss=0.18774408102035522, test/num_examples=2472, test/wer=0.060102, total_duration=59928.429037, train/ctc_loss=0.12866996228694916, train/wer=0.047467, validation/ctc_loss=0.34032580256462097, validation/num_examples=5348, validation/wer=0.097425
I0328 15:39:33.894597 139831124244224 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.8425179719924927, loss=0.9393020272254944
I0328 15:46:08.503169 139831115851520 logging_writer.py:48] [67500] global_step=67500, grad_norm=3.7758378982543945, loss=0.9861263632774353
I0328 15:53:26.655667 139832209684224 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.8419551849365234, loss=1.0329958200454712
I0328 15:58:24.154419 140002444732224 spec.py:321] Evaluating on the training split.
I0328 15:59:16.683891 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 16:00:07.505198 140002444732224 spec.py:349] Evaluating on the test split.
I0328 16:00:33.104561 140002444732224 submission_runner.py:422] Time since start: 61498.04s, 	Step: 68388, 	{'train/ctc_loss': Array(0.12628658, dtype=float32), 'train/wer': 0.047460924397933854, 'validation/ctc_loss': Array(0.34032667, dtype=float32), 'validation/wer': 0.09741544937582668, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18774174, dtype=float32), 'test/wer': 0.06010196412975037, 'test/num_examples': 2472, 'score': 56234.35003376007, 'total_duration': 61498.0438375473, 'accumulated_submission_time': 56234.35003376007, 'accumulated_eval_time': 5258.745967626572, 'accumulated_logging_time': 2.276702642440796}
I0328 16:00:33.143590 139832209684224 logging_writer.py:48] [68388] accumulated_eval_time=5258.745968, accumulated_logging_time=2.276703, accumulated_submission_time=56234.350034, global_step=68388, preemption_count=0, score=56234.350034, test/ctc_loss=0.1877417415380478, test/num_examples=2472, test/wer=0.060102, total_duration=61498.043838, train/ctc_loss=0.126286581158638, train/wer=0.047461, validation/ctc_loss=0.3403266668319702, validation/num_examples=5348, validation/wer=0.097415
I0328 16:02:00.047315 139832201291520 logging_writer.py:48] [68500] global_step=68500, grad_norm=2.086622714996338, loss=1.0179979801177979
I0328 16:08:56.872727 139832209684224 logging_writer.py:48] [69000] global_step=69000, grad_norm=3.0247955322265625, loss=0.9912875890731812
I0328 16:15:28.188918 139832209684224 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.0338290929794312, loss=0.9776244759559631
I0328 16:22:45.167108 139832201291520 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.6192877888679504, loss=1.022305965423584
I0328 16:24:33.358970 140002444732224 spec.py:321] Evaluating on the training split.
I0328 16:25:26.332001 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 16:26:17.004920 140002444732224 spec.py:349] Evaluating on the test split.
I0328 16:26:42.568316 140002444732224 submission_runner.py:422] Time since start: 63067.51s, 	Step: 70133, 	{'train/ctc_loss': Array(0.1284171, dtype=float32), 'train/wer': 0.04833800073054913, 'validation/ctc_loss': Array(0.3403205, dtype=float32), 'validation/wer': 0.09743475868194676, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18773627, dtype=float32), 'test/wer': 0.06010196412975037, 'test/num_examples': 2472, 'score': 57674.48403573036, 'total_duration': 63067.507434129715, 'accumulated_submission_time': 57674.48403573036, 'accumulated_eval_time': 5387.949546098709, 'accumulated_logging_time': 2.332932949066162}
I0328 16:26:42.609901 139831160084224 logging_writer.py:48] [70133] accumulated_eval_time=5387.949546, accumulated_logging_time=2.332933, accumulated_submission_time=57674.484036, global_step=70133, preemption_count=0, score=57674.484036, test/ctc_loss=0.18773627281188965, test/num_examples=2472, test/wer=0.060102, total_duration=63067.507434, train/ctc_loss=0.12841710448265076, train/wer=0.048338, validation/ctc_loss=0.34032049775123596, validation/num_examples=5348, validation/wer=0.097435
I0328 16:31:25.692293 139831151691520 logging_writer.py:48] [70500] global_step=70500, grad_norm=2.386392831802368, loss=1.0075562000274658
I0328 16:38:23.772854 139831160084224 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.9892030358314514, loss=0.9963032603263855
I0328 16:44:58.818987 139831160084224 logging_writer.py:48] [71500] global_step=71500, grad_norm=2.2972910404205322, loss=0.9810674786567688
I0328 16:50:43.042704 140002444732224 spec.py:321] Evaluating on the training split.
I0328 16:51:35.341985 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 16:52:26.559265 140002444732224 spec.py:349] Evaluating on the test split.
I0328 16:52:52.421926 140002444732224 submission_runner.py:422] Time since start: 64637.36s, 	Step: 71901, 	{'train/ctc_loss': Array(0.11133227, dtype=float32), 'train/wer': 0.04208104119905243, 'validation/ctc_loss': Array(0.340324, dtype=float32), 'validation/wer': 0.09746372264112689, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1877455, dtype=float32), 'test/wer': 0.06010196412975037, 'test/num_examples': 2472, 'score': 59114.83821392059, 'total_duration': 64637.35920190811, 'accumulated_submission_time': 59114.83821392059, 'accumulated_eval_time': 5517.321028709412, 'accumulated_logging_time': 2.3915865421295166}
I0328 16:52:52.475631 139830571280128 logging_writer.py:48] [71901] accumulated_eval_time=5517.321029, accumulated_logging_time=2.391587, accumulated_submission_time=59114.838214, global_step=71901, preemption_count=0, score=59114.838214, test/ctc_loss=0.18774549663066864, test/num_examples=2472, test/wer=0.060102, total_duration=64637.359202, train/ctc_loss=0.11133227497339249, train/wer=0.042081, validation/ctc_loss=0.3403240144252777, validation/num_examples=5348, validation/wer=0.097464
I0328 16:54:09.589292 139830562887424 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.8992742300033569, loss=1.0230673551559448
I0328 17:00:37.358260 139832209684224 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.222001552581787, loss=0.9441413283348083
I0328 17:07:46.362208 139832201291520 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.0137999057769775, loss=0.9785270094871521
I0328 17:14:27.030294 139830243600128 logging_writer.py:48] [73500] global_step=73500, grad_norm=2.2158305644989014, loss=0.9895327091217041
I0328 17:16:53.140514 140002444732224 spec.py:321] Evaluating on the training split.
I0328 17:17:45.285707 140002444732224 spec.py:333] Evaluating on the validation split.
I0328 17:18:36.496964 140002444732224 spec.py:349] Evaluating on the test split.
I0328 17:19:02.689531 140002444732224 submission_runner.py:422] Time since start: 66207.63s, 	Step: 73681, 	{'train/ctc_loss': Array(0.10995505, dtype=float32), 'train/wer': 0.04095653996250418, 'validation/ctc_loss': Array(0.34032497, dtype=float32), 'validation/wer': 0.09741544937582668, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1877446, dtype=float32), 'test/wer': 0.060081652550118825, 'test/num_examples': 2472, 'score': 60555.421325683594, 'total_duration': 66207.62722039223, 'accumulated_submission_time': 60555.421325683594, 'accumulated_eval_time': 5646.862709522247, 'accumulated_logging_time': 2.4634487628936768}
I0328 17:19:02.729999 139831590164224 logging_writer.py:48] [73681] accumulated_eval_time=5646.862710, accumulated_logging_time=2.463449, accumulated_submission_time=60555.421326, global_step=73681, preemption_count=0, score=60555.421326, test/ctc_loss=0.187744602560997, test/num_examples=2472, test/wer=0.060082, total_duration=66207.627220, train/ctc_loss=0.10995505005121231, train/wer=0.040957, validation/ctc_loss=0.3403249680995941, validation/num_examples=5348, validation/wer=0.097415
I0328 17:23:11.437558 139831581771520 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.8851785659790039, loss=0.9971897006034851
I0328 17:27:36.091521 139831262484224 logging_writer.py:48] [74318] global_step=74318, preemption_count=0, score=61068.724503
I0328 17:27:38.117822 140002444732224 submission_runner.py:596] Tuning trial 1/1
I0328 17:27:38.118093 140002444732224 submission_runner.py:597] Hyperparameters: Hyperparameters(learning_rate=0.0014446807792420305, beta1=0.7427148812902895, beta2=0.8993064520764248, warmup_steps=3000, weight_decay=0.06875136511682291)
I0328 17:27:38.138117 140002444732224 submission_runner.py:598] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.144068, dtype=float32), 'train/wer': 1.5954490556608292, 'validation/ctc_loss': Array(30.784756, dtype=float32), 'validation/wer': 1.5437114417293414, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.686705, dtype=float32), 'test/wer': 1.5518656185891577, 'test/num_examples': 2472, 'score': 59.595560789108276, 'total_duration': 245.51749229431152, 'accumulated_submission_time': 59.595560789108276, 'accumulated_eval_time': 185.92183756828308, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1803, {'train/ctc_loss': Array(4.095391, dtype=float32), 'train/wer': 0.840152623014974, 'validation/ctc_loss': Array(4.45944, dtype=float32), 'validation/wer': 0.8330613939388088, 'validation/num_examples': 5348, 'test/ctc_loss': Array(4.2168097, dtype=float32), 'test/wer': 0.8155708569455447, 'test/num_examples': 2472, 'score': 1500.1719553470612, 'total_duration': 1800.3977222442627, 'accumulated_submission_time': 1500.1719553470612, 'accumulated_eval_time': 300.12022733688354, 'accumulated_logging_time': 0.04186701774597168, 'global_step': 1803, 'preemption_count': 0}), (3626, {'train/ctc_loss': Array(0.79150915, dtype=float32), 'train/wer': 0.267293503449378, 'validation/ctc_loss': Array(1.1359301, dtype=float32), 'validation/wer': 0.32738928526603395, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.8338466, dtype=float32), 'test/wer': 0.2679909816586436, 'test/num_examples': 2472, 'score': 2940.422672510147, 'total_duration': 3369.5566635131836, 'accumulated_submission_time': 2940.422672510147, 'accumulated_eval_time': 428.9050886631012, 'accumulated_logging_time': 0.09521222114562988, 'global_step': 3626, 'preemption_count': 0}), (5428, {'train/ctc_loss': Array(0.48875508, dtype=float32), 'train/wer': 0.1729999841864731, 'validation/ctc_loss': Array(0.8318542, dtype=float32), 'validation/wer': 0.2478542533574056, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.55740714, dtype=float32), 'test/wer': 0.1852619178193488, 'test/num_examples': 2472, 'score': 4380.319248199463, 'total_duration': 4939.229757785797, 'accumulated_submission_time': 4380.319248199463, 'accumulated_eval_time': 558.5591344833374, 'accumulated_logging_time': 0.14757418632507324, 'global_step': 5428, 'preemption_count': 0}), (7189, {'train/ctc_loss': Array(0.40930986, dtype=float32), 'train/wer': 0.1436186367016096, 'validation/ctc_loss': Array(0.7136427, dtype=float32), 'validation/wer': 0.21319404887185378, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46206, dtype=float32), 'test/wer': 0.1526008977718197, 'test/num_examples': 2472, 'score': 5820.772198915482, 'total_duration': 6511.6455771923065, 'accumulated_submission_time': 5820.772198915482, 'accumulated_eval_time': 690.3928747177124, 'accumulated_logging_time': 0.20771169662475586, 'global_step': 7189, 'preemption_count': 0}), (8949, {'train/ctc_loss': Array(0.35587066, dtype=float32), 'train/wer': 0.12813933766770313, 'validation/ctc_loss': Array(0.6459704, dtype=float32), 'validation/wer': 0.19563223495563686, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.41139576, dtype=float32), 'test/wer': 0.14155139845225764, 'test/num_examples': 2472, 'score': 7261.275091409683, 'total_duration': 8082.823434591293, 'accumulated_submission_time': 7261.275091409683, 'accumulated_eval_time': 820.9422154426575, 'accumulated_logging_time': 0.26285457611083984, 'global_step': 8949, 'preemption_count': 0}), (10718, {'train/ctc_loss': Array(0.3555755, dtype=float32), 'train/wer': 0.12436316690761852, 'validation/ctc_loss': Array(0.63100934, dtype=float32), 'validation/wer': 0.1882464253647045, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38889992, dtype=float32), 'test/wer': 0.13074563808827414, 'test/num_examples': 2472, 'score': 8701.822609901428, 'total_duration': 9653.478703975677, 'accumulated_submission_time': 8701.822609901428, 'accumulated_eval_time': 950.9268486499786, 'accumulated_logging_time': 0.31924915313720703, 'global_step': 10718, 'preemption_count': 0}), (12441, {'train/ctc_loss': Array(0.30882823, dtype=float32), 'train/wer': 0.11118733116372068, 'validation/ctc_loss': Array(0.6040676, dtype=float32), 'validation/wer': 0.18083165181459204, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36773804, dtype=float32), 'test/wer': 0.12410375154875794, 'test/num_examples': 2472, 'score': 10142.258544921875, 'total_duration': 11224.908006191254, 'accumulated_submission_time': 10142.258544921875, 'accumulated_eval_time': 1081.8001081943512, 'accumulated_logging_time': 0.3719062805175781, 'global_step': 12441, 'preemption_count': 0}), (14190, {'train/ctc_loss': Array(0.2673932, dtype=float32), 'train/wer': 0.10080345302789984, 'validation/ctc_loss': Array(0.5752747, dtype=float32), 'validation/wer': 0.1739285748766618, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34668887, dtype=float32), 'test/wer': 0.11894461032234477, 'test/num_examples': 2472, 'score': 11582.391997814178, 'total_duration': 12797.393310070038, 'accumulated_submission_time': 11582.391997814178, 'accumulated_eval_time': 1214.0290806293488, 'accumulated_logging_time': 0.4271676540374756, 'global_step': 14190, 'preemption_count': 0}), (15963, {'train/ctc_loss': Array(0.2514518, dtype=float32), 'train/wer': 0.09180237166766861, 'validation/ctc_loss': Array(0.5608643, dtype=float32), 'validation/wer': 0.16682275022447068, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33763376, dtype=float32), 'test/wer': 0.11276989011435419, 'test/num_examples': 2472, 'score': 13023.056012392044, 'total_duration': 14368.632350683212, 'accumulated_submission_time': 13023.056012392044, 'accumulated_eval_time': 1344.4869689941406, 'accumulated_logging_time': 0.47449159622192383, 'global_step': 15963, 'preemption_count': 0}), (17707, {'train/ctc_loss': Array(0.23556301, dtype=float32), 'train/wer': 0.08935093072768908, 'validation/ctc_loss': Array(0.5287409, dtype=float32), 'validation/wer': 0.1579211601031117, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32146436, dtype=float32), 'test/wer': 0.10712327097678387, 'test/num_examples': 2472, 'score': 14463.13798880577, 'total_duration': 15938.392670154572, 'accumulated_submission_time': 14463.13798880577, 'accumulated_eval_time': 1474.01935172081, 'accumulated_logging_time': 0.551727294921875, 'global_step': 17707, 'preemption_count': 0}), (19447, {'train/ctc_loss': Array(0.25388935, dtype=float32), 'train/wer': 0.09018747190565589, 'validation/ctc_loss': Array(0.54082644, dtype=float32), 'validation/wer': 0.15938866736823812, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32557067, dtype=float32), 'test/wer': 0.10604675725631182, 'test/num_examples': 2472, 'score': 15903.59057211876, 'total_duration': 17510.60142850876, 'accumulated_submission_time': 15903.59057211876, 'accumulated_eval_time': 1605.6605203151703, 'accumulated_logging_time': 0.5993692874908447, 'global_step': 19447, 'preemption_count': 0}), (21204, {'train/ctc_loss': Array(0.23959489, dtype=float32), 'train/wer': 0.08956657081805472, 'validation/ctc_loss': Array(0.5109227, dtype=float32), 'validation/wer': 0.153200034756751, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30394173, dtype=float32), 'test/wer': 0.10257347713931712, 'test/num_examples': 2472, 'score': 17343.781029462814, 'total_duration': 19082.02796602249, 'accumulated_submission_time': 17343.781029462814, 'accumulated_eval_time': 1736.7732248306274, 'accumulated_logging_time': 0.6557137966156006, 'global_step': 21204, 'preemption_count': 0}), (22977, {'train/ctc_loss': Array(0.21597603, dtype=float32), 'train/wer': 0.08100113180368351, 'validation/ctc_loss': Array(0.5011605, dtype=float32), 'validation/wer': 0.14936713749191421, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29222873, dtype=float32), 'test/wer': 0.09871427700932302, 'test/num_examples': 2472, 'score': 18784.199924230576, 'total_duration': 20653.634942293167, 'accumulated_submission_time': 18784.199924230576, 'accumulated_eval_time': 1867.8287971019745, 'accumulated_logging_time': 0.7179415225982666, 'global_step': 22977, 'preemption_count': 0}), (24700, {'train/ctc_loss': Array(0.19944268, dtype=float32), 'train/wer': 0.07526603606266627, 'validation/ctc_loss': Array(0.48467597, dtype=float32), 'validation/wer': 0.1470500207575041, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28198373, dtype=float32), 'test/wer': 0.09668311904616822, 'test/num_examples': 2472, 'score': 20224.769287347794, 'total_duration': 22226.36704683304, 'accumulated_submission_time': 20224.769287347794, 'accumulated_eval_time': 1999.8688099384308, 'accumulated_logging_time': 0.7733640670776367, 'global_step': 24700, 'preemption_count': 0}), (26447, {'train/ctc_loss': Array(0.19055541, dtype=float32), 'train/wer': 0.07278228435796448, 'validation/ctc_loss': Array(0.474879, dtype=float32), 'validation/wer': 0.14236751402338357, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2783278, dtype=float32), 'test/wer': 0.09357544736254138, 'test/num_examples': 2472, 'score': 21665.098954916, 'total_duration': 23798.49133992195, 'accumulated_submission_time': 21665.098954916, 'accumulated_eval_time': 2131.528559446335, 'accumulated_logging_time': 0.8407466411590576, 'global_step': 26447, 'preemption_count': 0}), (28234, {'train/ctc_loss': Array(0.18396299, dtype=float32), 'train/wer': 0.0676376430638942, 'validation/ctc_loss': Array(0.46728623, dtype=float32), 'validation/wer': 0.1378105177790436, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27716658, dtype=float32), 'test/wer': 0.09225519468649077, 'test/num_examples': 2472, 'score': 23105.721066474915, 'total_duration': 25370.701897144318, 'accumulated_submission_time': 23105.721066474915, 'accumulated_eval_time': 2262.988550186157, 'accumulated_logging_time': 0.8999254703521729, 'global_step': 28234, 'preemption_count': 0}), (29992, {'train/ctc_loss': Array(0.20105429, dtype=float32), 'train/wer': 0.07587428497139885, 'validation/ctc_loss': Array(0.46363172, dtype=float32), 'validation/wer': 0.1387663284319878, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.27467206, dtype=float32), 'test/wer': 0.09264111469949018, 'test/num_examples': 2472, 'score': 24546.02538204193, 'total_duration': 26942.068878173828, 'accumulated_submission_time': 24546.02538204193, 'accumulated_eval_time': 2393.9246048927307, 'accumulated_logging_time': 0.957648515701294, 'global_step': 29992, 'preemption_count': 0}), (31728, {'train/ctc_loss': Array(0.17956269, dtype=float32), 'train/wer': 0.06554566221194906, 'validation/ctc_loss': Array(0.45461053, dtype=float32), 'validation/wer': 0.13447966247332901, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25810274, dtype=float32), 'test/wer': 0.08628359027481568, 'test/num_examples': 2472, 'score': 25986.407766342163, 'total_duration': 28513.783675193787, 'accumulated_submission_time': 25986.407766342163, 'accumulated_eval_time': 2525.128483772278, 'accumulated_logging_time': 1.0193750858306885, 'global_step': 31728, 'preemption_count': 0}), (33488, {'train/ctc_loss': Array(0.18582933, dtype=float32), 'train/wer': 0.06800260955776215, 'validation/ctc_loss': Array(0.4406355, dtype=float32), 'validation/wer': 0.1310715699431341, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25114754, dtype=float32), 'test/wer': 0.08478053338208112, 'test/num_examples': 2472, 'score': 27426.630671024323, 'total_duration': 30085.866382598877, 'accumulated_submission_time': 27426.630671024323, 'accumulated_eval_time': 2656.8701095581055, 'accumulated_logging_time': 1.0686450004577637, 'global_step': 33488, 'preemption_count': 0}), (35229, {'train/ctc_loss': Array(0.17369977, dtype=float32), 'train/wer': 0.06156420175410552, 'validation/ctc_loss': Array(0.42272478, dtype=float32), 'validation/wer': 0.123058207903299, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2449416, dtype=float32), 'test/wer': 0.08065728271687689, 'test/num_examples': 2472, 'score': 28867.240092277527, 'total_duration': 31657.06634402275, 'accumulated_submission_time': 28867.240092277527, 'accumulated_eval_time': 2787.3269016742706, 'accumulated_logging_time': 1.1331031322479248, 'global_step': 35229, 'preemption_count': 0}), (36958, {'train/ctc_loss': Array(0.1301626, dtype=float32), 'train/wer': 0.04947572337873989, 'validation/ctc_loss': Array(0.41261932, dtype=float32), 'validation/wer': 0.12281684157679794, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23866884, dtype=float32), 'test/wer': 0.07974326163345724, 'test/num_examples': 2472, 'score': 30307.72377705574, 'total_duration': 33229.89807128906, 'accumulated_submission_time': 30307.72377705574, 'accumulated_eval_time': 2919.556672334671, 'accumulated_logging_time': 1.184589147567749, 'global_step': 36958, 'preemption_count': 0}), (38702, {'train/ctc_loss': Array(0.14963634, dtype=float32), 'train/wer': 0.05615509704124517, 'validation/ctc_loss': Array(0.41541314, dtype=float32), 'validation/wer': 0.12419745696438399, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23655531, dtype=float32), 'test/wer': 0.08000731216866735, 'test/num_examples': 2472, 'score': 31747.961825847626, 'total_duration': 34800.60106897354, 'accumulated_submission_time': 31747.961825847626, 'accumulated_eval_time': 3049.883457660675, 'accumulated_logging_time': 1.2530457973480225, 'global_step': 38702, 'preemption_count': 0}), (40465, {'train/ctc_loss': Array(0.17602299, dtype=float32), 'train/wer': 0.06591344250202047, 'validation/ctc_loss': Array(0.4037802, dtype=float32), 'validation/wer': 0.11797020574065671, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22502472, dtype=float32), 'test/wer': 0.07482785936262264, 'test/num_examples': 2472, 'score': 33188.00236105919, 'total_duration': 36371.32774734497, 'accumulated_submission_time': 33188.00236105919, 'accumulated_eval_time': 3180.441960334778, 'accumulated_logging_time': 1.3102242946624756, 'global_step': 40465, 'preemption_count': 0}), (42185, {'train/ctc_loss': Array(0.17704529, dtype=float32), 'train/wer': 0.06581102161132915, 'validation/ctc_loss': Array(0.39160708, dtype=float32), 'validation/wer': 0.11502553655734381, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22013047, dtype=float32), 'test/wer': 0.07287794771799402, 'test/num_examples': 2472, 'score': 34628.517916202545, 'total_duration': 37942.206201314926, 'accumulated_submission_time': 34628.517916202545, 'accumulated_eval_time': 3310.6726155281067, 'accumulated_logging_time': 1.3750653266906738, 'global_step': 42185, 'preemption_count': 0}), (43922, {'train/ctc_loss': Array(0.19677651, dtype=float32), 'train/wer': 0.07444238407496885, 'validation/ctc_loss': Array(0.38611034, dtype=float32), 'validation/wer': 0.11226430578217171, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21493606, dtype=float32), 'test/wer': 0.07033900026405053, 'test/num_examples': 2472, 'score': 36068.43892288208, 'total_duration': 39511.36161732674, 'accumulated_submission_time': 36068.43892288208, 'accumulated_eval_time': 3439.7774698734283, 'accumulated_logging_time': 1.4349782466888428, 'global_step': 43922, 'preemption_count': 0}), (45670, {'train/ctc_loss': Array(0.16540317, dtype=float32), 'train/wer': 0.06011221732244523, 'validation/ctc_loss': Array(0.36965913, dtype=float32), 'validation/wer': 0.10758179904805121, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20512938, dtype=float32), 'test/wer': 0.06749537911563382, 'test/num_examples': 2472, 'score': 37508.522341012955, 'total_duration': 41079.81269240379, 'accumulated_submission_time': 37508.522341012955, 'accumulated_eval_time': 3568.022716522217, 'accumulated_logging_time': 1.490485429763794, 'global_step': 45670, 'preemption_count': 0}), (47396, {'train/ctc_loss': Array(0.14902788, dtype=float32), 'train/wer': 0.056166701925111064, 'validation/ctc_loss': Array(0.368786, dtype=float32), 'validation/wer': 0.10731146876237002, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20425586, dtype=float32), 'test/wer': 0.06607356854142547, 'test/num_examples': 2472, 'score': 38948.955758333206, 'total_duration': 42650.75806093216, 'accumulated_submission_time': 38948.955758333206, 'accumulated_eval_time': 3698.4091176986694, 'accumulated_logging_time': 1.5476596355438232, 'global_step': 47396, 'preemption_count': 0}), (49143, {'train/ctc_loss': Array(0.12587143, dtype=float32), 'train/wer': 0.04756441070593481, 'validation/ctc_loss': Array(0.36090443, dtype=float32), 'validation/wer': 0.10338202496693281, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20120494, dtype=float32), 'test/wer': 0.06664229277110881, 'test/num_examples': 2472, 'score': 40389.83613157272, 'total_duration': 44223.31549620628, 'accumulated_submission_time': 40389.83613157272, 'accumulated_eval_time': 3829.954564809799, 'accumulated_logging_time': 1.6105077266693115, 'global_step': 49143, 'preemption_count': 0}), (50914, {'train/ctc_loss': Array(0.13706723, dtype=float32), 'train/wer': 0.051920685179251275, 'validation/ctc_loss': Array(0.3541431, dtype=float32), 'validation/wer': 0.1030151481506512, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19621822, dtype=float32), 'test/wer': 0.06396116425974448, 'test/num_examples': 2472, 'score': 41830.231493234634, 'total_duration': 45793.258259534836, 'accumulated_submission_time': 41830.231493234634, 'accumulated_eval_time': 3959.3782358169556, 'accumulated_logging_time': 1.6653227806091309, 'global_step': 50914, 'preemption_count': 0}), (52639, {'train/ctc_loss': Array(0.1219428, dtype=float32), 'train/wer': 0.04634620340147741, 'validation/ctc_loss': Array(0.348766, dtype=float32), 'validation/wer': 0.10078492329378144, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19256645, dtype=float32), 'test/wer': 0.06274246948185161, 'test/num_examples': 2472, 'score': 43270.73646521568, 'total_duration': 47365.765083789825, 'accumulated_submission_time': 43270.73646521568, 'accumulated_eval_time': 4091.2563354969025, 'accumulated_logging_time': 1.7211670875549316, 'global_step': 52639, 'preemption_count': 0}), (54370, {'train/ctc_loss': Array(0.12319872, dtype=float32), 'train/wer': 0.04637173325811736, 'validation/ctc_loss': Array(0.3454306, dtype=float32), 'validation/wer': 0.09896984851849348, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1905708, dtype=float32), 'test/wer': 0.061239412589117054, 'test/num_examples': 2472, 'score': 44711.165451049805, 'total_duration': 48936.92869734764, 'accumulated_submission_time': 44711.165451049805, 'accumulated_eval_time': 4221.863181114197, 'accumulated_logging_time': 1.7806179523468018, 'global_step': 54370, 'preemption_count': 0}), (56132, {'train/ctc_loss': Array(0.11897294, dtype=float32), 'train/wer': 0.04465424430641822, 'validation/ctc_loss': Array(0.342649, dtype=float32), 'validation/wer': 0.09799472855942921, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18938068, dtype=float32), 'test/wer': 0.06042694940385514, 'test/num_examples': 2472, 'score': 46151.37986826897, 'total_duration': 50507.73611855507, 'accumulated_submission_time': 46151.37986826897, 'accumulated_eval_time': 4352.316321372986, 'accumulated_logging_time': 1.8505048751831055, 'global_step': 56132, 'preemption_count': 0}), (57861, {'train/ctc_loss': Array(0.12764521, dtype=float32), 'train/wer': 0.04722250838455639, 'validation/ctc_loss': Array(0.34064272, dtype=float32), 'validation/wer': 0.09756026917172732, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18807088, dtype=float32), 'test/wer': 0.059959783072329534, 'test/num_examples': 2472, 'score': 47591.754549741745, 'total_duration': 52076.789647340775, 'accumulated_submission_time': 47591.754549741745, 'accumulated_eval_time': 4480.866909980774, 'accumulated_logging_time': 1.910968542098999, 'global_step': 57861, 'preemption_count': 0}), (59587, {'train/ctc_loss': Array(0.1374247, dtype=float32), 'train/wer': 0.050577767128807766, 'validation/ctc_loss': Array(0.34034002, dtype=float32), 'validation/wer': 0.09730924819216621, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1877432, dtype=float32), 'test/wer': 0.060081652550118825, 'test/num_examples': 2472, 'score': 49031.96312832832, 'total_duration': 53647.88099217415, 'accumulated_submission_time': 49031.96312832832, 'accumulated_eval_time': 4611.625438451767, 'accumulated_logging_time': 1.9681634902954102, 'global_step': 59587, 'preemption_count': 0}), (61341, {'train/ctc_loss': Array(0.11774658, dtype=float32), 'train/wer': 0.045093819355706506, 'validation/ctc_loss': Array(0.3403337, dtype=float32), 'validation/wer': 0.09740579472276664, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18774082, dtype=float32), 'test/wer': 0.060081652550118825, 'test/num_examples': 2472, 'score': 50472.50412297249, 'total_duration': 55219.55290055275, 'accumulated_submission_time': 50472.50412297249, 'accumulated_eval_time': 4742.62815785408, 'accumulated_logging_time': 2.026418447494507, 'global_step': 61341, 'preemption_count': 0}), (63093, {'train/ctc_loss': Array(0.12637022, dtype=float32), 'train/wer': 0.04678586098306787, 'validation/ctc_loss': Array(0.34032816, dtype=float32), 'validation/wer': 0.09743475868194676, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18774717, dtype=float32), 'test/wer': 0.06014258728901346, 'test/num_examples': 2472, 'score': 51912.91632771492, 'total_duration': 56789.27572798729, 'accumulated_submission_time': 51912.91632771492, 'accumulated_eval_time': 4871.80082154274, 'accumulated_logging_time': 2.093965530395508, 'global_step': 63093, 'preemption_count': 0}), (64848, {'train/ctc_loss': Array(0.12142679, dtype=float32), 'train/wer': 0.045630049546935676, 'validation/ctc_loss': Array(0.34032327, dtype=float32), 'validation/wer': 0.09743475868194676, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18774422, dtype=float32), 'test/wer': 0.06010196412975037, 'test/num_examples': 2472, 'score': 53353.246735572815, 'total_duration': 58359.24052262306, 'accumulated_submission_time': 53353.246735572815, 'accumulated_eval_time': 5001.304198741913, 'accumulated_logging_time': 2.156931161880493, 'global_step': 64848, 'preemption_count': 0}), (66604, {'train/ctc_loss': Array(0.12866996, dtype=float32), 'train/wer': 0.04746721932166913, 'validation/ctc_loss': Array(0.3403258, dtype=float32), 'validation/wer': 0.09742510402888672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18774408, dtype=float32), 'test/wer': 0.06010196412975037, 'test/num_examples': 2472, 'score': 54793.808423519135, 'total_duration': 59928.429037332535, 'accumulated_submission_time': 54793.808423519135, 'accumulated_eval_time': 5129.801570892334, 'accumulated_logging_time': 2.217597007751465, 'global_step': 66604, 'preemption_count': 0}), (68388, {'train/ctc_loss': Array(0.12628658, dtype=float32), 'train/wer': 0.047460924397933854, 'validation/ctc_loss': Array(0.34032667, dtype=float32), 'validation/wer': 0.09741544937582668, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18774174, dtype=float32), 'test/wer': 0.06010196412975037, 'test/num_examples': 2472, 'score': 56234.35003376007, 'total_duration': 61498.0438375473, 'accumulated_submission_time': 56234.35003376007, 'accumulated_eval_time': 5258.745967626572, 'accumulated_logging_time': 2.276702642440796, 'global_step': 68388, 'preemption_count': 0}), (70133, {'train/ctc_loss': Array(0.1284171, dtype=float32), 'train/wer': 0.04833800073054913, 'validation/ctc_loss': Array(0.3403205, dtype=float32), 'validation/wer': 0.09743475868194676, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18773627, dtype=float32), 'test/wer': 0.06010196412975037, 'test/num_examples': 2472, 'score': 57674.48403573036, 'total_duration': 63067.507434129715, 'accumulated_submission_time': 57674.48403573036, 'accumulated_eval_time': 5387.949546098709, 'accumulated_logging_time': 2.332932949066162, 'global_step': 70133, 'preemption_count': 0}), (71901, {'train/ctc_loss': Array(0.11133227, dtype=float32), 'train/wer': 0.04208104119905243, 'validation/ctc_loss': Array(0.340324, dtype=float32), 'validation/wer': 0.09746372264112689, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1877455, dtype=float32), 'test/wer': 0.06010196412975037, 'test/num_examples': 2472, 'score': 59114.83821392059, 'total_duration': 64637.35920190811, 'accumulated_submission_time': 59114.83821392059, 'accumulated_eval_time': 5517.321028709412, 'accumulated_logging_time': 2.3915865421295166, 'global_step': 71901, 'preemption_count': 0}), (73681, {'train/ctc_loss': Array(0.10995505, dtype=float32), 'train/wer': 0.04095653996250418, 'validation/ctc_loss': Array(0.34032497, dtype=float32), 'validation/wer': 0.09741544937582668, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1877446, dtype=float32), 'test/wer': 0.060081652550118825, 'test/num_examples': 2472, 'score': 60555.421325683594, 'total_duration': 66207.62722039223, 'accumulated_submission_time': 60555.421325683594, 'accumulated_eval_time': 5646.862709522247, 'accumulated_logging_time': 2.4634487628936768, 'global_step': 73681, 'preemption_count': 0})], 'global_step': 74318}
I0328 17:27:38.138415 140002444732224 submission_runner.py:599] Timing: 61068.724503040314
I0328 17:27:38.138485 140002444732224 submission_runner.py:601] Total number of evals: 43
I0328 17:27:38.138554 140002444732224 submission_runner.py:602] ====================
I0328 17:27:38.145353 140002444732224 submission_runner.py:686] Final librispeech_conformer_layernorm score: 0
