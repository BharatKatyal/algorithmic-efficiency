python3 submission_runner.py --framework=jax --workload=imagenet_resnet_silu --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --rng_seed=2511472366 --max_global_steps=186666 --imagenet_v2_data_dir=/data/imagenet/jax --tuning_ruleset=external --tuning_search_space=reference_algorithms/target_setting_algorithms/imagenet_resnet_silu/tuning_search_space.json --num_tuning_trials=1 2>&1 | tee -a /logs/imagenet_resnet_silu_jax_03-16-2024-00-38-48.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0316 00:39:10.153499 139725430036288 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/imagenet_resnet_silu_jax.
I0316 00:39:11.230504 139725430036288 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Host Interpreter CUDA
I0316 00:39:11.231310 139725430036288 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0316 00:39:11.231469 139725430036288 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0316 00:39:11.236950 139725430036288 submission_runner.py:554] Using RNG seed 2511472366
I0316 00:39:12.365423 139725430036288 submission_runner.py:563] --- Tuning run 1/1 ---
I0316 00:39:12.365651 139725430036288 submission_runner.py:568] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/imagenet_resnet_silu_jax/trial_1.
I0316 00:39:12.365841 139725430036288 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/imagenet_resnet_silu_jax/trial_1/hparams.json.
I0316 00:39:12.547989 139725430036288 submission_runner.py:209] Initializing dataset.
I0316 00:39:12.564726 139725430036288 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0316 00:39:12.575212 139725430036288 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0316 00:39:12.964237 139725430036288 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0316 00:39:14.175926 139725430036288 submission_runner.py:220] Initializing model.
I0316 00:39:24.055166 139725430036288 submission_runner.py:262] Initializing optimizer.
I0316 00:39:25.752305 139725430036288 submission_runner.py:269] Initializing metrics bundle.
I0316 00:39:25.752503 139725430036288 submission_runner.py:287] Initializing checkpoint and logger.
I0316 00:39:25.753678 139725430036288 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/imagenet_resnet_silu_jax/trial_1 with prefix checkpoint_
I0316 00:39:25.753816 139725430036288 submission_runner.py:307] Saving meta data to /experiment_runs/variants_target_setting/study_0/imagenet_resnet_silu_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0316 00:39:26.102760 139725430036288 logger_utils.py:220] Unable to record git information. Continuing without it.
I0316 00:39:26.420360 139725430036288 submission_runner.py:311] Saving flags to /experiment_runs/variants_target_setting/study_0/imagenet_resnet_silu_jax/trial_1/flags_0.json.
I0316 00:39:26.431135 139725430036288 submission_runner.py:321] Starting training loop.
I0316 00:40:11.182390 139563662018304 logging_writer.py:48] [0] global_step=0, grad_norm=0.44761475920677185, loss=6.910325050354004
I0316 00:40:11.200643 139725430036288 spec.py:321] Evaluating on the training split.
I0316 00:40:12.171489 139725430036288 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0316 00:40:12.180685 139725430036288 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0316 00:40:12.263756 139725430036288 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0316 00:40:30.808327 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 00:40:32.459710 139725430036288 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0316 00:40:32.479734 139725430036288 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0316 00:40:32.529320 139725430036288 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0316 00:40:48.066251 139725430036288 spec.py:349] Evaluating on the test split.
I0316 00:40:48.903252 139725430036288 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0316 00:40:48.908346 139725430036288 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0316 00:40:48.948608 139725430036288 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0316 00:40:52.969250 139725430036288 submission_runner.py:420] Time since start: 86.54s, 	Step: 1, 	{'train/accuracy': 0.0009566326625645161, 'train/loss': 6.90775728225708, 'validation/accuracy': 0.0009399999980814755, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.9077558517456055, 'test/num_examples': 10000, 'score': 44.76942777633667, 'total_duration': 86.53807091712952, 'accumulated_submission_time': 44.76942777633667, 'accumulated_eval_time': 41.76855397224426, 'accumulated_logging_time': 0}
I0316 00:40:52.987706 139546004010752 logging_writer.py:48] [1] accumulated_eval_time=41.768554, accumulated_logging_time=0, accumulated_submission_time=44.769428, global_step=1, preemption_count=0, score=44.769428, test/accuracy=0.000900, test/loss=6.907756, test/num_examples=10000, total_duration=86.538071, train/accuracy=0.000957, train/loss=6.907757, validation/accuracy=0.000940, validation/loss=6.907756, validation/num_examples=50000
I0316 00:40:53.187545 139545995618048 logging_writer.py:48] [1] global_step=1, grad_norm=0.5097753405570984, loss=6.915291786193848
I0316 00:40:53.382147 139546004010752 logging_writer.py:48] [2] global_step=2, grad_norm=0.47820714116096497, loss=6.92249870300293
I0316 00:40:53.578324 139545995618048 logging_writer.py:48] [3] global_step=3, grad_norm=0.4557245671749115, loss=6.911190986633301
I0316 00:40:53.773344 139546004010752 logging_writer.py:48] [4] global_step=4, grad_norm=0.48275434970855713, loss=6.911101818084717
I0316 00:40:53.976577 139545995618048 logging_writer.py:48] [5] global_step=5, grad_norm=0.46631115674972534, loss=6.9145426750183105
I0316 00:40:54.169882 139546004010752 logging_writer.py:48] [6] global_step=6, grad_norm=0.442141056060791, loss=6.908900260925293
I0316 00:40:54.364647 139545995618048 logging_writer.py:48] [7] global_step=7, grad_norm=0.4594440460205078, loss=6.913821697235107
I0316 00:40:54.559278 139546004010752 logging_writer.py:48] [8] global_step=8, grad_norm=0.4455295205116272, loss=6.9099440574646
I0316 00:40:54.755244 139545995618048 logging_writer.py:48] [9] global_step=9, grad_norm=0.4405551850795746, loss=6.909232139587402
I0316 00:40:54.952768 139546004010752 logging_writer.py:48] [10] global_step=10, grad_norm=0.4517093598842621, loss=6.907080173492432
I0316 00:40:55.151456 139545995618048 logging_writer.py:48] [11] global_step=11, grad_norm=0.47411802411079407, loss=6.9071807861328125
I0316 00:40:55.345530 139546004010752 logging_writer.py:48] [12] global_step=12, grad_norm=0.4819190800189972, loss=6.906920909881592
I0316 00:40:55.539441 139545995618048 logging_writer.py:48] [13] global_step=13, grad_norm=0.4710509181022644, loss=6.914799690246582
I0316 00:40:55.741532 139546004010752 logging_writer.py:48] [14] global_step=14, grad_norm=0.45317962765693665, loss=6.911277770996094
I0316 00:40:55.941846 139545995618048 logging_writer.py:48] [15] global_step=15, grad_norm=0.434704065322876, loss=6.902480125427246
I0316 00:40:56.140573 139546004010752 logging_writer.py:48] [16] global_step=16, grad_norm=0.43113166093826294, loss=6.9077911376953125
I0316 00:40:56.336735 139545995618048 logging_writer.py:48] [17] global_step=17, grad_norm=0.44668957591056824, loss=6.904680252075195
I0316 00:40:56.530972 139546004010752 logging_writer.py:48] [18] global_step=18, grad_norm=0.4313472807407379, loss=6.902581691741943
I0316 00:40:56.723610 139545995618048 logging_writer.py:48] [19] global_step=19, grad_norm=0.459104061126709, loss=6.903518199920654
I0316 00:40:56.917790 139546004010752 logging_writer.py:48] [20] global_step=20, grad_norm=0.44483861327171326, loss=6.903264045715332
I0316 00:40:57.115979 139545995618048 logging_writer.py:48] [21] global_step=21, grad_norm=0.45808082818984985, loss=6.89649772644043
I0316 00:40:57.314807 139546004010752 logging_writer.py:48] [22] global_step=22, grad_norm=0.44123366475105286, loss=6.893574237823486
I0316 00:40:57.506225 139545995618048 logging_writer.py:48] [23] global_step=23, grad_norm=0.428854376077652, loss=6.896161079406738
I0316 00:40:57.701094 139546004010752 logging_writer.py:48] [24] global_step=24, grad_norm=0.4859212338924408, loss=6.901924133300781
I0316 00:40:57.901857 139545995618048 logging_writer.py:48] [25] global_step=25, grad_norm=0.4494147002696991, loss=6.889927387237549
I0316 00:40:58.100354 139546004010752 logging_writer.py:48] [26] global_step=26, grad_norm=0.46129631996154785, loss=6.897960662841797
I0316 00:40:58.293510 139545995618048 logging_writer.py:48] [27] global_step=27, grad_norm=0.47489461302757263, loss=6.899106979370117
I0316 00:40:58.492855 139546004010752 logging_writer.py:48] [28] global_step=28, grad_norm=0.46162909269332886, loss=6.8863444328308105
I0316 00:40:58.684833 139545995618048 logging_writer.py:48] [29] global_step=29, grad_norm=0.531148374080658, loss=6.891022682189941
I0316 00:40:58.886242 139546004010752 logging_writer.py:48] [30] global_step=30, grad_norm=0.45846059918403625, loss=6.889740467071533
I0316 00:40:59.078868 139545995618048 logging_writer.py:48] [31] global_step=31, grad_norm=0.45737671852111816, loss=6.8771653175354
I0316 00:40:59.273780 139546004010752 logging_writer.py:48] [32] global_step=32, grad_norm=0.44774600863456726, loss=6.888289928436279
I0316 00:40:59.475959 139545995618048 logging_writer.py:48] [33] global_step=33, grad_norm=0.45121264457702637, loss=6.882292747497559
I0316 00:40:59.669503 139546004010752 logging_writer.py:48] [34] global_step=34, grad_norm=0.46223244071006775, loss=6.891407489776611
I0316 00:40:59.862972 139545995618048 logging_writer.py:48] [35] global_step=35, grad_norm=0.43384408950805664, loss=6.891791820526123
I0316 00:41:00.064661 139546004010752 logging_writer.py:48] [36] global_step=36, grad_norm=0.46336308121681213, loss=6.87567138671875
I0316 00:41:00.258448 139545995618048 logging_writer.py:48] [37] global_step=37, grad_norm=0.46264809370040894, loss=6.8780694007873535
I0316 00:41:00.454448 139546004010752 logging_writer.py:48] [38] global_step=38, grad_norm=0.4626908600330353, loss=6.884636878967285
I0316 00:41:00.657308 139545995618048 logging_writer.py:48] [39] global_step=39, grad_norm=0.42988958954811096, loss=6.867935657501221
I0316 00:41:00.861346 139546004010752 logging_writer.py:48] [40] global_step=40, grad_norm=0.4603066146373749, loss=6.87777042388916
I0316 00:41:01.057829 139545995618048 logging_writer.py:48] [41] global_step=41, grad_norm=0.5377367734909058, loss=6.864468574523926
I0316 00:41:01.261658 139546004010752 logging_writer.py:48] [42] global_step=42, grad_norm=0.46237775683403015, loss=6.878806114196777
I0316 00:41:01.468775 139545995618048 logging_writer.py:48] [43] global_step=43, grad_norm=0.5184493660926819, loss=6.851365566253662
I0316 00:41:01.662658 139546004010752 logging_writer.py:48] [44] global_step=44, grad_norm=0.45345044136047363, loss=6.871462345123291
I0316 00:41:01.865185 139545995618048 logging_writer.py:48] [45] global_step=45, grad_norm=0.4739192724227905, loss=6.896018981933594
I0316 00:41:02.058399 139546004010752 logging_writer.py:48] [46] global_step=46, grad_norm=0.4549780786037445, loss=6.88211727142334
I0316 00:41:02.251848 139545995618048 logging_writer.py:48] [47] global_step=47, grad_norm=0.4785477817058563, loss=6.854720115661621
I0316 00:41:02.449227 139546004010752 logging_writer.py:48] [48] global_step=48, grad_norm=0.46559473872184753, loss=6.874294281005859
I0316 00:41:02.652783 139545995618048 logging_writer.py:48] [49] global_step=49, grad_norm=0.4460773169994354, loss=6.8848876953125
I0316 00:41:02.851732 139546004010752 logging_writer.py:48] [50] global_step=50, grad_norm=0.4540644884109497, loss=6.875492095947266
I0316 00:41:03.051677 139545995618048 logging_writer.py:48] [51] global_step=51, grad_norm=0.4411490559577942, loss=6.88043212890625
I0316 00:41:03.250712 139546004010752 logging_writer.py:48] [52] global_step=52, grad_norm=0.506409227848053, loss=6.879937171936035
I0316 00:41:03.445243 139545995618048 logging_writer.py:48] [53] global_step=53, grad_norm=0.4588538110256195, loss=6.870689868927002
I0316 00:41:03.640816 139546004010752 logging_writer.py:48] [54] global_step=54, grad_norm=0.4978507459163666, loss=6.858374118804932
I0316 00:41:03.836692 139545995618048 logging_writer.py:48] [55] global_step=55, grad_norm=0.49315693974494934, loss=6.870591163635254
I0316 00:41:04.039416 139546004010752 logging_writer.py:48] [56] global_step=56, grad_norm=0.47320306301116943, loss=6.869428634643555
I0316 00:41:04.240231 139545995618048 logging_writer.py:48] [57] global_step=57, grad_norm=0.5062956809997559, loss=6.842345237731934
I0316 00:41:04.441638 139546004010752 logging_writer.py:48] [58] global_step=58, grad_norm=0.47659847140312195, loss=6.87547492980957
I0316 00:41:04.633468 139545995618048 logging_writer.py:48] [59] global_step=59, grad_norm=0.5085817575454712, loss=6.861530780792236
I0316 00:41:04.831204 139546004010752 logging_writer.py:48] [60] global_step=60, grad_norm=0.45922258496284485, loss=6.862748146057129
I0316 00:41:05.025996 139545995618048 logging_writer.py:48] [61] global_step=61, grad_norm=0.5096933841705322, loss=6.829862594604492
I0316 00:41:05.220309 139546004010752 logging_writer.py:48] [62] global_step=62, grad_norm=0.48858219385147095, loss=6.850942611694336
I0316 00:41:05.418841 139545995618048 logging_writer.py:48] [63] global_step=63, grad_norm=0.5512053966522217, loss=6.867725849151611
I0316 00:41:05.613991 139546004010752 logging_writer.py:48] [64] global_step=64, grad_norm=0.5288534760475159, loss=6.870292663574219
I0316 00:41:05.810333 139545995618048 logging_writer.py:48] [65] global_step=65, grad_norm=0.5250971913337708, loss=6.8412299156188965
I0316 00:41:06.004548 139546004010752 logging_writer.py:48] [66] global_step=66, grad_norm=0.5268698930740356, loss=6.8615617752075195
I0316 00:41:06.199383 139545995618048 logging_writer.py:48] [67] global_step=67, grad_norm=0.5113334655761719, loss=6.845077037811279
I0316 00:41:06.403409 139546004010752 logging_writer.py:48] [68] global_step=68, grad_norm=0.49565672874450684, loss=6.884483814239502
I0316 00:41:06.605616 139545995618048 logging_writer.py:48] [69] global_step=69, grad_norm=0.5526397824287415, loss=6.8551483154296875
I0316 00:41:06.807869 139546004010752 logging_writer.py:48] [70] global_step=70, grad_norm=0.5560685992240906, loss=6.8669657707214355
I0316 00:41:06.999983 139545995618048 logging_writer.py:48] [71] global_step=71, grad_norm=0.4971069097518921, loss=6.86415433883667
I0316 00:41:07.196816 139546004010752 logging_writer.py:48] [72] global_step=72, grad_norm=0.48235177993774414, loss=6.869974136352539
I0316 00:41:07.400513 139545995618048 logging_writer.py:48] [73] global_step=73, grad_norm=0.5273216962814331, loss=6.872042655944824
I0316 00:41:07.597817 139546004010752 logging_writer.py:48] [74] global_step=74, grad_norm=0.5100458860397339, loss=6.849030494689941
I0316 00:41:07.792247 139545995618048 logging_writer.py:48] [75] global_step=75, grad_norm=0.5104387402534485, loss=6.841569900512695
I0316 00:41:07.985449 139546004010752 logging_writer.py:48] [76] global_step=76, grad_norm=0.5441670417785645, loss=6.855031490325928
I0316 00:41:08.180567 139545995618048 logging_writer.py:48] [77] global_step=77, grad_norm=0.5090103149414062, loss=6.846078395843506
I0316 00:41:08.376224 139546004010752 logging_writer.py:48] [78] global_step=78, grad_norm=0.5622164011001587, loss=6.845359802246094
I0316 00:41:08.572072 139545995618048 logging_writer.py:48] [79] global_step=79, grad_norm=0.5570395588874817, loss=6.842798709869385
I0316 00:41:08.768603 139546004010752 logging_writer.py:48] [80] global_step=80, grad_norm=0.5007999539375305, loss=6.859825611114502
I0316 00:41:08.971192 139545995618048 logging_writer.py:48] [81] global_step=81, grad_norm=0.5461638569831848, loss=6.846014499664307
I0316 00:41:09.166038 139546004010752 logging_writer.py:48] [82] global_step=82, grad_norm=0.5272499918937683, loss=6.824295520782471
I0316 00:41:09.370474 139545995618048 logging_writer.py:48] [83] global_step=83, grad_norm=0.4863263666629791, loss=6.860474109649658
I0316 00:41:09.568903 139546004010752 logging_writer.py:48] [84] global_step=84, grad_norm=0.6053535342216492, loss=6.837072849273682
I0316 00:41:09.771594 139545995618048 logging_writer.py:48] [85] global_step=85, grad_norm=0.5240848064422607, loss=6.8347883224487305
I0316 00:41:09.966399 139546004010752 logging_writer.py:48] [86] global_step=86, grad_norm=0.5687527656555176, loss=6.842604160308838
I0316 00:41:10.162522 139545995618048 logging_writer.py:48] [87] global_step=87, grad_norm=0.547153651714325, loss=6.836157321929932
I0316 00:41:10.359416 139546004010752 logging_writer.py:48] [88] global_step=88, grad_norm=0.5408716797828674, loss=6.829889297485352
I0316 00:41:10.558443 139545995618048 logging_writer.py:48] [89] global_step=89, grad_norm=0.49455034732818604, loss=6.833117961883545
I0316 00:41:10.752841 139546004010752 logging_writer.py:48] [90] global_step=90, grad_norm=0.5690198540687561, loss=6.834661483764648
I0316 00:41:10.953339 139545995618048 logging_writer.py:48] [91] global_step=91, grad_norm=0.5803727507591248, loss=6.816472053527832
I0316 00:41:11.146100 139546004010752 logging_writer.py:48] [92] global_step=92, grad_norm=0.535946786403656, loss=6.834010124206543
I0316 00:41:11.338635 139545995618048 logging_writer.py:48] [93] global_step=93, grad_norm=0.5843982100486755, loss=6.841358184814453
I0316 00:41:11.540821 139546004010752 logging_writer.py:48] [94] global_step=94, grad_norm=0.5820639729499817, loss=6.823349475860596
I0316 00:41:11.735096 139545995618048 logging_writer.py:48] [95] global_step=95, grad_norm=0.507600724697113, loss=6.844691276550293
I0316 00:41:11.931600 139546004010752 logging_writer.py:48] [96] global_step=96, grad_norm=0.5829574465751648, loss=6.8531293869018555
I0316 00:41:12.126340 139545995618048 logging_writer.py:48] [97] global_step=97, grad_norm=0.513925313949585, loss=6.800652503967285
I0316 00:41:12.326134 139546004010752 logging_writer.py:48] [98] global_step=98, grad_norm=0.5420833826065063, loss=6.848116874694824
I0316 00:41:12.519553 139545995618048 logging_writer.py:48] [99] global_step=99, grad_norm=0.5322269797325134, loss=6.791481971740723
I0316 00:41:12.721806 139546004010752 logging_writer.py:48] [100] global_step=100, grad_norm=0.5145546197891235, loss=6.849993705749512
I0316 00:42:25.340493 139545995618048 logging_writer.py:48] [500] global_step=500, grad_norm=1.7471048831939697, loss=5.532251834869385
I0316 00:43:56.370751 139546004010752 logging_writer.py:48] [1000] global_step=1000, grad_norm=3.4122872352600098, loss=4.861424446105957
I0316 00:45:27.281702 139545995618048 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.7808468341827393, loss=4.271796226501465
I0316 00:46:58.207077 139546004010752 logging_writer.py:48] [2000] global_step=2000, grad_norm=2.1562023162841797, loss=4.080905914306641
I0316 00:48:29.351486 139545995618048 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.9531463384628296, loss=3.6441566944122314
I0316 00:49:23.073157 139725430036288 spec.py:321] Evaluating on the training split.
I0316 00:49:30.596713 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 00:49:39.020542 139725430036288 spec.py:349] Evaluating on the test split.
I0316 00:49:41.367162 139725430036288 submission_runner.py:420] Time since start: 614.94s, 	Step: 2797, 	{'train/accuracy': 0.3777303695678711, 'train/loss': 2.8826675415039062, 'validation/accuracy': 0.3177799880504608, 'validation/loss': 3.285593032836914, 'validation/num_examples': 50000, 'test/accuracy': 0.2346000075340271, 'test/loss': 4.022057056427002, 'test/num_examples': 10000, 'score': 554.7455728054047, 'total_duration': 614.9359719753265, 'accumulated_submission_time': 554.7455728054047, 'accumulated_eval_time': 60.06253504753113, 'accumulated_logging_time': 0.027496814727783203}
I0316 00:49:41.385531 139546012403456 logging_writer.py:48] [2797] accumulated_eval_time=60.062535, accumulated_logging_time=0.027497, accumulated_submission_time=554.745573, global_step=2797, preemption_count=0, score=554.745573, test/accuracy=0.234600, test/loss=4.022057, test/num_examples=10000, total_duration=614.935972, train/accuracy=0.377730, train/loss=2.882668, validation/accuracy=0.317780, validation/loss=3.285593, validation/num_examples=50000
I0316 00:50:18.549183 139546020796160 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.56679368019104, loss=3.4906258583068848
I0316 00:51:49.613836 139546012403456 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.3840467929840088, loss=3.371029853820801
I0316 00:53:20.481599 139546020796160 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.025091528892517, loss=3.3132944107055664
I0316 00:54:51.617120 139546012403456 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.640139102935791, loss=3.2392942905426025
I0316 00:56:22.536524 139546020796160 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8922497034072876, loss=2.9706966876983643
I0316 00:57:53.702322 139546012403456 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6336598992347717, loss=3.2105181217193604
I0316 00:58:11.429845 139725430036288 spec.py:321] Evaluating on the training split.
I0316 00:58:18.803029 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 00:58:27.263489 139725430036288 spec.py:349] Evaluating on the test split.
I0316 00:58:29.556116 139725430036288 submission_runner.py:420] Time since start: 1143.12s, 	Step: 5599, 	{'train/accuracy': 0.4461694657802582, 'train/loss': 2.4868123531341553, 'validation/accuracy': 0.40153998136520386, 'validation/loss': 2.776494264602661, 'validation/num_examples': 50000, 'test/accuracy': 0.3099000155925751, 'test/loss': 3.5185117721557617, 'test/num_examples': 10000, 'score': 1064.6769313812256, 'total_duration': 1143.124930858612, 'accumulated_submission_time': 1064.6769313812256, 'accumulated_eval_time': 78.1887674331665, 'accumulated_logging_time': 0.05712008476257324}
I0316 00:58:29.573938 139560897931008 logging_writer.py:48] [5599] accumulated_eval_time=78.188767, accumulated_logging_time=0.057120, accumulated_submission_time=1064.676931, global_step=5599, preemption_count=0, score=1064.676931, test/accuracy=0.309900, test/loss=3.518512, test/num_examples=10000, total_duration=1143.124931, train/accuracy=0.446169, train/loss=2.486812, validation/accuracy=0.401540, validation/loss=2.776494, validation/num_examples=50000
I0316 00:59:42.645906 139560906323712 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5669639110565186, loss=2.904559373855591
I0316 01:01:13.716983 139560897931008 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6964489221572876, loss=3.033022880554199
I0316 01:02:44.632349 139560906323712 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6846809983253479, loss=3.0929410457611084
I0316 01:04:15.666721 139560897931008 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6654901504516602, loss=2.8472204208374023
I0316 01:05:46.570770 139560906323712 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.7086573839187622, loss=3.079829216003418
I0316 01:06:59.573490 139725430036288 spec.py:321] Evaluating on the training split.
I0316 01:07:06.945943 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 01:07:15.594352 139725430036288 spec.py:349] Evaluating on the test split.
I0316 01:07:17.869206 139725430036288 submission_runner.py:420] Time since start: 1671.44s, 	Step: 8403, 	{'train/accuracy': 0.4722974896430969, 'train/loss': 2.3668148517608643, 'validation/accuracy': 0.4332599937915802, 'validation/loss': 2.6302802562713623, 'validation/num_examples': 50000, 'test/accuracy': 0.3272000253200531, 'test/loss': 3.3977982997894287, 'test/num_examples': 10000, 'score': 1574.5632791519165, 'total_duration': 1671.438006401062, 'accumulated_submission_time': 1574.5632791519165, 'accumulated_eval_time': 96.48443961143494, 'accumulated_logging_time': 0.08557748794555664}
I0316 01:07:17.889348 139561256572672 logging_writer.py:48] [8403] accumulated_eval_time=96.484440, accumulated_logging_time=0.085577, accumulated_submission_time=1574.563279, global_step=8403, preemption_count=0, score=1574.563279, test/accuracy=0.327200, test/loss=3.397798, test/num_examples=10000, total_duration=1671.438006, train/accuracy=0.472297, train/loss=2.366815, validation/accuracy=0.433260, validation/loss=2.630280, validation/num_examples=50000
I0316 01:07:35.889098 139561264965376 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.6896384358406067, loss=3.0423643589019775
I0316 01:09:06.740294 139561256572672 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6727538704872131, loss=2.8825182914733887
I0316 01:10:37.639360 139561264965376 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.6488291025161743, loss=2.956292152404785
I0316 01:12:08.665349 139561256572672 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.6344509124755859, loss=2.7609140872955322
I0316 01:13:39.509137 139561264965376 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.6862961053848267, loss=2.6531600952148438
I0316 01:15:10.569560 139561256572672 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.6963955163955688, loss=2.6113297939300537
I0316 01:15:47.875898 139725430036288 spec.py:321] Evaluating on the training split.
I0316 01:15:55.262003 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 01:16:03.674589 139725430036288 spec.py:349] Evaluating on the test split.
I0316 01:16:06.001527 139725430036288 submission_runner.py:420] Time since start: 2199.57s, 	Step: 11207, 	{'train/accuracy': 0.4318399131298065, 'train/loss': 2.6891977787017822, 'validation/accuracy': 0.4001599848270416, 'validation/loss': 2.936760663986206, 'validation/num_examples': 50000, 'test/accuracy': 0.2972000241279602, 'test/loss': 3.787325859069824, 'test/num_examples': 10000, 'score': 2084.4396369457245, 'total_duration': 2199.5703365802765, 'accumulated_submission_time': 2084.4396369457245, 'accumulated_eval_time': 114.61002850532532, 'accumulated_logging_time': 0.11574101448059082}
I0316 01:16:06.020464 139563460708096 logging_writer.py:48] [11207] accumulated_eval_time=114.610029, accumulated_logging_time=0.115741, accumulated_submission_time=2084.439637, global_step=11207, preemption_count=0, score=2084.439637, test/accuracy=0.297200, test/loss=3.787326, test/num_examples=10000, total_duration=2199.570337, train/accuracy=0.431840, train/loss=2.689198, validation/accuracy=0.400160, validation/loss=2.936761, validation/num_examples=50000
I0316 01:16:59.436302 139563578091264 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6425326466560364, loss=2.5767805576324463
I0316 01:18:30.423716 139563460708096 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.602746307849884, loss=2.6655895709991455
I0316 01:20:01.293038 139563578091264 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.6378493309020996, loss=2.6068315505981445
I0316 01:21:32.340364 139563460708096 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6087181568145752, loss=2.5956223011016846
I0316 01:23:03.118818 139563578091264 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.6550493240356445, loss=2.644341468811035
I0316 01:24:34.159338 139563460708096 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.6435882449150085, loss=2.759854793548584
I0316 01:24:36.057116 139725430036288 spec.py:321] Evaluating on the training split.
I0316 01:24:43.657632 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 01:24:51.956104 139725430036288 spec.py:349] Evaluating on the test split.
I0316 01:24:54.300454 139725430036288 submission_runner.py:420] Time since start: 2727.87s, 	Step: 14012, 	{'train/accuracy': 0.5277822017669678, 'train/loss': 2.0577874183654785, 'validation/accuracy': 0.49167999625205994, 'validation/loss': 2.2797157764434814, 'validation/num_examples': 50000, 'test/accuracy': 0.3775000274181366, 'test/loss': 3.0655620098114014, 'test/num_examples': 10000, 'score': 2594.3650550842285, 'total_duration': 2727.8692593574524, 'accumulated_submission_time': 2594.3650550842285, 'accumulated_eval_time': 132.8533160686493, 'accumulated_logging_time': 0.14440321922302246}
I0316 01:24:54.319108 139563460708096 logging_writer.py:48] [14012] accumulated_eval_time=132.853316, accumulated_logging_time=0.144403, accumulated_submission_time=2594.365055, global_step=14012, preemption_count=0, score=2594.365055, test/accuracy=0.377500, test/loss=3.065562, test/num_examples=10000, total_duration=2727.869259, train/accuracy=0.527782, train/loss=2.057787, validation/accuracy=0.491680, validation/loss=2.279716, validation/num_examples=50000
I0316 01:26:23.224651 139563469100800 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.6155243515968323, loss=2.6234183311462402
I0316 01:27:54.295624 139563460708096 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.6432491540908813, loss=2.746570348739624
I0316 01:29:25.205774 139563469100800 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6301121115684509, loss=2.585993528366089
I0316 01:30:56.066632 139563460708096 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.6349121928215027, loss=2.6710097789764404
I0316 01:32:27.143962 139563469100800 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.6135082840919495, loss=2.542712688446045
I0316 01:33:24.463024 139725430036288 spec.py:321] Evaluating on the training split.
I0316 01:33:31.951021 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 01:33:40.337554 139725430036288 spec.py:349] Evaluating on the test split.
I0316 01:33:42.697767 139725430036288 submission_runner.py:420] Time since start: 3256.27s, 	Step: 16817, 	{'train/accuracy': 0.5407366156578064, 'train/loss': 1.9808214902877808, 'validation/accuracy': 0.5069000124931335, 'validation/loss': 2.204259157180786, 'validation/num_examples': 50000, 'test/accuracy': 0.39000001549720764, 'test/loss': 3.021458864212036, 'test/num_examples': 10000, 'score': 3104.3994331359863, 'total_duration': 3256.2665758132935, 'accumulated_submission_time': 3104.3994331359863, 'accumulated_eval_time': 151.0880241394043, 'accumulated_logging_time': 0.17274951934814453}
I0316 01:33:42.717208 139563569698560 logging_writer.py:48] [16817] accumulated_eval_time=151.088024, accumulated_logging_time=0.172750, accumulated_submission_time=3104.399433, global_step=16817, preemption_count=0, score=3104.399433, test/accuracy=0.390000, test/loss=3.021459, test/num_examples=10000, total_duration=3256.266576, train/accuracy=0.540737, train/loss=1.980821, validation/accuracy=0.506900, validation/loss=2.204259, validation/num_examples=50000
I0316 01:34:16.164785 139563578091264 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.6229394674301147, loss=2.654979705810547
I0316 01:35:47.235256 139563569698560 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.6232717633247375, loss=2.5814931392669678
I0316 01:37:18.151820 139563578091264 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6396332383155823, loss=2.4973864555358887
I0316 01:38:49.232522 139563569698560 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.6168171167373657, loss=2.5097548961639404
I0316 01:40:20.069259 139563578091264 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.5902329683303833, loss=2.532705783843994
I0316 01:41:51.073033 139563569698560 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.6263679265975952, loss=2.4780819416046143
I0316 01:42:12.787827 139725430036288 spec.py:321] Evaluating on the training split.
I0316 01:42:20.262742 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 01:42:28.760703 139725430036288 spec.py:349] Evaluating on the test split.
I0316 01:42:31.103517 139725430036288 submission_runner.py:420] Time since start: 3784.67s, 	Step: 19621, 	{'train/accuracy': 0.5181361436843872, 'train/loss': 2.109130620956421, 'validation/accuracy': 0.48955997824668884, 'validation/loss': 2.2983715534210205, 'validation/num_examples': 50000, 'test/accuracy': 0.3757000267505646, 'test/loss': 3.088669776916504, 'test/num_examples': 10000, 'score': 3614.357518196106, 'total_duration': 3784.6723296642303, 'accumulated_submission_time': 3614.357518196106, 'accumulated_eval_time': 169.4036750793457, 'accumulated_logging_time': 0.2043294906616211}
I0316 01:42:31.123650 139563460708096 logging_writer.py:48] [19621] accumulated_eval_time=169.403675, accumulated_logging_time=0.204329, accumulated_submission_time=3614.357518, global_step=19621, preemption_count=0, score=3614.357518, test/accuracy=0.375700, test/loss=3.088670, test/num_examples=10000, total_duration=3784.672330, train/accuracy=0.518136, train/loss=2.109131, validation/accuracy=0.489560, validation/loss=2.298372, validation/num_examples=50000
I0316 01:43:40.235585 139563469100800 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.6329939365386963, loss=2.5940232276916504
I0316 01:45:11.295766 139563460708096 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.6037129759788513, loss=2.5590810775756836
I0316 01:46:42.172504 139563469100800 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.622468113899231, loss=2.6699109077453613
I0316 01:48:13.222854 139563460708096 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.642966628074646, loss=2.632887125015259
I0316 01:49:44.082228 139563469100800 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.610288679599762, loss=2.301820993423462
I0316 01:51:01.175288 139725430036288 spec.py:321] Evaluating on the training split.
I0316 01:51:08.834953 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 01:51:18.519761 139725430036288 spec.py:349] Evaluating on the test split.
I0316 01:51:20.874541 139725430036288 submission_runner.py:420] Time since start: 4314.44s, 	Step: 22426, 	{'train/accuracy': 0.5439851880073547, 'train/loss': 1.972494125366211, 'validation/accuracy': 0.5137199759483337, 'validation/loss': 2.164656162261963, 'validation/num_examples': 50000, 'test/accuracy': 0.39890003204345703, 'test/loss': 2.9415578842163086, 'test/num_examples': 10000, 'score': 4124.296654462814, 'total_duration': 4314.44334936142, 'accumulated_submission_time': 4124.296654462814, 'accumulated_eval_time': 189.10289025306702, 'accumulated_logging_time': 0.23410701751708984}
I0316 01:51:20.893616 139563569698560 logging_writer.py:48] [22426] accumulated_eval_time=189.102890, accumulated_logging_time=0.234107, accumulated_submission_time=4124.296654, global_step=22426, preemption_count=0, score=4124.296654, test/accuracy=0.398900, test/loss=2.941558, test/num_examples=10000, total_duration=4314.443349, train/accuracy=0.543985, train/loss=1.972494, validation/accuracy=0.513720, validation/loss=2.164656, validation/num_examples=50000
I0316 01:51:34.741080 139563578091264 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.6310220956802368, loss=2.66494083404541
I0316 01:53:05.580234 139563569698560 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.6432271599769592, loss=2.599383592605591
I0316 01:54:36.463279 139563578091264 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.6524807810783386, loss=2.6654069423675537
I0316 01:56:07.484174 139563569698560 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.5858092904090881, loss=2.568678855895996
I0316 01:57:38.322994 139563578091264 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.597420871257782, loss=2.516573905944824
I0316 01:59:09.355856 139563569698560 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.6179133057594299, loss=2.554333448410034
I0316 01:59:51.053198 139725430036288 spec.py:321] Evaluating on the training split.
I0316 01:59:58.480926 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 02:00:07.049081 139725430036288 spec.py:349] Evaluating on the test split.
I0316 02:00:09.386289 139725430036288 submission_runner.py:420] Time since start: 4842.96s, 	Step: 25231, 	{'train/accuracy': 0.5458186864852905, 'train/loss': 1.956695556640625, 'validation/accuracy': 0.5180000066757202, 'validation/loss': 2.1391093730926514, 'validation/num_examples': 50000, 'test/accuracy': 0.4059000313282013, 'test/loss': 2.9370806217193604, 'test/num_examples': 10000, 'score': 4634.342221260071, 'total_duration': 4842.955098152161, 'accumulated_submission_time': 4634.342221260071, 'accumulated_eval_time': 207.43594336509705, 'accumulated_logging_time': 0.26413846015930176}
I0316 02:00:09.407813 139563460708096 logging_writer.py:48] [25231] accumulated_eval_time=207.435943, accumulated_logging_time=0.264138, accumulated_submission_time=4634.342221, global_step=25231, preemption_count=0, score=4634.342221, test/accuracy=0.405900, test/loss=2.937081, test/num_examples=10000, total_duration=4842.955098, train/accuracy=0.545819, train/loss=1.956696, validation/accuracy=0.518000, validation/loss=2.139109, validation/num_examples=50000
I0316 02:00:58.482472 139563469100800 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.5840051174163818, loss=2.4531543254852295
I0316 02:02:29.559562 139563460708096 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.6237355470657349, loss=2.4751815795898438
I0316 02:04:00.450864 139563469100800 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.6272836327552795, loss=2.5297598838806152
I0316 02:05:31.514155 139563460708096 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.6055216789245605, loss=2.3511710166931152
I0316 02:07:02.304242 139563469100800 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.5770517587661743, loss=2.3661468029022217
I0316 02:08:33.445088 139563460708096 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.6133106350898743, loss=2.4638288021087646
I0316 02:08:39.506553 139725430036288 spec.py:321] Evaluating on the training split.
I0316 02:08:46.993889 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 02:08:55.423517 139725430036288 spec.py:349] Evaluating on the test split.
I0316 02:08:57.760926 139725430036288 submission_runner.py:420] Time since start: 5371.33s, 	Step: 28035, 	{'train/accuracy': 0.5399991869926453, 'train/loss': 2.0099799633026123, 'validation/accuracy': 0.5093199610710144, 'validation/loss': 2.1760058403015137, 'validation/num_examples': 50000, 'test/accuracy': 0.3921000063419342, 'test/loss': 2.9691014289855957, 'test/num_examples': 10000, 'score': 5144.328633308411, 'total_duration': 5371.329715490341, 'accumulated_submission_time': 5144.328633308411, 'accumulated_eval_time': 225.69025087356567, 'accumulated_logging_time': 0.29558563232421875}
I0316 02:08:57.784733 139563586483968 logging_writer.py:48] [28035] accumulated_eval_time=225.690251, accumulated_logging_time=0.295586, accumulated_submission_time=5144.328633, global_step=28035, preemption_count=0, score=5144.328633, test/accuracy=0.392100, test/loss=2.969101, test/num_examples=10000, total_duration=5371.329715, train/accuracy=0.539999, train/loss=2.009980, validation/accuracy=0.509320, validation/loss=2.176006, validation/num_examples=50000
I0316 02:10:22.524235 139563594876672 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.579586923122406, loss=2.6001596450805664
I0316 02:11:53.587712 139563586483968 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.6072957515716553, loss=2.2892754077911377
I0316 02:13:24.437577 139563594876672 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.6031361222267151, loss=2.384009599685669
I0316 02:14:55.306917 139563586483968 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.5972999334335327, loss=2.3250551223754883
I0316 02:16:26.357887 139563594876672 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.5704667568206787, loss=2.3194117546081543
I0316 02:17:27.840044 139725430036288 spec.py:321] Evaluating on the training split.
I0316 02:17:35.218249 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 02:17:43.830972 139725430036288 spec.py:349] Evaluating on the test split.
I0316 02:17:46.103082 139725430036288 submission_runner.py:420] Time since start: 5899.67s, 	Step: 30840, 	{'train/accuracy': 0.5283203125, 'train/loss': 2.044196844100952, 'validation/accuracy': 0.5013200044631958, 'validation/loss': 2.2105205059051514, 'validation/num_examples': 50000, 'test/accuracy': 0.38610002398490906, 'test/loss': 2.9892220497131348, 'test/num_examples': 10000, 'score': 5654.273295402527, 'total_duration': 5899.6718645095825, 'accumulated_submission_time': 5654.273295402527, 'accumulated_eval_time': 243.95322513580322, 'accumulated_logging_time': 0.32945823669433594}
I0316 02:17:46.125554 139562726704896 logging_writer.py:48] [30840] accumulated_eval_time=243.953225, accumulated_logging_time=0.329458, accumulated_submission_time=5654.273295, global_step=30840, preemption_count=0, score=5654.273295, test/accuracy=0.386100, test/loss=2.989222, test/num_examples=10000, total_duration=5899.671865, train/accuracy=0.528320, train/loss=2.044197, validation/accuracy=0.501320, validation/loss=2.210521, validation/num_examples=50000
I0316 02:18:15.386906 139562735097600 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.5812851786613464, loss=2.2693705558776855
I0316 02:19:46.432201 139562726704896 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.5866109132766724, loss=2.213675022125244
I0316 02:21:17.306941 139562735097600 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.6154854893684387, loss=2.6122162342071533
I0316 02:22:48.395248 139562726704896 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.587249219417572, loss=2.3471176624298096
I0316 02:24:19.244286 139562735097600 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.5888159275054932, loss=2.4281156063079834
I0316 02:25:50.321028 139562726704896 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.5803989171981812, loss=2.4414916038513184
I0316 02:26:16.203053 139725430036288 spec.py:321] Evaluating on the training split.
I0316 02:26:23.654953 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 02:26:32.215435 139725430036288 spec.py:349] Evaluating on the test split.
I0316 02:26:34.538597 139725430036288 submission_runner.py:420] Time since start: 6428.11s, 	Step: 33644, 	{'train/accuracy': 0.55078125, 'train/loss': 1.9237500429153442, 'validation/accuracy': 0.5217399597167969, 'validation/loss': 2.1103367805480957, 'validation/num_examples': 50000, 'test/accuracy': 0.4102000296115875, 'test/loss': 2.8517088890075684, 'test/num_examples': 10000, 'score': 6164.236007452011, 'total_duration': 6428.107389688492, 'accumulated_submission_time': 6164.236007452011, 'accumulated_eval_time': 262.2887086868286, 'accumulated_logging_time': 0.3625681400299072}
I0316 02:26:34.557372 139562709919488 logging_writer.py:48] [33644] accumulated_eval_time=262.288709, accumulated_logging_time=0.362568, accumulated_submission_time=6164.236007, global_step=33644, preemption_count=0, score=6164.236007, test/accuracy=0.410200, test/loss=2.851709, test/num_examples=10000, total_duration=6428.107390, train/accuracy=0.550781, train/loss=1.923750, validation/accuracy=0.521740, validation/loss=2.110337, validation/num_examples=50000
I0316 02:27:39.465687 139562718312192 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.6148477792739868, loss=2.397278070449829
I0316 02:29:10.485958 139562709919488 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.5541295409202576, loss=2.409315824508667
I0316 02:30:41.305252 139562718312192 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.5836238265037537, loss=2.2137842178344727
I0316 02:32:12.354693 139562709919488 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.6172060966491699, loss=2.57352614402771
I0316 02:33:43.186784 139562718312192 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.5554679036140442, loss=2.0593183040618896
I0316 02:35:04.695106 139725430036288 spec.py:321] Evaluating on the training split.
I0316 02:35:12.172956 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 02:35:20.626247 139725430036288 spec.py:349] Evaluating on the test split.
I0316 02:35:23.030133 139725430036288 submission_runner.py:420] Time since start: 6956.60s, 	Step: 36450, 	{'train/accuracy': 0.6231265664100647, 'train/loss': 1.5569429397583008, 'validation/accuracy': 0.5211600065231323, 'validation/loss': 2.109849214553833, 'validation/num_examples': 50000, 'test/accuracy': 0.40610000491142273, 'test/loss': 2.878661632537842, 'test/num_examples': 10000, 'score': 6674.259563446045, 'total_duration': 6956.598942995071, 'accumulated_submission_time': 6674.259563446045, 'accumulated_eval_time': 280.6237015724182, 'accumulated_logging_time': 0.3942446708679199}
I0316 02:35:23.053867 139563586483968 logging_writer.py:48] [36450] accumulated_eval_time=280.623702, accumulated_logging_time=0.394245, accumulated_submission_time=6674.259563, global_step=36450, preemption_count=0, score=6674.259563, test/accuracy=0.406100, test/loss=2.878662, test/num_examples=10000, total_duration=6956.598943, train/accuracy=0.623127, train/loss=1.556943, validation/accuracy=0.521160, validation/loss=2.109849, validation/num_examples=50000
I0316 02:35:32.341461 139563594876672 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.5822371244430542, loss=2.2580385208129883
I0316 02:37:03.395609 139563586483968 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.6000664234161377, loss=2.396773338317871
I0316 02:38:34.250767 139563594876672 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.5760064721107483, loss=2.4094367027282715
I0316 02:40:05.285693 139563586483968 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.5474331974983215, loss=2.353682279586792
I0316 02:41:36.158447 139563594876672 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.5891013145446777, loss=2.403421401977539
I0316 02:43:07.187924 139563586483968 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.5994011163711548, loss=2.458346366882324
I0316 02:43:53.032980 139725430036288 spec.py:321] Evaluating on the training split.
I0316 02:44:00.648579 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 02:44:09.190874 139725430036288 spec.py:349] Evaluating on the test split.
I0316 02:44:11.446130 139725430036288 submission_runner.py:420] Time since start: 7485.01s, 	Step: 39254, 	{'train/accuracy': 0.5848612785339355, 'train/loss': 1.7456644773483276, 'validation/accuracy': 0.5212000012397766, 'validation/loss': 2.116684913635254, 'validation/num_examples': 50000, 'test/accuracy': 0.40470001101493835, 'test/loss': 2.9121763706207275, 'test/num_examples': 10000, 'score': 7184.125803232193, 'total_duration': 7485.014935970306, 'accumulated_submission_time': 7184.125803232193, 'accumulated_eval_time': 299.0368101596832, 'accumulated_logging_time': 0.4279322624206543}
I0316 02:44:11.465813 139562709919488 logging_writer.py:48] [39254] accumulated_eval_time=299.036810, accumulated_logging_time=0.427932, accumulated_submission_time=7184.125803, global_step=39254, preemption_count=0, score=7184.125803, test/accuracy=0.404700, test/loss=2.912176, test/num_examples=10000, total_duration=7485.014936, train/accuracy=0.584861, train/loss=1.745664, validation/accuracy=0.521200, validation/loss=2.116685, validation/num_examples=50000
I0316 02:44:56.345237 139562718312192 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.5825587511062622, loss=2.121248245239258
I0316 02:46:27.371642 139562709919488 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.6079080700874329, loss=2.4550602436065674
I0316 02:47:58.186933 139562718312192 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.6027303338050842, loss=2.4129345417022705
I0316 02:49:29.294142 139562709919488 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.5627144575119019, loss=2.133045196533203
I0316 02:51:00.118398 139562718312192 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.5936693549156189, loss=2.281466245651245
I0316 02:52:31.211182 139562709919488 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.5910301208496094, loss=2.3224682807922363
I0316 02:52:41.485148 139725430036288 spec.py:321] Evaluating on the training split.
I0316 02:52:49.041469 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 02:52:57.585517 139725430036288 spec.py:349] Evaluating on the test split.
I0316 02:52:59.918643 139725430036288 submission_runner.py:420] Time since start: 8013.49s, 	Step: 42058, 	{'train/accuracy': 0.5817123651504517, 'train/loss': 1.7716667652130127, 'validation/accuracy': 0.5311200022697449, 'validation/loss': 2.07171630859375, 'validation/num_examples': 50000, 'test/accuracy': 0.41210001707077026, 'test/loss': 2.8512117862701416, 'test/num_examples': 10000, 'score': 7694.032695770264, 'total_duration': 8013.487437486649, 'accumulated_submission_time': 7694.032695770264, 'accumulated_eval_time': 317.4702606201172, 'accumulated_logging_time': 0.4591078758239746}
I0316 02:52:59.942553 139562709919488 logging_writer.py:48] [42058] accumulated_eval_time=317.470261, accumulated_logging_time=0.459108, accumulated_submission_time=7694.032696, global_step=42058, preemption_count=0, score=7694.032696, test/accuracy=0.412100, test/loss=2.851212, test/num_examples=10000, total_duration=8013.487437, train/accuracy=0.581712, train/loss=1.771667, validation/accuracy=0.531120, validation/loss=2.071716, validation/num_examples=50000
I0316 02:54:20.451127 139563578091264 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.5840718746185303, loss=2.1867637634277344
I0316 02:55:51.337504 139562709919488 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.5507592558860779, loss=2.3706555366516113
I0316 02:57:22.328720 139563578091264 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.5961565375328064, loss=2.3039705753326416
I0316 02:58:53.185642 139562709919488 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.6054562330245972, loss=2.3796825408935547
I0316 03:00:24.167474 139563578091264 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.5625378489494324, loss=2.302389144897461
I0316 03:01:30.001003 139725430036288 spec.py:321] Evaluating on the training split.
I0316 03:01:37.407184 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 03:01:46.049783 139725430036288 spec.py:349] Evaluating on the test split.
I0316 03:01:48.384130 139725430036288 submission_runner.py:420] Time since start: 8541.95s, 	Step: 44864, 	{'train/accuracy': 0.6045320630073547, 'train/loss': 1.64271080493927, 'validation/accuracy': 0.5548999905586243, 'validation/loss': 1.9526820182800293, 'validation/num_examples': 50000, 'test/accuracy': 0.4333000183105469, 'test/loss': 2.7427008152008057, 'test/num_examples': 10000, 'score': 8203.979739427567, 'total_duration': 8541.952942848206, 'accumulated_submission_time': 8203.979739427567, 'accumulated_eval_time': 335.8533761501312, 'accumulated_logging_time': 0.49260902404785156}
I0316 03:01:48.407584 139562718312192 logging_writer.py:48] [44864] accumulated_eval_time=335.853376, accumulated_logging_time=0.492609, accumulated_submission_time=8203.979739, global_step=44864, preemption_count=0, score=8203.979739, test/accuracy=0.433300, test/loss=2.742701, test/num_examples=10000, total_duration=8541.952943, train/accuracy=0.604532, train/loss=1.642711, validation/accuracy=0.554900, validation/loss=1.952682, validation/num_examples=50000
I0316 03:02:13.339123 139562726704896 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.5872277617454529, loss=2.1890459060668945
I0316 03:03:44.385291 139562718312192 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.5622026920318604, loss=2.2912392616271973
I0316 03:05:15.231480 139562726704896 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.5625321269035339, loss=2.439673900604248
I0316 03:06:46.270750 139562718312192 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.5705539584159851, loss=2.1540608406066895
I0316 03:08:17.098664 139562726704896 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.5594844222068787, loss=2.359779119491577
I0316 03:09:48.094051 139562718312192 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.5779052972793579, loss=2.286222457885742
I0316 03:10:18.502165 139725430036288 spec.py:321] Evaluating on the training split.
I0316 03:10:26.142652 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 03:10:34.701246 139725430036288 spec.py:349] Evaluating on the test split.
I0316 03:10:37.032952 139725430036288 submission_runner.py:420] Time since start: 9070.60s, 	Step: 47669, 	{'train/accuracy': 0.5823700428009033, 'train/loss': 1.7545100450515747, 'validation/accuracy': 0.5369399785995483, 'validation/loss': 2.0408244132995605, 'validation/num_examples': 50000, 'test/accuracy': 0.4198000133037567, 'test/loss': 2.8394970893859863, 'test/num_examples': 10000, 'score': 8713.962616920471, 'total_duration': 9070.60176706314, 'accumulated_submission_time': 8713.962616920471, 'accumulated_eval_time': 354.3841245174408, 'accumulated_logging_time': 0.5262198448181152}
I0316 03:10:37.053406 139562709919488 logging_writer.py:48] [47669] accumulated_eval_time=354.384125, accumulated_logging_time=0.526220, accumulated_submission_time=8713.962617, global_step=47669, preemption_count=0, score=8713.962617, test/accuracy=0.419800, test/loss=2.839497, test/num_examples=10000, total_duration=9070.601767, train/accuracy=0.582370, train/loss=1.754510, validation/accuracy=0.536940, validation/loss=2.040824, validation/num_examples=50000
I0316 03:11:37.394119 139563578091264 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.5685532093048096, loss=2.3172531127929688
I0316 03:13:08.379455 139562709919488 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.5084497928619385, loss=2.0929503440856934
I0316 03:14:39.144485 139563578091264 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.6137064695358276, loss=2.2703309059143066
I0316 03:16:10.124340 139562709919488 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.5444047451019287, loss=2.1511149406433105
I0316 03:17:40.906840 139563578091264 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.562208890914917, loss=2.142713785171509
I0316 03:19:07.058581 139725430036288 spec.py:321] Evaluating on the training split.
I0316 03:19:14.603895 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 03:19:24.797161 139725430036288 spec.py:349] Evaluating on the test split.
I0316 03:19:27.135011 139725430036288 submission_runner.py:420] Time since start: 9600.70s, 	Step: 50476, 	{'train/accuracy': 0.5855987071990967, 'train/loss': 1.7411925792694092, 'validation/accuracy': 0.5447800159454346, 'validation/loss': 1.9759788513183594, 'validation/num_examples': 50000, 'test/accuracy': 0.4279000163078308, 'test/loss': 2.75671124458313, 'test/num_examples': 10000, 'score': 9223.852549552917, 'total_duration': 9600.703824520111, 'accumulated_submission_time': 9223.852549552917, 'accumulated_eval_time': 374.4605236053467, 'accumulated_logging_time': 0.5567066669464111}
I0316 03:19:27.155416 139562103858944 logging_writer.py:48] [50476] accumulated_eval_time=374.460524, accumulated_logging_time=0.556707, accumulated_submission_time=9223.852550, global_step=50476, preemption_count=0, score=9223.852550, test/accuracy=0.427900, test/loss=2.756711, test/num_examples=10000, total_duration=9600.703825, train/accuracy=0.585599, train/loss=1.741193, validation/accuracy=0.544780, validation/loss=1.975979, validation/num_examples=50000
I0316 03:19:31.720284 139562709919488 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.6021803021430969, loss=2.250983715057373
I0316 03:21:02.709964 139562103858944 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.566299557685852, loss=2.098097324371338
I0316 03:22:33.576338 139562709919488 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.5782656669616699, loss=2.2591209411621094
I0316 03:24:04.592398 139562103858944 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.5787199139595032, loss=2.1796228885650635
I0316 03:25:35.422698 139562709919488 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.5938265323638916, loss=2.458195924758911
I0316 03:27:06.426376 139562103858944 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.5876614451408386, loss=2.278620719909668
I0316 03:27:57.138589 139725430036288 spec.py:321] Evaluating on the training split.
I0316 03:28:04.971075 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 03:28:17.263936 139725430036288 spec.py:349] Evaluating on the test split.
I0316 03:28:19.565071 139725430036288 submission_runner.py:420] Time since start: 10133.13s, 	Step: 53281, 	{'train/accuracy': 0.6027781963348389, 'train/loss': 1.6589690446853638, 'validation/accuracy': 0.5643399953842163, 'validation/loss': 1.9020472764968872, 'validation/num_examples': 50000, 'test/accuracy': 0.4481000304222107, 'test/loss': 2.6654083728790283, 'test/num_examples': 10000, 'score': 9733.72113442421, 'total_duration': 10133.133841276169, 'accumulated_submission_time': 9733.72113442421, 'accumulated_eval_time': 396.8869295120239, 'accumulated_logging_time': 0.5897741317749023}
I0316 03:28:19.583389 139562949015296 logging_writer.py:48] [53281] accumulated_eval_time=396.886930, accumulated_logging_time=0.589774, accumulated_submission_time=9733.721134, global_step=53281, preemption_count=0, score=9733.721134, test/accuracy=0.448100, test/loss=2.665408, test/num_examples=10000, total_duration=10133.133841, train/accuracy=0.602778, train/loss=1.658969, validation/accuracy=0.564340, validation/loss=1.902047, validation/num_examples=50000
I0316 03:28:59.552057 139563026597632 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.5808900594711304, loss=2.385666847229004
I0316 03:30:30.577128 139562949015296 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.5504426956176758, loss=2.2248826026916504
I0316 03:32:01.422332 139563026597632 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.5480431914329529, loss=2.0958213806152344
I0316 03:33:32.502399 139562949015296 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.5407748818397522, loss=2.087155342102051
I0316 03:35:03.401761 139563026597632 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.5424485206604004, loss=2.0967462062835693
I0316 03:36:34.407812 139562949015296 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.5776008367538452, loss=2.2688605785369873
I0316 03:36:49.587623 139725430036288 spec.py:321] Evaluating on the training split.
I0316 03:36:57.815417 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 03:37:12.477796 139725430036288 spec.py:349] Evaluating on the test split.
I0316 03:37:14.691879 139725430036288 submission_runner.py:420] Time since start: 10668.26s, 	Step: 56085, 	{'train/accuracy': 0.5968989133834839, 'train/loss': 1.692585825920105, 'validation/accuracy': 0.5556600093841553, 'validation/loss': 1.945448398590088, 'validation/num_examples': 50000, 'test/accuracy': 0.44050002098083496, 'test/loss': 2.7063052654266357, 'test/num_examples': 10000, 'score': 10243.608927488327, 'total_duration': 10668.260700702667, 'accumulated_submission_time': 10243.608927488327, 'accumulated_eval_time': 421.9911689758301, 'accumulated_logging_time': 0.6212289333343506}
I0316 03:37:14.711210 139563034990336 logging_writer.py:48] [56085] accumulated_eval_time=421.991169, accumulated_logging_time=0.621229, accumulated_submission_time=10243.608927, global_step=56085, preemption_count=0, score=10243.608927, test/accuracy=0.440500, test/loss=2.706305, test/num_examples=10000, total_duration=10668.260701, train/accuracy=0.596899, train/loss=1.692586, validation/accuracy=0.555660, validation/loss=1.945448, validation/num_examples=50000
I0316 03:38:30.280904 139563578091264 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.5647571086883545, loss=2.2017722129821777
I0316 03:40:01.118987 139563034990336 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.5507593750953674, loss=2.090935707092285
I0316 03:41:32.150478 139563578091264 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.5600723028182983, loss=2.221515417098999
I0316 03:43:02.980829 139563034990336 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.5612632036209106, loss=2.2342894077301025
I0316 03:44:34.026927 139563578091264 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.5526781678199768, loss=2.1358566284179688
I0316 03:45:44.831669 139725430036288 spec.py:321] Evaluating on the training split.
I0316 03:45:52.930325 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 03:46:07.815868 139725430036288 spec.py:349] Evaluating on the test split.
I0316 03:46:10.023610 139725430036288 submission_runner.py:420] Time since start: 11203.59s, 	Step: 58891, 	{'train/accuracy': 0.5959023833274841, 'train/loss': 1.6968411207199097, 'validation/accuracy': 0.5623799562454224, 'validation/loss': 1.9091722965240479, 'validation/num_examples': 50000, 'test/accuracy': 0.4401000142097473, 'test/loss': 2.6764233112335205, 'test/num_examples': 10000, 'score': 10753.614179611206, 'total_duration': 11203.592434883118, 'accumulated_submission_time': 10753.614179611206, 'accumulated_eval_time': 447.18309020996094, 'accumulated_logging_time': 0.6515779495239258}
I0316 03:46:10.042127 139563026597632 logging_writer.py:48] [58891] accumulated_eval_time=447.183090, accumulated_logging_time=0.651578, accumulated_submission_time=10753.614180, global_step=58891, preemption_count=0, score=10753.614180, test/accuracy=0.440100, test/loss=2.676423, test/num_examples=10000, total_duration=11203.592435, train/accuracy=0.595902, train/loss=1.696841, validation/accuracy=0.562380, validation/loss=1.909172, validation/num_examples=50000
I0316 03:46:30.046782 139563034990336 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.5849315524101257, loss=2.218137741088867
I0316 03:48:01.078622 139563026597632 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.5869332551956177, loss=2.105423927307129
I0316 03:49:31.908614 139563034990336 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.5469114184379578, loss=2.1343493461608887
I0316 03:51:02.987855 139563026597632 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.5404883623123169, loss=2.037458658218384
I0316 03:52:33.919196 139563034990336 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.5341022610664368, loss=2.103201389312744
I0316 03:54:05.059247 139563026597632 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.5201241970062256, loss=2.011101245880127
I0316 03:54:40.033711 139725430036288 spec.py:321] Evaluating on the training split.
I0316 03:54:48.199008 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 03:55:03.139387 139725430036288 spec.py:349] Evaluating on the test split.
I0316 03:55:05.367716 139725430036288 submission_runner.py:420] Time since start: 11738.94s, 	Step: 61694, 	{'train/accuracy': 0.5912388563156128, 'train/loss': 1.7321046590805054, 'validation/accuracy': 0.5537399649620056, 'validation/loss': 1.9472651481628418, 'validation/num_examples': 50000, 'test/accuracy': 0.43480002880096436, 'test/loss': 2.7636868953704834, 'test/num_examples': 10000, 'score': 11263.492208719254, 'total_duration': 11738.936523675919, 'accumulated_submission_time': 11263.492208719254, 'accumulated_eval_time': 472.5170512199402, 'accumulated_logging_time': 0.6798145771026611}
I0316 03:55:05.386344 139564215691008 logging_writer.py:48] [61694] accumulated_eval_time=472.517051, accumulated_logging_time=0.679815, accumulated_submission_time=11263.492209, global_step=61694, preemption_count=0, score=11263.492209, test/accuracy=0.434800, test/loss=2.763687, test/num_examples=10000, total_duration=11738.936524, train/accuracy=0.591239, train/loss=1.732105, validation/accuracy=0.553740, validation/loss=1.947265, validation/num_examples=50000
I0316 03:56:01.147310 139564316305152 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.5349366068840027, loss=2.036993980407715
I0316 03:57:32.175713 139564215691008 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.5427669286727905, loss=2.1176199913024902
I0316 03:59:02.961489 139564316305152 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.5839278101921082, loss=2.1806325912475586
I0316 04:00:33.756726 139564215691008 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.5346437096595764, loss=2.097527503967285
I0316 04:02:04.727265 139564316305152 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.5307667851448059, loss=2.0514426231384277
I0316 04:03:35.555720 139564215691008 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.5609830021858215, loss=2.036576747894287
I0316 04:03:35.562726 139725430036288 spec.py:321] Evaluating on the training split.
I0316 04:03:43.871345 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 04:03:57.462916 139725430036288 spec.py:349] Evaluating on the test split.
I0316 04:03:59.638170 139725430036288 submission_runner.py:420] Time since start: 12273.21s, 	Step: 64501, 	{'train/accuracy': 0.6083784699440002, 'train/loss': 1.6426235437393188, 'validation/accuracy': 0.574459969997406, 'validation/loss': 1.8577344417572021, 'validation/num_examples': 50000, 'test/accuracy': 0.453000009059906, 'test/loss': 2.6313228607177734, 'test/num_examples': 10000, 'score': 11773.553865909576, 'total_duration': 12273.206993341446, 'accumulated_submission_time': 11773.553865909576, 'accumulated_eval_time': 496.5924394130707, 'accumulated_logging_time': 0.708660364151001}
I0316 04:03:59.656306 139564207298304 logging_writer.py:48] [64501] accumulated_eval_time=496.592439, accumulated_logging_time=0.708660, accumulated_submission_time=11773.553866, global_step=64501, preemption_count=0, score=11773.553866, test/accuracy=0.453000, test/loss=2.631323, test/num_examples=10000, total_duration=12273.206993, train/accuracy=0.608378, train/loss=1.642624, validation/accuracy=0.574460, validation/loss=1.857734, validation/num_examples=50000
I0316 04:05:30.706016 139564215691008 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.5384786128997803, loss=2.023552656173706
I0316 04:07:01.437395 139564207298304 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.576054573059082, loss=2.240748167037964
I0316 04:08:32.521255 139564215691008 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.5184885859489441, loss=1.8875492811203003
I0316 04:10:03.350159 139564207298304 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.5423668026924133, loss=2.105435609817505
I0316 04:11:34.322900 139564215691008 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.547203004360199, loss=2.062117338180542
I0316 04:12:29.765051 139725430036288 spec.py:321] Evaluating on the training split.
I0316 04:12:38.286159 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 04:12:53.493914 139725430036288 spec.py:349] Evaluating on the test split.
I0316 04:12:55.687937 139725430036288 submission_runner.py:420] Time since start: 12809.26s, 	Step: 67307, 	{'train/accuracy': 0.630301296710968, 'train/loss': 1.5318502187728882, 'validation/accuracy': 0.5893200039863586, 'validation/loss': 1.756237268447876, 'validation/num_examples': 50000, 'test/accuracy': 0.46220001578330994, 'test/loss': 2.512631416320801, 'test/num_examples': 10000, 'score': 12283.551887750626, 'total_duration': 12809.256762266159, 'accumulated_submission_time': 12283.551887750626, 'accumulated_eval_time': 522.5153031349182, 'accumulated_logging_time': 0.7359635829925537}
I0316 04:12:55.708154 139564324697856 logging_writer.py:48] [67307] accumulated_eval_time=522.515303, accumulated_logging_time=0.735964, accumulated_submission_time=12283.551888, global_step=67307, preemption_count=0, score=12283.551888, test/accuracy=0.462200, test/loss=2.512631, test/num_examples=10000, total_duration=12809.256762, train/accuracy=0.630301, train/loss=1.531850, validation/accuracy=0.589320, validation/loss=1.756237, validation/num_examples=50000
I0316 04:13:30.935671 139564333090560 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.5536673665046692, loss=2.0569393634796143
I0316 04:15:01.879463 139564324697856 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.5421137809753418, loss=2.0513336658477783
I0316 04:16:32.692044 139564333090560 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.5811235308647156, loss=1.8460943698883057
I0316 04:18:03.929199 139564324697856 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.5728899836540222, loss=2.0139622688293457
I0316 04:19:34.760144 139564333090560 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.5383332371711731, loss=2.077362537384033
I0316 04:21:05.577924 139564324697856 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.617108941078186, loss=2.104353666305542
I0316 04:21:25.816956 139725430036288 spec.py:321] Evaluating on the training split.
I0316 04:21:33.471577 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 04:21:46.323006 139725430036288 spec.py:349] Evaluating on the test split.
I0316 04:21:48.626838 139725430036288 submission_runner.py:420] Time since start: 13342.20s, 	Step: 70112, 	{'train/accuracy': 0.6595184803009033, 'train/loss': 1.371079921722412, 'validation/accuracy': 0.5632199645042419, 'validation/loss': 1.9180924892425537, 'validation/num_examples': 50000, 'test/accuracy': 0.4400000274181366, 'test/loss': 2.6956000328063965, 'test/num_examples': 10000, 'score': 12793.549216508865, 'total_duration': 13342.195656776428, 'accumulated_submission_time': 12793.549216508865, 'accumulated_eval_time': 545.3251523971558, 'accumulated_logging_time': 0.765692949295044}
I0316 04:21:48.645889 139564307912448 logging_writer.py:48] [70112] accumulated_eval_time=545.325152, accumulated_logging_time=0.765693, accumulated_submission_time=12793.549217, global_step=70112, preemption_count=0, score=12793.549217, test/accuracy=0.440000, test/loss=2.695600, test/num_examples=10000, total_duration=13342.195657, train/accuracy=0.659518, train/loss=1.371080, validation/accuracy=0.563220, validation/loss=1.918092, validation/num_examples=50000
I0316 04:22:59.270193 139564316305152 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.5186900496482849, loss=1.8879860639572144
I0316 04:24:30.067726 139564307912448 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.5420776009559631, loss=1.9781906604766846
I0316 04:26:01.026431 139564316305152 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.5300925970077515, loss=2.0547690391540527
I0316 04:27:31.844794 139564307912448 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.547433614730835, loss=1.9954947233200073
I0316 04:29:02.812171 139564316305152 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.538454532623291, loss=2.085843563079834
I0316 04:30:18.653519 139725430036288 spec.py:321] Evaluating on the training split.
I0316 04:30:26.330774 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 04:30:39.778693 139725430036288 spec.py:349] Evaluating on the test split.
I0316 04:30:42.008162 139725430036288 submission_runner.py:420] Time since start: 13875.58s, 	Step: 72919, 	{'train/accuracy': 0.6660555005073547, 'train/loss': 1.3387036323547363, 'validation/accuracy': 0.5880999565124512, 'validation/loss': 1.769765853881836, 'validation/num_examples': 50000, 'test/accuracy': 0.4571000337600708, 'test/loss': 2.5662119388580322, 'test/num_examples': 10000, 'score': 13303.445712327957, 'total_duration': 13875.576974868774, 'accumulated_submission_time': 13303.445712327957, 'accumulated_eval_time': 568.6797630786896, 'accumulated_logging_time': 0.7939844131469727}
I0316 04:30:42.028589 139564341483264 logging_writer.py:48] [72919] accumulated_eval_time=568.679763, accumulated_logging_time=0.793984, accumulated_submission_time=13303.445712, global_step=72919, preemption_count=0, score=13303.445712, test/accuracy=0.457100, test/loss=2.566212, test/num_examples=10000, total_duration=13875.576975, train/accuracy=0.666056, train/loss=1.338704, validation/accuracy=0.588100, validation/loss=1.769766, validation/num_examples=50000
I0316 04:30:56.917655 139564349875968 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.559699296951294, loss=1.9922153949737549
I0316 04:32:27.918566 139564341483264 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.551824152469635, loss=2.0477654933929443
I0316 04:33:58.763715 139564349875968 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.5116992592811584, loss=1.8977302312850952
I0316 04:35:29.840167 139564341483264 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.5190946459770203, loss=1.8866018056869507
I0316 04:37:00.645515 139564349875968 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.529412031173706, loss=1.8679754734039307
I0316 04:38:31.645083 139564341483264 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.4858180582523346, loss=1.6933414936065674
I0316 04:39:12.073248 139725430036288 spec.py:321] Evaluating on the training split.
I0316 04:39:19.593577 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 04:39:33.758072 139725430036288 spec.py:349] Evaluating on the test split.
I0316 04:39:35.948188 139725430036288 submission_runner.py:420] Time since start: 14409.52s, 	Step: 75724, 	{'train/accuracy': 0.6802455186843872, 'train/loss': 1.2749207019805908, 'validation/accuracy': 0.6038999557495117, 'validation/loss': 1.6779900789260864, 'validation/num_examples': 50000, 'test/accuracy': 0.47470003366470337, 'test/loss': 2.478337526321411, 'test/num_examples': 10000, 'score': 13813.378238916397, 'total_duration': 14409.517012834549, 'accumulated_submission_time': 13813.378238916397, 'accumulated_eval_time': 592.5546867847443, 'accumulated_logging_time': 0.8258340358734131}
I0316 04:39:35.967222 139564089865984 logging_writer.py:48] [75724] accumulated_eval_time=592.554687, accumulated_logging_time=0.825834, accumulated_submission_time=13813.378239, global_step=75724, preemption_count=0, score=13813.378239, test/accuracy=0.474700, test/loss=2.478338, test/num_examples=10000, total_duration=14409.517013, train/accuracy=0.680246, train/loss=1.274921, validation/accuracy=0.603900, validation/loss=1.677990, validation/num_examples=50000
I0316 04:40:26.231076 139564307912448 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.5208898782730103, loss=1.87563157081604
I0316 04:41:57.253870 139564089865984 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.542458176612854, loss=1.8177828788757324
I0316 04:43:28.101396 139564307912448 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.5503816604614258, loss=1.985304355621338
I0316 04:44:58.880210 139564089865984 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.5645361542701721, loss=1.7662644386291504
I0316 04:46:29.909222 139564307912448 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.49549615383148193, loss=1.8504775762557983
I0316 04:48:00.703178 139564089865984 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.5233400464057922, loss=1.8671529293060303
I0316 04:48:06.051072 139725430036288 spec.py:321] Evaluating on the training split.
I0316 04:48:13.372152 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 04:48:28.438472 139725430036288 spec.py:349] Evaluating on the test split.
I0316 04:48:30.629172 139725430036288 submission_runner.py:420] Time since start: 14944.20s, 	Step: 78531, 	{'train/accuracy': 0.6827168464660645, 'train/loss': 1.266086220741272, 'validation/accuracy': 0.6155799627304077, 'validation/loss': 1.6203733682632446, 'validation/num_examples': 50000, 'test/accuracy': 0.49500003457069397, 'test/loss': 2.3525707721710205, 'test/num_examples': 10000, 'score': 14323.351205825806, 'total_duration': 14944.197999477386, 'accumulated_submission_time': 14323.351205825806, 'accumulated_eval_time': 617.1327600479126, 'accumulated_logging_time': 0.8554675579071045}
I0316 04:48:30.647953 139563628447488 logging_writer.py:48] [78531] accumulated_eval_time=617.132760, accumulated_logging_time=0.855468, accumulated_submission_time=14323.351206, global_step=78531, preemption_count=0, score=14323.351206, test/accuracy=0.495000, test/loss=2.352571, test/num_examples=10000, total_duration=14944.197999, train/accuracy=0.682717, train/loss=1.266086, validation/accuracy=0.615580, validation/loss=1.620373, validation/num_examples=50000
I0316 04:49:56.159992 139564089865984 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.5773617029190063, loss=1.8113980293273926
I0316 04:51:26.919871 139563628447488 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.5314375758171082, loss=1.8950562477111816
I0316 04:52:57.894427 139564089865984 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.5281901955604553, loss=1.8963879346847534
I0316 04:54:28.667020 139563628447488 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.5210610032081604, loss=1.763685703277588
I0316 04:55:59.745071 139564089865984 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.5653931498527527, loss=1.9523640871047974
I0316 04:57:00.768292 139725430036288 spec.py:321] Evaluating on the training split.
I0316 04:57:08.062108 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 04:57:22.017160 139725430036288 spec.py:349] Evaluating on the test split.
I0316 04:57:24.211018 139725430036288 submission_runner.py:420] Time since start: 15477.78s, 	Step: 81338, 	{'train/accuracy': 0.6678491830825806, 'train/loss': 1.3394726514816284, 'validation/accuracy': 0.6077399849891663, 'validation/loss': 1.6649376153945923, 'validation/num_examples': 50000, 'test/accuracy': 0.4856000244617462, 'test/loss': 2.405643939971924, 'test/num_examples': 10000, 'score': 14833.359668016434, 'total_duration': 15477.779819011688, 'accumulated_submission_time': 14833.359668016434, 'accumulated_eval_time': 640.5754406452179, 'accumulated_logging_time': 0.8834488391876221}
I0316 04:57:24.229815 139563620054784 logging_writer.py:48] [81338] accumulated_eval_time=640.575441, accumulated_logging_time=0.883449, accumulated_submission_time=14833.359668, global_step=81338, preemption_count=0, score=14833.359668, test/accuracy=0.485600, test/loss=2.405644, test/num_examples=10000, total_duration=15477.779819, train/accuracy=0.667849, train/loss=1.339473, validation/accuracy=0.607740, validation/loss=1.664938, validation/num_examples=50000
I0316 04:57:53.813019 139564316305152 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.5745599269866943, loss=2.045715808868408
I0316 04:59:24.716047 139563620054784 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.5639973282814026, loss=1.9007102251052856
I0316 05:00:55.479871 139564316305152 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.5194619297981262, loss=1.6717171669006348
I0316 05:02:26.416878 139563620054784 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.5350288152694702, loss=1.771962285041809
I0316 05:03:57.123536 139564316305152 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.5294731259346008, loss=1.640014410018921
I0316 05:05:27.942669 139563620054784 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.5529766082763672, loss=1.696578860282898
I0316 05:05:54.280560 139725430036288 spec.py:321] Evaluating on the training split.
I0316 05:06:01.557369 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 05:06:15.540060 139725430036288 spec.py:349] Evaluating on the test split.
I0316 05:06:17.748930 139725430036288 submission_runner.py:420] Time since start: 16011.32s, 	Step: 84145, 	{'train/accuracy': 0.6775350570678711, 'train/loss': 1.2906715869903564, 'validation/accuracy': 0.6220799684524536, 'validation/loss': 1.5991394519805908, 'validation/num_examples': 50000, 'test/accuracy': 0.49500003457069397, 'test/loss': 2.3471944332122803, 'test/num_examples': 10000, 'score': 15343.299053192139, 'total_duration': 16011.31775546074, 'accumulated_submission_time': 15343.299053192139, 'accumulated_eval_time': 664.0437948703766, 'accumulated_logging_time': 0.9118335247039795}
I0316 05:06:17.768634 139564324697856 logging_writer.py:48] [84145] accumulated_eval_time=664.043795, accumulated_logging_time=0.911834, accumulated_submission_time=15343.299053, global_step=84145, preemption_count=0, score=15343.299053, test/accuracy=0.495000, test/loss=2.347194, test/num_examples=10000, total_duration=16011.317755, train/accuracy=0.677535, train/loss=1.290672, validation/accuracy=0.622080, validation/loss=1.599139, validation/num_examples=50000
I0316 05:07:22.440122 139564349875968 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.5932234525680542, loss=1.862157940864563
I0316 05:08:53.266823 139564324697856 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.5493646264076233, loss=1.791825771331787
I0316 05:10:24.279927 139564349875968 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.5476499199867249, loss=1.8493220806121826
I0316 05:11:55.046963 139564324697856 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.5377261638641357, loss=1.7290440797805786
I0316 05:13:26.137585 139564349875968 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.5067258477210999, loss=1.715381383895874
I0316 05:14:47.905862 139725430036288 spec.py:321] Evaluating on the training split.
I0316 05:14:55.075797 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 05:15:08.733132 139725430036288 spec.py:349] Evaluating on the test split.
I0316 05:15:11.011485 139725430036288 submission_runner.py:420] Time since start: 16544.58s, 	Step: 86952, 	{'train/accuracy': 0.6671316623687744, 'train/loss': 1.3346246480941772, 'validation/accuracy': 0.6101399660110474, 'validation/loss': 1.6558141708374023, 'validation/num_examples': 50000, 'test/accuracy': 0.48570001125335693, 'test/loss': 2.394282341003418, 'test/num_examples': 10000, 'score': 15853.323334932327, 'total_duration': 16544.580283880234, 'accumulated_submission_time': 15853.323334932327, 'accumulated_eval_time': 687.1493873596191, 'accumulated_logging_time': 0.9420502185821533}
I0316 05:15:11.034625 139563628447488 logging_writer.py:48] [86952] accumulated_eval_time=687.149387, accumulated_logging_time=0.942050, accumulated_submission_time=15853.323335, global_step=86952, preemption_count=0, score=15853.323335, test/accuracy=0.485700, test/loss=2.394282, test/num_examples=10000, total_duration=16544.580284, train/accuracy=0.667132, train/loss=1.334625, validation/accuracy=0.610140, validation/loss=1.655814, validation/num_examples=50000
I0316 05:15:19.967919 139564089865984 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.5579468011856079, loss=1.569187879562378
I0316 05:16:50.968818 139563628447488 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.5706928372383118, loss=1.7508152723312378
I0316 05:18:21.763479 139564089865984 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.5493432879447937, loss=1.740142822265625
I0316 05:19:52.746029 139563628447488 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.5736432075500488, loss=1.7525300979614258
I0316 05:21:23.540546 139564089865984 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.5299144387245178, loss=1.6147483587265015
I0316 05:22:54.519806 139563628447488 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.5380628705024719, loss=1.7839807271957397
I0316 05:23:41.135927 139725430036288 spec.py:321] Evaluating on the training split.
I0316 05:23:48.170578 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 05:24:01.356281 139725430036288 spec.py:349] Evaluating on the test split.
I0316 05:24:03.608374 139725430036288 submission_runner.py:420] Time since start: 17077.18s, 	Step: 89758, 	{'train/accuracy': 0.654715359210968, 'train/loss': 1.392137050628662, 'validation/accuracy': 0.5983399748802185, 'validation/loss': 1.7241629362106323, 'validation/num_examples': 50000, 'test/accuracy': 0.4775000214576721, 'test/loss': 2.459275245666504, 'test/num_examples': 10000, 'score': 16363.312257289886, 'total_duration': 17077.17719435692, 'accumulated_submission_time': 16363.312257289886, 'accumulated_eval_time': 709.6218059062958, 'accumulated_logging_time': 0.9764280319213867}
I0316 05:24:03.630849 139564333090560 logging_writer.py:48] [89758] accumulated_eval_time=709.621806, accumulated_logging_time=0.976428, accumulated_submission_time=16363.312257, global_step=89758, preemption_count=0, score=16363.312257, test/accuracy=0.477500, test/loss=2.459275, test/num_examples=10000, total_duration=17077.177194, train/accuracy=0.654715, train/loss=1.392137, validation/accuracy=0.598340, validation/loss=1.724163, validation/num_examples=50000
I0316 05:24:47.786820 139564341483264 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.5289528965950012, loss=1.644599437713623
I0316 05:26:18.614006 139564333090560 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.5623263716697693, loss=1.810093641281128
I0316 05:27:49.717419 139564341483264 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.5177108645439148, loss=1.5243771076202393
I0316 05:29:20.546353 139564333090560 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.5517160892486572, loss=1.7178455591201782
I0316 05:30:51.627268 139564341483264 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.5542411804199219, loss=1.8242496252059937
I0316 05:32:22.462416 139564333090560 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.502996027469635, loss=1.6462993621826172
I0316 05:32:33.639510 139725430036288 spec.py:321] Evaluating on the training split.
I0316 05:32:40.716107 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 05:32:51.611035 139725430036288 spec.py:349] Evaluating on the test split.
I0316 05:32:54.040034 139725430036288 submission_runner.py:420] Time since start: 17607.61s, 	Step: 92563, 	{'train/accuracy': 0.7001753449440002, 'train/loss': 1.1904690265655518, 'validation/accuracy': 0.6431800127029419, 'validation/loss': 1.5130765438079834, 'validation/num_examples': 50000, 'test/accuracy': 0.5164999961853027, 'test/loss': 2.2567412853240967, 'test/num_examples': 10000, 'score': 16873.20974779129, 'total_duration': 17607.608857393265, 'accumulated_submission_time': 16873.20974779129, 'accumulated_eval_time': 730.0223150253296, 'accumulated_logging_time': 1.0082745552062988}
I0316 05:32:54.061384 139563620054784 logging_writer.py:48] [92563] accumulated_eval_time=730.022315, accumulated_logging_time=1.008275, accumulated_submission_time=16873.209748, global_step=92563, preemption_count=0, score=16873.209748, test/accuracy=0.516500, test/loss=2.256741, test/num_examples=10000, total_duration=17607.608857, train/accuracy=0.700175, train/loss=1.190469, validation/accuracy=0.643180, validation/loss=1.513077, validation/num_examples=50000
I0316 05:34:13.926647 139563628447488 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.5780076384544373, loss=1.6307611465454102
I0316 05:35:44.772827 139563620054784 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.5571697950363159, loss=1.8286386728286743
I0316 05:37:15.778723 139563628447488 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.5149836540222168, loss=1.6140400171279907
I0316 05:38:46.624168 139563620054784 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.533258318901062, loss=1.5920698642730713
I0316 05:40:17.609208 139563628447488 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.5355112552642822, loss=1.6656889915466309
I0316 05:41:24.175941 139725430036288 spec.py:321] Evaluating on the training split.
I0316 05:41:31.089638 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 05:41:41.823523 139725430036288 spec.py:349] Evaluating on the test split.
I0316 05:41:44.147316 139725430036288 submission_runner.py:420] Time since start: 18137.72s, 	Step: 95368, 	{'train/accuracy': 0.7003945708274841, 'train/loss': 1.1815835237503052, 'validation/accuracy': 0.6406799554824829, 'validation/loss': 1.5170526504516602, 'validation/num_examples': 50000, 'test/accuracy': 0.5115000009536743, 'test/loss': 2.2971582412719727, 'test/num_examples': 10000, 'score': 17383.213754177094, 'total_duration': 18137.716124534607, 'accumulated_submission_time': 17383.213754177094, 'accumulated_eval_time': 749.993668794632, 'accumulated_logging_time': 1.0387895107269287}
I0316 05:41:44.169730 139564333090560 logging_writer.py:48] [95368] accumulated_eval_time=749.993669, accumulated_logging_time=1.038790, accumulated_submission_time=17383.213754, global_step=95368, preemption_count=0, score=17383.213754, test/accuracy=0.511500, test/loss=2.297158, test/num_examples=10000, total_duration=18137.716125, train/accuracy=0.700395, train/loss=1.181584, validation/accuracy=0.640680, validation/loss=1.517053, validation/num_examples=50000
I0316 05:42:08.325536 139564341483264 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.5198657512664795, loss=1.5284706354141235
I0316 05:43:39.430185 139564333090560 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.5502397418022156, loss=1.674099326133728
I0316 05:45:10.216504 139564341483264 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.5151243209838867, loss=1.5538660287857056
I0316 05:46:40.997123 139564333090560 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.5333176255226135, loss=1.6779944896697998
I0316 05:48:12.026239 139564341483264 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.5669654607772827, loss=1.5610369443893433
I0316 05:49:42.835078 139564333090560 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.5719839334487915, loss=1.4982926845550537
I0316 05:50:14.290471 139725430036288 spec.py:321] Evaluating on the training split.
I0316 05:50:21.045249 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 05:50:30.419192 139725430036288 spec.py:349] Evaluating on the test split.
I0316 05:50:32.761013 139725430036288 submission_runner.py:420] Time since start: 18666.33s, 	Step: 98174, 	{'train/accuracy': 0.7204838991165161, 'train/loss': 1.1050729751586914, 'validation/accuracy': 0.6505599617958069, 'validation/loss': 1.4714226722717285, 'validation/num_examples': 50000, 'test/accuracy': 0.5278000235557556, 'test/loss': 2.1921775341033936, 'test/num_examples': 10000, 'score': 17893.218606472015, 'total_duration': 18666.329828500748, 'accumulated_submission_time': 17893.218606472015, 'accumulated_eval_time': 768.4641678333282, 'accumulated_logging_time': 1.0741848945617676}
I0316 05:50:32.784477 139563620054784 logging_writer.py:48] [98174] accumulated_eval_time=768.464168, accumulated_logging_time=1.074185, accumulated_submission_time=17893.218606, global_step=98174, preemption_count=0, score=17893.218606, test/accuracy=0.527800, test/loss=2.192178, test/num_examples=10000, total_duration=18666.329829, train/accuracy=0.720484, train/loss=1.105073, validation/accuracy=0.650560, validation/loss=1.471423, validation/num_examples=50000
I0316 05:51:32.160929 139563628447488 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.5377265214920044, loss=1.5538686513900757
I0316 05:53:03.026025 139563620054784 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.5340163111686707, loss=1.5845963954925537
I0316 05:54:34.012826 139563628447488 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.5284721851348877, loss=1.5126982927322388
I0316 05:56:04.897507 139563620054784 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.523817777633667, loss=1.387112021446228
I0316 05:57:35.890396 139563628447488 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.5430809855461121, loss=1.4957646131515503
I0316 05:59:02.780549 139725430036288 spec.py:321] Evaluating on the training split.
I0316 05:59:09.498550 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 05:59:19.241816 139725430036288 spec.py:349] Evaluating on the test split.
I0316 05:59:21.579667 139725430036288 submission_runner.py:420] Time since start: 19195.15s, 	Step: 100980, 	{'train/accuracy': 0.7105987071990967, 'train/loss': 1.1519235372543335, 'validation/accuracy': 0.645039975643158, 'validation/loss': 1.4904940128326416, 'validation/num_examples': 50000, 'test/accuracy': 0.515500009059906, 'test/loss': 2.267381429672241, 'test/num_examples': 10000, 'score': 18403.10390162468, 'total_duration': 19195.148364067078, 'accumulated_submission_time': 18403.10390162468, 'accumulated_eval_time': 787.2631387710571, 'accumulated_logging_time': 1.1079041957855225}
I0316 05:59:21.606314 139564341483264 logging_writer.py:48] [100980] accumulated_eval_time=787.263139, accumulated_logging_time=1.107904, accumulated_submission_time=18403.103902, global_step=100980, preemption_count=0, score=18403.103902, test/accuracy=0.515500, test/loss=2.267381, test/num_examples=10000, total_duration=19195.148364, train/accuracy=0.710599, train/loss=1.151924, validation/accuracy=0.645040, validation/loss=1.490494, validation/num_examples=50000
I0316 05:59:25.436405 139564349875968 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.5707379579544067, loss=1.4325553178787231
I0316 06:00:56.475573 139564341483264 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.5271320939064026, loss=1.407593011856079
I0316 06:02:27.374791 139564349875968 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.5535398125648499, loss=1.5186265707015991
I0316 06:03:58.451258 139564341483264 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.5851614475250244, loss=1.588512897491455
I0316 06:05:29.280227 139564349875968 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.5825507640838623, loss=1.4577305316925049
I0316 06:07:00.104471 139564341483264 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.5194058418273926, loss=1.390358567237854
I0316 06:07:51.611739 139725430036288 spec.py:321] Evaluating on the training split.
I0316 06:07:58.112354 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 06:08:10.604415 139725430036288 spec.py:349] Evaluating on the test split.
I0316 06:08:12.917299 139725430036288 submission_runner.py:420] Time since start: 19726.49s, 	Step: 103784, 	{'train/accuracy': 0.7798947691917419, 'train/loss': 0.8267284631729126, 'validation/accuracy': 0.667419970035553, 'validation/loss': 1.3963357210159302, 'validation/num_examples': 50000, 'test/accuracy': 0.5393000245094299, 'test/loss': 2.16798996925354, 'test/num_examples': 10000, 'score': 18912.9945166111, 'total_duration': 19726.486125946045, 'accumulated_submission_time': 18912.9945166111, 'accumulated_eval_time': 808.5686769485474, 'accumulated_logging_time': 1.1457602977752686}
I0316 06:08:12.940410 139563628447488 logging_writer.py:48] [103784] accumulated_eval_time=808.568677, accumulated_logging_time=1.145760, accumulated_submission_time=18912.994517, global_step=103784, preemption_count=0, score=18912.994517, test/accuracy=0.539300, test/loss=2.167990, test/num_examples=10000, total_duration=19726.486126, train/accuracy=0.779895, train/loss=0.826728, validation/accuracy=0.667420, validation/loss=1.396336, validation/num_examples=50000
I0316 06:08:52.343523 139564089865984 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.5355071425437927, loss=1.400649070739746
I0316 06:10:23.132661 139563628447488 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.54800945520401, loss=1.4612025022506714
I0316 06:11:54.122981 139564089865984 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.566333532333374, loss=1.4935498237609863
I0316 06:13:24.911277 139563628447488 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.6757996678352356, loss=1.4897377490997314
I0316 06:14:55.920819 139564089865984 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.5632238388061523, loss=1.3599841594696045
I0316 06:16:26.741242 139563628447488 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.5610829591751099, loss=1.315350890159607
I0316 06:16:43.000621 139725430036288 spec.py:321] Evaluating on the training split.
I0316 06:16:49.388245 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 06:16:59.932863 139725430036288 spec.py:349] Evaluating on the test split.
I0316 06:17:02.268172 139725430036288 submission_runner.py:420] Time since start: 20255.84s, 	Step: 106591, 	{'train/accuracy': 0.8012595772743225, 'train/loss': 0.7305102348327637, 'validation/accuracy': 0.6799399852752686, 'validation/loss': 1.3346288204193115, 'validation/num_examples': 50000, 'test/accuracy': 0.5610000491142273, 'test/loss': 2.0588128566741943, 'test/num_examples': 10000, 'score': 19422.943388223648, 'total_duration': 20255.836973428726, 'accumulated_submission_time': 19422.943388223648, 'accumulated_eval_time': 827.8361773490906, 'accumulated_logging_time': 1.1785557270050049}
I0316 06:17:02.291089 139564333090560 logging_writer.py:48] [106591] accumulated_eval_time=827.836177, accumulated_logging_time=1.178556, accumulated_submission_time=19422.943388, global_step=106591, preemption_count=0, score=19422.943388, test/accuracy=0.561000, test/loss=2.058813, test/num_examples=10000, total_duration=20255.836973, train/accuracy=0.801260, train/loss=0.730510, validation/accuracy=0.679940, validation/loss=1.334629, validation/num_examples=50000
I0316 06:18:16.964352 139564341483264 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.5457786321640015, loss=1.2950323820114136
I0316 06:19:47.809093 139564333090560 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.5365842580795288, loss=1.292922019958496
I0316 06:21:18.875531 139564341483264 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.6026999950408936, loss=1.3079856634140015
I0316 06:22:49.676736 139564333090560 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.6222770810127258, loss=1.4528429508209229
I0316 06:24:20.717397 139564341483264 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.6056073904037476, loss=1.46695876121521
I0316 06:25:32.412082 139725430036288 spec.py:321] Evaluating on the training split.
I0316 06:25:38.733030 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 06:25:48.022923 139725430036288 spec.py:349] Evaluating on the test split.
I0316 06:25:50.327847 139725430036288 submission_runner.py:420] Time since start: 20783.90s, 	Step: 109396, 	{'train/accuracy': 0.7942641973495483, 'train/loss': 0.7458347082138062, 'validation/accuracy': 0.6802999973297119, 'validation/loss': 1.3340208530426025, 'validation/num_examples': 50000, 'test/accuracy': 0.5577000379562378, 'test/loss': 2.072763442993164, 'test/num_examples': 10000, 'score': 19932.948579072952, 'total_duration': 20783.896659851074, 'accumulated_submission_time': 19932.948579072952, 'accumulated_eval_time': 845.7519087791443, 'accumulated_logging_time': 1.2124686241149902}
I0316 06:25:50.355050 139564307912448 logging_writer.py:48] [109396] accumulated_eval_time=845.751909, accumulated_logging_time=1.212469, accumulated_submission_time=19932.948579, global_step=109396, preemption_count=0, score=19932.948579, test/accuracy=0.557700, test/loss=2.072763, test/num_examples=10000, total_duration=20783.896660, train/accuracy=0.794264, train/loss=0.745835, validation/accuracy=0.680300, validation/loss=1.334021, validation/num_examples=50000
I0316 06:26:09.452081 139564316305152 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.596279501914978, loss=1.3166887760162354
I0316 06:27:40.514808 139564307912448 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.6567075252532959, loss=1.5049986839294434
I0316 06:29:11.339590 139564316305152 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.6171251535415649, loss=1.4233675003051758
I0316 06:30:42.132085 139564307912448 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.605033814907074, loss=1.3111718893051147
I0316 06:32:13.191288 139564316305152 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.5860946774482727, loss=1.3482561111450195
I0316 06:33:43.979166 139564307912448 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.549060046672821, loss=1.1406381130218506
I0316 06:34:20.413398 139725430036288 spec.py:321] Evaluating on the training split.
I0316 06:34:26.804949 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 06:34:36.690865 139725430036288 spec.py:349] Evaluating on the test split.
I0316 06:34:39.048112 139725430036288 submission_runner.py:420] Time since start: 21312.62s, 	Step: 112201, 	{'train/accuracy': 0.7999641299247742, 'train/loss': 0.7351735234260559, 'validation/accuracy': 0.686519980430603, 'validation/loss': 1.315731167793274, 'validation/num_examples': 50000, 'test/accuracy': 0.5611000061035156, 'test/loss': 2.063603639602661, 'test/num_examples': 10000, 'score': 20442.890431404114, 'total_duration': 21312.616918325424, 'accumulated_submission_time': 20442.890431404114, 'accumulated_eval_time': 864.3865873813629, 'accumulated_logging_time': 1.2514734268188477}
I0316 06:34:39.072324 139564089865984 logging_writer.py:48] [112201] accumulated_eval_time=864.386587, accumulated_logging_time=1.251473, accumulated_submission_time=20442.890431, global_step=112201, preemption_count=0, score=20442.890431, test/accuracy=0.561100, test/loss=2.063604, test/num_examples=10000, total_duration=21312.616918, train/accuracy=0.799964, train/loss=0.735174, validation/accuracy=0.686520, validation/loss=1.315731, validation/num_examples=50000
I0316 06:35:33.590319 139564333090560 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.6286363005638123, loss=1.2789117097854614
I0316 06:37:04.405178 139564089865984 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.6291837692260742, loss=1.3144451379776
I0316 06:38:35.414866 139564333090560 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.5660291314125061, loss=1.1786682605743408
I0316 06:40:06.204788 139564089865984 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.6015546917915344, loss=1.178836703300476
I0316 06:41:37.279130 139564333090560 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.5944655537605286, loss=1.198651671409607
I0316 06:43:08.093658 139564089865984 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.5916997194290161, loss=1.057225227355957
I0316 06:43:09.104802 139725430036288 spec.py:321] Evaluating on the training split.
I0316 06:43:15.407983 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 06:43:24.368468 139725430036288 spec.py:349] Evaluating on the test split.
I0316 06:43:26.692425 139725430036288 submission_runner.py:420] Time since start: 21840.26s, 	Step: 115007, 	{'train/accuracy': 0.8163862824440002, 'train/loss': 0.6614758372306824, 'validation/accuracy': 0.7037400007247925, 'validation/loss': 1.2468513250350952, 'validation/num_examples': 50000, 'test/accuracy': 0.572700023651123, 'test/loss': 2.0381808280944824, 'test/num_examples': 10000, 'score': 20952.809463977814, 'total_duration': 21840.261241674423, 'accumulated_submission_time': 20952.809463977814, 'accumulated_eval_time': 881.9741714000702, 'accumulated_logging_time': 1.2862813472747803}
I0316 06:43:26.718888 139563620054784 logging_writer.py:48] [115007] accumulated_eval_time=881.974171, accumulated_logging_time=1.286281, accumulated_submission_time=20952.809464, global_step=115007, preemption_count=0, score=20952.809464, test/accuracy=0.572700, test/loss=2.038181, test/num_examples=10000, total_duration=21840.261242, train/accuracy=0.816386, train/loss=0.661476, validation/accuracy=0.703740, validation/loss=1.246851, validation/num_examples=50000
I0316 06:44:56.742538 139563628447488 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.6341275572776794, loss=1.3104690313339233
I0316 06:46:27.574889 139563620054784 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.6303390264511108, loss=1.2203471660614014
I0316 06:47:58.656342 139563628447488 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.6048852205276489, loss=1.2290822267532349
I0316 06:49:29.490594 139563620054784 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.6372970342636108, loss=1.1115095615386963
I0316 06:51:00.303041 139563628447488 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.6363246440887451, loss=1.1198863983154297
I0316 06:51:56.736499 139725430036288 spec.py:321] Evaluating on the training split.
I0316 06:52:02.998100 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 06:52:13.562400 139725430036288 spec.py:349] Evaluating on the test split.
I0316 06:52:15.909166 139725430036288 submission_runner.py:420] Time since start: 22369.48s, 	Step: 117811, 	{'train/accuracy': 0.8378308415412903, 'train/loss': 0.5822914838790894, 'validation/accuracy': 0.7119999527931213, 'validation/loss': 1.1972250938415527, 'validation/num_examples': 50000, 'test/accuracy': 0.589900016784668, 'test/loss': 1.922194004058838, 'test/num_examples': 10000, 'score': 21462.71215581894, 'total_duration': 22369.477986335754, 'accumulated_submission_time': 21462.71215581894, 'accumulated_eval_time': 901.1468102931976, 'accumulated_logging_time': 1.3231489658355713}
I0316 06:52:15.932898 139564089865984 logging_writer.py:48] [117811] accumulated_eval_time=901.146810, accumulated_logging_time=1.323149, accumulated_submission_time=21462.712156, global_step=117811, preemption_count=0, score=21462.712156, test/accuracy=0.589900, test/loss=1.922194, test/num_examples=10000, total_duration=22369.477986, train/accuracy=0.837831, train/loss=0.582291, validation/accuracy=0.712000, validation/loss=1.197225, validation/num_examples=50000
I0316 06:52:50.475893 139564307912448 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.5984247326850891, loss=1.1340054273605347
I0316 06:54:21.327320 139564089865984 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.6584746837615967, loss=1.1498609781265259
I0316 06:55:52.367969 139564307912448 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.6731297373771667, loss=1.0532245635986328
I0316 06:57:23.160317 139564089865984 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.656358003616333, loss=1.1266270875930786
I0316 06:58:54.192889 139564307912448 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.6122862100601196, loss=0.9165579676628113
I0316 07:00:25.003834 139564089865984 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.5933855772018433, loss=0.9595524072647095
I0316 07:00:46.005272 139725430036288 spec.py:321] Evaluating on the training split.
I0316 07:00:52.158044 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 07:01:02.045272 139725430036288 spec.py:349] Evaluating on the test split.
I0316 07:01:04.393501 139725430036288 submission_runner.py:420] Time since start: 22897.96s, 	Step: 120617, 	{'train/accuracy': 0.8485331535339355, 'train/loss': 0.5348440408706665, 'validation/accuracy': 0.7240599989891052, 'validation/loss': 1.1545007228851318, 'validation/num_examples': 50000, 'test/accuracy': 0.5979000329971313, 'test/loss': 1.9008315801620483, 'test/num_examples': 10000, 'score': 21972.669156074524, 'total_duration': 22897.962318897247, 'accumulated_submission_time': 21972.669156074524, 'accumulated_eval_time': 919.5350027084351, 'accumulated_logging_time': 1.357767105102539}
I0316 07:01:04.419136 139563628447488 logging_writer.py:48] [120617] accumulated_eval_time=919.535003, accumulated_logging_time=1.357767, accumulated_submission_time=21972.669156, global_step=120617, preemption_count=0, score=21972.669156, test/accuracy=0.597900, test/loss=1.900832, test/num_examples=10000, total_duration=22897.962319, train/accuracy=0.848533, train/loss=0.534844, validation/accuracy=0.724060, validation/loss=1.154501, validation/num_examples=50000
I0316 07:02:14.349576 139564089865984 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.5611120462417603, loss=0.9768239855766296
I0316 07:03:45.197466 139563628447488 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.633584201335907, loss=0.9999504089355469
I0316 07:05:16.176042 139564089865984 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.6881098747253418, loss=1.0102978944778442
I0316 07:06:46.995921 139563628447488 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.5938153266906738, loss=1.1046580076217651
I0316 07:08:17.960929 139564089865984 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.7211833596229553, loss=0.9418295621871948
I0316 07:09:34.514413 139725430036288 spec.py:321] Evaluating on the training split.
I0316 07:09:40.715823 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 07:09:49.727321 139725430036288 spec.py:349] Evaluating on the test split.
I0316 07:09:52.003389 139725430036288 submission_runner.py:420] Time since start: 23425.57s, 	Step: 123423, 	{'train/accuracy': 0.8653938174247742, 'train/loss': 0.48598766326904297, 'validation/accuracy': 0.7308200001716614, 'validation/loss': 1.1270761489868164, 'validation/num_examples': 50000, 'test/accuracy': 0.603600025177002, 'test/loss': 1.8526098728179932, 'test/num_examples': 10000, 'score': 22482.648923158646, 'total_duration': 23425.57219672203, 'accumulated_submission_time': 22482.648923158646, 'accumulated_eval_time': 937.0239474773407, 'accumulated_logging_time': 1.3936655521392822}
I0316 07:09:52.029362 139564307912448 logging_writer.py:48] [123423] accumulated_eval_time=937.023947, accumulated_logging_time=1.393666, accumulated_submission_time=22482.648923, global_step=123423, preemption_count=0, score=22482.648923, test/accuracy=0.603600, test/loss=1.852610, test/num_examples=10000, total_duration=23425.572197, train/accuracy=0.865394, train/loss=0.485988, validation/accuracy=0.730820, validation/loss=1.127076, validation/num_examples=50000
I0316 07:10:06.223714 139564333090560 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.6100102066993713, loss=1.045255422592163
I0316 07:11:37.061246 139564307912448 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.7387357354164124, loss=1.1097207069396973
I0316 07:13:08.157385 139564333090560 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.7195104956626892, loss=0.8588534593582153
I0316 07:14:38.990485 139564307912448 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.6146708726882935, loss=1.0262099504470825
I0316 07:16:10.008434 139564333090560 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.6691508293151855, loss=1.0076377391815186
I0316 07:17:40.844290 139564307912448 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.6643010973930359, loss=0.8896152973175049
I0316 07:18:22.175238 139725430036288 spec.py:321] Evaluating on the training split.
I0316 07:18:28.362704 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 07:18:37.337224 139725430036288 spec.py:349] Evaluating on the test split.
I0316 07:18:39.674768 139725430036288 submission_runner.py:420] Time since start: 23953.24s, 	Step: 126228, 	{'train/accuracy': 0.8854233026504517, 'train/loss': 0.4137967824935913, 'validation/accuracy': 0.7409200072288513, 'validation/loss': 1.0861350297927856, 'validation/num_examples': 50000, 'test/accuracy': 0.6169000267982483, 'test/loss': 1.8041653633117676, 'test/num_examples': 10000, 'score': 22992.67735028267, 'total_duration': 23953.243550539017, 'accumulated_submission_time': 22992.67735028267, 'accumulated_eval_time': 954.5234022140503, 'accumulated_logging_time': 1.4333734512329102}
I0316 07:18:39.702435 139563628447488 logging_writer.py:48] [126228] accumulated_eval_time=954.523402, accumulated_logging_time=1.433373, accumulated_submission_time=22992.677350, global_step=126228, preemption_count=0, score=22992.677350, test/accuracy=0.616900, test/loss=1.804165, test/num_examples=10000, total_duration=23953.243551, train/accuracy=0.885423, train/loss=0.413797, validation/accuracy=0.740920, validation/loss=1.086135, validation/num_examples=50000
I0316 07:19:29.309949 139564316305152 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.6934093236923218, loss=0.945575475692749
I0316 07:21:00.102603 139563628447488 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.6708923578262329, loss=0.8892594575881958
I0316 07:22:31.114899 139564316305152 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.6689514517784119, loss=0.9208406805992126
I0316 07:24:01.944978 139563628447488 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.6785967946052551, loss=0.8681586980819702
I0316 07:25:33.046910 139564316305152 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.8277997970581055, loss=1.0675764083862305
I0316 07:27:03.829010 139563628447488 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.795697808265686, loss=0.9434146881103516
I0316 07:27:09.728039 139725430036288 spec.py:321] Evaluating on the training split.
I0316 07:27:16.144361 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 07:27:25.184532 139725430036288 spec.py:349] Evaluating on the test split.
I0316 07:27:27.494977 139725430036288 submission_runner.py:420] Time since start: 24481.06s, 	Step: 129034, 	{'train/accuracy': 0.897879421710968, 'train/loss': 0.3687564730644226, 'validation/accuracy': 0.7457000017166138, 'validation/loss': 1.0661872625350952, 'validation/num_examples': 50000, 'test/accuracy': 0.6183000206947327, 'test/loss': 1.7954069375991821, 'test/num_examples': 10000, 'score': 23502.58859872818, 'total_duration': 24481.063786029816, 'accumulated_submission_time': 23502.58859872818, 'accumulated_eval_time': 972.2902998924255, 'accumulated_logging_time': 1.4721760749816895}
I0316 07:27:27.520611 139564341483264 logging_writer.py:48] [129034] accumulated_eval_time=972.290300, accumulated_logging_time=1.472176, accumulated_submission_time=23502.588599, global_step=129034, preemption_count=0, score=23502.588599, test/accuracy=0.618300, test/loss=1.795407, test/num_examples=10000, total_duration=24481.063786, train/accuracy=0.897879, train/loss=0.368756, validation/accuracy=0.745700, validation/loss=1.066187, validation/num_examples=50000
I0316 07:28:52.659809 139564349875968 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.7065736055374146, loss=0.8415083885192871
I0316 07:30:23.480662 139564341483264 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.84573894739151, loss=0.7986040115356445
I0316 07:31:54.535407 139564349875968 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.709592342376709, loss=0.899451732635498
I0316 07:33:25.406059 139564341483264 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.7653840184211731, loss=0.9268827438354492
I0316 07:34:56.272581 139564349875968 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.7589657306671143, loss=0.9282770752906799
I0316 07:35:57.585383 139725430036288 spec.py:321] Evaluating on the training split.
I0316 07:36:03.832991 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 07:36:12.848396 139725430036288 spec.py:349] Evaluating on the test split.
I0316 07:36:15.186943 139725430036288 submission_runner.py:420] Time since start: 25008.76s, 	Step: 131838, 	{'train/accuracy': 0.9066087007522583, 'train/loss': 0.3339364528656006, 'validation/accuracy': 0.7515199780464172, 'validation/loss': 1.0488859415054321, 'validation/num_examples': 50000, 'test/accuracy': 0.6264000535011292, 'test/loss': 1.7780516147613525, 'test/num_examples': 10000, 'score': 24012.539065122604, 'total_duration': 25008.755753993988, 'accumulated_submission_time': 24012.539065122604, 'accumulated_eval_time': 989.8918237686157, 'accumulated_logging_time': 1.5100431442260742}
I0316 07:36:15.213498 139564089865984 logging_writer.py:48] [131838] accumulated_eval_time=989.891824, accumulated_logging_time=1.510043, accumulated_submission_time=24012.539065, global_step=131838, preemption_count=0, score=24012.539065, test/accuracy=0.626400, test/loss=1.778052, test/num_examples=10000, total_duration=25008.755754, train/accuracy=0.906609, train/loss=0.333936, validation/accuracy=0.751520, validation/loss=1.048886, validation/num_examples=50000
I0316 07:36:44.850108 139564307912448 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.7674474716186523, loss=0.9323089122772217
I0316 07:38:15.699035 139564089865984 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.6225941181182861, loss=0.7821975350379944
I0316 07:39:46.822908 139564307912448 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.7093611359596252, loss=0.8501914739608765
I0316 07:41:17.617118 139564089865984 logging_writer.py:48] [133500] global_step=133500, grad_norm=0.6802899837493896, loss=0.8557538986206055
I0316 07:42:48.679818 139564307912448 logging_writer.py:48] [134000] global_step=134000, grad_norm=0.6636516451835632, loss=0.8540666103363037
I0316 07:44:19.535954 139564089865984 logging_writer.py:48] [134500] global_step=134500, grad_norm=0.745938241481781, loss=0.7545986771583557
I0316 07:44:45.236603 139725430036288 spec.py:321] Evaluating on the training split.
I0316 07:44:51.437198 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 07:45:00.445737 139725430036288 spec.py:349] Evaluating on the test split.
I0316 07:45:02.801509 139725430036288 submission_runner.py:420] Time since start: 25536.37s, 	Step: 134643, 	{'train/accuracy': 0.9126076102256775, 'train/loss': 0.31133192777633667, 'validation/accuracy': 0.752839982509613, 'validation/loss': 1.0438672304153442, 'validation/num_examples': 50000, 'test/accuracy': 0.6296000480651855, 'test/loss': 1.773249864578247, 'test/num_examples': 10000, 'score': 24522.4491918087, 'total_duration': 25536.370287418365, 'accumulated_submission_time': 24522.4491918087, 'accumulated_eval_time': 1007.456659078598, 'accumulated_logging_time': 1.547370195388794}
I0316 07:45:02.834923 139563628447488 logging_writer.py:48] [134643] accumulated_eval_time=1007.456659, accumulated_logging_time=1.547370, accumulated_submission_time=24522.449192, global_step=134643, preemption_count=0, score=24522.449192, test/accuracy=0.629600, test/loss=1.773250, test/num_examples=10000, total_duration=25536.370287, train/accuracy=0.912608, train/loss=0.311332, validation/accuracy=0.752840, validation/loss=1.043867, validation/num_examples=50000
I0316 07:46:08.189911 139564324697856 logging_writer.py:48] [135000] global_step=135000, grad_norm=0.815572202205658, loss=0.9396737217903137
I0316 07:47:39.021607 139563628447488 logging_writer.py:48] [135500] global_step=135500, grad_norm=0.7346750497817993, loss=0.8680108785629272
I0316 07:49:10.067424 139564324697856 logging_writer.py:48] [136000] global_step=136000, grad_norm=0.6897454261779785, loss=0.800541341304779
I0316 07:50:40.905545 139563628447488 logging_writer.py:48] [136500] global_step=136500, grad_norm=0.6608800888061523, loss=0.7646988034248352
I0316 07:52:11.887778 139564324697856 logging_writer.py:48] [137000] global_step=137000, grad_norm=0.7032042145729065, loss=0.8426575064659119
I0316 07:53:32.803775 139725430036288 spec.py:321] Evaluating on the training split.
I0316 07:53:39.020028 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 07:53:47.915481 139725430036288 spec.py:349] Evaluating on the test split.
I0316 07:53:50.232232 139725430036288 submission_runner.py:420] Time since start: 26063.80s, 	Step: 137447, 	{'train/accuracy': 0.9162946343421936, 'train/loss': 0.3012593686580658, 'validation/accuracy': 0.7541599869728088, 'validation/loss': 1.0395097732543945, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.7657780647277832, 'test/num_examples': 10000, 'score': 25032.304614782333, 'total_duration': 26063.80104780197, 'accumulated_submission_time': 25032.304614782333, 'accumulated_eval_time': 1024.8850872516632, 'accumulated_logging_time': 1.5916309356689453}
I0316 07:53:50.257802 139563628447488 logging_writer.py:48] [137447] accumulated_eval_time=1024.885087, accumulated_logging_time=1.591631, accumulated_submission_time=25032.304615, global_step=137447, preemption_count=0, score=25032.304615, test/accuracy=0.629800, test/loss=1.765778, test/num_examples=10000, total_duration=26063.801048, train/accuracy=0.916295, train/loss=0.301259, validation/accuracy=0.754160, validation/loss=1.039510, validation/num_examples=50000
I0316 07:54:00.091491 139564089865984 logging_writer.py:48] [137500] global_step=137500, grad_norm=0.6068389415740967, loss=0.7401057481765747
I0316 07:55:30.871143 139563628447488 logging_writer.py:48] [138000] global_step=138000, grad_norm=0.7414254546165466, loss=0.9057151675224304
I0316 07:57:01.833531 139564089865984 logging_writer.py:48] [138500] global_step=138500, grad_norm=0.6156983971595764, loss=0.8503227829933167
I0316 07:58:32.650998 139563628447488 logging_writer.py:48] [139000] global_step=139000, grad_norm=0.6761829853057861, loss=0.7355616092681885
I0316 08:00:03.678573 139564089865984 logging_writer.py:48] [139500] global_step=139500, grad_norm=0.6638569235801697, loss=0.7457917332649231
I0316 08:01:34.505777 139563628447488 logging_writer.py:48] [140000] global_step=140000, grad_norm=0.7143927216529846, loss=0.916667640209198
I0316 08:02:20.242882 139725430036288 spec.py:321] Evaluating on the training split.
I0316 08:02:27.195415 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 08:02:36.301585 139725430036288 spec.py:349] Evaluating on the test split.
I0316 08:02:38.710971 139725430036288 submission_runner.py:420] Time since start: 26592.28s, 	Step: 140252, 	{'train/accuracy': 0.9176498651504517, 'train/loss': 0.2996687889099121, 'validation/accuracy': 0.754040002822876, 'validation/loss': 1.0387669801712036, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.7635995149612427, 'test/num_examples': 10000, 'score': 25542.17391395569, 'total_duration': 26592.27976679802, 'accumulated_submission_time': 25542.17391395569, 'accumulated_eval_time': 1043.353123664856, 'accumulated_logging_time': 1.6281332969665527}
I0316 08:02:38.746576 139563620054784 logging_writer.py:48] [140252] accumulated_eval_time=1043.353124, accumulated_logging_time=1.628133, accumulated_submission_time=25542.173914, global_step=140252, preemption_count=0, score=25542.173914, test/accuracy=0.630700, test/loss=1.763600, test/num_examples=10000, total_duration=26592.279767, train/accuracy=0.917650, train/loss=0.299669, validation/accuracy=0.754040, validation/loss=1.038767, validation/num_examples=50000
I0316 08:03:23.994188 139563628447488 logging_writer.py:48] [140500] global_step=140500, grad_norm=0.7527120113372803, loss=0.7525848150253296
I0316 08:04:54.798767 139563620054784 logging_writer.py:48] [141000] global_step=141000, grad_norm=0.6751615405082703, loss=0.8105723261833191
I0316 08:06:25.776228 139563628447488 logging_writer.py:48] [141500] global_step=141500, grad_norm=0.7200832962989807, loss=0.860338568687439
I0316 08:07:56.606206 139563620054784 logging_writer.py:48] [142000] global_step=142000, grad_norm=0.6859225630760193, loss=0.8120474219322205
I0316 08:09:27.614191 139563628447488 logging_writer.py:48] [142500] global_step=142500, grad_norm=0.6868689656257629, loss=0.7858834862709045
I0316 08:10:58.417306 139563620054784 logging_writer.py:48] [143000] global_step=143000, grad_norm=0.7611920237541199, loss=0.8851451873779297
I0316 08:11:08.875030 139725430036288 spec.py:321] Evaluating on the training split.
I0316 08:11:15.278918 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 08:11:24.293967 139725430036288 spec.py:349] Evaluating on the test split.
I0316 08:11:26.576274 139725430036288 submission_runner.py:420] Time since start: 27120.15s, 	Step: 143059, 	{'train/accuracy': 0.9155173897743225, 'train/loss': 0.302499920129776, 'validation/accuracy': 0.7542200088500977, 'validation/loss': 1.0374367237091064, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.7649126052856445, 'test/num_examples': 10000, 'score': 26052.18441581726, 'total_duration': 27120.14508652687, 'accumulated_submission_time': 26052.18441581726, 'accumulated_eval_time': 1061.0543432235718, 'accumulated_logging_time': 1.676928997039795}
I0316 08:11:26.604141 139563628447488 logging_writer.py:48] [143059] accumulated_eval_time=1061.054343, accumulated_logging_time=1.676929, accumulated_submission_time=26052.184416, global_step=143059, preemption_count=0, score=26052.184416, test/accuracy=0.630800, test/loss=1.764913, test/num_examples=10000, total_duration=27120.145087, train/accuracy=0.915517, train/loss=0.302500, validation/accuracy=0.754220, validation/loss=1.037437, validation/num_examples=50000
I0316 08:12:47.077820 139564089865984 logging_writer.py:48] [143500] global_step=143500, grad_norm=0.7696661353111267, loss=0.8561187386512756
I0316 08:14:17.877476 139563628447488 logging_writer.py:48] [144000] global_step=144000, grad_norm=0.8131527304649353, loss=0.9440009593963623
I0316 08:15:48.826179 139564089865984 logging_writer.py:48] [144500] global_step=144500, grad_norm=0.7005102038383484, loss=0.937340259552002
I0316 08:17:19.555135 139563628447488 logging_writer.py:48] [145000] global_step=145000, grad_norm=0.6264556646347046, loss=0.7170202136039734
I0316 08:18:50.326082 139564089865984 logging_writer.py:48] [145500] global_step=145500, grad_norm=0.712471067905426, loss=0.806333065032959
I0316 08:19:56.691960 139725430036288 spec.py:321] Evaluating on the training split.
I0316 08:20:02.867473 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 08:20:11.968829 139725430036288 spec.py:349] Evaluating on the test split.
I0316 08:20:14.241027 139725430036288 submission_runner.py:420] Time since start: 27647.81s, 	Step: 145866, 	{'train/accuracy': 0.9177096486091614, 'train/loss': 0.2954210042953491, 'validation/accuracy': 0.7541599869728088, 'validation/loss': 1.0382142066955566, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.7643870115280151, 'test/num_examples': 10000, 'score': 26562.158358812332, 'total_duration': 27647.80983400345, 'accumulated_submission_time': 26562.158358812332, 'accumulated_eval_time': 1078.6033670902252, 'accumulated_logging_time': 1.7154245376586914}
I0316 08:20:14.267755 139564341483264 logging_writer.py:48] [145866] accumulated_eval_time=1078.603367, accumulated_logging_time=1.715425, accumulated_submission_time=26562.158359, global_step=145866, preemption_count=0, score=26562.158359, test/accuracy=0.630400, test/loss=1.764387, test/num_examples=10000, total_duration=27647.809834, train/accuracy=0.917710, train/loss=0.295421, validation/accuracy=0.754160, validation/loss=1.038214, validation/num_examples=50000
I0316 08:20:38.783567 139564349875968 logging_writer.py:48] [146000] global_step=146000, grad_norm=0.696160614490509, loss=0.9084281921386719
I0316 08:22:09.547802 139564341483264 logging_writer.py:48] [146500] global_step=146500, grad_norm=0.6991137266159058, loss=0.8736972808837891
I0316 08:23:40.556980 139564349875968 logging_writer.py:48] [147000] global_step=147000, grad_norm=0.6499474048614502, loss=0.825432300567627
I0316 08:25:11.371145 139564341483264 logging_writer.py:48] [147500] global_step=147500, grad_norm=0.7347280979156494, loss=0.8733905553817749
I0316 08:26:42.283660 139564349875968 logging_writer.py:48] [148000] global_step=148000, grad_norm=0.7350257039070129, loss=0.8008481860160828
I0316 08:28:13.099783 139564341483264 logging_writer.py:48] [148500] global_step=148500, grad_norm=0.6847607493400574, loss=0.7929593920707703
I0316 08:28:44.400901 139725430036288 spec.py:321] Evaluating on the training split.
I0316 08:28:50.514899 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 08:28:59.593133 139725430036288 spec.py:349] Evaluating on the test split.
I0316 08:29:01.923321 139725430036288 submission_runner.py:420] Time since start: 28175.49s, 	Step: 148674, 	{'train/accuracy': 0.918387234210968, 'train/loss': 0.2957543432712555, 'validation/accuracy': 0.7542600035667419, 'validation/loss': 1.0386526584625244, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.7660565376281738, 'test/num_examples': 10000, 'score': 27072.17339706421, 'total_duration': 28175.49212384224, 'accumulated_submission_time': 27072.17339706421, 'accumulated_eval_time': 1096.125738620758, 'accumulated_logging_time': 1.754462718963623}
I0316 08:29:01.949115 139564089865984 logging_writer.py:48] [148674] accumulated_eval_time=1096.125739, accumulated_logging_time=1.754463, accumulated_submission_time=27072.173397, global_step=148674, preemption_count=0, score=27072.173397, test/accuracy=0.631200, test/loss=1.766057, test/num_examples=10000, total_duration=28175.492124, train/accuracy=0.918387, train/loss=0.295754, validation/accuracy=0.754260, validation/loss=1.038653, validation/num_examples=50000
I0316 08:30:01.650820 139564307912448 logging_writer.py:48] [149000] global_step=149000, grad_norm=0.7194421887397766, loss=0.8925405144691467
I0316 08:31:32.447691 139564089865984 logging_writer.py:48] [149500] global_step=149500, grad_norm=0.7928081750869751, loss=0.8326708078384399
I0316 08:33:03.466272 139564307912448 logging_writer.py:48] [150000] global_step=150000, grad_norm=0.7145406603813171, loss=0.8371790647506714
I0316 08:34:34.291335 139564089865984 logging_writer.py:48] [150500] global_step=150500, grad_norm=0.7515827417373657, loss=0.9631219506263733
I0316 08:36:05.303979 139564307912448 logging_writer.py:48] [151000] global_step=151000, grad_norm=0.706386148929596, loss=0.8913401961326599
I0316 08:37:32.063441 139725430036288 spec.py:321] Evaluating on the training split.
I0316 08:37:38.285905 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 08:37:47.156283 139725430036288 spec.py:349] Evaluating on the test split.
I0316 08:37:49.508563 139725430036288 submission_runner.py:420] Time since start: 28703.08s, 	Step: 151479, 	{'train/accuracy': 0.9175302982330322, 'train/loss': 0.2996428906917572, 'validation/accuracy': 0.7544599771499634, 'validation/loss': 1.0373268127441406, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.7634209394454956, 'test/num_examples': 10000, 'score': 27582.174550294876, 'total_duration': 28703.077377319336, 'accumulated_submission_time': 27582.174550294876, 'accumulated_eval_time': 1113.5708310604095, 'accumulated_logging_time': 1.7905094623565674}
I0316 08:37:49.534955 139563620054784 logging_writer.py:48] [151479] accumulated_eval_time=1113.570831, accumulated_logging_time=1.790509, accumulated_submission_time=27582.174550, global_step=151479, preemption_count=0, score=27582.174550, test/accuracy=0.630700, test/loss=1.763421, test/num_examples=10000, total_duration=28703.077377, train/accuracy=0.917530, train/loss=0.299643, validation/accuracy=0.754460, validation/loss=1.037327, validation/num_examples=50000
I0316 08:37:53.561695 139563628447488 logging_writer.py:48] [151500] global_step=151500, grad_norm=0.698360800743103, loss=0.8340806365013123
I0316 08:39:24.411777 139563620054784 logging_writer.py:48] [152000] global_step=152000, grad_norm=0.6591158509254456, loss=0.8283318877220154
I0316 08:40:55.414842 139563628447488 logging_writer.py:48] [152500] global_step=152500, grad_norm=0.7092608213424683, loss=0.825628936290741
I0316 08:42:26.206271 139563620054784 logging_writer.py:48] [153000] global_step=153000, grad_norm=0.7714143395423889, loss=0.7310261130332947
I0316 08:43:57.317071 139563628447488 logging_writer.py:48] [153500] global_step=153500, grad_norm=0.7916770577430725, loss=0.9141113758087158
I0316 08:45:28.104570 139563620054784 logging_writer.py:48] [154000] global_step=154000, grad_norm=0.6485216617584229, loss=0.7130747437477112
I0316 08:46:19.609145 139725430036288 spec.py:321] Evaluating on the training split.
I0316 08:46:25.755352 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 08:46:34.806149 139725430036288 spec.py:349] Evaluating on the test split.
I0316 08:46:37.127068 139725430036288 submission_runner.py:420] Time since start: 29230.70s, 	Step: 154284, 	{'train/accuracy': 0.9184072017669678, 'train/loss': 0.2942565083503723, 'validation/accuracy': 0.7542799711227417, 'validation/loss': 1.038588285446167, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.765087366104126, 'test/num_examples': 10000, 'score': 28092.13354229927, 'total_duration': 29230.695883512497, 'accumulated_submission_time': 28092.13354229927, 'accumulated_eval_time': 1131.0887157917023, 'accumulated_logging_time': 1.826988935470581}
I0316 08:46:37.153876 139564324697856 logging_writer.py:48] [154284] accumulated_eval_time=1131.088716, accumulated_logging_time=1.826989, accumulated_submission_time=28092.133542, global_step=154284, preemption_count=0, score=28092.133542, test/accuracy=0.630400, test/loss=1.765087, test/num_examples=10000, total_duration=29230.695884, train/accuracy=0.918407, train/loss=0.294257, validation/accuracy=0.754280, validation/loss=1.038588, validation/num_examples=50000
I0316 08:47:16.573917 139564349875968 logging_writer.py:48] [154500] global_step=154500, grad_norm=0.6584895849227905, loss=0.7832823395729065
I0316 08:48:47.414640 139564324697856 logging_writer.py:48] [155000] global_step=155000, grad_norm=0.6818944811820984, loss=0.9296526312828064
I0316 08:50:18.531937 139564349875968 logging_writer.py:48] [155500] global_step=155500, grad_norm=0.6880626678466797, loss=0.8316749930381775
I0316 08:51:49.325380 139564324697856 logging_writer.py:48] [156000] global_step=156000, grad_norm=0.6603097319602966, loss=0.7946260571479797
I0316 08:53:20.390119 139564349875968 logging_writer.py:48] [156500] global_step=156500, grad_norm=0.6963866949081421, loss=0.8008550405502319
I0316 08:54:51.173326 139564324697856 logging_writer.py:48] [157000] global_step=157000, grad_norm=0.6727290153503418, loss=0.7652443051338196
I0316 08:55:07.262024 139725430036288 spec.py:321] Evaluating on the training split.
I0316 08:55:13.492368 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 08:55:22.519759 139725430036288 spec.py:349] Evaluating on the test split.
I0316 08:55:24.873284 139725430036288 submission_runner.py:420] Time since start: 29758.44s, 	Step: 157090, 	{'train/accuracy': 0.9148397445678711, 'train/loss': 0.3033747375011444, 'validation/accuracy': 0.7536999583244324, 'validation/loss': 1.0386899709701538, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.7648581266403198, 'test/num_examples': 10000, 'score': 28602.121263980865, 'total_duration': 29758.442100286484, 'accumulated_submission_time': 28602.121263980865, 'accumulated_eval_time': 1148.6999378204346, 'accumulated_logging_time': 1.8656847476959229}
I0316 08:55:24.901542 139564333090560 logging_writer.py:48] [157090] accumulated_eval_time=1148.699938, accumulated_logging_time=1.865685, accumulated_submission_time=28602.121264, global_step=157090, preemption_count=0, score=28602.121264, test/accuracy=0.630200, test/loss=1.764858, test/num_examples=10000, total_duration=29758.442100, train/accuracy=0.914840, train/loss=0.303375, validation/accuracy=0.753700, validation/loss=1.038690, validation/num_examples=50000
I0316 08:56:39.744107 139564349875968 logging_writer.py:48] [157500] global_step=157500, grad_norm=0.710229218006134, loss=0.8076192140579224
I0316 08:58:10.580229 139564333090560 logging_writer.py:48] [158000] global_step=158000, grad_norm=0.7317501306533813, loss=0.7548960447311401
I0316 08:59:41.616847 139564349875968 logging_writer.py:48] [158500] global_step=158500, grad_norm=0.7343026399612427, loss=0.813846230506897
I0316 09:01:12.448363 139564333090560 logging_writer.py:48] [159000] global_step=159000, grad_norm=0.8242912888526917, loss=0.9463234543800354
I0316 09:02:43.299941 139564349875968 logging_writer.py:48] [159500] global_step=159500, grad_norm=0.7588870525360107, loss=0.7429735064506531
I0316 09:03:54.942134 139725430036288 spec.py:321] Evaluating on the training split.
I0316 09:04:01.087220 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 09:04:10.161139 139725430036288 spec.py:349] Evaluating on the test split.
I0316 09:04:12.506385 139725430036288 submission_runner.py:420] Time since start: 30286.08s, 	Step: 159895, 	{'train/accuracy': 0.9173309803009033, 'train/loss': 0.2961660921573639, 'validation/accuracy': 0.7539599537849426, 'validation/loss': 1.0384106636047363, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.7647697925567627, 'test/num_examples': 10000, 'score': 29112.043640851974, 'total_duration': 30286.075167179108, 'accumulated_submission_time': 29112.043640851974, 'accumulated_eval_time': 1166.2641248703003, 'accumulated_logging_time': 1.9065217971801758}
I0316 09:04:12.533851 139564307912448 logging_writer.py:48] [159895] accumulated_eval_time=1166.264125, accumulated_logging_time=1.906522, accumulated_submission_time=29112.043641, global_step=159895, preemption_count=0, score=29112.043641, test/accuracy=0.631300, test/loss=1.764770, test/num_examples=10000, total_duration=30286.075167, train/accuracy=0.917331, train/loss=0.296166, validation/accuracy=0.753960, validation/loss=1.038411, validation/num_examples=50000
I0316 09:04:31.803857 139564316305152 logging_writer.py:48] [160000] global_step=160000, grad_norm=0.7653154134750366, loss=0.8669164776802063
I0316 09:06:02.626706 139564307912448 logging_writer.py:48] [160500] global_step=160500, grad_norm=0.681885302066803, loss=0.7646649479866028
I0316 09:07:33.601527 139564316305152 logging_writer.py:48] [161000] global_step=161000, grad_norm=0.7196430563926697, loss=0.8465477228164673
I0316 09:09:04.435163 139564307912448 logging_writer.py:48] [161500] global_step=161500, grad_norm=0.7158703804016113, loss=0.7852195501327515
I0316 09:10:35.451446 139564316305152 logging_writer.py:48] [162000] global_step=162000, grad_norm=0.6481751799583435, loss=0.679109513759613
I0316 09:12:06.295273 139564307912448 logging_writer.py:48] [162500] global_step=162500, grad_norm=0.6895599365234375, loss=0.8464685082435608
I0316 09:12:42.575607 139725430036288 spec.py:321] Evaluating on the training split.
I0316 09:12:48.727635 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 09:12:57.667269 139725430036288 spec.py:349] Evaluating on the test split.
I0316 09:13:00.005748 139725430036288 submission_runner.py:420] Time since start: 30813.57s, 	Step: 162701, 	{'train/accuracy': 0.9182676672935486, 'train/loss': 0.2924569845199585, 'validation/accuracy': 0.7541799545288086, 'validation/loss': 1.0382978916168213, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.7653626203536987, 'test/num_examples': 10000, 'score': 29621.970261335373, 'total_duration': 30813.574528455734, 'accumulated_submission_time': 29621.970261335373, 'accumulated_eval_time': 1183.6941940784454, 'accumulated_logging_time': 1.9452533721923828}
I0316 09:13:00.043162 139563628447488 logging_writer.py:48] [162701] accumulated_eval_time=1183.694194, accumulated_logging_time=1.945253, accumulated_submission_time=29621.970261, global_step=162701, preemption_count=0, score=29621.970261, test/accuracy=0.630900, test/loss=1.765363, test/num_examples=10000, total_duration=30813.574528, train/accuracy=0.918268, train/loss=0.292457, validation/accuracy=0.754180, validation/loss=1.038298, validation/num_examples=50000
I0316 09:13:54.731940 139564089865984 logging_writer.py:48] [163000] global_step=163000, grad_norm=0.7489859461784363, loss=0.8392220139503479
I0316 09:15:25.549288 139563628447488 logging_writer.py:48] [163500] global_step=163500, grad_norm=0.6956849098205566, loss=0.7177483439445496
I0316 09:16:56.527817 139564089865984 logging_writer.py:48] [164000] global_step=164000, grad_norm=0.789260745048523, loss=0.9302884936332703
I0316 09:18:27.336163 139563628447488 logging_writer.py:48] [164500] global_step=164500, grad_norm=0.7436217069625854, loss=1.0189710855484009
I0316 09:19:58.305268 139564089865984 logging_writer.py:48] [165000] global_step=165000, grad_norm=0.7413505911827087, loss=0.8926626443862915
I0316 09:21:29.065698 139563628447488 logging_writer.py:48] [165500] global_step=165500, grad_norm=0.7250461578369141, loss=0.8140773773193359
I0316 09:21:30.072173 139725430036288 spec.py:321] Evaluating on the training split.
I0316 09:21:36.395339 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 09:21:45.171860 139725430036288 spec.py:349] Evaluating on the test split.
I0316 09:21:47.544932 139725430036288 submission_runner.py:420] Time since start: 31341.11s, 	Step: 165507, 	{'train/accuracy': 0.9168327450752258, 'train/loss': 0.30003082752227783, 'validation/accuracy': 0.7541999816894531, 'validation/loss': 1.0381540060043335, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.7633862495422363, 'test/num_examples': 10000, 'score': 30131.885346651077, 'total_duration': 31341.113740682602, 'accumulated_submission_time': 30131.885346651077, 'accumulated_eval_time': 1201.1669166088104, 'accumulated_logging_time': 1.9930555820465088}
I0316 09:21:47.574216 139563620054784 logging_writer.py:48] [165507] accumulated_eval_time=1201.166917, accumulated_logging_time=1.993056, accumulated_submission_time=30131.885347, global_step=165507, preemption_count=0, score=30131.885347, test/accuracy=0.630900, test/loss=1.763386, test/num_examples=10000, total_duration=31341.113741, train/accuracy=0.916833, train/loss=0.300031, validation/accuracy=0.754200, validation/loss=1.038154, validation/num_examples=50000
I0316 09:23:17.288797 139563628447488 logging_writer.py:48] [166000] global_step=166000, grad_norm=0.6665670871734619, loss=0.8191283941268921
I0316 09:24:48.364210 139563620054784 logging_writer.py:48] [166500] global_step=166500, grad_norm=0.673334538936615, loss=0.8267421722412109
I0316 09:26:19.183090 139563628447488 logging_writer.py:48] [167000] global_step=167000, grad_norm=0.6908839344978333, loss=0.7410213947296143
I0316 09:27:50.133193 139563620054784 logging_writer.py:48] [167500] global_step=167500, grad_norm=0.6345812678337097, loss=0.7777022123336792
I0316 09:29:20.916308 139563628447488 logging_writer.py:48] [168000] global_step=168000, grad_norm=0.7007169127464294, loss=0.8775027990341187
I0316 09:30:17.653214 139725430036288 spec.py:321] Evaluating on the training split.
I0316 09:30:23.861170 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 09:30:32.816542 139725430036288 spec.py:349] Evaluating on the test split.
I0316 09:30:35.168025 139725430036288 submission_runner.py:420] Time since start: 31868.74s, 	Step: 168313, 	{'train/accuracy': 0.9159956574440002, 'train/loss': 0.30313050746917725, 'validation/accuracy': 0.7540199756622314, 'validation/loss': 1.0380730628967285, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.7652491331100464, 'test/num_examples': 10000, 'score': 30641.84900689125, 'total_duration': 31868.736838817596, 'accumulated_submission_time': 30641.84900689125, 'accumulated_eval_time': 1218.6816873550415, 'accumulated_logging_time': 2.032628297805786}
I0316 09:30:35.194773 139563628447488 logging_writer.py:48] [168313] accumulated_eval_time=1218.681687, accumulated_logging_time=2.032628, accumulated_submission_time=30641.849007, global_step=168313, preemption_count=0, score=30641.849007, test/accuracy=0.630500, test/loss=1.765249, test/num_examples=10000, total_duration=31868.736839, train/accuracy=0.915996, train/loss=0.303131, validation/accuracy=0.754020, validation/loss=1.038073, validation/num_examples=50000
I0316 09:31:09.350125 139564089865984 logging_writer.py:48] [168500] global_step=168500, grad_norm=0.6940661668777466, loss=0.8905355930328369
I0316 09:32:40.102413 139563628447488 logging_writer.py:48] [169000] global_step=169000, grad_norm=0.7059973478317261, loss=0.8133347034454346
I0316 09:34:11.080598 139564089865984 logging_writer.py:48] [169500] global_step=169500, grad_norm=0.7457658052444458, loss=0.8138691186904907
I0316 09:35:41.853374 139563628447488 logging_writer.py:48] [170000] global_step=170000, grad_norm=0.6650568842887878, loss=0.7601898908615112
I0316 09:37:12.825465 139564089865984 logging_writer.py:48] [170500] global_step=170500, grad_norm=0.6689815521240234, loss=0.8536592721939087
I0316 09:38:43.641906 139563628447488 logging_writer.py:48] [171000] global_step=171000, grad_norm=0.7221453189849854, loss=0.8984997272491455
I0316 09:39:05.185369 139725430036288 spec.py:321] Evaluating on the training split.
I0316 09:39:11.368046 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 09:39:20.404540 139725430036288 spec.py:349] Evaluating on the test split.
I0316 09:39:22.742355 139725430036288 submission_runner.py:420] Time since start: 32396.31s, 	Step: 171120, 	{'train/accuracy': 0.9181481003761292, 'train/loss': 0.29753434658050537, 'validation/accuracy': 0.7540599703788757, 'validation/loss': 1.0391547679901123, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.7662850618362427, 'test/num_examples': 10000, 'score': 31151.722822904587, 'total_duration': 32396.311151981354, 'accumulated_submission_time': 31151.722822904587, 'accumulated_eval_time': 1236.238615512848, 'accumulated_logging_time': 2.071288585662842}
I0316 09:39:22.770284 139563628447488 logging_writer.py:48] [171120] accumulated_eval_time=1236.238616, accumulated_logging_time=2.071289, accumulated_submission_time=31151.722823, global_step=171120, preemption_count=0, score=31151.722823, test/accuracy=0.631500, test/loss=1.766285, test/num_examples=10000, total_duration=32396.311152, train/accuracy=0.918148, train/loss=0.297534, validation/accuracy=0.754060, validation/loss=1.039155, validation/num_examples=50000
I0316 09:40:32.177294 139564089865984 logging_writer.py:48] [171500] global_step=171500, grad_norm=0.7347131967544556, loss=0.801835298538208
I0316 09:42:02.954469 139563628447488 logging_writer.py:48] [172000] global_step=172000, grad_norm=0.7088420391082764, loss=0.829902172088623
I0316 09:43:33.789365 139564089865984 logging_writer.py:48] [172500] global_step=172500, grad_norm=0.6721984148025513, loss=0.7850382328033447
I0316 09:45:04.825724 139563628447488 logging_writer.py:48] [173000] global_step=173000, grad_norm=0.6576178073883057, loss=0.7901608347892761
I0316 09:46:35.615126 139564089865984 logging_writer.py:48] [173500] global_step=173500, grad_norm=0.7178006768226624, loss=0.800559937953949
I0316 09:47:52.797842 139725430036288 spec.py:321] Evaluating on the training split.
I0316 09:47:58.912288 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 09:48:07.899045 139725430036288 spec.py:349] Evaluating on the test split.
I0316 09:48:10.238793 139725430036288 submission_runner.py:420] Time since start: 32923.81s, 	Step: 173925, 	{'train/accuracy': 0.91605544090271, 'train/loss': 0.29945990443229675, 'validation/accuracy': 0.7540199756622314, 'validation/loss': 1.0377227067947388, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.7649096250534058, 'test/num_examples': 10000, 'score': 31661.63643503189, 'total_duration': 32923.807606220245, 'accumulated_submission_time': 31661.63643503189, 'accumulated_eval_time': 1253.6795370578766, 'accumulated_logging_time': 2.110353708267212}
I0316 09:48:10.267124 139564333090560 logging_writer.py:48] [173925] accumulated_eval_time=1253.679537, accumulated_logging_time=2.110354, accumulated_submission_time=31661.636435, global_step=173925, preemption_count=0, score=31661.636435, test/accuracy=0.631000, test/loss=1.764910, test/num_examples=10000, total_duration=32923.807606, train/accuracy=0.916055, train/loss=0.299460, validation/accuracy=0.754020, validation/loss=1.037723, validation/num_examples=50000
I0316 09:48:24.099534 139564341483264 logging_writer.py:48] [174000] global_step=174000, grad_norm=0.7720032930374146, loss=0.9153072834014893
I0316 09:49:54.951176 139564333090560 logging_writer.py:48] [174500] global_step=174500, grad_norm=0.6634495258331299, loss=0.8196321129798889
I0316 09:51:26.019721 139564341483264 logging_writer.py:48] [175000] global_step=175000, grad_norm=0.7154852151870728, loss=0.789573073387146
I0316 09:52:56.785679 139564333090560 logging_writer.py:48] [175500] global_step=175500, grad_norm=0.6847909092903137, loss=0.8159950971603394
I0316 09:54:27.729282 139564341483264 logging_writer.py:48] [176000] global_step=176000, grad_norm=0.5964089632034302, loss=0.7000983357429504
I0316 09:55:58.531651 139564333090560 logging_writer.py:48] [176500] global_step=176500, grad_norm=0.7274363040924072, loss=0.8509983420372009
I0316 09:56:40.411294 139725430036288 spec.py:321] Evaluating on the training split.
I0316 09:56:46.657077 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 09:56:55.661287 139725430036288 spec.py:349] Evaluating on the test split.
I0316 09:56:57.994472 139725430036288 submission_runner.py:420] Time since start: 33451.56s, 	Step: 176732, 	{'train/accuracy': 0.9172911047935486, 'train/loss': 0.3004980981349945, 'validation/accuracy': 0.7536999583244324, 'validation/loss': 1.0384106636047363, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.7646987438201904, 'test/num_examples': 10000, 'score': 32171.66368174553, 'total_duration': 33451.56328821182, 'accumulated_submission_time': 32171.66368174553, 'accumulated_eval_time': 1271.26269698143, 'accumulated_logging_time': 2.1495463848114014}
I0316 09:56:58.024213 139563620054784 logging_writer.py:48] [176732] accumulated_eval_time=1271.262697, accumulated_logging_time=2.149546, accumulated_submission_time=32171.663682, global_step=176732, preemption_count=0, score=32171.663682, test/accuracy=0.631500, test/loss=1.764699, test/num_examples=10000, total_duration=33451.563288, train/accuracy=0.917291, train/loss=0.300498, validation/accuracy=0.753700, validation/loss=1.038411, validation/num_examples=50000
I0316 09:57:47.099589 139563628447488 logging_writer.py:48] [177000] global_step=177000, grad_norm=0.6478115320205688, loss=0.7016187906265259
I0316 09:59:17.881448 139563620054784 logging_writer.py:48] [177500] global_step=177500, grad_norm=0.7008726596832275, loss=0.8899295926094055
I0316 10:00:48.841400 139563628447488 logging_writer.py:48] [178000] global_step=178000, grad_norm=0.7195479869842529, loss=0.9096183180809021
I0316 10:02:19.639642 139563620054784 logging_writer.py:48] [178500] global_step=178500, grad_norm=0.6574999690055847, loss=0.7347880601882935
I0316 10:03:50.485466 139563628447488 logging_writer.py:48] [179000] global_step=179000, grad_norm=0.7047602534294128, loss=0.7722309827804565
I0316 10:05:21.479112 139563620054784 logging_writer.py:48] [179500] global_step=179500, grad_norm=0.6918190717697144, loss=0.8885670304298401
I0316 10:05:28.116843 139725430036288 spec.py:321] Evaluating on the training split.
I0316 10:05:34.306722 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 10:05:43.285665 139725430036288 spec.py:349] Evaluating on the test split.
I0316 10:05:45.635220 139725430036288 submission_runner.py:420] Time since start: 33979.20s, 	Step: 179538, 	{'train/accuracy': 0.9173508882522583, 'train/loss': 0.3010532259941101, 'validation/accuracy': 0.75382000207901, 'validation/loss': 1.0383822917938232, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.7664769887924194, 'test/num_examples': 10000, 'score': 32681.64035630226, 'total_duration': 33979.20403385162, 'accumulated_submission_time': 32681.64035630226, 'accumulated_eval_time': 1288.781033039093, 'accumulated_logging_time': 2.189389228820801}
I0316 10:05:45.662858 139563620054784 logging_writer.py:48] [179538] accumulated_eval_time=1288.781033, accumulated_logging_time=2.189389, accumulated_submission_time=32681.640356, global_step=179538, preemption_count=0, score=32681.640356, test/accuracy=0.631300, test/loss=1.766477, test/num_examples=10000, total_duration=33979.204034, train/accuracy=0.917351, train/loss=0.301053, validation/accuracy=0.753820, validation/loss=1.038382, validation/num_examples=50000
I0316 10:07:09.784751 139563628447488 logging_writer.py:48] [180000] global_step=180000, grad_norm=0.6663945913314819, loss=0.7691124081611633
I0316 10:08:40.833785 139563620054784 logging_writer.py:48] [180500] global_step=180500, grad_norm=0.7433281540870667, loss=0.9233909845352173
I0316 10:10:11.664415 139563628447488 logging_writer.py:48] [181000] global_step=181000, grad_norm=0.8119332790374756, loss=0.931843101978302
I0316 10:11:42.743682 139563620054784 logging_writer.py:48] [181500] global_step=181500, grad_norm=0.704652726650238, loss=0.7832055687904358
I0316 10:13:13.582976 139563628447488 logging_writer.py:48] [182000] global_step=182000, grad_norm=0.7005051374435425, loss=0.8660744428634644
I0316 10:14:15.754272 139725430036288 spec.py:321] Evaluating on the training split.
I0316 10:14:21.901514 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 10:14:30.781436 139725430036288 spec.py:349] Evaluating on the test split.
I0316 10:14:33.134207 139725430036288 submission_runner.py:420] Time since start: 34506.70s, 	Step: 182342, 	{'train/accuracy': 0.9167330861091614, 'train/loss': 0.3020401895046234, 'validation/accuracy': 0.754319965839386, 'validation/loss': 1.03815758228302, 'validation/num_examples': 50000, 'test/accuracy': 0.6323000192642212, 'test/loss': 1.7642401456832886, 'test/num_examples': 10000, 'score': 33191.615904569626, 'total_duration': 34506.703023433685, 'accumulated_submission_time': 33191.615904569626, 'accumulated_eval_time': 1306.1609332561493, 'accumulated_logging_time': 2.2296924591064453}
I0316 10:14:33.161761 139563628447488 logging_writer.py:48] [182342] accumulated_eval_time=1306.160933, accumulated_logging_time=2.229692, accumulated_submission_time=33191.615905, global_step=182342, preemption_count=0, score=33191.615905, test/accuracy=0.632300, test/loss=1.764240, test/num_examples=10000, total_duration=34506.703023, train/accuracy=0.916733, train/loss=0.302040, validation/accuracy=0.754320, validation/loss=1.038158, validation/num_examples=50000
I0316 10:15:02.076674 139564307912448 logging_writer.py:48] [182500] global_step=182500, grad_norm=0.7487936615943909, loss=0.8130898475646973
I0316 10:16:32.845233 139563628447488 logging_writer.py:48] [183000] global_step=183000, grad_norm=0.7871741652488708, loss=0.8682219982147217
I0316 10:18:03.904225 139564307912448 logging_writer.py:48] [183500] global_step=183500, grad_norm=0.7071731686592102, loss=0.894916832447052
I0316 10:19:34.677021 139563628447488 logging_writer.py:48] [184000] global_step=184000, grad_norm=0.6434023976325989, loss=0.7121776938438416
I0316 10:21:05.667808 139564307912448 logging_writer.py:48] [184500] global_step=184500, grad_norm=0.6873692274093628, loss=0.811095118522644
I0316 10:22:36.471216 139563628447488 logging_writer.py:48] [185000] global_step=185000, grad_norm=0.704468846321106, loss=0.8306275010108948
I0316 10:23:03.239968 139725430036288 spec.py:321] Evaluating on the training split.
I0316 10:23:09.464989 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 10:23:18.288090 139725430036288 spec.py:349] Evaluating on the test split.
I0316 10:23:20.604786 139725430036288 submission_runner.py:420] Time since start: 35034.17s, 	Step: 185149, 	{'train/accuracy': 0.915437638759613, 'train/loss': 0.3011271059513092, 'validation/accuracy': 0.7538999915122986, 'validation/loss': 1.037919282913208, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.7637271881103516, 'test/num_examples': 10000, 'score': 33701.57856965065, 'total_duration': 35034.17359876633, 'accumulated_submission_time': 33701.57856965065, 'accumulated_eval_time': 1323.525725364685, 'accumulated_logging_time': 2.268526554107666}
I0316 10:23:20.634706 139563628447488 logging_writer.py:48] [185149] accumulated_eval_time=1323.525725, accumulated_logging_time=2.268527, accumulated_submission_time=33701.578570, global_step=185149, preemption_count=0, score=33701.578570, test/accuracy=0.630700, test/loss=1.763727, test/num_examples=10000, total_duration=35034.173599, train/accuracy=0.915438, train/loss=0.301127, validation/accuracy=0.753900, validation/loss=1.037919, validation/num_examples=50000
I0316 10:24:24.788800 139564089865984 logging_writer.py:48] [185500] global_step=185500, grad_norm=0.6620498299598694, loss=0.7221150398254395
I0316 10:25:55.570850 139563628447488 logging_writer.py:48] [186000] global_step=186000, grad_norm=0.7725915908813477, loss=0.849913477897644
I0316 10:27:26.296869 139564089865984 logging_writer.py:48] [186500] global_step=186500, grad_norm=0.7293543219566345, loss=0.8317250609397888
I0316 10:27:56.464189 139725430036288 spec.py:321] Evaluating on the training split.
I0316 10:28:02.708319 139725430036288 spec.py:333] Evaluating on the validation split.
I0316 10:28:11.620312 139725430036288 spec.py:349] Evaluating on the test split.
I0316 10:28:14.049681 139725430036288 submission_runner.py:420] Time since start: 35327.62s, 	Step: 186666, 	{'train/accuracy': 0.9185666441917419, 'train/loss': 0.29453206062316895, 'validation/accuracy': 0.7536999583244324, 'validation/loss': 1.0383217334747314, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.7669010162353516, 'test/num_examples': 10000, 'score': 33977.34105300903, 'total_duration': 35327.618497133255, 'accumulated_submission_time': 33977.34105300903, 'accumulated_eval_time': 1341.1111905574799, 'accumulated_logging_time': 2.3088231086730957}
I0316 10:28:14.078880 139564307912448 logging_writer.py:48] [186666] accumulated_eval_time=1341.111191, accumulated_logging_time=2.308823, accumulated_submission_time=33977.341053, global_step=186666, preemption_count=0, score=33977.341053, test/accuracy=0.630500, test/loss=1.766901, test/num_examples=10000, total_duration=35327.618497, train/accuracy=0.918567, train/loss=0.294532, validation/accuracy=0.753700, validation/loss=1.038322, validation/num_examples=50000
I0316 10:28:14.102899 139564316305152 logging_writer.py:48] [186666] global_step=186666, preemption_count=0, score=33977.341053
I0316 10:28:14.493918 139725430036288 checkpoints.py:490] Saving checkpoint at step: 186666
I0316 10:28:15.703161 139725430036288 checkpoints.py:422] Saved checkpoint at /experiment_runs/variants_target_setting/study_0/imagenet_resnet_silu_jax/trial_1/checkpoint_186666
I0316 10:28:15.734172 139725430036288 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/variants_target_setting/study_0/imagenet_resnet_silu_jax/trial_1/checkpoint_186666.
I0316 10:28:16.211805 139725430036288 submission_runner.py:593] Tuning trial 1/1
I0316 10:28:16.212038 139725430036288 submission_runner.py:594] Hyperparameters: Hyperparameters(learning_rate=0.01897755400372091, beta1=0.9666072782043229, beta2=0.99681600289198, warmup_steps=6999, weight_decay=0.015653883841116094)
I0316 10:28:16.215188 139725430036288 submission_runner.py:595] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0009566326625645161, 'train/loss': 6.90775728225708, 'validation/accuracy': 0.0009399999980814755, 'validation/loss': 6.9077558517456055, 'validation/num_examples': 50000, 'test/accuracy': 0.0009000000427477062, 'test/loss': 6.9077558517456055, 'test/num_examples': 10000, 'score': 44.76942777633667, 'total_duration': 86.53807091712952, 'accumulated_submission_time': 44.76942777633667, 'accumulated_eval_time': 41.76855397224426, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2797, {'train/accuracy': 0.3777303695678711, 'train/loss': 2.8826675415039062, 'validation/accuracy': 0.3177799880504608, 'validation/loss': 3.285593032836914, 'validation/num_examples': 50000, 'test/accuracy': 0.2346000075340271, 'test/loss': 4.022057056427002, 'test/num_examples': 10000, 'score': 554.7455728054047, 'total_duration': 614.9359719753265, 'accumulated_submission_time': 554.7455728054047, 'accumulated_eval_time': 60.06253504753113, 'accumulated_logging_time': 0.027496814727783203, 'global_step': 2797, 'preemption_count': 0}), (5599, {'train/accuracy': 0.4461694657802582, 'train/loss': 2.4868123531341553, 'validation/accuracy': 0.40153998136520386, 'validation/loss': 2.776494264602661, 'validation/num_examples': 50000, 'test/accuracy': 0.3099000155925751, 'test/loss': 3.5185117721557617, 'test/num_examples': 10000, 'score': 1064.6769313812256, 'total_duration': 1143.124930858612, 'accumulated_submission_time': 1064.6769313812256, 'accumulated_eval_time': 78.1887674331665, 'accumulated_logging_time': 0.05712008476257324, 'global_step': 5599, 'preemption_count': 0}), (8403, {'train/accuracy': 0.4722974896430969, 'train/loss': 2.3668148517608643, 'validation/accuracy': 0.4332599937915802, 'validation/loss': 2.6302802562713623, 'validation/num_examples': 50000, 'test/accuracy': 0.3272000253200531, 'test/loss': 3.3977982997894287, 'test/num_examples': 10000, 'score': 1574.5632791519165, 'total_duration': 1671.438006401062, 'accumulated_submission_time': 1574.5632791519165, 'accumulated_eval_time': 96.48443961143494, 'accumulated_logging_time': 0.08557748794555664, 'global_step': 8403, 'preemption_count': 0}), (11207, {'train/accuracy': 0.4318399131298065, 'train/loss': 2.6891977787017822, 'validation/accuracy': 0.4001599848270416, 'validation/loss': 2.936760663986206, 'validation/num_examples': 50000, 'test/accuracy': 0.2972000241279602, 'test/loss': 3.787325859069824, 'test/num_examples': 10000, 'score': 2084.4396369457245, 'total_duration': 2199.5703365802765, 'accumulated_submission_time': 2084.4396369457245, 'accumulated_eval_time': 114.61002850532532, 'accumulated_logging_time': 0.11574101448059082, 'global_step': 11207, 'preemption_count': 0}), (14012, {'train/accuracy': 0.5277822017669678, 'train/loss': 2.0577874183654785, 'validation/accuracy': 0.49167999625205994, 'validation/loss': 2.2797157764434814, 'validation/num_examples': 50000, 'test/accuracy': 0.3775000274181366, 'test/loss': 3.0655620098114014, 'test/num_examples': 10000, 'score': 2594.3650550842285, 'total_duration': 2727.8692593574524, 'accumulated_submission_time': 2594.3650550842285, 'accumulated_eval_time': 132.8533160686493, 'accumulated_logging_time': 0.14440321922302246, 'global_step': 14012, 'preemption_count': 0}), (16817, {'train/accuracy': 0.5407366156578064, 'train/loss': 1.9808214902877808, 'validation/accuracy': 0.5069000124931335, 'validation/loss': 2.204259157180786, 'validation/num_examples': 50000, 'test/accuracy': 0.39000001549720764, 'test/loss': 3.021458864212036, 'test/num_examples': 10000, 'score': 3104.3994331359863, 'total_duration': 3256.2665758132935, 'accumulated_submission_time': 3104.3994331359863, 'accumulated_eval_time': 151.0880241394043, 'accumulated_logging_time': 0.17274951934814453, 'global_step': 16817, 'preemption_count': 0}), (19621, {'train/accuracy': 0.5181361436843872, 'train/loss': 2.109130620956421, 'validation/accuracy': 0.48955997824668884, 'validation/loss': 2.2983715534210205, 'validation/num_examples': 50000, 'test/accuracy': 0.3757000267505646, 'test/loss': 3.088669776916504, 'test/num_examples': 10000, 'score': 3614.357518196106, 'total_duration': 3784.6723296642303, 'accumulated_submission_time': 3614.357518196106, 'accumulated_eval_time': 169.4036750793457, 'accumulated_logging_time': 0.2043294906616211, 'global_step': 19621, 'preemption_count': 0}), (22426, {'train/accuracy': 0.5439851880073547, 'train/loss': 1.972494125366211, 'validation/accuracy': 0.5137199759483337, 'validation/loss': 2.164656162261963, 'validation/num_examples': 50000, 'test/accuracy': 0.39890003204345703, 'test/loss': 2.9415578842163086, 'test/num_examples': 10000, 'score': 4124.296654462814, 'total_duration': 4314.44334936142, 'accumulated_submission_time': 4124.296654462814, 'accumulated_eval_time': 189.10289025306702, 'accumulated_logging_time': 0.23410701751708984, 'global_step': 22426, 'preemption_count': 0}), (25231, {'train/accuracy': 0.5458186864852905, 'train/loss': 1.956695556640625, 'validation/accuracy': 0.5180000066757202, 'validation/loss': 2.1391093730926514, 'validation/num_examples': 50000, 'test/accuracy': 0.4059000313282013, 'test/loss': 2.9370806217193604, 'test/num_examples': 10000, 'score': 4634.342221260071, 'total_duration': 4842.955098152161, 'accumulated_submission_time': 4634.342221260071, 'accumulated_eval_time': 207.43594336509705, 'accumulated_logging_time': 0.26413846015930176, 'global_step': 25231, 'preemption_count': 0}), (28035, {'train/accuracy': 0.5399991869926453, 'train/loss': 2.0099799633026123, 'validation/accuracy': 0.5093199610710144, 'validation/loss': 2.1760058403015137, 'validation/num_examples': 50000, 'test/accuracy': 0.3921000063419342, 'test/loss': 2.9691014289855957, 'test/num_examples': 10000, 'score': 5144.328633308411, 'total_duration': 5371.329715490341, 'accumulated_submission_time': 5144.328633308411, 'accumulated_eval_time': 225.69025087356567, 'accumulated_logging_time': 0.29558563232421875, 'global_step': 28035, 'preemption_count': 0}), (30840, {'train/accuracy': 0.5283203125, 'train/loss': 2.044196844100952, 'validation/accuracy': 0.5013200044631958, 'validation/loss': 2.2105205059051514, 'validation/num_examples': 50000, 'test/accuracy': 0.38610002398490906, 'test/loss': 2.9892220497131348, 'test/num_examples': 10000, 'score': 5654.273295402527, 'total_duration': 5899.6718645095825, 'accumulated_submission_time': 5654.273295402527, 'accumulated_eval_time': 243.95322513580322, 'accumulated_logging_time': 0.32945823669433594, 'global_step': 30840, 'preemption_count': 0}), (33644, {'train/accuracy': 0.55078125, 'train/loss': 1.9237500429153442, 'validation/accuracy': 0.5217399597167969, 'validation/loss': 2.1103367805480957, 'validation/num_examples': 50000, 'test/accuracy': 0.4102000296115875, 'test/loss': 2.8517088890075684, 'test/num_examples': 10000, 'score': 6164.236007452011, 'total_duration': 6428.107389688492, 'accumulated_submission_time': 6164.236007452011, 'accumulated_eval_time': 262.2887086868286, 'accumulated_logging_time': 0.3625681400299072, 'global_step': 33644, 'preemption_count': 0}), (36450, {'train/accuracy': 0.6231265664100647, 'train/loss': 1.5569429397583008, 'validation/accuracy': 0.5211600065231323, 'validation/loss': 2.109849214553833, 'validation/num_examples': 50000, 'test/accuracy': 0.40610000491142273, 'test/loss': 2.878661632537842, 'test/num_examples': 10000, 'score': 6674.259563446045, 'total_duration': 6956.598942995071, 'accumulated_submission_time': 6674.259563446045, 'accumulated_eval_time': 280.6237015724182, 'accumulated_logging_time': 0.3942446708679199, 'global_step': 36450, 'preemption_count': 0}), (39254, {'train/accuracy': 0.5848612785339355, 'train/loss': 1.7456644773483276, 'validation/accuracy': 0.5212000012397766, 'validation/loss': 2.116684913635254, 'validation/num_examples': 50000, 'test/accuracy': 0.40470001101493835, 'test/loss': 2.9121763706207275, 'test/num_examples': 10000, 'score': 7184.125803232193, 'total_duration': 7485.014935970306, 'accumulated_submission_time': 7184.125803232193, 'accumulated_eval_time': 299.0368101596832, 'accumulated_logging_time': 0.4279322624206543, 'global_step': 39254, 'preemption_count': 0}), (42058, {'train/accuracy': 0.5817123651504517, 'train/loss': 1.7716667652130127, 'validation/accuracy': 0.5311200022697449, 'validation/loss': 2.07171630859375, 'validation/num_examples': 50000, 'test/accuracy': 0.41210001707077026, 'test/loss': 2.8512117862701416, 'test/num_examples': 10000, 'score': 7694.032695770264, 'total_duration': 8013.487437486649, 'accumulated_submission_time': 7694.032695770264, 'accumulated_eval_time': 317.4702606201172, 'accumulated_logging_time': 0.4591078758239746, 'global_step': 42058, 'preemption_count': 0}), (44864, {'train/accuracy': 0.6045320630073547, 'train/loss': 1.64271080493927, 'validation/accuracy': 0.5548999905586243, 'validation/loss': 1.9526820182800293, 'validation/num_examples': 50000, 'test/accuracy': 0.4333000183105469, 'test/loss': 2.7427008152008057, 'test/num_examples': 10000, 'score': 8203.979739427567, 'total_duration': 8541.952942848206, 'accumulated_submission_time': 8203.979739427567, 'accumulated_eval_time': 335.8533761501312, 'accumulated_logging_time': 0.49260902404785156, 'global_step': 44864, 'preemption_count': 0}), (47669, {'train/accuracy': 0.5823700428009033, 'train/loss': 1.7545100450515747, 'validation/accuracy': 0.5369399785995483, 'validation/loss': 2.0408244132995605, 'validation/num_examples': 50000, 'test/accuracy': 0.4198000133037567, 'test/loss': 2.8394970893859863, 'test/num_examples': 10000, 'score': 8713.962616920471, 'total_duration': 9070.60176706314, 'accumulated_submission_time': 8713.962616920471, 'accumulated_eval_time': 354.3841245174408, 'accumulated_logging_time': 0.5262198448181152, 'global_step': 47669, 'preemption_count': 0}), (50476, {'train/accuracy': 0.5855987071990967, 'train/loss': 1.7411925792694092, 'validation/accuracy': 0.5447800159454346, 'validation/loss': 1.9759788513183594, 'validation/num_examples': 50000, 'test/accuracy': 0.4279000163078308, 'test/loss': 2.75671124458313, 'test/num_examples': 10000, 'score': 9223.852549552917, 'total_duration': 9600.703824520111, 'accumulated_submission_time': 9223.852549552917, 'accumulated_eval_time': 374.4605236053467, 'accumulated_logging_time': 0.5567066669464111, 'global_step': 50476, 'preemption_count': 0}), (53281, {'train/accuracy': 0.6027781963348389, 'train/loss': 1.6589690446853638, 'validation/accuracy': 0.5643399953842163, 'validation/loss': 1.9020472764968872, 'validation/num_examples': 50000, 'test/accuracy': 0.4481000304222107, 'test/loss': 2.6654083728790283, 'test/num_examples': 10000, 'score': 9733.72113442421, 'total_duration': 10133.133841276169, 'accumulated_submission_time': 9733.72113442421, 'accumulated_eval_time': 396.8869295120239, 'accumulated_logging_time': 0.5897741317749023, 'global_step': 53281, 'preemption_count': 0}), (56085, {'train/accuracy': 0.5968989133834839, 'train/loss': 1.692585825920105, 'validation/accuracy': 0.5556600093841553, 'validation/loss': 1.945448398590088, 'validation/num_examples': 50000, 'test/accuracy': 0.44050002098083496, 'test/loss': 2.7063052654266357, 'test/num_examples': 10000, 'score': 10243.608927488327, 'total_duration': 10668.260700702667, 'accumulated_submission_time': 10243.608927488327, 'accumulated_eval_time': 421.9911689758301, 'accumulated_logging_time': 0.6212289333343506, 'global_step': 56085, 'preemption_count': 0}), (58891, {'train/accuracy': 0.5959023833274841, 'train/loss': 1.6968411207199097, 'validation/accuracy': 0.5623799562454224, 'validation/loss': 1.9091722965240479, 'validation/num_examples': 50000, 'test/accuracy': 0.4401000142097473, 'test/loss': 2.6764233112335205, 'test/num_examples': 10000, 'score': 10753.614179611206, 'total_duration': 11203.592434883118, 'accumulated_submission_time': 10753.614179611206, 'accumulated_eval_time': 447.18309020996094, 'accumulated_logging_time': 0.6515779495239258, 'global_step': 58891, 'preemption_count': 0}), (61694, {'train/accuracy': 0.5912388563156128, 'train/loss': 1.7321046590805054, 'validation/accuracy': 0.5537399649620056, 'validation/loss': 1.9472651481628418, 'validation/num_examples': 50000, 'test/accuracy': 0.43480002880096436, 'test/loss': 2.7636868953704834, 'test/num_examples': 10000, 'score': 11263.492208719254, 'total_duration': 11738.936523675919, 'accumulated_submission_time': 11263.492208719254, 'accumulated_eval_time': 472.5170512199402, 'accumulated_logging_time': 0.6798145771026611, 'global_step': 61694, 'preemption_count': 0}), (64501, {'train/accuracy': 0.6083784699440002, 'train/loss': 1.6426235437393188, 'validation/accuracy': 0.574459969997406, 'validation/loss': 1.8577344417572021, 'validation/num_examples': 50000, 'test/accuracy': 0.453000009059906, 'test/loss': 2.6313228607177734, 'test/num_examples': 10000, 'score': 11773.553865909576, 'total_duration': 12273.206993341446, 'accumulated_submission_time': 11773.553865909576, 'accumulated_eval_time': 496.5924394130707, 'accumulated_logging_time': 0.708660364151001, 'global_step': 64501, 'preemption_count': 0}), (67307, {'train/accuracy': 0.630301296710968, 'train/loss': 1.5318502187728882, 'validation/accuracy': 0.5893200039863586, 'validation/loss': 1.756237268447876, 'validation/num_examples': 50000, 'test/accuracy': 0.46220001578330994, 'test/loss': 2.512631416320801, 'test/num_examples': 10000, 'score': 12283.551887750626, 'total_duration': 12809.256762266159, 'accumulated_submission_time': 12283.551887750626, 'accumulated_eval_time': 522.5153031349182, 'accumulated_logging_time': 0.7359635829925537, 'global_step': 67307, 'preemption_count': 0}), (70112, {'train/accuracy': 0.6595184803009033, 'train/loss': 1.371079921722412, 'validation/accuracy': 0.5632199645042419, 'validation/loss': 1.9180924892425537, 'validation/num_examples': 50000, 'test/accuracy': 0.4400000274181366, 'test/loss': 2.6956000328063965, 'test/num_examples': 10000, 'score': 12793.549216508865, 'total_duration': 13342.195656776428, 'accumulated_submission_time': 12793.549216508865, 'accumulated_eval_time': 545.3251523971558, 'accumulated_logging_time': 0.765692949295044, 'global_step': 70112, 'preemption_count': 0}), (72919, {'train/accuracy': 0.6660555005073547, 'train/loss': 1.3387036323547363, 'validation/accuracy': 0.5880999565124512, 'validation/loss': 1.769765853881836, 'validation/num_examples': 50000, 'test/accuracy': 0.4571000337600708, 'test/loss': 2.5662119388580322, 'test/num_examples': 10000, 'score': 13303.445712327957, 'total_duration': 13875.576974868774, 'accumulated_submission_time': 13303.445712327957, 'accumulated_eval_time': 568.6797630786896, 'accumulated_logging_time': 0.7939844131469727, 'global_step': 72919, 'preemption_count': 0}), (75724, {'train/accuracy': 0.6802455186843872, 'train/loss': 1.2749207019805908, 'validation/accuracy': 0.6038999557495117, 'validation/loss': 1.6779900789260864, 'validation/num_examples': 50000, 'test/accuracy': 0.47470003366470337, 'test/loss': 2.478337526321411, 'test/num_examples': 10000, 'score': 13813.378238916397, 'total_duration': 14409.517012834549, 'accumulated_submission_time': 13813.378238916397, 'accumulated_eval_time': 592.5546867847443, 'accumulated_logging_time': 0.8258340358734131, 'global_step': 75724, 'preemption_count': 0}), (78531, {'train/accuracy': 0.6827168464660645, 'train/loss': 1.266086220741272, 'validation/accuracy': 0.6155799627304077, 'validation/loss': 1.6203733682632446, 'validation/num_examples': 50000, 'test/accuracy': 0.49500003457069397, 'test/loss': 2.3525707721710205, 'test/num_examples': 10000, 'score': 14323.351205825806, 'total_duration': 14944.197999477386, 'accumulated_submission_time': 14323.351205825806, 'accumulated_eval_time': 617.1327600479126, 'accumulated_logging_time': 0.8554675579071045, 'global_step': 78531, 'preemption_count': 0}), (81338, {'train/accuracy': 0.6678491830825806, 'train/loss': 1.3394726514816284, 'validation/accuracy': 0.6077399849891663, 'validation/loss': 1.6649376153945923, 'validation/num_examples': 50000, 'test/accuracy': 0.4856000244617462, 'test/loss': 2.405643939971924, 'test/num_examples': 10000, 'score': 14833.359668016434, 'total_duration': 15477.779819011688, 'accumulated_submission_time': 14833.359668016434, 'accumulated_eval_time': 640.5754406452179, 'accumulated_logging_time': 0.8834488391876221, 'global_step': 81338, 'preemption_count': 0}), (84145, {'train/accuracy': 0.6775350570678711, 'train/loss': 1.2906715869903564, 'validation/accuracy': 0.6220799684524536, 'validation/loss': 1.5991394519805908, 'validation/num_examples': 50000, 'test/accuracy': 0.49500003457069397, 'test/loss': 2.3471944332122803, 'test/num_examples': 10000, 'score': 15343.299053192139, 'total_duration': 16011.31775546074, 'accumulated_submission_time': 15343.299053192139, 'accumulated_eval_time': 664.0437948703766, 'accumulated_logging_time': 0.9118335247039795, 'global_step': 84145, 'preemption_count': 0}), (86952, {'train/accuracy': 0.6671316623687744, 'train/loss': 1.3346246480941772, 'validation/accuracy': 0.6101399660110474, 'validation/loss': 1.6558141708374023, 'validation/num_examples': 50000, 'test/accuracy': 0.48570001125335693, 'test/loss': 2.394282341003418, 'test/num_examples': 10000, 'score': 15853.323334932327, 'total_duration': 16544.580283880234, 'accumulated_submission_time': 15853.323334932327, 'accumulated_eval_time': 687.1493873596191, 'accumulated_logging_time': 0.9420502185821533, 'global_step': 86952, 'preemption_count': 0}), (89758, {'train/accuracy': 0.654715359210968, 'train/loss': 1.392137050628662, 'validation/accuracy': 0.5983399748802185, 'validation/loss': 1.7241629362106323, 'validation/num_examples': 50000, 'test/accuracy': 0.4775000214576721, 'test/loss': 2.459275245666504, 'test/num_examples': 10000, 'score': 16363.312257289886, 'total_duration': 17077.17719435692, 'accumulated_submission_time': 16363.312257289886, 'accumulated_eval_time': 709.6218059062958, 'accumulated_logging_time': 0.9764280319213867, 'global_step': 89758, 'preemption_count': 0}), (92563, {'train/accuracy': 0.7001753449440002, 'train/loss': 1.1904690265655518, 'validation/accuracy': 0.6431800127029419, 'validation/loss': 1.5130765438079834, 'validation/num_examples': 50000, 'test/accuracy': 0.5164999961853027, 'test/loss': 2.2567412853240967, 'test/num_examples': 10000, 'score': 16873.20974779129, 'total_duration': 17607.608857393265, 'accumulated_submission_time': 16873.20974779129, 'accumulated_eval_time': 730.0223150253296, 'accumulated_logging_time': 1.0082745552062988, 'global_step': 92563, 'preemption_count': 0}), (95368, {'train/accuracy': 0.7003945708274841, 'train/loss': 1.1815835237503052, 'validation/accuracy': 0.6406799554824829, 'validation/loss': 1.5170526504516602, 'validation/num_examples': 50000, 'test/accuracy': 0.5115000009536743, 'test/loss': 2.2971582412719727, 'test/num_examples': 10000, 'score': 17383.213754177094, 'total_duration': 18137.716124534607, 'accumulated_submission_time': 17383.213754177094, 'accumulated_eval_time': 749.993668794632, 'accumulated_logging_time': 1.0387895107269287, 'global_step': 95368, 'preemption_count': 0}), (98174, {'train/accuracy': 0.7204838991165161, 'train/loss': 1.1050729751586914, 'validation/accuracy': 0.6505599617958069, 'validation/loss': 1.4714226722717285, 'validation/num_examples': 50000, 'test/accuracy': 0.5278000235557556, 'test/loss': 2.1921775341033936, 'test/num_examples': 10000, 'score': 17893.218606472015, 'total_duration': 18666.329828500748, 'accumulated_submission_time': 17893.218606472015, 'accumulated_eval_time': 768.4641678333282, 'accumulated_logging_time': 1.0741848945617676, 'global_step': 98174, 'preemption_count': 0}), (100980, {'train/accuracy': 0.7105987071990967, 'train/loss': 1.1519235372543335, 'validation/accuracy': 0.645039975643158, 'validation/loss': 1.4904940128326416, 'validation/num_examples': 50000, 'test/accuracy': 0.515500009059906, 'test/loss': 2.267381429672241, 'test/num_examples': 10000, 'score': 18403.10390162468, 'total_duration': 19195.148364067078, 'accumulated_submission_time': 18403.10390162468, 'accumulated_eval_time': 787.2631387710571, 'accumulated_logging_time': 1.1079041957855225, 'global_step': 100980, 'preemption_count': 0}), (103784, {'train/accuracy': 0.7798947691917419, 'train/loss': 0.8267284631729126, 'validation/accuracy': 0.667419970035553, 'validation/loss': 1.3963357210159302, 'validation/num_examples': 50000, 'test/accuracy': 0.5393000245094299, 'test/loss': 2.16798996925354, 'test/num_examples': 10000, 'score': 18912.9945166111, 'total_duration': 19726.486125946045, 'accumulated_submission_time': 18912.9945166111, 'accumulated_eval_time': 808.5686769485474, 'accumulated_logging_time': 1.1457602977752686, 'global_step': 103784, 'preemption_count': 0}), (106591, {'train/accuracy': 0.8012595772743225, 'train/loss': 0.7305102348327637, 'validation/accuracy': 0.6799399852752686, 'validation/loss': 1.3346288204193115, 'validation/num_examples': 50000, 'test/accuracy': 0.5610000491142273, 'test/loss': 2.0588128566741943, 'test/num_examples': 10000, 'score': 19422.943388223648, 'total_duration': 20255.836973428726, 'accumulated_submission_time': 19422.943388223648, 'accumulated_eval_time': 827.8361773490906, 'accumulated_logging_time': 1.1785557270050049, 'global_step': 106591, 'preemption_count': 0}), (109396, {'train/accuracy': 0.7942641973495483, 'train/loss': 0.7458347082138062, 'validation/accuracy': 0.6802999973297119, 'validation/loss': 1.3340208530426025, 'validation/num_examples': 50000, 'test/accuracy': 0.5577000379562378, 'test/loss': 2.072763442993164, 'test/num_examples': 10000, 'score': 19932.948579072952, 'total_duration': 20783.896659851074, 'accumulated_submission_time': 19932.948579072952, 'accumulated_eval_time': 845.7519087791443, 'accumulated_logging_time': 1.2124686241149902, 'global_step': 109396, 'preemption_count': 0}), (112201, {'train/accuracy': 0.7999641299247742, 'train/loss': 0.7351735234260559, 'validation/accuracy': 0.686519980430603, 'validation/loss': 1.315731167793274, 'validation/num_examples': 50000, 'test/accuracy': 0.5611000061035156, 'test/loss': 2.063603639602661, 'test/num_examples': 10000, 'score': 20442.890431404114, 'total_duration': 21312.616918325424, 'accumulated_submission_time': 20442.890431404114, 'accumulated_eval_time': 864.3865873813629, 'accumulated_logging_time': 1.2514734268188477, 'global_step': 112201, 'preemption_count': 0}), (115007, {'train/accuracy': 0.8163862824440002, 'train/loss': 0.6614758372306824, 'validation/accuracy': 0.7037400007247925, 'validation/loss': 1.2468513250350952, 'validation/num_examples': 50000, 'test/accuracy': 0.572700023651123, 'test/loss': 2.0381808280944824, 'test/num_examples': 10000, 'score': 20952.809463977814, 'total_duration': 21840.261241674423, 'accumulated_submission_time': 20952.809463977814, 'accumulated_eval_time': 881.9741714000702, 'accumulated_logging_time': 1.2862813472747803, 'global_step': 115007, 'preemption_count': 0}), (117811, {'train/accuracy': 0.8378308415412903, 'train/loss': 0.5822914838790894, 'validation/accuracy': 0.7119999527931213, 'validation/loss': 1.1972250938415527, 'validation/num_examples': 50000, 'test/accuracy': 0.589900016784668, 'test/loss': 1.922194004058838, 'test/num_examples': 10000, 'score': 21462.71215581894, 'total_duration': 22369.477986335754, 'accumulated_submission_time': 21462.71215581894, 'accumulated_eval_time': 901.1468102931976, 'accumulated_logging_time': 1.3231489658355713, 'global_step': 117811, 'preemption_count': 0}), (120617, {'train/accuracy': 0.8485331535339355, 'train/loss': 0.5348440408706665, 'validation/accuracy': 0.7240599989891052, 'validation/loss': 1.1545007228851318, 'validation/num_examples': 50000, 'test/accuracy': 0.5979000329971313, 'test/loss': 1.9008315801620483, 'test/num_examples': 10000, 'score': 21972.669156074524, 'total_duration': 22897.962318897247, 'accumulated_submission_time': 21972.669156074524, 'accumulated_eval_time': 919.5350027084351, 'accumulated_logging_time': 1.357767105102539, 'global_step': 120617, 'preemption_count': 0}), (123423, {'train/accuracy': 0.8653938174247742, 'train/loss': 0.48598766326904297, 'validation/accuracy': 0.7308200001716614, 'validation/loss': 1.1270761489868164, 'validation/num_examples': 50000, 'test/accuracy': 0.603600025177002, 'test/loss': 1.8526098728179932, 'test/num_examples': 10000, 'score': 22482.648923158646, 'total_duration': 23425.57219672203, 'accumulated_submission_time': 22482.648923158646, 'accumulated_eval_time': 937.0239474773407, 'accumulated_logging_time': 1.3936655521392822, 'global_step': 123423, 'preemption_count': 0}), (126228, {'train/accuracy': 0.8854233026504517, 'train/loss': 0.4137967824935913, 'validation/accuracy': 0.7409200072288513, 'validation/loss': 1.0861350297927856, 'validation/num_examples': 50000, 'test/accuracy': 0.6169000267982483, 'test/loss': 1.8041653633117676, 'test/num_examples': 10000, 'score': 22992.67735028267, 'total_duration': 23953.243550539017, 'accumulated_submission_time': 22992.67735028267, 'accumulated_eval_time': 954.5234022140503, 'accumulated_logging_time': 1.4333734512329102, 'global_step': 126228, 'preemption_count': 0}), (129034, {'train/accuracy': 0.897879421710968, 'train/loss': 0.3687564730644226, 'validation/accuracy': 0.7457000017166138, 'validation/loss': 1.0661872625350952, 'validation/num_examples': 50000, 'test/accuracy': 0.6183000206947327, 'test/loss': 1.7954069375991821, 'test/num_examples': 10000, 'score': 23502.58859872818, 'total_duration': 24481.063786029816, 'accumulated_submission_time': 23502.58859872818, 'accumulated_eval_time': 972.2902998924255, 'accumulated_logging_time': 1.4721760749816895, 'global_step': 129034, 'preemption_count': 0}), (131838, {'train/accuracy': 0.9066087007522583, 'train/loss': 0.3339364528656006, 'validation/accuracy': 0.7515199780464172, 'validation/loss': 1.0488859415054321, 'validation/num_examples': 50000, 'test/accuracy': 0.6264000535011292, 'test/loss': 1.7780516147613525, 'test/num_examples': 10000, 'score': 24012.539065122604, 'total_duration': 25008.755753993988, 'accumulated_submission_time': 24012.539065122604, 'accumulated_eval_time': 989.8918237686157, 'accumulated_logging_time': 1.5100431442260742, 'global_step': 131838, 'preemption_count': 0}), (134643, {'train/accuracy': 0.9126076102256775, 'train/loss': 0.31133192777633667, 'validation/accuracy': 0.752839982509613, 'validation/loss': 1.0438672304153442, 'validation/num_examples': 50000, 'test/accuracy': 0.6296000480651855, 'test/loss': 1.773249864578247, 'test/num_examples': 10000, 'score': 24522.4491918087, 'total_duration': 25536.370287418365, 'accumulated_submission_time': 24522.4491918087, 'accumulated_eval_time': 1007.456659078598, 'accumulated_logging_time': 1.547370195388794, 'global_step': 134643, 'preemption_count': 0}), (137447, {'train/accuracy': 0.9162946343421936, 'train/loss': 0.3012593686580658, 'validation/accuracy': 0.7541599869728088, 'validation/loss': 1.0395097732543945, 'validation/num_examples': 50000, 'test/accuracy': 0.629800021648407, 'test/loss': 1.7657780647277832, 'test/num_examples': 10000, 'score': 25032.304614782333, 'total_duration': 26063.80104780197, 'accumulated_submission_time': 25032.304614782333, 'accumulated_eval_time': 1024.8850872516632, 'accumulated_logging_time': 1.5916309356689453, 'global_step': 137447, 'preemption_count': 0}), (140252, {'train/accuracy': 0.9176498651504517, 'train/loss': 0.2996687889099121, 'validation/accuracy': 0.754040002822876, 'validation/loss': 1.0387669801712036, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.7635995149612427, 'test/num_examples': 10000, 'score': 25542.17391395569, 'total_duration': 26592.27976679802, 'accumulated_submission_time': 25542.17391395569, 'accumulated_eval_time': 1043.353123664856, 'accumulated_logging_time': 1.6281332969665527, 'global_step': 140252, 'preemption_count': 0}), (143059, {'train/accuracy': 0.9155173897743225, 'train/loss': 0.302499920129776, 'validation/accuracy': 0.7542200088500977, 'validation/loss': 1.0374367237091064, 'validation/num_examples': 50000, 'test/accuracy': 0.6308000087738037, 'test/loss': 1.7649126052856445, 'test/num_examples': 10000, 'score': 26052.18441581726, 'total_duration': 27120.14508652687, 'accumulated_submission_time': 26052.18441581726, 'accumulated_eval_time': 1061.0543432235718, 'accumulated_logging_time': 1.676928997039795, 'global_step': 143059, 'preemption_count': 0}), (145866, {'train/accuracy': 0.9177096486091614, 'train/loss': 0.2954210042953491, 'validation/accuracy': 0.7541599869728088, 'validation/loss': 1.0382142066955566, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.7643870115280151, 'test/num_examples': 10000, 'score': 26562.158358812332, 'total_duration': 27647.80983400345, 'accumulated_submission_time': 26562.158358812332, 'accumulated_eval_time': 1078.6033670902252, 'accumulated_logging_time': 1.7154245376586914, 'global_step': 145866, 'preemption_count': 0}), (148674, {'train/accuracy': 0.918387234210968, 'train/loss': 0.2957543432712555, 'validation/accuracy': 0.7542600035667419, 'validation/loss': 1.0386526584625244, 'validation/num_examples': 50000, 'test/accuracy': 0.6312000155448914, 'test/loss': 1.7660565376281738, 'test/num_examples': 10000, 'score': 27072.17339706421, 'total_duration': 28175.49212384224, 'accumulated_submission_time': 27072.17339706421, 'accumulated_eval_time': 1096.125738620758, 'accumulated_logging_time': 1.754462718963623, 'global_step': 148674, 'preemption_count': 0}), (151479, {'train/accuracy': 0.9175302982330322, 'train/loss': 0.2996428906917572, 'validation/accuracy': 0.7544599771499634, 'validation/loss': 1.0373268127441406, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.7634209394454956, 'test/num_examples': 10000, 'score': 27582.174550294876, 'total_duration': 28703.077377319336, 'accumulated_submission_time': 27582.174550294876, 'accumulated_eval_time': 1113.5708310604095, 'accumulated_logging_time': 1.7905094623565674, 'global_step': 151479, 'preemption_count': 0}), (154284, {'train/accuracy': 0.9184072017669678, 'train/loss': 0.2942565083503723, 'validation/accuracy': 0.7542799711227417, 'validation/loss': 1.038588285446167, 'validation/num_examples': 50000, 'test/accuracy': 0.6304000020027161, 'test/loss': 1.765087366104126, 'test/num_examples': 10000, 'score': 28092.13354229927, 'total_duration': 29230.695883512497, 'accumulated_submission_time': 28092.13354229927, 'accumulated_eval_time': 1131.0887157917023, 'accumulated_logging_time': 1.826988935470581, 'global_step': 154284, 'preemption_count': 0}), (157090, {'train/accuracy': 0.9148397445678711, 'train/loss': 0.3033747375011444, 'validation/accuracy': 0.7536999583244324, 'validation/loss': 1.0386899709701538, 'validation/num_examples': 50000, 'test/accuracy': 0.6302000284194946, 'test/loss': 1.7648581266403198, 'test/num_examples': 10000, 'score': 28602.121263980865, 'total_duration': 29758.442100286484, 'accumulated_submission_time': 28602.121263980865, 'accumulated_eval_time': 1148.6999378204346, 'accumulated_logging_time': 1.8656847476959229, 'global_step': 157090, 'preemption_count': 0}), (159895, {'train/accuracy': 0.9173309803009033, 'train/loss': 0.2961660921573639, 'validation/accuracy': 0.7539599537849426, 'validation/loss': 1.0384106636047363, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.7647697925567627, 'test/num_examples': 10000, 'score': 29112.043640851974, 'total_duration': 30286.075167179108, 'accumulated_submission_time': 29112.043640851974, 'accumulated_eval_time': 1166.2641248703003, 'accumulated_logging_time': 1.9065217971801758, 'global_step': 159895, 'preemption_count': 0}), (162701, {'train/accuracy': 0.9182676672935486, 'train/loss': 0.2924569845199585, 'validation/accuracy': 0.7541799545288086, 'validation/loss': 1.0382978916168213, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.7653626203536987, 'test/num_examples': 10000, 'score': 29621.970261335373, 'total_duration': 30813.574528455734, 'accumulated_submission_time': 29621.970261335373, 'accumulated_eval_time': 1183.6941940784454, 'accumulated_logging_time': 1.9452533721923828, 'global_step': 162701, 'preemption_count': 0}), (165507, {'train/accuracy': 0.9168327450752258, 'train/loss': 0.30003082752227783, 'validation/accuracy': 0.7541999816894531, 'validation/loss': 1.0381540060043335, 'validation/num_examples': 50000, 'test/accuracy': 0.6309000253677368, 'test/loss': 1.7633862495422363, 'test/num_examples': 10000, 'score': 30131.885346651077, 'total_duration': 31341.113740682602, 'accumulated_submission_time': 30131.885346651077, 'accumulated_eval_time': 1201.1669166088104, 'accumulated_logging_time': 1.9930555820465088, 'global_step': 165507, 'preemption_count': 0}), (168313, {'train/accuracy': 0.9159956574440002, 'train/loss': 0.30313050746917725, 'validation/accuracy': 0.7540199756622314, 'validation/loss': 1.0380730628967285, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.7652491331100464, 'test/num_examples': 10000, 'score': 30641.84900689125, 'total_duration': 31868.736838817596, 'accumulated_submission_time': 30641.84900689125, 'accumulated_eval_time': 1218.6816873550415, 'accumulated_logging_time': 2.032628297805786, 'global_step': 168313, 'preemption_count': 0}), (171120, {'train/accuracy': 0.9181481003761292, 'train/loss': 0.29753434658050537, 'validation/accuracy': 0.7540599703788757, 'validation/loss': 1.0391547679901123, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.7662850618362427, 'test/num_examples': 10000, 'score': 31151.722822904587, 'total_duration': 32396.311151981354, 'accumulated_submission_time': 31151.722822904587, 'accumulated_eval_time': 1236.238615512848, 'accumulated_logging_time': 2.071288585662842, 'global_step': 171120, 'preemption_count': 0}), (173925, {'train/accuracy': 0.91605544090271, 'train/loss': 0.29945990443229675, 'validation/accuracy': 0.7540199756622314, 'validation/loss': 1.0377227067947388, 'validation/num_examples': 50000, 'test/accuracy': 0.6310000419616699, 'test/loss': 1.7649096250534058, 'test/num_examples': 10000, 'score': 31661.63643503189, 'total_duration': 32923.807606220245, 'accumulated_submission_time': 31661.63643503189, 'accumulated_eval_time': 1253.6795370578766, 'accumulated_logging_time': 2.110353708267212, 'global_step': 173925, 'preemption_count': 0}), (176732, {'train/accuracy': 0.9172911047935486, 'train/loss': 0.3004980981349945, 'validation/accuracy': 0.7536999583244324, 'validation/loss': 1.0384106636047363, 'validation/num_examples': 50000, 'test/accuracy': 0.6315000057220459, 'test/loss': 1.7646987438201904, 'test/num_examples': 10000, 'score': 32171.66368174553, 'total_duration': 33451.56328821182, 'accumulated_submission_time': 32171.66368174553, 'accumulated_eval_time': 1271.26269698143, 'accumulated_logging_time': 2.1495463848114014, 'global_step': 176732, 'preemption_count': 0}), (179538, {'train/accuracy': 0.9173508882522583, 'train/loss': 0.3010532259941101, 'validation/accuracy': 0.75382000207901, 'validation/loss': 1.0383822917938232, 'validation/num_examples': 50000, 'test/accuracy': 0.6313000321388245, 'test/loss': 1.7664769887924194, 'test/num_examples': 10000, 'score': 32681.64035630226, 'total_duration': 33979.20403385162, 'accumulated_submission_time': 32681.64035630226, 'accumulated_eval_time': 1288.781033039093, 'accumulated_logging_time': 2.189389228820801, 'global_step': 179538, 'preemption_count': 0}), (182342, {'train/accuracy': 0.9167330861091614, 'train/loss': 0.3020401895046234, 'validation/accuracy': 0.754319965839386, 'validation/loss': 1.03815758228302, 'validation/num_examples': 50000, 'test/accuracy': 0.6323000192642212, 'test/loss': 1.7642401456832886, 'test/num_examples': 10000, 'score': 33191.615904569626, 'total_duration': 34506.703023433685, 'accumulated_submission_time': 33191.615904569626, 'accumulated_eval_time': 1306.1609332561493, 'accumulated_logging_time': 2.2296924591064453, 'global_step': 182342, 'preemption_count': 0}), (185149, {'train/accuracy': 0.915437638759613, 'train/loss': 0.3011271059513092, 'validation/accuracy': 0.7538999915122986, 'validation/loss': 1.037919282913208, 'validation/num_examples': 50000, 'test/accuracy': 0.6307000517845154, 'test/loss': 1.7637271881103516, 'test/num_examples': 10000, 'score': 33701.57856965065, 'total_duration': 35034.17359876633, 'accumulated_submission_time': 33701.57856965065, 'accumulated_eval_time': 1323.525725364685, 'accumulated_logging_time': 2.268526554107666, 'global_step': 185149, 'preemption_count': 0}), (186666, {'train/accuracy': 0.9185666441917419, 'train/loss': 0.29453206062316895, 'validation/accuracy': 0.7536999583244324, 'validation/loss': 1.0383217334747314, 'validation/num_examples': 50000, 'test/accuracy': 0.6305000185966492, 'test/loss': 1.7669010162353516, 'test/num_examples': 10000, 'score': 33977.34105300903, 'total_duration': 35327.618497133255, 'accumulated_submission_time': 33977.34105300903, 'accumulated_eval_time': 1341.1111905574799, 'accumulated_logging_time': 2.3088231086730957, 'global_step': 186666, 'preemption_count': 0})], 'global_step': 186666}
I0316 10:28:16.215385 139725430036288 submission_runner.py:596] Timing: 33977.34105300903
I0316 10:28:16.215443 139725430036288 submission_runner.py:598] Total number of evals: 68
I0316 10:28:16.215490 139725430036288 submission_runner.py:599] ====================
I0316 10:28:16.215724 139725430036288 submission_runner.py:683] Final imagenet_resnet_silu score: 0
