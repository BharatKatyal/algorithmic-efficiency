python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech_norm_and_spec_aug --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --rng_seed=1094522399 --max_global_steps=48900 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab --tuning_ruleset=external --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_deepspeech_norm_and_spec_aug/tuning_search_space.json --num_tuning_trials=1 2>&1 | tee -a /logs/librispeech_deepspeech_norm_and_spec_aug_jax_03-12-2024-01-06-52.log
I0312 01:07:14.519505 140011093206848 logger_utils.py:61] Removing existing experiment directory /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_norm_and_spec_aug_jax because --overwrite was set.
I0312 01:07:14.521857 140011093206848 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_norm_and_spec_aug_jax.
I0312 01:07:15.549226 140011093206848 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0312 01:07:15.549972 140011093206848 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0312 01:07:15.550114 140011093206848 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0312 01:07:15.555274 140011093206848 submission_runner.py:547] Using RNG seed 1094522399
I0312 01:07:16.805504 140011093206848 submission_runner.py:556] --- Tuning run 1/1 ---
I0312 01:07:16.805729 140011093206848 submission_runner.py:561] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_norm_and_spec_aug_jax/trial_1.
I0312 01:07:16.805926 140011093206848 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_norm_and_spec_aug_jax/trial_1/hparams.json.
I0312 01:07:16.988411 140011093206848 submission_runner.py:206] Initializing dataset.
I0312 01:07:16.988645 140011093206848 submission_runner.py:213] Initializing model.
I0312 01:07:19.495752 140011093206848 submission_runner.py:255] Initializing optimizer.
I0312 01:07:20.197288 140011093206848 submission_runner.py:262] Initializing metrics bundle.
I0312 01:07:20.197511 140011093206848 submission_runner.py:280] Initializing checkpoint and logger.
I0312 01:07:20.198589 140011093206848 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_norm_and_spec_aug_jax/trial_1 with prefix checkpoint_
I0312 01:07:20.198729 140011093206848 submission_runner.py:300] Saving meta data to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_norm_and_spec_aug_jax/trial_1/meta_data_0.json.
I0312 01:07:20.198950 140011093206848 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0312 01:07:20.199012 140011093206848 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0312 01:07:20.464109 140011093206848 logger_utils.py:220] Unable to record git information. Continuing without it.
I0312 01:07:20.705085 140011093206848 submission_runner.py:304] Saving flags to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_norm_and_spec_aug_jax/trial_1/flags_0.json.
I0312 01:07:20.718461 140011093206848 submission_runner.py:314] Starting training loop.
I0312 01:07:21.009238 140011093206848 input_pipeline.py:20] Loading split = train-clean-100
I0312 01:07:21.045725 140011093206848 input_pipeline.py:20] Loading split = train-clean-360
I0312 01:07:21.172598 140011093206848 input_pipeline.py:20] Loading split = train-other-500
I0312 01:08:00.887606 139848052631296 logging_writer.py:48] [0] global_step=0, grad_norm=2291.079833984375, loss=30.452552795410156
I0312 01:08:00.918111 140011093206848 spec.py:321] Evaluating on the training split.
I0312 01:08:00.918289 140011093206848 input_pipeline.py:20] Loading split = train-clean-100
I0312 01:08:00.953618 140011093206848 input_pipeline.py:20] Loading split = train-clean-360
I0312 01:08:01.314821 140011093206848 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0312 01:09:37.615036 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 01:09:37.615565 140011093206848 input_pipeline.py:20] Loading split = dev-clean
I0312 01:09:37.622270 140011093206848 input_pipeline.py:20] Loading split = dev-other
I0312 01:10:41.821400 140011093206848 spec.py:349] Evaluating on the test split.
I0312 01:10:41.821884 140011093206848 input_pipeline.py:20] Loading split = test-clean
I0312 01:11:15.824180 140011093206848 submission_runner.py:413] Time since start: 235.10s, 	Step: 1, 	{'train/ctc_loss': Array(30.565628, dtype=float32), 'train/wer': 2.764732523998917, 'validation/ctc_loss': Array(30.097767, dtype=float32), 'validation/wer': 2.601166282089653, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.133963, dtype=float32), 'test/wer': 2.7303840919708326, 'test/num_examples': 2472, 'score': 40.19957900047302, 'total_duration': 235.10354018211365, 'accumulated_submission_time': 40.19957900047302, 'accumulated_eval_time': 194.90390253067017, 'accumulated_logging_time': 0}
I0312 01:11:15.850260 139842348377856 logging_writer.py:48] [1] accumulated_eval_time=194.903903, accumulated_logging_time=0, accumulated_submission_time=40.199579, global_step=1, preemption_count=0, score=40.199579, test/ctc_loss=30.133962631225586, test/num_examples=2472, test/wer=2.730384, total_duration=235.103540, train/ctc_loss=30.565628051757812, train/wer=2.764733, validation/ctc_loss=30.097766876220703, validation/num_examples=5348, validation/wer=2.601166
I0312 01:11:23.203036 139855356368640 logging_writer.py:48] [1] global_step=1, grad_norm=2220.368408203125, loss=31.568788528442383
I0312 01:11:24.084411 139855364761344 logging_writer.py:48] [2] global_step=2, grad_norm=3596.19384765625, loss=31.50653648376465
I0312 01:11:24.970191 139855356368640 logging_writer.py:48] [3] global_step=3, grad_norm=3761.2236328125, loss=31.155080795288086
I0312 01:11:25.853766 139855364761344 logging_writer.py:48] [4] global_step=4, grad_norm=4640.1845703125, loss=31.283138275146484
I0312 01:11:26.749241 139855356368640 logging_writer.py:48] [5] global_step=5, grad_norm=5417.578125, loss=31.056795120239258
I0312 01:11:27.643002 139855364761344 logging_writer.py:48] [6] global_step=6, grad_norm=6175.82861328125, loss=31.018470764160156
I0312 01:11:28.534982 139855356368640 logging_writer.py:48] [7] global_step=7, grad_norm=6804.69140625, loss=30.142805099487305
I0312 01:11:29.426972 139855364761344 logging_writer.py:48] [8] global_step=8, grad_norm=8314.5048828125, loss=29.959556579589844
I0312 01:11:30.323650 139855356368640 logging_writer.py:48] [9] global_step=9, grad_norm=9786.09765625, loss=28.826290130615234
I0312 01:11:31.216636 139855364761344 logging_writer.py:48] [10] global_step=10, grad_norm=9497.9384765625, loss=28.753528594970703
I0312 01:11:32.106392 139855356368640 logging_writer.py:48] [11] global_step=11, grad_norm=10120.3427734375, loss=27.08137321472168
I0312 01:11:32.997532 139855364761344 logging_writer.py:48] [12] global_step=12, grad_norm=9274.2314453125, loss=26.755889892578125
I0312 01:11:33.876621 139855356368640 logging_writer.py:48] [13] global_step=13, grad_norm=8824.564453125, loss=25.630224227905273
I0312 01:11:34.757254 139855364761344 logging_writer.py:48] [14] global_step=14, grad_norm=8018.16357421875, loss=24.335628509521484
I0312 01:11:35.651038 139855356368640 logging_writer.py:48] [15] global_step=15, grad_norm=7369.369140625, loss=24.144716262817383
I0312 01:11:36.542883 139855364761344 logging_writer.py:48] [16] global_step=16, grad_norm=6816.04248046875, loss=22.177156448364258
I0312 01:11:37.426111 139855356368640 logging_writer.py:48] [17] global_step=17, grad_norm=6343.13720703125, loss=20.866926193237305
I0312 01:11:38.305503 139855364761344 logging_writer.py:48] [18] global_step=18, grad_norm=5567.5361328125, loss=19.848569869995117
I0312 01:11:39.195402 139855356368640 logging_writer.py:48] [19] global_step=19, grad_norm=4747.91259765625, loss=18.0401611328125
I0312 01:11:40.087188 139855364761344 logging_writer.py:48] [20] global_step=20, grad_norm=3914.188232421875, loss=17.0222225189209
I0312 01:11:40.983416 139855356368640 logging_writer.py:48] [21] global_step=21, grad_norm=3254.32958984375, loss=15.917281150817871
I0312 01:11:41.873421 139855364761344 logging_writer.py:48] [22] global_step=22, grad_norm=2651.15673828125, loss=14.682517051696777
I0312 01:11:42.739334 139855356368640 logging_writer.py:48] [23] global_step=23, grad_norm=2188.952392578125, loss=13.587316513061523
I0312 01:11:43.626978 139855364761344 logging_writer.py:48] [24] global_step=24, grad_norm=1704.8341064453125, loss=12.298206329345703
I0312 01:11:44.527706 139855356368640 logging_writer.py:48] [25] global_step=25, grad_norm=1335.787353515625, loss=11.366950988769531
I0312 01:11:45.420969 139855364761344 logging_writer.py:48] [26] global_step=26, grad_norm=935.8896484375, loss=10.271340370178223
I0312 01:11:46.301894 139855356368640 logging_writer.py:48] [27] global_step=27, grad_norm=706.8723754882812, loss=9.784004211425781
I0312 01:11:47.202324 139855364761344 logging_writer.py:48] [28] global_step=28, grad_norm=519.8101806640625, loss=9.165452003479004
I0312 01:11:48.093626 139855356368640 logging_writer.py:48] [29] global_step=29, grad_norm=367.4482421875, loss=8.83636474609375
I0312 01:11:48.993795 139855364761344 logging_writer.py:48] [30] global_step=30, grad_norm=233.06881713867188, loss=8.417301177978516
I0312 01:11:49.862685 139855356368640 logging_writer.py:48] [31] global_step=31, grad_norm=114.81642150878906, loss=8.121188163757324
I0312 01:11:50.748169 139855364761344 logging_writer.py:48] [32] global_step=32, grad_norm=85.38098907470703, loss=8.15042495727539
I0312 01:11:51.618010 139855356368640 logging_writer.py:48] [33] global_step=33, grad_norm=58.45090866088867, loss=8.04887866973877
I0312 01:11:52.512791 139855364761344 logging_writer.py:48] [34] global_step=34, grad_norm=92.42431640625, loss=8.088890075683594
I0312 01:11:53.400876 139855356368640 logging_writer.py:48] [35] global_step=35, grad_norm=107.8631362915039, loss=8.134941101074219
I0312 01:11:54.295707 139855364761344 logging_writer.py:48] [36] global_step=36, grad_norm=133.0707244873047, loss=8.20626163482666
I0312 01:11:55.185727 139855356368640 logging_writer.py:48] [37] global_step=37, grad_norm=163.6722412109375, loss=8.347729682922363
I0312 01:11:56.078024 139855364761344 logging_writer.py:48] [38] global_step=38, grad_norm=176.42518615722656, loss=8.41527271270752
I0312 01:11:56.969324 139855356368640 logging_writer.py:48] [39] global_step=39, grad_norm=190.43455505371094, loss=8.506224632263184
I0312 01:11:57.860750 139855364761344 logging_writer.py:48] [40] global_step=40, grad_norm=197.7443389892578, loss=8.589699745178223
I0312 01:11:58.734020 139855356368640 logging_writer.py:48] [41] global_step=41, grad_norm=211.1900177001953, loss=8.676491737365723
I0312 01:11:59.618090 139855364761344 logging_writer.py:48] [42] global_step=42, grad_norm=217.19435119628906, loss=8.748046875
I0312 01:12:00.497253 139855356368640 logging_writer.py:48] [43] global_step=43, grad_norm=225.74114990234375, loss=8.801900863647461
I0312 01:12:01.391073 139855364761344 logging_writer.py:48] [44] global_step=44, grad_norm=229.60324096679688, loss=8.886221885681152
I0312 01:12:02.286855 139855356368640 logging_writer.py:48] [45] global_step=45, grad_norm=231.25006103515625, loss=8.828001976013184
I0312 01:12:03.175405 139855364761344 logging_writer.py:48] [46] global_step=46, grad_norm=230.9424285888672, loss=8.83200740814209
I0312 01:12:04.044153 139855356368640 logging_writer.py:48] [47] global_step=47, grad_norm=229.095703125, loss=8.8109769821167
I0312 01:12:04.931948 139855364761344 logging_writer.py:48] [48] global_step=48, grad_norm=223.5222930908203, loss=8.693290710449219
I0312 01:12:05.819997 139855356368640 logging_writer.py:48] [49] global_step=49, grad_norm=226.93490600585938, loss=8.669293403625488
I0312 01:12:06.714647 139855364761344 logging_writer.py:48] [50] global_step=50, grad_norm=227.09326171875, loss=8.576756477355957
I0312 01:12:07.606590 139855356368640 logging_writer.py:48] [51] global_step=51, grad_norm=219.669921875, loss=8.473821640014648
I0312 01:12:08.499194 139855364761344 logging_writer.py:48] [52] global_step=52, grad_norm=198.34849548339844, loss=8.30565071105957
I0312 01:12:09.380621 139855356368640 logging_writer.py:48] [53] global_step=53, grad_norm=192.99493408203125, loss=8.194125175476074
I0312 01:12:10.281760 139855364761344 logging_writer.py:48] [54] global_step=54, grad_norm=180.3973846435547, loss=8.051219940185547
I0312 01:12:11.162262 139855356368640 logging_writer.py:48] [55] global_step=55, grad_norm=158.63140869140625, loss=7.9295735359191895
I0312 01:12:12.061323 139855364761344 logging_writer.py:48] [56] global_step=56, grad_norm=128.85386657714844, loss=7.827465534210205
I0312 01:12:12.939435 139855356368640 logging_writer.py:48] [57] global_step=57, grad_norm=83.2701644897461, loss=7.720008373260498
I0312 01:12:13.814189 139855364761344 logging_writer.py:48] [58] global_step=58, grad_norm=46.15327072143555, loss=7.688928127288818
I0312 01:12:14.707400 139855356368640 logging_writer.py:48] [59] global_step=59, grad_norm=54.252201080322266, loss=7.67735481262207
I0312 01:12:15.580406 139855364761344 logging_writer.py:48] [60] global_step=60, grad_norm=115.7226333618164, loss=7.709682464599609
I0312 01:12:16.459800 139855356368640 logging_writer.py:48] [61] global_step=61, grad_norm=193.23580932617188, loss=7.744588851928711
I0312 01:12:17.359635 139855364761344 logging_writer.py:48] [62] global_step=62, grad_norm=264.78411865234375, loss=7.817220211029053
I0312 01:12:18.245557 139855356368640 logging_writer.py:48] [63] global_step=63, grad_norm=306.0885314941406, loss=7.882815837860107
I0312 01:12:19.144527 139855364761344 logging_writer.py:48] [64] global_step=64, grad_norm=346.10430908203125, loss=7.947061061859131
I0312 01:12:20.031781 139855356368640 logging_writer.py:48] [65] global_step=65, grad_norm=305.8934326171875, loss=7.877760410308838
I0312 01:12:20.920267 139855364761344 logging_writer.py:48] [66] global_step=66, grad_norm=273.217041015625, loss=7.8031721115112305
I0312 01:12:21.799685 139855356368640 logging_writer.py:48] [67] global_step=67, grad_norm=252.8291778564453, loss=7.761329174041748
I0312 01:12:22.694829 139855364761344 logging_writer.py:48] [68] global_step=68, grad_norm=198.01931762695312, loss=7.647805690765381
I0312 01:12:23.573406 139855356368640 logging_writer.py:48] [69] global_step=69, grad_norm=154.5857391357422, loss=7.570034980773926
I0312 01:12:24.454353 139855364761344 logging_writer.py:48] [70] global_step=70, grad_norm=116.64684295654297, loss=7.52531623840332
I0312 01:12:25.338681 139855356368640 logging_writer.py:48] [71] global_step=71, grad_norm=79.745361328125, loss=7.424187660217285
I0312 01:12:26.220899 139855364761344 logging_writer.py:48] [72] global_step=72, grad_norm=34.73708724975586, loss=7.35872745513916
I0312 01:12:27.112738 139855356368640 logging_writer.py:48] [73] global_step=73, grad_norm=23.283369064331055, loss=7.354251384735107
I0312 01:12:27.996407 139855364761344 logging_writer.py:48] [74] global_step=74, grad_norm=38.39105987548828, loss=7.338114261627197
I0312 01:12:28.884736 139855356368640 logging_writer.py:48] [75] global_step=75, grad_norm=49.28579330444336, loss=7.333502292633057
I0312 01:12:29.776649 139855364761344 logging_writer.py:48] [76] global_step=76, grad_norm=63.7392692565918, loss=7.323805809020996
I0312 01:12:30.657754 139855356368640 logging_writer.py:48] [77] global_step=77, grad_norm=70.49838256835938, loss=7.332714557647705
I0312 01:12:31.546010 139855364761344 logging_writer.py:48] [78] global_step=78, grad_norm=75.51742553710938, loss=7.3011016845703125
I0312 01:12:32.434443 139855356368640 logging_writer.py:48] [79] global_step=79, grad_norm=78.21147918701172, loss=7.3051958084106445
I0312 01:12:33.330365 139855364761344 logging_writer.py:48] [80] global_step=80, grad_norm=82.91959381103516, loss=7.285182476043701
I0312 01:12:34.219751 139855356368640 logging_writer.py:48] [81] global_step=81, grad_norm=80.59349060058594, loss=7.269814968109131
I0312 01:12:35.120554 139855364761344 logging_writer.py:48] [82] global_step=82, grad_norm=77.24359130859375, loss=7.234210014343262
I0312 01:12:35.990724 139855356368640 logging_writer.py:48] [83] global_step=83, grad_norm=79.07743835449219, loss=7.203665256500244
I0312 01:12:36.882845 139855364761344 logging_writer.py:48] [84] global_step=84, grad_norm=69.74005889892578, loss=7.164090156555176
I0312 01:12:37.775450 139855356368640 logging_writer.py:48] [85] global_step=85, grad_norm=66.9224624633789, loss=7.131525039672852
I0312 01:12:38.669551 139855364761344 logging_writer.py:48] [86] global_step=86, grad_norm=55.10191345214844, loss=7.099344253540039
I0312 01:12:39.564686 139855356368640 logging_writer.py:48] [87] global_step=87, grad_norm=46.30613708496094, loss=7.0378241539001465
I0312 01:12:40.446753 139855364761344 logging_writer.py:48] [88] global_step=88, grad_norm=27.063568115234375, loss=7.000737190246582
I0312 01:12:41.329740 139855356368640 logging_writer.py:48] [89] global_step=89, grad_norm=14.643708229064941, loss=6.982892990112305
I0312 01:12:42.222444 139855364761344 logging_writer.py:48] [90] global_step=90, grad_norm=14.780733108520508, loss=6.962896823883057
I0312 01:12:43.119322 139855356368640 logging_writer.py:48] [91] global_step=91, grad_norm=25.6531982421875, loss=6.943745136260986
I0312 01:12:44.002253 139855364761344 logging_writer.py:48] [92] global_step=92, grad_norm=45.01055145263672, loss=6.9503068923950195
I0312 01:12:44.881634 139855356368640 logging_writer.py:48] [93] global_step=93, grad_norm=47.858909606933594, loss=6.912580966949463
I0312 01:12:45.773628 139855364761344 logging_writer.py:48] [94] global_step=94, grad_norm=54.295196533203125, loss=6.936470985412598
I0312 01:12:46.664216 139855356368640 logging_writer.py:48] [95] global_step=95, grad_norm=63.27363967895508, loss=6.918976783752441
I0312 01:12:47.551944 139855364761344 logging_writer.py:48] [96] global_step=96, grad_norm=63.60580825805664, loss=6.883470058441162
I0312 01:12:48.436337 139855356368640 logging_writer.py:48] [97] global_step=97, grad_norm=59.20986557006836, loss=6.869670391082764
I0312 01:12:49.329298 139855364761344 logging_writer.py:48] [98] global_step=98, grad_norm=41.47767639160156, loss=6.804433345794678
I0312 01:12:50.218780 139855356368640 logging_writer.py:48] [99] global_step=99, grad_norm=32.19724655151367, loss=6.788832187652588
I0312 01:12:51.104581 139855364761344 logging_writer.py:48] [100] global_step=100, grad_norm=21.919588088989258, loss=6.7542853355407715
I0312 01:17:57.485868 139855356368640 logging_writer.py:48] [500] global_step=500, grad_norm=1.3862879276275635, loss=5.79779052734375
I0312 01:24:49.935966 139855364761344 logging_writer.py:48] [1000] global_step=1000, grad_norm=6.088079452514648, loss=5.604693412780762
I0312 01:31:16.315881 139855389939456 logging_writer.py:48] [1500] global_step=1500, grad_norm=10.134567260742188, loss=5.372481822967529
I0312 01:35:15.997165 140011093206848 spec.py:321] Evaluating on the training split.
I0312 01:36:04.287038 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 01:36:50.289388 140011093206848 spec.py:349] Evaluating on the test split.
I0312 01:37:12.412132 140011093206848 submission_runner.py:413] Time since start: 1791.69s, 	Step: 1815, 	{'train/ctc_loss': Array(3.6631207, dtype=float32), 'train/wer': 0.7533315482688308, 'validation/ctc_loss': Array(3.8746686, dtype=float32), 'validation/wer': 0.7599563609681687, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.6510215, dtype=float32), 'test/wer': 0.7332074015396177, 'test/num_examples': 2472, 'score': 1480.2582232952118, 'total_duration': 1791.6881017684937, 'accumulated_submission_time': 1480.2582232952118, 'accumulated_eval_time': 311.3134095668793, 'accumulated_logging_time': 0.040225982666015625}
I0312 01:37:12.447701 139855389939456 logging_writer.py:48] [1815] accumulated_eval_time=311.313410, accumulated_logging_time=0.040226, accumulated_submission_time=1480.258223, global_step=1815, preemption_count=0, score=1480.258223, test/ctc_loss=3.6510214805603027, test/num_examples=2472, test/wer=0.733207, total_duration=1791.688102, train/ctc_loss=3.663120746612549, train/wer=0.753332, validation/ctc_loss=3.874668598175049, validation/num_examples=5348, validation/wer=0.759956
I0312 01:39:33.709246 139855381546752 logging_writer.py:48] [2000] global_step=2000, grad_norm=3.495781421661377, loss=4.335855484008789
I0312 01:45:56.911966 139855389939456 logging_writer.py:48] [2500] global_step=2500, grad_norm=4.249317169189453, loss=3.7560999393463135
I0312 01:52:59.177286 139855381546752 logging_writer.py:48] [3000] global_step=3000, grad_norm=4.605288028717041, loss=3.3791298866271973
I0312 01:59:28.771747 139855389939456 logging_writer.py:48] [3500] global_step=3500, grad_norm=3.3248071670532227, loss=3.2383787631988525
I0312 02:01:12.452066 140011093206848 spec.py:321] Evaluating on the training split.
I0312 02:02:07.593812 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 02:02:56.258788 140011093206848 spec.py:349] Evaluating on the test split.
I0312 02:03:19.993246 140011093206848 submission_runner.py:413] Time since start: 3359.27s, 	Step: 3639, 	{'train/ctc_loss': Array(1.2549866, dtype=float32), 'train/wer': 0.3648616253362811, 'validation/ctc_loss': Array(1.5888001, dtype=float32), 'validation/wer': 0.4131322590922695, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.2625551, dtype=float32), 'test/wer': 0.3578900331078748, 'test/num_examples': 2472, 'score': 2920.1678557395935, 'total_duration': 3359.268629550934, 'accumulated_submission_time': 2920.1678557395935, 'accumulated_eval_time': 438.84852933883667, 'accumulated_logging_time': 0.09029245376586914}
I0312 02:03:20.033501 139855389939456 logging_writer.py:48] [3639] accumulated_eval_time=438.848529, accumulated_logging_time=0.090292, accumulated_submission_time=2920.167856, global_step=3639, preemption_count=0, score=2920.167856, test/ctc_loss=1.2625551223754883, test/num_examples=2472, test/wer=0.357890, total_duration=3359.268630, train/ctc_loss=1.2549866437911987, train/wer=0.364862, validation/ctc_loss=1.588800072669983, validation/num_examples=5348, validation/wer=0.413132
I0312 02:08:05.104387 139855381546752 logging_writer.py:48] [4000] global_step=4000, grad_norm=3.3392512798309326, loss=3.038261651992798
I0312 02:14:38.012691 139855389939456 logging_writer.py:48] [4500] global_step=4500, grad_norm=5.407558441162109, loss=2.9117465019226074
I0312 02:21:31.203483 139855381546752 logging_writer.py:48] [5000] global_step=5000, grad_norm=3.368142604827881, loss=2.9306225776672363
I0312 02:27:20.347943 140011093206848 spec.py:321] Evaluating on the training split.
I0312 02:28:16.098216 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 02:29:04.752462 140011093206848 spec.py:349] Evaluating on the test split.
I0312 02:29:28.753572 140011093206848 submission_runner.py:413] Time since start: 4928.03s, 	Step: 5436, 	{'train/ctc_loss': Array(0.8079109, dtype=float32), 'train/wer': 0.2565533891023715, 'validation/ctc_loss': Array(1.1846188, dtype=float32), 'validation/wer': 0.3250625138785638, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.85878384, dtype=float32), 'test/wer': 0.2599882192838137, 'test/num_examples': 2472, 'score': 4360.387193918228, 'total_duration': 4928.032552957535, 'accumulated_submission_time': 4360.387193918228, 'accumulated_eval_time': 567.2518880367279, 'accumulated_logging_time': 0.14759349822998047}
I0312 02:29:28.779439 139855389939456 logging_writer.py:48] [5436] accumulated_eval_time=567.251888, accumulated_logging_time=0.147593, accumulated_submission_time=4360.387194, global_step=5436, preemption_count=0, score=4360.387194, test/ctc_loss=0.8587838411331177, test/num_examples=2472, test/wer=0.259988, total_duration=4928.032553, train/ctc_loss=0.8079109191894531, train/wer=0.256553, validation/ctc_loss=1.1846188306808472, validation/num_examples=5348, validation/wer=0.325063
I0312 02:30:17.698793 139855381546752 logging_writer.py:48] [5500] global_step=5500, grad_norm=2.10611629486084, loss=2.7295539379119873
I0312 02:36:52.649526 139855389939456 logging_writer.py:48] [6000] global_step=6000, grad_norm=3.5014824867248535, loss=2.7220842838287354
I0312 02:43:30.592960 139855389939456 logging_writer.py:48] [6500] global_step=6500, grad_norm=3.84240984916687, loss=2.797316551208496
I0312 02:50:14.920847 139855381546752 logging_writer.py:48] [7000] global_step=7000, grad_norm=3.337325096130371, loss=2.6628828048706055
I0312 02:53:29.502359 140011093206848 spec.py:321] Evaluating on the training split.
I0312 02:54:25.265089 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 02:55:13.584408 140011093206848 spec.py:349] Evaluating on the test split.
I0312 02:55:37.154075 140011093206848 submission_runner.py:413] Time since start: 6496.43s, 	Step: 7229, 	{'train/ctc_loss': Array(0.6611109, dtype=float32), 'train/wer': 0.21087245651331568, 'validation/ctc_loss': Array(0.99669725, dtype=float32), 'validation/wer': 0.2786912152311807, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6835765, dtype=float32), 'test/wer': 0.21170759449962423, 'test/num_examples': 2472, 'score': 5801.019718408585, 'total_duration': 6496.429447650909, 'accumulated_submission_time': 5801.019718408585, 'accumulated_eval_time': 694.8976786136627, 'accumulated_logging_time': 0.18665361404418945}
I0312 02:55:37.189118 139855389939456 logging_writer.py:48] [7229] accumulated_eval_time=694.897679, accumulated_logging_time=0.186654, accumulated_submission_time=5801.019718, global_step=7229, preemption_count=0, score=5801.019718, test/ctc_loss=0.6835765242576599, test/num_examples=2472, test/wer=0.211708, total_duration=6496.429448, train/ctc_loss=0.6611108779907227, train/wer=0.210872, validation/ctc_loss=0.9966972470283508, validation/num_examples=5348, validation/wer=0.278691
I0312 02:59:02.388863 139855381546752 logging_writer.py:48] [7500] global_step=7500, grad_norm=3.5615336894989014, loss=2.604078531265259
I0312 03:05:36.139338 139855389939456 logging_writer.py:48] [8000] global_step=8000, grad_norm=4.379870891571045, loss=2.4885125160217285
I0312 03:12:24.421926 139855389939456 logging_writer.py:48] [8500] global_step=8500, grad_norm=3.0065255165100098, loss=2.60414457321167
I0312 03:19:15.273596 139855381546752 logging_writer.py:48] [9000] global_step=9000, grad_norm=2.482851505279541, loss=2.5355782508850098
I0312 03:19:37.155300 140011093206848 spec.py:321] Evaluating on the training split.
I0312 03:20:31.911050 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 03:21:20.314069 140011093206848 spec.py:349] Evaluating on the test split.
I0312 03:21:44.201374 140011093206848 submission_runner.py:413] Time since start: 8063.48s, 	Step: 9026, 	{'train/ctc_loss': Array(0.5731816, dtype=float32), 'train/wer': 0.18660984148383308, 'validation/ctc_loss': Array(0.8929109, dtype=float32), 'validation/wer': 0.25249814147928595, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5971978, dtype=float32), 'test/wer': 0.1849572441248756, 'test/num_examples': 2472, 'score': 7240.888499736786, 'total_duration': 8063.478012561798, 'accumulated_submission_time': 7240.888499736786, 'accumulated_eval_time': 821.9389078617096, 'accumulated_logging_time': 0.23938465118408203}
I0312 03:21:44.236131 139855389939456 logging_writer.py:48] [9026] accumulated_eval_time=821.938908, accumulated_logging_time=0.239385, accumulated_submission_time=7240.888500, global_step=9026, preemption_count=0, score=7240.888500, test/ctc_loss=0.5971977710723877, test/num_examples=2472, test/wer=0.184957, total_duration=8063.478013, train/ctc_loss=0.5731816291809082, train/wer=0.186610, validation/ctc_loss=0.892910897731781, validation/num_examples=5348, validation/wer=0.252498
I0312 03:27:45.016520 139855389939456 logging_writer.py:48] [9500] global_step=9500, grad_norm=2.4485695362091064, loss=2.5020034313201904
I0312 03:34:30.049204 139855381546752 logging_writer.py:48] [10000] global_step=10000, grad_norm=4.137574672698975, loss=2.565241813659668
I0312 03:41:28.321497 139855389939456 logging_writer.py:48] [10500] global_step=10500, grad_norm=3.762122631072998, loss=2.4195728302001953
I0312 03:45:44.986343 140011093206848 spec.py:321] Evaluating on the training split.
I0312 03:46:40.118144 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 03:47:29.214435 140011093206848 spec.py:349] Evaluating on the test split.
I0312 03:47:53.188909 140011093206848 submission_runner.py:413] Time since start: 9632.46s, 	Step: 10837, 	{'train/ctc_loss': Array(0.54191834, dtype=float32), 'train/wer': 0.17545385193997365, 'validation/ctc_loss': Array(0.82358056, dtype=float32), 'validation/wer': 0.2340384448284851, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5422325, dtype=float32), 'test/wer': 0.16972355940121464, 'test/num_examples': 2472, 'score': 8681.543917417526, 'total_duration': 9632.462462186813, 'accumulated_submission_time': 8681.543917417526, 'accumulated_eval_time': 950.1336026191711, 'accumulated_logging_time': 0.29027676582336426}
I0312 03:47:53.227333 139855389939456 logging_writer.py:48] [10837] accumulated_eval_time=950.133603, accumulated_logging_time=0.290277, accumulated_submission_time=8681.543917, global_step=10837, preemption_count=0, score=8681.543917, test/ctc_loss=0.5422325134277344, test/num_examples=2472, test/wer=0.169724, total_duration=9632.462462, train/ctc_loss=0.5419183373451233, train/wer=0.175454, validation/ctc_loss=0.8235805630683899, validation/num_examples=5348, validation/wer=0.234038
I0312 03:49:56.751404 139855381546752 logging_writer.py:48] [11000] global_step=11000, grad_norm=4.362105846405029, loss=2.477689504623413
I0312 03:56:47.579047 139855389939456 logging_writer.py:48] [11500] global_step=11500, grad_norm=3.678243398666382, loss=2.3950772285461426
I0312 04:03:25.086310 139855381546752 logging_writer.py:48] [12000] global_step=12000, grad_norm=4.8738112449646, loss=2.4123528003692627
I0312 04:10:28.456583 139855389939456 logging_writer.py:48] [12500] global_step=12500, grad_norm=3.4207024574279785, loss=2.343440055847168
I0312 04:11:53.307833 140011093206848 spec.py:321] Evaluating on the training split.
I0312 04:12:49.020114 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 04:13:38.333797 140011093206848 spec.py:349] Evaluating on the test split.
I0312 04:14:01.897592 140011093206848 submission_runner.py:413] Time since start: 11201.17s, 	Step: 12614, 	{'train/ctc_loss': Array(0.4917271, dtype=float32), 'train/wer': 0.1593974007189501, 'validation/ctc_loss': Array(0.7904846, dtype=float32), 'validation/wer': 0.22684572829875357, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.51778907, dtype=float32), 'test/wer': 0.16328478865801394, 'test/num_examples': 2472, 'score': 10121.531110286713, 'total_duration': 11201.173258543015, 'accumulated_submission_time': 10121.531110286713, 'accumulated_eval_time': 1078.7175657749176, 'accumulated_logging_time': 0.3447730541229248}
I0312 04:14:01.926973 139855389939456 logging_writer.py:48] [12614] accumulated_eval_time=1078.717566, accumulated_logging_time=0.344773, accumulated_submission_time=10121.531110, global_step=12614, preemption_count=0, score=10121.531110, test/ctc_loss=0.5177890658378601, test/num_examples=2472, test/wer=0.163285, total_duration=11201.173259, train/ctc_loss=0.4917271137237549, train/wer=0.159397, validation/ctc_loss=0.790484607219696, validation/num_examples=5348, validation/wer=0.226846
I0312 04:18:57.122266 139855381546752 logging_writer.py:48] [13000] global_step=13000, grad_norm=3.423502206802368, loss=2.426424503326416
I0312 04:26:01.247220 139855389939456 logging_writer.py:48] [13500] global_step=13500, grad_norm=3.485222101211548, loss=2.3115861415863037
I0312 04:32:27.986225 139855381546752 logging_writer.py:48] [14000] global_step=14000, grad_norm=5.438370227813721, loss=2.2866482734680176
I0312 04:38:02.258553 140011093206848 spec.py:321] Evaluating on the training split.
I0312 04:38:58.335535 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 04:39:47.870622 140011093206848 spec.py:349] Evaluating on the test split.
I0312 04:40:11.795431 140011093206848 submission_runner.py:413] Time since start: 12771.07s, 	Step: 14382, 	{'train/ctc_loss': Array(0.43487144, dtype=float32), 'train/wer': 0.1466808809381419, 'validation/ctc_loss': Array(0.77115315, dtype=float32), 'validation/wer': 0.2193826814833409, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49584833, dtype=float32), 'test/wer': 0.15755692320191741, 'test/num_examples': 2472, 'score': 11561.77420091629, 'total_duration': 12771.071817874908, 'accumulated_submission_time': 11561.77420091629, 'accumulated_eval_time': 1208.2493934631348, 'accumulated_logging_time': 0.3865022659301758}
I0312 04:40:11.827658 139855389939456 logging_writer.py:48] [14382] accumulated_eval_time=1208.249393, accumulated_logging_time=0.386502, accumulated_submission_time=11561.774201, global_step=14382, preemption_count=0, score=11561.774201, test/ctc_loss=0.49584832787513733, test/num_examples=2472, test/wer=0.157557, total_duration=12771.071818, train/ctc_loss=0.4348714351654053, train/wer=0.146681, validation/ctc_loss=0.7711531519889832, validation/num_examples=5348, validation/wer=0.219383
I0312 04:41:45.097685 139855389939456 logging_writer.py:48] [14500] global_step=14500, grad_norm=3.900998115539551, loss=2.2932279109954834
I0312 04:48:12.790946 139855381546752 logging_writer.py:48] [15000] global_step=15000, grad_norm=4.312876224517822, loss=2.3093039989471436
I0312 04:55:28.048693 139855389939456 logging_writer.py:48] [15500] global_step=15500, grad_norm=3.9454500675201416, loss=2.3472111225128174
I0312 05:01:49.059967 139855381546752 logging_writer.py:48] [16000] global_step=16000, grad_norm=3.2535810470581055, loss=2.346203565597534
I0312 05:04:12.095798 140011093206848 spec.py:321] Evaluating on the training split.
I0312 05:05:07.745912 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 05:05:56.499700 140011093206848 spec.py:349] Evaluating on the test split.
I0312 05:06:20.403067 140011093206848 submission_runner.py:413] Time since start: 14339.68s, 	Step: 16163, 	{'train/ctc_loss': Array(0.4050046, dtype=float32), 'train/wer': 0.13411785985987051, 'validation/ctc_loss': Array(0.73362416, dtype=float32), 'validation/wer': 0.20901358409685547, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46837115, dtype=float32), 'test/wer': 0.1482745313103, 'test/num_examples': 2472, 'score': 13001.947366952896, 'total_duration': 14339.678297758102, 'accumulated_submission_time': 13001.947366952896, 'accumulated_eval_time': 1336.550460100174, 'accumulated_logging_time': 0.4358994960784912}
I0312 05:06:20.438474 139855389939456 logging_writer.py:48] [16163] accumulated_eval_time=1336.550460, accumulated_logging_time=0.435899, accumulated_submission_time=13001.947367, global_step=16163, preemption_count=0, score=13001.947367, test/ctc_loss=0.4683711528778076, test/num_examples=2472, test/wer=0.148275, total_duration=14339.678298, train/ctc_loss=0.4050045907497406, train/wer=0.134118, validation/ctc_loss=0.7336241602897644, validation/num_examples=5348, validation/wer=0.209014
I0312 05:10:41.885234 139855389939456 logging_writer.py:48] [16500] global_step=16500, grad_norm=5.9622626304626465, loss=2.2864584922790527
I0312 05:17:00.604432 139855381546752 logging_writer.py:48] [17000] global_step=17000, grad_norm=3.9792068004608154, loss=2.2675812244415283
I0312 05:24:16.957519 139855389939456 logging_writer.py:48] [17500] global_step=17500, grad_norm=2.8841824531555176, loss=2.2460567951202393
I0312 05:30:21.008757 140011093206848 spec.py:321] Evaluating on the training split.
I0312 05:31:16.662364 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 05:32:05.164268 140011093206848 spec.py:349] Evaluating on the test split.
I0312 05:32:29.074781 140011093206848 submission_runner.py:413] Time since start: 15908.35s, 	Step: 17980, 	{'train/ctc_loss': Array(0.39788875, dtype=float32), 'train/wer': 0.13503193594577056, 'validation/ctc_loss': Array(0.6985907, dtype=float32), 'validation/wer': 0.20087471156723982, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44714493, dtype=float32), 'test/wer': 0.14264822375236122, 'test/num_examples': 2472, 'score': 14442.421786546707, 'total_duration': 15908.350621700287, 'accumulated_submission_time': 14442.421786546707, 'accumulated_eval_time': 1464.6110398769379, 'accumulated_logging_time': 0.48676466941833496}
I0312 05:32:29.110634 139855389939456 logging_writer.py:48] [17980] accumulated_eval_time=1464.611040, accumulated_logging_time=0.486765, accumulated_submission_time=14442.421787, global_step=17980, preemption_count=0, score=14442.421787, test/ctc_loss=0.44714492559432983, test/num_examples=2472, test/wer=0.142648, total_duration=15908.350622, train/ctc_loss=0.39788874983787537, train/wer=0.135032, validation/ctc_loss=0.6985906958580017, validation/num_examples=5348, validation/wer=0.200875
I0312 05:32:45.043192 139855381546752 logging_writer.py:48] [18000] global_step=18000, grad_norm=5.018885135650635, loss=2.318455219268799
I0312 05:39:29.447498 139855389939456 logging_writer.py:48] [18500] global_step=18500, grad_norm=3.728584051132202, loss=2.228745698928833
I0312 05:45:55.192899 139855389939456 logging_writer.py:48] [19000] global_step=19000, grad_norm=4.7357025146484375, loss=2.144108295440674
I0312 05:53:03.448623 139855381546752 logging_writer.py:48] [19500] global_step=19500, grad_norm=5.506226539611816, loss=2.2152650356292725
I0312 05:56:29.421432 140011093206848 spec.py:321] Evaluating on the training split.
I0312 05:57:25.251842 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 05:58:13.967558 140011093206848 spec.py:349] Evaluating on the test split.
I0312 05:58:37.763192 140011093206848 submission_runner.py:413] Time since start: 17477.04s, 	Step: 19760, 	{'train/ctc_loss': Array(0.39307576, dtype=float32), 'train/wer': 0.12833231973346731, 'validation/ctc_loss': Array(0.67581445, dtype=float32), 'validation/wer': 0.19375923226198866, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4251886, dtype=float32), 'test/wer': 0.13529543192574087, 'test/num_examples': 2472, 'score': 15882.638124465942, 'total_duration': 17477.041954040527, 'accumulated_submission_time': 15882.638124465942, 'accumulated_eval_time': 1592.9503252506256, 'accumulated_logging_time': 0.5391883850097656}
I0312 05:58:37.790456 139855389939456 logging_writer.py:48] [19760] accumulated_eval_time=1592.950325, accumulated_logging_time=0.539188, accumulated_submission_time=15882.638124, global_step=19760, preemption_count=0, score=15882.638124, test/ctc_loss=0.4251886010169983, test/num_examples=2472, test/wer=0.135295, total_duration=17477.041954, train/ctc_loss=0.39307576417922974, train/wer=0.128332, validation/ctc_loss=0.6758144497871399, validation/num_examples=5348, validation/wer=0.193759
I0312 06:01:39.626433 139855381546752 logging_writer.py:48] [20000] global_step=20000, grad_norm=2.2661495208740234, loss=2.2352473735809326
I0312 06:08:36.625206 139855389939456 logging_writer.py:48] [20500] global_step=20500, grad_norm=3.253755807876587, loss=2.22603440284729
I0312 06:15:10.209714 139855389939456 logging_writer.py:48] [21000] global_step=21000, grad_norm=3.2630228996276855, loss=2.2584383487701416
I0312 06:22:18.910526 139855381546752 logging_writer.py:48] [21500] global_step=21500, grad_norm=4.7490692138671875, loss=2.159268379211426
I0312 06:22:38.460063 140011093206848 spec.py:321] Evaluating on the training split.
I0312 06:23:34.528852 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 06:24:23.117109 140011093206848 spec.py:349] Evaluating on the test split.
I0312 06:24:46.586015 140011093206848 submission_runner.py:413] Time since start: 19045.86s, 	Step: 21522, 	{'train/ctc_loss': Array(0.3864148, dtype=float32), 'train/wer': 0.12680507557970824, 'validation/ctc_loss': Array(0.6487764, dtype=float32), 'validation/wer': 0.1871457949158597, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40044314, dtype=float32), 'test/wer': 0.12960818962890744, 'test/num_examples': 2472, 'score': 17323.217123508453, 'total_duration': 19045.860335111618, 'accumulated_submission_time': 17323.217123508453, 'accumulated_eval_time': 1721.0691142082214, 'accumulated_logging_time': 0.5809519290924072}
I0312 06:24:46.618813 139855389939456 logging_writer.py:48] [21522] accumulated_eval_time=1721.069114, accumulated_logging_time=0.580952, accumulated_submission_time=17323.217124, global_step=21522, preemption_count=0, score=17323.217124, test/ctc_loss=0.4004431366920471, test/num_examples=2472, test/wer=0.129608, total_duration=19045.860335, train/ctc_loss=0.3864147961139679, train/wer=0.126805, validation/ctc_loss=0.6487764120101929, validation/num_examples=5348, validation/wer=0.187146
I0312 06:30:50.958014 139855389939456 logging_writer.py:48] [22000] global_step=22000, grad_norm=3.831416130065918, loss=2.15533185005188
I0312 06:37:48.576331 139855381546752 logging_writer.py:48] [22500] global_step=22500, grad_norm=6.232051372528076, loss=2.137181043624878
I0312 06:44:26.277285 139855389939456 logging_writer.py:48] [23000] global_step=23000, grad_norm=6.552724361419678, loss=2.145003080368042
I0312 06:48:47.718566 140011093206848 spec.py:321] Evaluating on the training split.
I0312 06:49:45.136095 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 06:50:33.917918 140011093206848 spec.py:349] Evaluating on the test split.
I0312 06:50:57.506387 140011093206848 submission_runner.py:413] Time since start: 20616.78s, 	Step: 23322, 	{'train/ctc_loss': Array(0.35554844, dtype=float32), 'train/wer': 0.11743492128819838, 'validation/ctc_loss': Array(0.62565005, dtype=float32), 'validation/wer': 0.17986618650858782, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3876742, dtype=float32), 'test/wer': 0.1245912294599151, 'test/num_examples': 2472, 'score': 18764.22445678711, 'total_duration': 20616.782017946243, 'accumulated_submission_time': 18764.22445678711, 'accumulated_eval_time': 1850.8511288166046, 'accumulated_logging_time': 0.6287531852722168}
I0312 06:50:57.541828 139855389939456 logging_writer.py:48] [23322] accumulated_eval_time=1850.851129, accumulated_logging_time=0.628753, accumulated_submission_time=18764.224457, global_step=23322, preemption_count=0, score=18764.224457, test/ctc_loss=0.3876742124557495, test/num_examples=2472, test/wer=0.124591, total_duration=20616.782018, train/ctc_loss=0.3555484414100647, train/wer=0.117435, validation/ctc_loss=0.6256500482559204, validation/num_examples=5348, validation/wer=0.179866
I0312 06:53:12.466762 139855381546752 logging_writer.py:48] [23500] global_step=23500, grad_norm=3.0901594161987305, loss=2.1154913902282715
I0312 06:59:42.661253 139855389939456 logging_writer.py:48] [24000] global_step=24000, grad_norm=3.628896951675415, loss=2.088071584701538
I0312 07:06:45.828120 139855381546752 logging_writer.py:48] [24500] global_step=24500, grad_norm=4.290198802947998, loss=2.155458688735962
I0312 07:13:36.407543 139855389939456 logging_writer.py:48] [25000] global_step=25000, grad_norm=3.3684113025665283, loss=2.087556838989258
I0312 07:14:57.927156 140011093206848 spec.py:321] Evaluating on the training split.
I0312 07:15:54.147238 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 07:16:44.771987 140011093206848 spec.py:349] Evaluating on the test split.
I0312 07:17:08.986001 140011093206848 submission_runner.py:413] Time since start: 22188.26s, 	Step: 25110, 	{'train/ctc_loss': Array(0.3207983, dtype=float32), 'train/wer': 0.10658861534563574, 'validation/ctc_loss': Array(0.59721625, dtype=float32), 'validation/wer': 0.17366789924404066, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36801726, dtype=float32), 'test/wer': 0.11876180610566084, 'test/num_examples': 2472, 'score': 20204.516329288483, 'total_duration': 22188.264468431473, 'accumulated_submission_time': 20204.516329288483, 'accumulated_eval_time': 1981.9069805145264, 'accumulated_logging_time': 0.6795573234558105}
I0312 07:17:09.013592 139855389939456 logging_writer.py:48] [25110] accumulated_eval_time=1981.906981, accumulated_logging_time=0.679557, accumulated_submission_time=20204.516329, global_step=25110, preemption_count=0, score=20204.516329, test/ctc_loss=0.3680172562599182, test/num_examples=2472, test/wer=0.118762, total_duration=22188.264468, train/ctc_loss=0.3207983076572418, train/wer=0.106589, validation/ctc_loss=0.5972162485122681, validation/num_examples=5348, validation/wer=0.173668
I0312 07:22:26.716511 139855381546752 logging_writer.py:48] [25500] global_step=25500, grad_norm=5.707972049713135, loss=2.0666565895080566
I0312 07:29:18.472587 139855389939456 logging_writer.py:48] [26000] global_step=26000, grad_norm=5.096348762512207, loss=2.015353202819824
I0312 07:36:19.120533 139855381546752 logging_writer.py:48] [26500] global_step=26500, grad_norm=2.814227819442749, loss=2.0448248386383057
I0312 07:41:09.202745 140011093206848 spec.py:321] Evaluating on the training split.
I0312 07:42:05.332762 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 07:42:55.218280 140011093206848 spec.py:349] Evaluating on the test split.
I0312 07:43:19.088973 140011093206848 submission_runner.py:413] Time since start: 23758.36s, 	Step: 26830, 	{'train/ctc_loss': Array(0.2881051, dtype=float32), 'train/wer': 0.0992774981089752, 'validation/ctc_loss': Array(0.5721271, dtype=float32), 'validation/wer': 0.1668710234897709, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34499624, dtype=float32), 'test/wer': 0.11104340584567261, 'test/num_examples': 2472, 'score': 21644.616439819336, 'total_duration': 23758.364603042603, 'accumulated_submission_time': 21644.616439819336, 'accumulated_eval_time': 2111.7875645160675, 'accumulated_logging_time': 0.7207136154174805}
I0312 07:43:19.127249 139855389939456 logging_writer.py:48] [26830] accumulated_eval_time=2111.787565, accumulated_logging_time=0.720714, accumulated_submission_time=21644.616440, global_step=26830, preemption_count=0, score=21644.616440, test/ctc_loss=0.34499624371528625, test/num_examples=2472, test/wer=0.111043, total_duration=23758.364603, train/ctc_loss=0.2881051003932953, train/wer=0.099277, validation/ctc_loss=0.572127103805542, validation/num_examples=5348, validation/wer=0.166871
I0312 07:45:28.053196 139855381546752 logging_writer.py:48] [27000] global_step=27000, grad_norm=5.923144817352295, loss=1.998020052909851
I0312 07:52:08.537611 139855389939456 logging_writer.py:48] [27500] global_step=27500, grad_norm=4.775676727294922, loss=2.0034120082855225
I0312 07:59:10.884549 139855389939456 logging_writer.py:48] [28000] global_step=28000, grad_norm=3.0380985736846924, loss=1.937252163887024
I0312 08:06:03.775978 139855381546752 logging_writer.py:48] [28500] global_step=28500, grad_norm=3.921672821044922, loss=1.979480266571045
I0312 08:07:19.631219 140011093206848 spec.py:321] Evaluating on the training split.
I0312 08:08:18.794847 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 08:09:08.015681 140011093206848 spec.py:349] Evaluating on the test split.
I0312 08:09:31.824564 140011093206848 submission_runner.py:413] Time since start: 25331.10s, 	Step: 28585, 	{'train/ctc_loss': Array(0.27337345, dtype=float32), 'train/wer': 0.09159157623426169, 'validation/ctc_loss': Array(0.5488452, dtype=float32), 'validation/wer': 0.1587224963070952, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32773188, dtype=float32), 'test/wer': 0.105274917230313, 'test/num_examples': 2472, 'score': 23085.02828645706, 'total_duration': 25331.09957075119, 'accumulated_submission_time': 23085.02828645706, 'accumulated_eval_time': 2243.974442720413, 'accumulated_logging_time': 0.7734575271606445}
I0312 08:09:31.859337 139855389939456 logging_writer.py:48] [28585] accumulated_eval_time=2243.974443, accumulated_logging_time=0.773458, accumulated_submission_time=23085.028286, global_step=28585, preemption_count=0, score=23085.028286, test/ctc_loss=0.3277318775653839, test/num_examples=2472, test/wer=0.105275, total_duration=25331.099571, train/ctc_loss=0.27337345480918884, train/wer=0.091592, validation/ctc_loss=0.5488451719284058, validation/num_examples=5348, validation/wer=0.158722
I0312 08:14:55.884527 139855389939456 logging_writer.py:48] [29000] global_step=29000, grad_norm=5.7949957847595215, loss=1.9881069660186768
I0312 08:21:39.865457 139855381546752 logging_writer.py:48] [29500] global_step=29500, grad_norm=3.470097064971924, loss=1.9696146249771118
I0312 08:28:53.648770 139855389939456 logging_writer.py:48] [30000] global_step=30000, grad_norm=5.307895660400391, loss=1.988896131515503
I0312 08:33:32.050899 140011093206848 spec.py:321] Evaluating on the training split.
I0312 08:34:27.788545 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 08:35:17.417214 140011093206848 spec.py:349] Evaluating on the test split.
I0312 08:35:41.719052 140011093206848 submission_runner.py:413] Time since start: 26900.99s, 	Step: 30361, 	{'train/ctc_loss': Array(0.28952494, dtype=float32), 'train/wer': 0.09590591957011614, 'validation/ctc_loss': Array(0.5283387, dtype=float32), 'validation/wer': 0.1527173021037489, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31529206, dtype=float32), 'test/wer': 0.10190319501147604, 'test/num_examples': 2472, 'score': 24525.12541604042, 'total_duration': 26900.994467258453, 'accumulated_submission_time': 24525.12541604042, 'accumulated_eval_time': 2373.636598110199, 'accumulated_logging_time': 0.8246824741363525}
I0312 08:35:41.753232 139855389939456 logging_writer.py:48] [30361] accumulated_eval_time=2373.636598, accumulated_logging_time=0.824682, accumulated_submission_time=24525.125416, global_step=30361, preemption_count=0, score=24525.125416, test/ctc_loss=0.3152920603752136, test/num_examples=2472, test/wer=0.101903, total_duration=26900.994467, train/ctc_loss=0.28952494263648987, train/wer=0.095906, validation/ctc_loss=0.5283386707305908, validation/num_examples=5348, validation/wer=0.152717
I0312 08:37:27.279788 139855381546752 logging_writer.py:48] [30500] global_step=30500, grad_norm=5.412025451660156, loss=2.0326552391052246
I0312 08:44:26.804339 139855389939456 logging_writer.py:48] [31000] global_step=31000, grad_norm=5.100801467895508, loss=1.9243199825286865
I0312 08:50:58.629424 139855381546752 logging_writer.py:48] [31500] global_step=31500, grad_norm=7.033578872680664, loss=1.9655333757400513
I0312 08:58:18.309178 139855389939456 logging_writer.py:48] [32000] global_step=32000, grad_norm=4.308074951171875, loss=1.9492305517196655
I0312 08:59:42.045240 140011093206848 spec.py:321] Evaluating on the training split.
I0312 09:00:36.624099 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 09:01:25.413757 140011093206848 spec.py:349] Evaluating on the test split.
I0312 09:01:49.419314 140011093206848 submission_runner.py:413] Time since start: 28468.70s, 	Step: 32112, 	{'train/ctc_loss': Array(0.2633193, dtype=float32), 'train/wer': 0.08619146061563003, 'validation/ctc_loss': Array(0.51500183, dtype=float32), 'validation/wer': 0.14941541075721443, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3053639, dtype=float32), 'test/wer': 0.09897832754453313, 'test/num_examples': 2472, 'score': 25965.32535982132, 'total_duration': 28468.69791007042, 'accumulated_submission_time': 25965.32535982132, 'accumulated_eval_time': 2501.0078134536743, 'accumulated_logging_time': 0.8752501010894775}
I0312 09:01:49.446362 139855389939456 logging_writer.py:48] [32112] accumulated_eval_time=2501.007813, accumulated_logging_time=0.875250, accumulated_submission_time=25965.325360, global_step=32112, preemption_count=0, score=25965.325360, test/ctc_loss=0.30536389350891113, test/num_examples=2472, test/wer=0.098978, total_duration=28468.697910, train/ctc_loss=0.26331931352615356, train/wer=0.086191, validation/ctc_loss=0.5150018334388733, validation/num_examples=5348, validation/wer=0.149415
I0312 09:06:45.902042 139855381546752 logging_writer.py:48] [32500] global_step=32500, grad_norm=3.22871994972229, loss=1.9152604341506958
I0312 09:14:10.234375 139855389939456 logging_writer.py:48] [33000] global_step=33000, grad_norm=4.9753594398498535, loss=1.9438432455062866
I0312 09:20:37.242446 139855381546752 logging_writer.py:48] [33500] global_step=33500, grad_norm=5.833650588989258, loss=1.9294682741165161
I0312 09:25:50.077297 140011093206848 spec.py:321] Evaluating on the training split.
I0312 09:26:46.807117 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 09:27:36.120642 140011093206848 spec.py:349] Evaluating on the test split.
I0312 09:28:00.116054 140011093206848 submission_runner.py:413] Time since start: 30039.39s, 	Step: 33849, 	{'train/ctc_loss': Array(0.2809698, dtype=float32), 'train/wer': 0.09108281236355044, 'validation/ctc_loss': Array(0.50607127, dtype=float32), 'validation/wer': 0.14681830908406307, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2989146, dtype=float32), 'test/wer': 0.09796274856295574, 'test/num_examples': 2472, 'score': 27405.867421865463, 'total_duration': 30039.391716718674, 'accumulated_submission_time': 27405.867421865463, 'accumulated_eval_time': 2631.0407733917236, 'accumulated_logging_time': 0.9160759449005127}
I0312 09:28:00.152615 139855389939456 logging_writer.py:48] [33849] accumulated_eval_time=2631.040773, accumulated_logging_time=0.916076, accumulated_submission_time=27405.867422, global_step=33849, preemption_count=0, score=27405.867422, test/ctc_loss=0.2989146113395691, test/num_examples=2472, test/wer=0.097963, total_duration=30039.391717, train/ctc_loss=0.2809697985649109, train/wer=0.091083, validation/ctc_loss=0.5060712695121765, validation/num_examples=5348, validation/wer=0.146818
I0312 09:29:57.613701 139855389939456 logging_writer.py:48] [34000] global_step=34000, grad_norm=5.290474891662598, loss=1.9426854848861694
I0312 09:36:22.668319 139855381546752 logging_writer.py:48] [34500] global_step=34500, grad_norm=4.913428783416748, loss=1.8904811143875122
I0312 09:43:47.885733 139855389939456 logging_writer.py:48] [35000] global_step=35000, grad_norm=5.226501941680908, loss=1.8734934329986572
I0312 09:50:14.926141 139855389939456 logging_writer.py:48] [35500] global_step=35500, grad_norm=4.587311744689941, loss=1.8391196727752686
I0312 09:52:00.928617 140011093206848 spec.py:321] Evaluating on the training split.
I0312 09:52:57.025771 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 09:53:46.657096 140011093206848 spec.py:349] Evaluating on the test split.
I0312 09:54:10.448337 140011093206848 submission_runner.py:413] Time since start: 31609.72s, 	Step: 35621, 	{'train/ctc_loss': Array(0.27416888, dtype=float32), 'train/wer': 0.08805162142864754, 'validation/ctc_loss': Array(0.50336236, dtype=float32), 'validation/wer': 0.14606524614537977, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29698846, dtype=float32), 'test/wer': 0.0968456116832206, 'test/num_examples': 2472, 'score': 28846.551880836487, 'total_duration': 31609.72355532646, 'accumulated_submission_time': 28846.551880836487, 'accumulated_eval_time': 2760.554272174835, 'accumulated_logging_time': 0.9677901268005371}
I0312 09:54:10.480077 139855389939456 logging_writer.py:48] [35621] accumulated_eval_time=2760.554272, accumulated_logging_time=0.967790, accumulated_submission_time=28846.551881, global_step=35621, preemption_count=0, score=28846.551881, test/ctc_loss=0.29698845744132996, test/num_examples=2472, test/wer=0.096846, total_duration=31609.723555, train/ctc_loss=0.27416887879371643, train/wer=0.088052, validation/ctc_loss=0.5033623576164246, validation/num_examples=5348, validation/wer=0.146065
I0312 09:59:15.834948 139855381546752 logging_writer.py:48] [36000] global_step=36000, grad_norm=5.255312919616699, loss=1.7959579229354858
I0312 10:05:48.102641 139855389939456 logging_writer.py:48] [36500] global_step=36500, grad_norm=7.374746322631836, loss=1.9027341604232788
I0312 10:13:12.670810 139855381546752 logging_writer.py:48] [37000] global_step=37000, grad_norm=4.8381428718566895, loss=1.9050527811050415
I0312 10:18:10.691668 140011093206848 spec.py:321] Evaluating on the training split.
I0312 10:19:07.522585 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 10:19:56.442566 140011093206848 spec.py:349] Evaluating on the test split.
I0312 10:20:20.596385 140011093206848 submission_runner.py:413] Time since start: 33179.87s, 	Step: 37378, 	{'train/ctc_loss': Array(0.22316046, dtype=float32), 'train/wer': 0.0754691840144317, 'validation/ctc_loss': Array(0.5033837, dtype=float32), 'validation/wer': 0.14596869961477935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2969991, dtype=float32), 'test/wer': 0.09672374220543131, 'test/num_examples': 2472, 'score': 30286.670339107513, 'total_duration': 33179.874921798706, 'accumulated_submission_time': 30286.670339107513, 'accumulated_eval_time': 2890.4562380313873, 'accumulated_logging_time': 1.016021728515625}
I0312 10:20:20.622880 139855389939456 logging_writer.py:48] [37378] accumulated_eval_time=2890.456238, accumulated_logging_time=1.016022, accumulated_submission_time=30286.670339, global_step=37378, preemption_count=0, score=30286.670339, test/ctc_loss=0.29699909687042236, test/num_examples=2472, test/wer=0.096724, total_duration=33179.874922, train/ctc_loss=0.22316046059131622, train/wer=0.075469, validation/ctc_loss=0.5033836960792542, validation/num_examples=5348, validation/wer=0.145969
I0312 10:21:53.248877 139855381546752 logging_writer.py:48] [37500] global_step=37500, grad_norm=6.890858173370361, loss=1.8926109075546265
I0312 10:28:55.955860 139855389939456 logging_writer.py:48] [38000] global_step=38000, grad_norm=3.3995988368988037, loss=1.876787781715393
I0312 10:35:30.066936 139855389939456 logging_writer.py:48] [38500] global_step=38500, grad_norm=5.270531177520752, loss=1.8368709087371826
I0312 10:42:41.178149 139855381546752 logging_writer.py:48] [39000] global_step=39000, grad_norm=4.763490200042725, loss=1.851717233657837
I0312 10:44:20.795488 140011093206848 spec.py:321] Evaluating on the training split.
I0312 10:45:16.940288 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 10:46:05.968824 140011093206848 spec.py:349] Evaluating on the test split.
I0312 10:46:29.611673 140011093206848 submission_runner.py:413] Time since start: 34748.89s, 	Step: 39113, 	{'train/ctc_loss': Array(0.26100314, dtype=float32), 'train/wer': 0.08586935861967013, 'validation/ctc_loss': Array(0.5033837, dtype=float32), 'validation/wer': 0.14596869961477935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2969991, dtype=float32), 'test/wer': 0.09672374220543131, 'test/num_examples': 2472, 'score': 31726.753246307373, 'total_duration': 34748.8867020607, 'accumulated_submission_time': 31726.753246307373, 'accumulated_eval_time': 3019.2659919261932, 'accumulated_logging_time': 1.056718111038208}
I0312 10:46:29.643803 139855389939456 logging_writer.py:48] [39113] accumulated_eval_time=3019.265992, accumulated_logging_time=1.056718, accumulated_submission_time=31726.753246, global_step=39113, preemption_count=0, score=31726.753246, test/ctc_loss=0.29699909687042236, test/num_examples=2472, test/wer=0.096724, total_duration=34748.886702, train/ctc_loss=0.26100313663482666, train/wer=0.085869, validation/ctc_loss=0.5033836960792542, validation/num_examples=5348, validation/wer=0.145969
I0312 10:51:25.155166 139855389939456 logging_writer.py:48] [39500] global_step=39500, grad_norm=6.463230609893799, loss=1.8451024293899536
I0312 10:58:32.781012 139855381546752 logging_writer.py:48] [40000] global_step=40000, grad_norm=5.055985450744629, loss=1.8972035646438599
I0312 11:05:15.261154 139855389939456 logging_writer.py:48] [40500] global_step=40500, grad_norm=6.718005657196045, loss=1.885949730873108
I0312 11:10:30.198511 140011093206848 spec.py:321] Evaluating on the training split.
I0312 11:11:25.016354 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 11:12:15.864888 140011093206848 spec.py:349] Evaluating on the test split.
I0312 11:12:39.964991 140011093206848 submission_runner.py:413] Time since start: 36319.24s, 	Step: 40875, 	{'train/ctc_loss': Array(0.3403322, dtype=float32), 'train/wer': 0.11062414909716158, 'validation/ctc_loss': Array(0.5033837, dtype=float32), 'validation/wer': 0.14596869961477935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2969991, dtype=float32), 'test/wer': 0.09672374220543131, 'test/num_examples': 2472, 'score': 33167.217478990555, 'total_duration': 36319.239720106125, 'accumulated_submission_time': 33167.217478990555, 'accumulated_eval_time': 3149.025756597519, 'accumulated_logging_time': 1.103356122970581}
I0312 11:12:40.011705 139855389939456 logging_writer.py:48] [40875] accumulated_eval_time=3149.025757, accumulated_logging_time=1.103356, accumulated_submission_time=33167.217479, global_step=40875, preemption_count=0, score=33167.217479, test/ctc_loss=0.29699909687042236, test/num_examples=2472, test/wer=0.096724, total_duration=36319.239720, train/ctc_loss=0.3403322100639343, train/wer=0.110624, validation/ctc_loss=0.5033836960792542, validation/num_examples=5348, validation/wer=0.145969
I0312 11:14:14.868934 139855381546752 logging_writer.py:48] [41000] global_step=41000, grad_norm=5.001704692840576, loss=1.8637034893035889
I0312 11:20:53.173822 139855389939456 logging_writer.py:48] [41500] global_step=41500, grad_norm=3.5330584049224854, loss=1.8923418521881104
I0312 11:28:05.129024 139855381546752 logging_writer.py:48] [42000] global_step=42000, grad_norm=6.362880706787109, loss=1.831335425376892
I0312 11:35:00.605382 139855389939456 logging_writer.py:48] [42500] global_step=42500, grad_norm=4.219375133514404, loss=1.870042324066162
I0312 11:36:40.227642 140011093206848 spec.py:321] Evaluating on the training split.
I0312 11:37:34.796334 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 11:38:23.210998 140011093206848 spec.py:349] Evaluating on the test split.
I0312 11:38:47.237470 140011093206848 submission_runner.py:413] Time since start: 37886.51s, 	Step: 42632, 	{'train/ctc_loss': Array(0.36612552, dtype=float32), 'train/wer': 0.11803238646141548, 'validation/ctc_loss': Array(0.5033837, dtype=float32), 'validation/wer': 0.14596869961477935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2969991, dtype=float32), 'test/wer': 0.09672374220543131, 'test/num_examples': 2472, 'score': 34607.330600738525, 'total_duration': 37886.513839006424, 'accumulated_submission_time': 34607.330600738525, 'accumulated_eval_time': 3276.030506849289, 'accumulated_logging_time': 1.1772422790527344}
I0312 11:38:47.268415 139855389939456 logging_writer.py:48] [42632] accumulated_eval_time=3276.030507, accumulated_logging_time=1.177242, accumulated_submission_time=34607.330601, global_step=42632, preemption_count=0, score=34607.330601, test/ctc_loss=0.29699909687042236, test/num_examples=2472, test/wer=0.096724, total_duration=37886.513839, train/ctc_loss=0.36612552404403687, train/wer=0.118032, validation/ctc_loss=0.5033836960792542, validation/num_examples=5348, validation/wer=0.145969
I0312 11:43:40.122632 139855381546752 logging_writer.py:48] [43000] global_step=43000, grad_norm=7.360703468322754, loss=1.880624771118164
I0312 11:50:30.682779 139855389939456 logging_writer.py:48] [43500] global_step=43500, grad_norm=5.925439834594727, loss=1.8468482494354248
I0312 11:57:23.774514 139855381546752 logging_writer.py:48] [44000] global_step=44000, grad_norm=4.668118953704834, loss=1.9245439767837524
I0312 12:02:47.619824 140011093206848 spec.py:321] Evaluating on the training split.
I0312 12:03:40.668547 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 12:04:30.399014 140011093206848 spec.py:349] Evaluating on the test split.
I0312 12:04:54.717729 140011093206848 submission_runner.py:413] Time since start: 39453.99s, 	Step: 44374, 	{'train/ctc_loss': Array(0.42718977, dtype=float32), 'train/wer': 0.13838468333031104, 'validation/ctc_loss': Array(0.5033837, dtype=float32), 'validation/wer': 0.14596869961477935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2969991, dtype=float32), 'test/wer': 0.09672374220543131, 'test/num_examples': 2472, 'score': 36047.59180903435, 'total_duration': 39453.99339413643, 'accumulated_submission_time': 36047.59180903435, 'accumulated_eval_time': 3403.1228449344635, 'accumulated_logging_time': 1.2229571342468262}
I0312 12:04:54.749660 139855389939456 logging_writer.py:48] [44374] accumulated_eval_time=3403.122845, accumulated_logging_time=1.222957, accumulated_submission_time=36047.591809, global_step=44374, preemption_count=0, score=36047.591809, test/ctc_loss=0.29699909687042236, test/num_examples=2472, test/wer=0.096724, total_duration=39453.993394, train/ctc_loss=0.42718976736068726, train/wer=0.138385, validation/ctc_loss=0.5033836960792542, validation/num_examples=5348, validation/wer=0.145969
I0312 12:06:30.476316 139855381546752 logging_writer.py:48] [44500] global_step=44500, grad_norm=4.293214321136475, loss=1.950015664100647
I0312 12:13:13.258201 139855389939456 logging_writer.py:48] [45000] global_step=45000, grad_norm=3.738474130630493, loss=1.8486047983169556
I0312 12:20:19.059949 139855389939456 logging_writer.py:48] [45500] global_step=45500, grad_norm=8.418676376342773, loss=1.918835163116455
I0312 12:27:06.350353 139855381546752 logging_writer.py:48] [46000] global_step=46000, grad_norm=9.124725341796875, loss=1.9110639095306396
I0312 12:28:55.123924 140011093206848 spec.py:321] Evaluating on the training split.
I0312 12:29:48.783280 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 12:30:37.985013 140011093206848 spec.py:349] Evaluating on the test split.
I0312 12:31:01.976417 140011093206848 submission_runner.py:413] Time since start: 41021.25s, 	Step: 46124, 	{'train/ctc_loss': Array(0.39215004, dtype=float32), 'train/wer': 0.12360390699974684, 'validation/ctc_loss': Array(0.5033837, dtype=float32), 'validation/wer': 0.14596869961477935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2969991, dtype=float32), 'test/wer': 0.09672374220543131, 'test/num_examples': 2472, 'score': 37487.87303304672, 'total_duration': 41021.252115249634, 'accumulated_submission_time': 37487.87303304672, 'accumulated_eval_time': 3529.9695677757263, 'accumulated_logging_time': 1.2710790634155273}
I0312 12:31:02.010083 139855389939456 logging_writer.py:48] [46124] accumulated_eval_time=3529.969568, accumulated_logging_time=1.271079, accumulated_submission_time=37487.873033, global_step=46124, preemption_count=0, score=37487.873033, test/ctc_loss=0.29699909687042236, test/num_examples=2472, test/wer=0.096724, total_duration=41021.252115, train/ctc_loss=0.39215004444122314, train/wer=0.123604, validation/ctc_loss=0.5033836960792542, validation/num_examples=5348, validation/wer=0.145969
I0312 12:35:49.063743 139855389939456 logging_writer.py:48] [46500] global_step=46500, grad_norm=3.3124711513519287, loss=1.8435004949569702
I0312 12:42:25.734608 139855381546752 logging_writer.py:48] [47000] global_step=47000, grad_norm=3.216590404510498, loss=1.8926730155944824
I0312 12:49:32.754977 139855389939456 logging_writer.py:48] [47500] global_step=47500, grad_norm=4.1654839515686035, loss=1.9476478099822998
I0312 12:55:02.313895 140011093206848 spec.py:321] Evaluating on the training split.
I0312 12:55:57.653082 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 12:56:46.694105 140011093206848 spec.py:349] Evaluating on the test split.
I0312 12:57:10.388703 140011093206848 submission_runner.py:413] Time since start: 42589.66s, 	Step: 47927, 	{'train/ctc_loss': Array(0.36895666, dtype=float32), 'train/wer': 0.12063955106721744, 'validation/ctc_loss': Array(0.5033837, dtype=float32), 'validation/wer': 0.14596869961477935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2969991, dtype=float32), 'test/wer': 0.09672374220543131, 'test/num_examples': 2472, 'score': 38928.082442998886, 'total_duration': 42589.66422653198, 'accumulated_submission_time': 38928.082442998886, 'accumulated_eval_time': 3658.0384402275085, 'accumulated_logging_time': 1.321122169494629}
I0312 12:57:10.421618 139855389939456 logging_writer.py:48] [47927] accumulated_eval_time=3658.038440, accumulated_logging_time=1.321122, accumulated_submission_time=38928.082443, global_step=47927, preemption_count=0, score=38928.082443, test/ctc_loss=0.29699909687042236, test/num_examples=2472, test/wer=0.096724, total_duration=42589.664227, train/ctc_loss=0.36895665526390076, train/wer=0.120640, validation/ctc_loss=0.5033836960792542, validation/num_examples=5348, validation/wer=0.145969
I0312 12:58:06.063096 139855381546752 logging_writer.py:48] [48000] global_step=48000, grad_norm=5.516864776611328, loss=1.8559805154800415
I0312 13:04:56.525826 139855389939456 logging_writer.py:48] [48500] global_step=48500, grad_norm=3.6070444583892822, loss=1.8495383262634277
I0312 13:10:02.948595 140011093206848 spec.py:321] Evaluating on the training split.
I0312 13:10:57.845286 140011093206848 spec.py:333] Evaluating on the validation split.
I0312 13:11:46.692784 140011093206848 spec.py:349] Evaluating on the test split.
I0312 13:12:10.658921 140011093206848 submission_runner.py:413] Time since start: 43489.94s, 	Step: 48900, 	{'train/ctc_loss': Array(0.33606264, dtype=float32), 'train/wer': 0.11107788012920206, 'validation/ctc_loss': Array(0.5033837, dtype=float32), 'validation/wer': 0.14596869961477935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2969991, dtype=float32), 'test/wer': 0.09672374220543131, 'test/num_examples': 2472, 'score': 39700.548932790756, 'total_duration': 43489.93720316887, 'accumulated_submission_time': 39700.548932790756, 'accumulated_eval_time': 3785.7455933094025, 'accumulated_logging_time': 1.3719985485076904}
I0312 13:12:10.690402 139855389939456 logging_writer.py:48] [48900] accumulated_eval_time=3785.745593, accumulated_logging_time=1.371999, accumulated_submission_time=39700.548933, global_step=48900, preemption_count=0, score=39700.548933, test/ctc_loss=0.29699909687042236, test/num_examples=2472, test/wer=0.096724, total_duration=43489.937203, train/ctc_loss=0.33606263995170593, train/wer=0.111078, validation/ctc_loss=0.5033836960792542, validation/num_examples=5348, validation/wer=0.145969
I0312 13:12:10.713491 139855381546752 logging_writer.py:48] [48900] global_step=48900, preemption_count=0, score=39700.548933
I0312 13:12:10.864926 140011093206848 checkpoints.py:490] Saving checkpoint at step: 48900
I0312 13:12:11.794358 140011093206848 checkpoints.py:422] Saved checkpoint at /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_norm_and_spec_aug_jax/trial_1/checkpoint_48900
I0312 13:12:11.814269 140011093206848 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_norm_and_spec_aug_jax/trial_1/checkpoint_48900.
I0312 13:12:12.986585 140011093206848 submission_runner.py:588] Tuning trial 1/1
I0312 13:12:12.986888 140011093206848 submission_runner.py:589] Hyperparameters: Hyperparameters(learning_rate=0.001308209823469072, beta1=0.9731333693827139, beta2=0.9981232922116359, warmup_steps=6000, weight_decay=0.16375311233774334)
I0312 13:12:12.999248 140011093206848 submission_runner.py:590] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(30.565628, dtype=float32), 'train/wer': 2.764732523998917, 'validation/ctc_loss': Array(30.097767, dtype=float32), 'validation/wer': 2.601166282089653, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.133963, dtype=float32), 'test/wer': 2.7303840919708326, 'test/num_examples': 2472, 'score': 40.19957900047302, 'total_duration': 235.10354018211365, 'accumulated_submission_time': 40.19957900047302, 'accumulated_eval_time': 194.90390253067017, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1815, {'train/ctc_loss': Array(3.6631207, dtype=float32), 'train/wer': 0.7533315482688308, 'validation/ctc_loss': Array(3.8746686, dtype=float32), 'validation/wer': 0.7599563609681687, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.6510215, dtype=float32), 'test/wer': 0.7332074015396177, 'test/num_examples': 2472, 'score': 1480.2582232952118, 'total_duration': 1791.6881017684937, 'accumulated_submission_time': 1480.2582232952118, 'accumulated_eval_time': 311.3134095668793, 'accumulated_logging_time': 0.040225982666015625, 'global_step': 1815, 'preemption_count': 0}), (3639, {'train/ctc_loss': Array(1.2549866, dtype=float32), 'train/wer': 0.3648616253362811, 'validation/ctc_loss': Array(1.5888001, dtype=float32), 'validation/wer': 0.4131322590922695, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.2625551, dtype=float32), 'test/wer': 0.3578900331078748, 'test/num_examples': 2472, 'score': 2920.1678557395935, 'total_duration': 3359.268629550934, 'accumulated_submission_time': 2920.1678557395935, 'accumulated_eval_time': 438.84852933883667, 'accumulated_logging_time': 0.09029245376586914, 'global_step': 3639, 'preemption_count': 0}), (5436, {'train/ctc_loss': Array(0.8079109, dtype=float32), 'train/wer': 0.2565533891023715, 'validation/ctc_loss': Array(1.1846188, dtype=float32), 'validation/wer': 0.3250625138785638, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.85878384, dtype=float32), 'test/wer': 0.2599882192838137, 'test/num_examples': 2472, 'score': 4360.387193918228, 'total_duration': 4928.032552957535, 'accumulated_submission_time': 4360.387193918228, 'accumulated_eval_time': 567.2518880367279, 'accumulated_logging_time': 0.14759349822998047, 'global_step': 5436, 'preemption_count': 0}), (7229, {'train/ctc_loss': Array(0.6611109, dtype=float32), 'train/wer': 0.21087245651331568, 'validation/ctc_loss': Array(0.99669725, dtype=float32), 'validation/wer': 0.2786912152311807, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6835765, dtype=float32), 'test/wer': 0.21170759449962423, 'test/num_examples': 2472, 'score': 5801.019718408585, 'total_duration': 6496.429447650909, 'accumulated_submission_time': 5801.019718408585, 'accumulated_eval_time': 694.8976786136627, 'accumulated_logging_time': 0.18665361404418945, 'global_step': 7229, 'preemption_count': 0}), (9026, {'train/ctc_loss': Array(0.5731816, dtype=float32), 'train/wer': 0.18660984148383308, 'validation/ctc_loss': Array(0.8929109, dtype=float32), 'validation/wer': 0.25249814147928595, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5971978, dtype=float32), 'test/wer': 0.1849572441248756, 'test/num_examples': 2472, 'score': 7240.888499736786, 'total_duration': 8063.478012561798, 'accumulated_submission_time': 7240.888499736786, 'accumulated_eval_time': 821.9389078617096, 'accumulated_logging_time': 0.23938465118408203, 'global_step': 9026, 'preemption_count': 0}), (10837, {'train/ctc_loss': Array(0.54191834, dtype=float32), 'train/wer': 0.17545385193997365, 'validation/ctc_loss': Array(0.82358056, dtype=float32), 'validation/wer': 0.2340384448284851, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5422325, dtype=float32), 'test/wer': 0.16972355940121464, 'test/num_examples': 2472, 'score': 8681.543917417526, 'total_duration': 9632.462462186813, 'accumulated_submission_time': 8681.543917417526, 'accumulated_eval_time': 950.1336026191711, 'accumulated_logging_time': 0.29027676582336426, 'global_step': 10837, 'preemption_count': 0}), (12614, {'train/ctc_loss': Array(0.4917271, dtype=float32), 'train/wer': 0.1593974007189501, 'validation/ctc_loss': Array(0.7904846, dtype=float32), 'validation/wer': 0.22684572829875357, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.51778907, dtype=float32), 'test/wer': 0.16328478865801394, 'test/num_examples': 2472, 'score': 10121.531110286713, 'total_duration': 11201.173258543015, 'accumulated_submission_time': 10121.531110286713, 'accumulated_eval_time': 1078.7175657749176, 'accumulated_logging_time': 0.3447730541229248, 'global_step': 12614, 'preemption_count': 0}), (14382, {'train/ctc_loss': Array(0.43487144, dtype=float32), 'train/wer': 0.1466808809381419, 'validation/ctc_loss': Array(0.77115315, dtype=float32), 'validation/wer': 0.2193826814833409, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.49584833, dtype=float32), 'test/wer': 0.15755692320191741, 'test/num_examples': 2472, 'score': 11561.77420091629, 'total_duration': 12771.071817874908, 'accumulated_submission_time': 11561.77420091629, 'accumulated_eval_time': 1208.2493934631348, 'accumulated_logging_time': 0.3865022659301758, 'global_step': 14382, 'preemption_count': 0}), (16163, {'train/ctc_loss': Array(0.4050046, dtype=float32), 'train/wer': 0.13411785985987051, 'validation/ctc_loss': Array(0.73362416, dtype=float32), 'validation/wer': 0.20901358409685547, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.46837115, dtype=float32), 'test/wer': 0.1482745313103, 'test/num_examples': 2472, 'score': 13001.947366952896, 'total_duration': 14339.678297758102, 'accumulated_submission_time': 13001.947366952896, 'accumulated_eval_time': 1336.550460100174, 'accumulated_logging_time': 0.4358994960784912, 'global_step': 16163, 'preemption_count': 0}), (17980, {'train/ctc_loss': Array(0.39788875, dtype=float32), 'train/wer': 0.13503193594577056, 'validation/ctc_loss': Array(0.6985907, dtype=float32), 'validation/wer': 0.20087471156723982, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44714493, dtype=float32), 'test/wer': 0.14264822375236122, 'test/num_examples': 2472, 'score': 14442.421786546707, 'total_duration': 15908.350621700287, 'accumulated_submission_time': 14442.421786546707, 'accumulated_eval_time': 1464.6110398769379, 'accumulated_logging_time': 0.48676466941833496, 'global_step': 17980, 'preemption_count': 0}), (19760, {'train/ctc_loss': Array(0.39307576, dtype=float32), 'train/wer': 0.12833231973346731, 'validation/ctc_loss': Array(0.67581445, dtype=float32), 'validation/wer': 0.19375923226198866, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4251886, dtype=float32), 'test/wer': 0.13529543192574087, 'test/num_examples': 2472, 'score': 15882.638124465942, 'total_duration': 17477.041954040527, 'accumulated_submission_time': 15882.638124465942, 'accumulated_eval_time': 1592.9503252506256, 'accumulated_logging_time': 0.5391883850097656, 'global_step': 19760, 'preemption_count': 0}), (21522, {'train/ctc_loss': Array(0.3864148, dtype=float32), 'train/wer': 0.12680507557970824, 'validation/ctc_loss': Array(0.6487764, dtype=float32), 'validation/wer': 0.1871457949158597, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40044314, dtype=float32), 'test/wer': 0.12960818962890744, 'test/num_examples': 2472, 'score': 17323.217123508453, 'total_duration': 19045.860335111618, 'accumulated_submission_time': 17323.217123508453, 'accumulated_eval_time': 1721.0691142082214, 'accumulated_logging_time': 0.5809519290924072, 'global_step': 21522, 'preemption_count': 0}), (23322, {'train/ctc_loss': Array(0.35554844, dtype=float32), 'train/wer': 0.11743492128819838, 'validation/ctc_loss': Array(0.62565005, dtype=float32), 'validation/wer': 0.17986618650858782, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3876742, dtype=float32), 'test/wer': 0.1245912294599151, 'test/num_examples': 2472, 'score': 18764.22445678711, 'total_duration': 20616.782017946243, 'accumulated_submission_time': 18764.22445678711, 'accumulated_eval_time': 1850.8511288166046, 'accumulated_logging_time': 0.6287531852722168, 'global_step': 23322, 'preemption_count': 0}), (25110, {'train/ctc_loss': Array(0.3207983, dtype=float32), 'train/wer': 0.10658861534563574, 'validation/ctc_loss': Array(0.59721625, dtype=float32), 'validation/wer': 0.17366789924404066, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36801726, dtype=float32), 'test/wer': 0.11876180610566084, 'test/num_examples': 2472, 'score': 20204.516329288483, 'total_duration': 22188.264468431473, 'accumulated_submission_time': 20204.516329288483, 'accumulated_eval_time': 1981.9069805145264, 'accumulated_logging_time': 0.6795573234558105, 'global_step': 25110, 'preemption_count': 0}), (26830, {'train/ctc_loss': Array(0.2881051, dtype=float32), 'train/wer': 0.0992774981089752, 'validation/ctc_loss': Array(0.5721271, dtype=float32), 'validation/wer': 0.1668710234897709, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34499624, dtype=float32), 'test/wer': 0.11104340584567261, 'test/num_examples': 2472, 'score': 21644.616439819336, 'total_duration': 23758.364603042603, 'accumulated_submission_time': 21644.616439819336, 'accumulated_eval_time': 2111.7875645160675, 'accumulated_logging_time': 0.7207136154174805, 'global_step': 26830, 'preemption_count': 0}), (28585, {'train/ctc_loss': Array(0.27337345, dtype=float32), 'train/wer': 0.09159157623426169, 'validation/ctc_loss': Array(0.5488452, dtype=float32), 'validation/wer': 0.1587224963070952, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32773188, dtype=float32), 'test/wer': 0.105274917230313, 'test/num_examples': 2472, 'score': 23085.02828645706, 'total_duration': 25331.09957075119, 'accumulated_submission_time': 23085.02828645706, 'accumulated_eval_time': 2243.974442720413, 'accumulated_logging_time': 0.7734575271606445, 'global_step': 28585, 'preemption_count': 0}), (30361, {'train/ctc_loss': Array(0.28952494, dtype=float32), 'train/wer': 0.09590591957011614, 'validation/ctc_loss': Array(0.5283387, dtype=float32), 'validation/wer': 0.1527173021037489, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.31529206, dtype=float32), 'test/wer': 0.10190319501147604, 'test/num_examples': 2472, 'score': 24525.12541604042, 'total_duration': 26900.994467258453, 'accumulated_submission_time': 24525.12541604042, 'accumulated_eval_time': 2373.636598110199, 'accumulated_logging_time': 0.8246824741363525, 'global_step': 30361, 'preemption_count': 0}), (32112, {'train/ctc_loss': Array(0.2633193, dtype=float32), 'train/wer': 0.08619146061563003, 'validation/ctc_loss': Array(0.51500183, dtype=float32), 'validation/wer': 0.14941541075721443, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3053639, dtype=float32), 'test/wer': 0.09897832754453313, 'test/num_examples': 2472, 'score': 25965.32535982132, 'total_duration': 28468.69791007042, 'accumulated_submission_time': 25965.32535982132, 'accumulated_eval_time': 2501.0078134536743, 'accumulated_logging_time': 0.8752501010894775, 'global_step': 32112, 'preemption_count': 0}), (33849, {'train/ctc_loss': Array(0.2809698, dtype=float32), 'train/wer': 0.09108281236355044, 'validation/ctc_loss': Array(0.50607127, dtype=float32), 'validation/wer': 0.14681830908406307, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2989146, dtype=float32), 'test/wer': 0.09796274856295574, 'test/num_examples': 2472, 'score': 27405.867421865463, 'total_duration': 30039.391716718674, 'accumulated_submission_time': 27405.867421865463, 'accumulated_eval_time': 2631.0407733917236, 'accumulated_logging_time': 0.9160759449005127, 'global_step': 33849, 'preemption_count': 0}), (35621, {'train/ctc_loss': Array(0.27416888, dtype=float32), 'train/wer': 0.08805162142864754, 'validation/ctc_loss': Array(0.50336236, dtype=float32), 'validation/wer': 0.14606524614537977, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29698846, dtype=float32), 'test/wer': 0.0968456116832206, 'test/num_examples': 2472, 'score': 28846.551880836487, 'total_duration': 31609.72355532646, 'accumulated_submission_time': 28846.551880836487, 'accumulated_eval_time': 2760.554272174835, 'accumulated_logging_time': 0.9677901268005371, 'global_step': 35621, 'preemption_count': 0}), (37378, {'train/ctc_loss': Array(0.22316046, dtype=float32), 'train/wer': 0.0754691840144317, 'validation/ctc_loss': Array(0.5033837, dtype=float32), 'validation/wer': 0.14596869961477935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2969991, dtype=float32), 'test/wer': 0.09672374220543131, 'test/num_examples': 2472, 'score': 30286.670339107513, 'total_duration': 33179.874921798706, 'accumulated_submission_time': 30286.670339107513, 'accumulated_eval_time': 2890.4562380313873, 'accumulated_logging_time': 1.016021728515625, 'global_step': 37378, 'preemption_count': 0}), (39113, {'train/ctc_loss': Array(0.26100314, dtype=float32), 'train/wer': 0.08586935861967013, 'validation/ctc_loss': Array(0.5033837, dtype=float32), 'validation/wer': 0.14596869961477935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2969991, dtype=float32), 'test/wer': 0.09672374220543131, 'test/num_examples': 2472, 'score': 31726.753246307373, 'total_duration': 34748.8867020607, 'accumulated_submission_time': 31726.753246307373, 'accumulated_eval_time': 3019.2659919261932, 'accumulated_logging_time': 1.056718111038208, 'global_step': 39113, 'preemption_count': 0}), (40875, {'train/ctc_loss': Array(0.3403322, dtype=float32), 'train/wer': 0.11062414909716158, 'validation/ctc_loss': Array(0.5033837, dtype=float32), 'validation/wer': 0.14596869961477935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2969991, dtype=float32), 'test/wer': 0.09672374220543131, 'test/num_examples': 2472, 'score': 33167.217478990555, 'total_duration': 36319.239720106125, 'accumulated_submission_time': 33167.217478990555, 'accumulated_eval_time': 3149.025756597519, 'accumulated_logging_time': 1.103356122970581, 'global_step': 40875, 'preemption_count': 0}), (42632, {'train/ctc_loss': Array(0.36612552, dtype=float32), 'train/wer': 0.11803238646141548, 'validation/ctc_loss': Array(0.5033837, dtype=float32), 'validation/wer': 0.14596869961477935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2969991, dtype=float32), 'test/wer': 0.09672374220543131, 'test/num_examples': 2472, 'score': 34607.330600738525, 'total_duration': 37886.513839006424, 'accumulated_submission_time': 34607.330600738525, 'accumulated_eval_time': 3276.030506849289, 'accumulated_logging_time': 1.1772422790527344, 'global_step': 42632, 'preemption_count': 0}), (44374, {'train/ctc_loss': Array(0.42718977, dtype=float32), 'train/wer': 0.13838468333031104, 'validation/ctc_loss': Array(0.5033837, dtype=float32), 'validation/wer': 0.14596869961477935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2969991, dtype=float32), 'test/wer': 0.09672374220543131, 'test/num_examples': 2472, 'score': 36047.59180903435, 'total_duration': 39453.99339413643, 'accumulated_submission_time': 36047.59180903435, 'accumulated_eval_time': 3403.1228449344635, 'accumulated_logging_time': 1.2229571342468262, 'global_step': 44374, 'preemption_count': 0}), (46124, {'train/ctc_loss': Array(0.39215004, dtype=float32), 'train/wer': 0.12360390699974684, 'validation/ctc_loss': Array(0.5033837, dtype=float32), 'validation/wer': 0.14596869961477935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2969991, dtype=float32), 'test/wer': 0.09672374220543131, 'test/num_examples': 2472, 'score': 37487.87303304672, 'total_duration': 41021.252115249634, 'accumulated_submission_time': 37487.87303304672, 'accumulated_eval_time': 3529.9695677757263, 'accumulated_logging_time': 1.2710790634155273, 'global_step': 46124, 'preemption_count': 0}), (47927, {'train/ctc_loss': Array(0.36895666, dtype=float32), 'train/wer': 0.12063955106721744, 'validation/ctc_loss': Array(0.5033837, dtype=float32), 'validation/wer': 0.14596869961477935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2969991, dtype=float32), 'test/wer': 0.09672374220543131, 'test/num_examples': 2472, 'score': 38928.082442998886, 'total_duration': 42589.66422653198, 'accumulated_submission_time': 38928.082442998886, 'accumulated_eval_time': 3658.0384402275085, 'accumulated_logging_time': 1.321122169494629, 'global_step': 47927, 'preemption_count': 0}), (48900, {'train/ctc_loss': Array(0.33606264, dtype=float32), 'train/wer': 0.11107788012920206, 'validation/ctc_loss': Array(0.5033837, dtype=float32), 'validation/wer': 0.14596869961477935, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2969991, dtype=float32), 'test/wer': 0.09672374220543131, 'test/num_examples': 2472, 'score': 39700.548932790756, 'total_duration': 43489.93720316887, 'accumulated_submission_time': 39700.548932790756, 'accumulated_eval_time': 3785.7455933094025, 'accumulated_logging_time': 1.3719985485076904, 'global_step': 48900, 'preemption_count': 0})], 'global_step': 48900}
I0312 13:12:12.999436 140011093206848 submission_runner.py:591] Timing: 39700.548932790756
I0312 13:12:12.999491 140011093206848 submission_runner.py:593] Total number of evals: 29
I0312 13:12:12.999553 140011093206848 submission_runner.py:594] ====================
I0312 13:12:13.009028 140011093206848 submission_runner.py:678] Final librispeech_deepspeech_norm_and_spec_aug score: 39700.548932790756
