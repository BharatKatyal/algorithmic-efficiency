python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech_norm_and_spec_aug --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --rng_seed=3767268467 --max_global_steps=48000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab --tuning_ruleset=external --tuning_search_space=reference_algorithms/target_setting_algorithms/imagenet_resnet/tuning_search_space.json --num_tuning_trials=1 2>&1 | tee -a /logs/librispeech_deepspeech_norm_and_spec_aug_jax_03-27-2024-22-55-00.log
I0327 22:55:20.212921 140003960837952 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_norm_and_spec_aug_jax.
I0327 22:55:21.221322 140003960837952 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0327 22:55:21.222124 140003960837952 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0327 22:55:21.222271 140003960837952 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0327 22:55:21.228494 140003960837952 submission_runner.py:557] Using RNG seed 3767268467
I0327 22:55:22.295282 140003960837952 submission_runner.py:566] --- Tuning run 1/1 ---
I0327 22:55:22.295490 140003960837952 submission_runner.py:571] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_norm_and_spec_aug_jax/trial_1.
I0327 22:55:22.295872 140003960837952 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_norm_and_spec_aug_jax/trial_1/hparams.json.
I0327 22:55:22.480315 140003960837952 submission_runner.py:211] Initializing dataset.
I0327 22:55:22.480551 140003960837952 submission_runner.py:222] Initializing model.
I0327 22:55:24.881607 140003960837952 submission_runner.py:264] Initializing optimizer.
I0327 22:55:25.557898 140003960837952 submission_runner.py:271] Initializing metrics bundle.
I0327 22:55:25.558089 140003960837952 submission_runner.py:289] Initializing checkpoint and logger.
I0327 22:55:25.559172 140003960837952 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_norm_and_spec_aug_jax/trial_1 with prefix checkpoint_
I0327 22:55:25.559338 140003960837952 submission_runner.py:309] Saving meta data to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_norm_and_spec_aug_jax/trial_1/meta_data_0.json.
I0327 22:55:25.559582 140003960837952 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0327 22:55:25.559656 140003960837952 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0327 22:55:25.823802 140003960837952 logger_utils.py:220] Unable to record git information. Continuing without it.
I0327 22:55:26.059069 140003960837952 submission_runner.py:313] Saving flags to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_norm_and_spec_aug_jax/trial_1/flags_0.json.
I0327 22:55:26.072191 140003960837952 submission_runner.py:323] Starting training loop.
I0327 22:55:26.362800 140003960837952 input_pipeline.py:20] Loading split = train-clean-100
I0327 22:55:26.398730 140003960837952 input_pipeline.py:20] Loading split = train-clean-360
I0327 22:55:26.524400 140003960837952 input_pipeline.py:20] Loading split = train-other-500
I0327 22:56:06.276327 139846848214784 logging_writer.py:48] [0] global_step=0, grad_norm=3324.16845703125, loss=31.196712493896484
I0327 22:56:06.303804 140003960837952 spec.py:321] Evaluating on the training split.
I0327 22:56:06.304003 140003960837952 input_pipeline.py:20] Loading split = train-clean-100
I0327 22:56:06.339327 140003960837952 input_pipeline.py:20] Loading split = train-clean-360
I0327 22:56:06.696206 140003960837952 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0327 22:57:43.758638 140003960837952 spec.py:333] Evaluating on the validation split.
I0327 22:57:43.759170 140003960837952 input_pipeline.py:20] Loading split = dev-clean
I0327 22:57:43.764186 140003960837952 input_pipeline.py:20] Loading split = dev-other
I0327 22:58:48.813372 140003960837952 spec.py:349] Evaluating on the test split.
I0327 22:58:48.813844 140003960837952 input_pipeline.py:20] Loading split = test-clean
I0327 22:59:22.579703 140003960837952 submission_runner.py:422] Time since start: 236.51s, 	Step: 1, 	{'train/ctc_loss': Array(30.895857, dtype=float32), 'train/wer': 2.8071913458134645, 'validation/ctc_loss': Array(29.951494, dtype=float32), 'validation/wer': 2.573940160460334, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.239141, dtype=float32), 'test/wer': 2.687282919992688, 'test/num_examples': 2472, 'score': 40.231555700302124, 'total_duration': 236.50531101226807, 'accumulated_submission_time': 40.231555700302124, 'accumulated_eval_time': 196.273695230484, 'accumulated_logging_time': 0}
I0327 22:59:22.609516 139841150420736 logging_writer.py:48] [1] accumulated_eval_time=196.273695, accumulated_logging_time=0, accumulated_submission_time=40.231556, global_step=1, preemption_count=0, score=40.231556, test/ctc_loss=30.2391414642334, test/num_examples=2472, test/wer=2.687283, total_duration=236.505311, train/ctc_loss=30.895856857299805, train/wer=2.807191, validation/ctc_loss=29.951494216918945, validation/num_examples=5348, validation/wer=2.573940
I0327 22:59:29.878665 139847503574784 logging_writer.py:48] [1] global_step=1, grad_norm=3313.45751953125, loss=32.11448287963867
I0327 22:59:30.696854 139847511967488 logging_writer.py:48] [2] global_step=2, grad_norm=178.68820190429688, loss=15.82070255279541
I0327 22:59:31.572231 139847503574784 logging_writer.py:48] [3] global_step=3, grad_norm=213.70767211914062, loss=14.323076248168945
I0327 22:59:32.468495 139847511967488 logging_writer.py:48] [4] global_step=4, grad_norm=75.57638549804688, loss=35.213802337646484
I0327 22:59:33.344390 139847503574784 logging_writer.py:48] [5] global_step=5, grad_norm=72.38390350341797, loss=17.777727127075195
I0327 22:59:34.222465 139847511967488 logging_writer.py:48] [6] global_step=6, grad_norm=18.399213790893555, loss=9.935687065124512
I0327 22:59:35.087011 139847503574784 logging_writer.py:48] [7] global_step=7, grad_norm=40.68380355834961, loss=10.71324348449707
I0327 22:59:35.958570 139847511967488 logging_writer.py:48] [8] global_step=8, grad_norm=12.159820556640625, loss=10.062654495239258
I0327 22:59:36.849656 139847503574784 logging_writer.py:48] [9] global_step=9, grad_norm=10.98617172241211, loss=9.687919616699219
I0327 22:59:37.732192 139847511967488 logging_writer.py:48] [10] global_step=10, grad_norm=8.711223602294922, loss=9.878527641296387
I0327 22:59:38.602923 139847503574784 logging_writer.py:48] [11] global_step=11, grad_norm=41.696170806884766, loss=11.748693466186523
I0327 22:59:39.479729 139847511967488 logging_writer.py:48] [12] global_step=12, grad_norm=288.3951721191406, loss=12.52628231048584
I0327 22:59:40.365177 139847503574784 logging_writer.py:48] [13] global_step=13, grad_norm=10.140883445739746, loss=10.453609466552734
I0327 22:59:41.241395 139847511967488 logging_writer.py:48] [14] global_step=14, grad_norm=46.05556106567383, loss=13.157698631286621
I0327 22:59:42.123312 139847503574784 logging_writer.py:48] [15] global_step=15, grad_norm=17.660526275634766, loss=12.647370338439941
I0327 22:59:43.004203 139847511967488 logging_writer.py:48] [16] global_step=16, grad_norm=7.759006500244141, loss=11.194345474243164
I0327 22:59:43.868202 139847503574784 logging_writer.py:48] [17] global_step=17, grad_norm=27.714019775390625, loss=14.51793384552002
I0327 22:59:44.730490 139847511967488 logging_writer.py:48] [18] global_step=18, grad_norm=8.571414947509766, loss=11.904937744140625
I0327 22:59:45.600913 139847503574784 logging_writer.py:48] [19] global_step=19, grad_norm=27.74016761779785, loss=12.199605941772461
I0327 22:59:46.478514 139847511967488 logging_writer.py:48] [20] global_step=20, grad_norm=8.686798095703125, loss=11.028548240661621
I0327 22:59:47.363588 139847503574784 logging_writer.py:48] [21] global_step=21, grad_norm=19.210294723510742, loss=11.607693672180176
I0327 22:59:48.240252 139847511967488 logging_writer.py:48] [22] global_step=22, grad_norm=20.37032127380371, loss=10.681381225585938
I0327 22:59:49.108333 139847503574784 logging_writer.py:48] [23] global_step=23, grad_norm=18.63340950012207, loss=10.621926307678223
I0327 22:59:49.986106 139847511967488 logging_writer.py:48] [24] global_step=24, grad_norm=28.67148780822754, loss=10.918842315673828
I0327 22:59:50.851132 139847503574784 logging_writer.py:48] [25] global_step=25, grad_norm=31.81778335571289, loss=11.055864334106445
I0327 22:59:51.711740 139847511967488 logging_writer.py:48] [26] global_step=26, grad_norm=14.858107566833496, loss=11.044894218444824
I0327 22:59:52.576985 139847503574784 logging_writer.py:48] [27] global_step=27, grad_norm=10.694900512695312, loss=10.775104522705078
I0327 22:59:53.470445 139847511967488 logging_writer.py:48] [28] global_step=28, grad_norm=13.786264419555664, loss=9.839905738830566
I0327 22:59:54.357063 139847503574784 logging_writer.py:48] [29] global_step=29, grad_norm=18.405956268310547, loss=10.887711524963379
I0327 22:59:55.217389 139847511967488 logging_writer.py:48] [30] global_step=30, grad_norm=38.28849411010742, loss=10.404261589050293
I0327 22:59:56.109724 139847503574784 logging_writer.py:48] [31] global_step=31, grad_norm=87.47917175292969, loss=12.427943229675293
I0327 22:59:56.978642 139847511967488 logging_writer.py:48] [32] global_step=32, grad_norm=73.54204559326172, loss=14.411886215209961
I0327 22:59:57.839019 139847503574784 logging_writer.py:48] [33] global_step=33, grad_norm=55.70272445678711, loss=14.573647499084473
I0327 22:59:58.699718 139847511967488 logging_writer.py:48] [34] global_step=34, grad_norm=17.032577514648438, loss=14.402585983276367
I0327 22:59:59.573704 139847503574784 logging_writer.py:48] [35] global_step=35, grad_norm=41.544673919677734, loss=15.358576774597168
I0327 23:00:00.451317 139847511967488 logging_writer.py:48] [36] global_step=36, grad_norm=13.963292121887207, loss=14.454111099243164
I0327 23:00:01.333436 139847503574784 logging_writer.py:48] [37] global_step=37, grad_norm=19.98210906982422, loss=15.080974578857422
I0327 23:00:02.194041 139847511967488 logging_writer.py:48] [38] global_step=38, grad_norm=8.844076156616211, loss=13.660122871398926
I0327 23:00:03.074357 139847503574784 logging_writer.py:48] [39] global_step=39, grad_norm=20.57880401611328, loss=13.403436660766602
I0327 23:00:03.930354 139847511967488 logging_writer.py:48] [40] global_step=40, grad_norm=15.187793731689453, loss=12.825100898742676
I0327 23:00:04.808964 139847503574784 logging_writer.py:48] [41] global_step=41, grad_norm=20.421377182006836, loss=12.82901668548584
I0327 23:00:05.675888 139847511967488 logging_writer.py:48] [42] global_step=42, grad_norm=40.50543212890625, loss=13.821401596069336
I0327 23:00:06.540136 139847503574784 logging_writer.py:48] [43] global_step=43, grad_norm=26.353450775146484, loss=13.726828575134277
I0327 23:00:07.403500 139847511967488 logging_writer.py:48] [44] global_step=44, grad_norm=25.53194808959961, loss=14.188077926635742
I0327 23:00:08.284482 139847503574784 logging_writer.py:48] [45] global_step=45, grad_norm=17.57447052001953, loss=13.475526809692383
I0327 23:00:09.148708 139847511967488 logging_writer.py:48] [46] global_step=46, grad_norm=39.466617584228516, loss=15.812016487121582
I0327 23:00:10.009592 139847503574784 logging_writer.py:48] [47] global_step=47, grad_norm=38.8421630859375, loss=14.856212615966797
I0327 23:00:10.879592 139847511967488 logging_writer.py:48] [48] global_step=48, grad_norm=15.161595344543457, loss=14.369712829589844
I0327 23:00:11.756140 139847503574784 logging_writer.py:48] [49] global_step=49, grad_norm=22.923686981201172, loss=11.63542652130127
I0327 23:00:12.608963 139847511967488 logging_writer.py:48] [50] global_step=50, grad_norm=105.81871795654297, loss=15.523965835571289
I0327 23:00:13.482266 139847503574784 logging_writer.py:48] [51] global_step=51, grad_norm=186.0799560546875, loss=52.65365219116211
I0327 23:00:14.355798 139847511967488 logging_writer.py:48] [52] global_step=52, grad_norm=90.31532287597656, loss=22.09746551513672
I0327 23:00:15.216363 139847503574784 logging_writer.py:48] [53] global_step=53, grad_norm=59.71247100830078, loss=31.999835968017578
I0327 23:00:16.085076 139847511967488 logging_writer.py:48] [54] global_step=54, grad_norm=62.351951599121094, loss=27.091793060302734
I0327 23:00:16.961820 139847503574784 logging_writer.py:48] [55] global_step=55, grad_norm=44.41258239746094, loss=21.545026779174805
I0327 23:00:17.817721 139847511967488 logging_writer.py:48] [56] global_step=56, grad_norm=26.022947311401367, loss=16.287979125976562
I0327 23:00:18.682181 139847503574784 logging_writer.py:48] [57] global_step=57, grad_norm=52.87066650390625, loss=21.13958168029785
I0327 23:00:19.563523 139847511967488 logging_writer.py:48] [58] global_step=58, grad_norm=103.25662994384766, loss=23.959928512573242
I0327 23:00:20.420898 139847503574784 logging_writer.py:48] [59] global_step=59, grad_norm=222.28390502929688, loss=34.39128875732422
I0327 23:00:21.295242 139847511967488 logging_writer.py:48] [60] global_step=60, grad_norm=164.12326049804688, loss=41.195491790771484
I0327 23:00:22.169856 139847503574784 logging_writer.py:48] [61] global_step=61, grad_norm=982.0359497070312, loss=53.35024642944336
I0327 23:00:23.050499 139847511967488 logging_writer.py:48] [62] global_step=62, grad_norm=67.80565643310547, loss=43.81918716430664
I0327 23:00:23.915916 139847503574784 logging_writer.py:48] [63] global_step=63, grad_norm=480.22259521484375, loss=33.42660140991211
I0327 23:00:24.778501 139847511967488 logging_writer.py:48] [64] global_step=64, grad_norm=3198.760986328125, loss=229.37442016601562
I0327 23:00:25.663680 139847503574784 logging_writer.py:48] [65] global_step=65, grad_norm=335.882080078125, loss=117.92878723144531
I0327 23:00:26.545972 139847511967488 logging_writer.py:48] [66] global_step=66, grad_norm=352.7521057128906, loss=70.72872924804688
I0327 23:00:27.424494 139847503574784 logging_writer.py:48] [67] global_step=67, grad_norm=125.65018463134766, loss=104.09408569335938
I0327 23:00:28.301334 139847511967488 logging_writer.py:48] [68] global_step=68, grad_norm=53.07916259765625, loss=78.81808471679688
I0327 23:00:29.173797 139847503574784 logging_writer.py:48] [69] global_step=69, grad_norm=249.6244354248047, loss=84.71537017822266
I0327 23:00:30.041933 139847511967488 logging_writer.py:48] [70] global_step=70, grad_norm=6383.7021484375, loss=230.9144287109375
I0327 23:00:30.903928 139847503574784 logging_writer.py:48] [71] global_step=71, grad_norm=420.4770812988281, loss=154.87855529785156
I0327 23:00:31.778442 139847511967488 logging_writer.py:48] [72] global_step=72, grad_norm=101.1380844116211, loss=127.4129867553711
I0327 23:00:32.641667 139847503574784 logging_writer.py:48] [73] global_step=73, grad_norm=216.99462890625, loss=107.13562774658203
I0327 23:00:33.523496 139847511967488 logging_writer.py:48] [74] global_step=74, grad_norm=332.9178161621094, loss=113.36396789550781
I0327 23:00:34.405092 139847503574784 logging_writer.py:48] [75] global_step=75, grad_norm=114.78267669677734, loss=87.15442657470703
I0327 23:00:35.262450 139847511967488 logging_writer.py:48] [76] global_step=76, grad_norm=393.2835998535156, loss=106.93982696533203
I0327 23:00:36.122162 139847503574784 logging_writer.py:48] [77] global_step=77, grad_norm=977.8799438476562, loss=247.51597595214844
I0327 23:00:36.984563 139847511967488 logging_writer.py:48] [78] global_step=78, grad_norm=105.53280639648438, loss=144.42001342773438
I0327 23:00:37.846479 139847503574784 logging_writer.py:48] [79] global_step=79, grad_norm=153.080078125, loss=150.24453735351562
I0327 23:00:38.707412 139847511967488 logging_writer.py:48] [80] global_step=80, grad_norm=156.5703887939453, loss=168.1502685546875
I0327 23:00:39.589808 139847503574784 logging_writer.py:48] [81] global_step=81, grad_norm=4374.583984375, loss=193.65811157226562
I0327 23:00:40.466033 139847511967488 logging_writer.py:48] [82] global_step=82, grad_norm=213.599365234375, loss=183.77923583984375
I0327 23:00:41.329185 139847503574784 logging_writer.py:48] [83] global_step=83, grad_norm=172.8108673095703, loss=211.00340270996094
I0327 23:00:42.187322 139847511967488 logging_writer.py:48] [84] global_step=84, grad_norm=121.88984680175781, loss=148.10650634765625
I0327 23:00:43.058771 139847503574784 logging_writer.py:48] [85] global_step=85, grad_norm=172.74227905273438, loss=147.955078125
I0327 23:00:43.923128 139847511967488 logging_writer.py:48] [86] global_step=86, grad_norm=89.75588989257812, loss=134.62826538085938
I0327 23:00:44.776584 139847503574784 logging_writer.py:48] [87] global_step=87, grad_norm=139.57582092285156, loss=123.26229858398438
I0327 23:00:45.655023 139847511967488 logging_writer.py:48] [88] global_step=88, grad_norm=97.26786041259766, loss=126.78312683105469
I0327 23:00:46.506852 139847503574784 logging_writer.py:48] [89] global_step=89, grad_norm=2142.227294921875, loss=112.53228759765625
I0327 23:00:47.380972 139847511967488 logging_writer.py:48] [90] global_step=90, grad_norm=1001.1883544921875, loss=309.75762939453125
I0327 23:00:48.254702 139847503574784 logging_writer.py:48] [91] global_step=91, grad_norm=4422.123046875, loss=221.08534240722656
I0327 23:00:49.120502 139847511967488 logging_writer.py:48] [92] global_step=92, grad_norm=206.83401489257812, loss=143.5812530517578
I0327 23:00:49.991282 139847503574784 logging_writer.py:48] [93] global_step=93, grad_norm=297.8638000488281, loss=137.06951904296875
I0327 23:00:50.852908 139847511967488 logging_writer.py:48] [94] global_step=94, grad_norm=117.91765594482422, loss=137.50819396972656
I0327 23:00:51.718211 139847503574784 logging_writer.py:48] [95] global_step=95, grad_norm=101.0356216430664, loss=138.37599182128906
I0327 23:00:52.572049 139847511967488 logging_writer.py:48] [96] global_step=96, grad_norm=83.07832336425781, loss=139.7930908203125
I0327 23:00:53.428943 139847503574784 logging_writer.py:48] [97] global_step=97, grad_norm=191.2527618408203, loss=152.89129638671875
I0327 23:00:54.288351 139847511967488 logging_writer.py:48] [98] global_step=98, grad_norm=248.24310302734375, loss=147.79623413085938
I0327 23:00:55.154241 139847503574784 logging_writer.py:48] [99] global_step=99, grad_norm=2332.895263671875, loss=530.0921630859375
I0327 23:00:56.024444 139847511967488 logging_writer.py:48] [100] global_step=100, grad_norm=2002.96142578125, loss=440.34124755859375
I0327 23:05:53.576675 139847503574784 logging_writer.py:48] [500] global_step=500, grad_norm=0.0, loss=1870.6484375
I0327 23:12:18.154168 139847511967488 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.0, loss=1798.3717041015625
I0327 23:18:34.418866 139848200898304 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0, loss=1789.446533203125
I0327 23:23:22.720541 140003960837952 spec.py:321] Evaluating on the training split.
I0327 23:24:00.708888 140003960837952 spec.py:333] Evaluating on the validation split.
I0327 23:24:44.158421 140003960837952 spec.py:349] Evaluating on the test split.
I0327 23:25:04.826224 140003960837952 submission_runner.py:422] Time since start: 1778.75s, 	Step: 1889, 	{'train/ctc_loss': Array(1767.6814, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1480.2585592269897, 'total_duration': 1778.7481315135956, 'accumulated_submission_time': 1480.2585592269897, 'accumulated_eval_time': 298.37354612350464, 'accumulated_logging_time': 0.04358315467834473}
I0327 23:25:04.864681 139848200898304 logging_writer.py:48] [1889] accumulated_eval_time=298.373546, accumulated_logging_time=0.043583, accumulated_submission_time=1480.258559, global_step=1889, preemption_count=0, score=1480.258559, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=1778.748132, train/ctc_loss=1767.681396484375, train/wer=0.944636, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0327 23:26:28.027677 139848192505600 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.0, loss=1833.0103759765625
I0327 23:32:42.512932 139848200898304 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0, loss=1876.81982421875
I0327 23:39:11.253114 139848192505600 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0, loss=1808.9190673828125
I0327 23:45:30.972473 139848200898304 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0, loss=1809.430419921875
I0327 23:49:05.132987 140003960837952 spec.py:321] Evaluating on the training split.
I0327 23:49:43.463473 140003960837952 spec.py:333] Evaluating on the validation split.
I0327 23:50:26.892595 140003960837952 spec.py:349] Evaluating on the test split.
I0327 23:50:47.946739 140003960837952 submission_runner.py:422] Time since start: 3321.87s, 	Step: 3790, 	{'train/ctc_loss': Array(1761.5704, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2920.4347846508026, 'total_duration': 3321.8674907684326, 'accumulated_submission_time': 2920.4347846508026, 'accumulated_eval_time': 401.18030858039856, 'accumulated_logging_time': 0.09850454330444336}
I0327 23:50:47.997263 139848200898304 logging_writer.py:48] [3790] accumulated_eval_time=401.180309, accumulated_logging_time=0.098505, accumulated_submission_time=2920.434785, global_step=3790, preemption_count=0, score=2920.434785, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=3321.867491, train/ctc_loss=1761.5704345703125, train/wer=0.942722, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0327 23:53:24.625012 139848192505600 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0, loss=1821.9217529296875
I0327 23:59:38.920633 139848200898304 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0, loss=1912.5755615234375
I0328 00:06:05.526252 139848192505600 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1781.10498046875
I0328 00:12:29.856461 139848200898304 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0, loss=1778.5064697265625
I0328 00:14:48.172254 140003960837952 spec.py:321] Evaluating on the training split.
I0328 00:15:27.173009 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 00:16:10.646797 140003960837952 spec.py:349] Evaluating on the test split.
I0328 00:16:31.561881 140003960837952 submission_runner.py:422] Time since start: 4865.48s, 	Step: 5687, 	{'train/ctc_loss': Array(1741.2977, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4360.514379024506, 'total_duration': 4865.484000682831, 'accumulated_submission_time': 4360.514379024506, 'accumulated_eval_time': 504.5643231868744, 'accumulated_logging_time': 0.16921186447143555}
I0328 00:16:31.595735 139847678650112 logging_writer.py:48] [5687] accumulated_eval_time=504.564323, accumulated_logging_time=0.169212, accumulated_submission_time=4360.514379, global_step=5687, preemption_count=0, score=4360.514379, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=4865.484001, train/ctc_loss=1741.2977294921875, train/wer=0.943324, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 00:20:25.306896 139847670257408 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1868.327392578125
I0328 00:26:47.483927 139847678650112 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1959.27001953125
I0328 00:33:14.494485 139847670257408 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0, loss=1822.5703125
I0328 00:39:46.732891 139847678650112 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1838.935546875
I0328 00:40:31.813746 140003960837952 spec.py:321] Evaluating on the training split.
I0328 00:41:10.126666 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 00:41:53.293737 140003960837952 spec.py:349] Evaluating on the test split.
I0328 00:42:14.080591 140003960837952 submission_runner.py:422] Time since start: 6408.00s, 	Step: 7562, 	{'train/ctc_loss': Array(1724.8612, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 5800.64427113533, 'total_duration': 6408.002635240555, 'accumulated_submission_time': 5800.64427113533, 'accumulated_eval_time': 606.8254625797272, 'accumulated_logging_time': 0.21919870376586914}
I0328 00:42:14.115491 139847678650112 logging_writer.py:48] [7562] accumulated_eval_time=606.825463, accumulated_logging_time=0.219199, accumulated_submission_time=5800.644271, global_step=7562, preemption_count=0, score=5800.644271, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=6408.002635, train/ctc_loss=1724.8612060546875, train/wer=0.943700, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 00:47:42.543626 139847670257408 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.0, loss=1873.9346923828125
I0328 00:54:16.022578 139847678650112 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0, loss=1873.1119384765625
I0328 01:00:39.446782 139847670257408 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0, loss=1804.3292236328125
I0328 01:06:14.747223 140003960837952 spec.py:321] Evaluating on the training split.
I0328 01:06:53.758611 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 01:07:36.780831 140003960837952 spec.py:349] Evaluating on the test split.
I0328 01:07:57.851672 140003960837952 submission_runner.py:422] Time since start: 7951.77s, 	Step: 9416, 	{'train/ctc_loss': Array(1832.9286, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7241.184892416, 'total_duration': 7951.773111820221, 'accumulated_submission_time': 7241.184892416, 'accumulated_eval_time': 709.9238367080688, 'accumulated_logging_time': 0.27213096618652344}
I0328 01:07:57.889178 139847883450112 logging_writer.py:48] [9416] accumulated_eval_time=709.923837, accumulated_logging_time=0.272131, accumulated_submission_time=7241.184892, global_step=9416, preemption_count=0, score=7241.184892, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=7951.773112, train/ctc_loss=1832.9285888671875, train/wer=0.941551, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 01:09:01.171970 139847566018304 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.0, loss=1824.388671875
I0328 01:15:13.992331 139847883450112 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.0, loss=1863.431396484375
I0328 01:21:51.525898 139847883450112 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.0, loss=1819.849365234375
I0328 01:28:09.369521 139847566018304 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.0, loss=1782.4691162109375
I0328 01:31:58.136109 140003960837952 spec.py:321] Evaluating on the training split.
I0328 01:32:40.013010 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 01:33:23.308686 140003960837952 spec.py:349] Evaluating on the test split.
I0328 01:33:44.257421 140003960837952 submission_runner.py:422] Time since start: 9498.18s, 	Step: 11280, 	{'train/ctc_loss': Array(1752.8002, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 8681.343116283417, 'total_duration': 9498.179074287415, 'accumulated_submission_time': 8681.343116283417, 'accumulated_eval_time': 816.0390911102295, 'accumulated_logging_time': 0.324566125869751}
I0328 01:33:44.295264 139847934650112 logging_writer.py:48] [11280] accumulated_eval_time=816.039091, accumulated_logging_time=0.324566, accumulated_submission_time=8681.343116, global_step=11280, preemption_count=0, score=8681.343116, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=9498.179074, train/ctc_loss=1752.8001708984375, train/wer=0.942641, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 01:36:31.757851 139847934650112 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.0, loss=1797.3616943359375
I0328 01:42:49.807266 139847926257408 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.0, loss=1863.83837890625
I0328 01:49:36.313422 139847504570112 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.0, loss=1820.6259765625
I0328 01:55:49.377537 139846953727744 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.0, loss=1755.2158203125
I0328 01:57:44.795983 140003960837952 spec.py:321] Evaluating on the training split.
I0328 01:58:24.167456 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 01:59:07.481452 140003960837952 spec.py:349] Evaluating on the test split.
I0328 01:59:28.437831 140003960837952 submission_runner.py:422] Time since start: 11042.36s, 	Step: 13142, 	{'train/ctc_loss': Array(1746.1107, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 10121.755238056183, 'total_duration': 11042.359280586243, 'accumulated_submission_time': 10121.755238056183, 'accumulated_eval_time': 919.6746597290039, 'accumulated_logging_time': 0.37816405296325684}
I0328 01:59:28.475576 139847504570112 logging_writer.py:48] [13142] accumulated_eval_time=919.674660, accumulated_logging_time=0.378164, accumulated_submission_time=10121.755238, global_step=13142, preemption_count=0, score=10121.755238, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=11042.359281, train/ctc_loss=1746.1107177734375, train/wer=0.942824, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 02:03:57.845131 139847504570112 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.0, loss=1747.6668701171875
I0328 02:10:09.905663 139846953727744 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.0, loss=1859.7763671875
I0328 02:17:02.662522 139847504570112 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.0, loss=1836.4290771484375
I0328 02:23:14.510995 139846953727744 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.0, loss=1771.4913330078125
I0328 02:23:28.581239 140003960837952 spec.py:321] Evaluating on the training split.
I0328 02:24:08.801825 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 02:24:52.303905 140003960837952 spec.py:349] Evaluating on the test split.
I0328 02:25:13.503427 140003960837952 submission_runner.py:422] Time since start: 12587.42s, 	Step: 15020, 	{'train/ctc_loss': Array(1733.7391, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 11561.767615318298, 'total_duration': 12587.424719333649, 'accumulated_submission_time': 11561.767615318298, 'accumulated_eval_time': 1024.5904006958008, 'accumulated_logging_time': 0.4345240592956543}
I0328 02:25:13.537506 139847504570112 logging_writer.py:48] [15020] accumulated_eval_time=1024.590401, accumulated_logging_time=0.434524, accumulated_submission_time=11561.767615, global_step=15020, preemption_count=0, score=11561.767615, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=12587.424719, train/ctc_loss=1733.7391357421875, train/wer=0.944086, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 02:31:20.663775 139847504570112 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.0, loss=1822.1810302734375
I0328 02:37:32.225768 139846953727744 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.0, loss=1727.033935546875
I0328 02:44:24.530980 139847504570112 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.0, loss=1818.427490234375
I0328 02:49:13.522173 140003960837952 spec.py:321] Evaluating on the training split.
I0328 02:49:52.250547 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 02:50:35.500117 140003960837952 spec.py:349] Evaluating on the test split.
I0328 02:50:56.295197 140003960837952 submission_runner.py:422] Time since start: 14130.22s, 	Step: 16891, 	{'train/ctc_loss': Array(1786.8646, dtype=float32), 'train/wer': 0.9427990785714666, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 13001.662180423737, 'total_duration': 14130.217215776443, 'accumulated_submission_time': 13001.662180423737, 'accumulated_eval_time': 1127.3577146530151, 'accumulated_logging_time': 0.48458337783813477}
I0328 02:50:56.326960 139847504570112 logging_writer.py:48] [16891] accumulated_eval_time=1127.357715, accumulated_logging_time=0.484583, accumulated_submission_time=13001.662180, global_step=16891, preemption_count=0, score=13001.662180, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=14130.217216, train/ctc_loss=1786.8646240234375, train/wer=0.942799, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 02:52:18.130282 139846953727744 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.0, loss=1804.96533203125
I0328 02:58:49.117699 139847504570112 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.0, loss=1828.036376953125
I0328 03:05:03.825015 139847504570112 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.0, loss=1900.50634765625
I0328 03:11:46.474185 139846953727744 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.0, loss=1783.5869140625
I0328 03:14:56.968028 140003960837952 spec.py:321] Evaluating on the training split.
I0328 03:15:35.436502 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 03:16:19.205933 140003960837952 spec.py:349] Evaluating on the test split.
I0328 03:16:40.338491 140003960837952 submission_runner.py:422] Time since start: 15674.26s, 	Step: 18750, 	{'train/ctc_loss': Array(1755.9376, dtype=float32), 'train/wer': 0.9423383225986367, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14442.213552474976, 'total_duration': 15674.260643720627, 'accumulated_submission_time': 14442.213552474976, 'accumulated_eval_time': 1230.7227623462677, 'accumulated_logging_time': 0.5332691669464111}
I0328 03:16:40.375674 139847504570112 logging_writer.py:48] [18750] accumulated_eval_time=1230.722762, accumulated_logging_time=0.533269, accumulated_submission_time=14442.213552, global_step=18750, preemption_count=0, score=14442.213552, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=15674.260644, train/ctc_loss=1755.9376220703125, train/wer=0.942338, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 03:19:46.965583 139846953727744 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.0, loss=1894.8795166015625
I0328 03:26:19.017141 139847504570112 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.0, loss=1805.601806640625
I0328 03:32:39.157638 139847504570112 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.0, loss=1835.63916015625
I0328 03:39:18.463224 139846953727744 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.0, loss=1823.73876953125
I0328 03:40:43.736063 140003960837952 spec.py:321] Evaluating on the training split.
I0328 03:41:23.350895 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 03:42:06.963170 140003960837952 spec.py:349] Evaluating on the test split.
I0328 03:42:28.128354 140003960837952 submission_runner.py:422] Time since start: 17222.05s, 	Step: 20601, 	{'train/ctc_loss': Array(1731.249, dtype=float32), 'train/wer': 0.9431396916893625, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15885.487879753113, 'total_duration': 17222.049082756042, 'accumulated_submission_time': 15885.487879753113, 'accumulated_eval_time': 1335.1082394123077, 'accumulated_logging_time': 0.5847785472869873}
I0328 03:42:28.164388 139847806650112 logging_writer.py:48] [20601] accumulated_eval_time=1335.108239, accumulated_logging_time=0.584779, accumulated_submission_time=15885.487880, global_step=20601, preemption_count=0, score=15885.487880, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=17222.049083, train/ctc_loss=1731.2490234375, train/wer=0.943140, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 03:47:26.020654 139847798257408 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.0, loss=1786.2001953125
I0328 03:53:51.208065 139847806650112 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.0, loss=1775.2996826171875
I0328 04:00:16.730912 139847806650112 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.0, loss=1796.8570556640625
I0328 04:06:28.203995 140003960837952 spec.py:321] Evaluating on the training split.
I0328 04:07:07.504229 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 04:07:51.386860 140003960837952 spec.py:349] Evaluating on the test split.
I0328 04:08:12.862779 140003960837952 submission_runner.py:422] Time since start: 18766.78s, 	Step: 22476, 	{'train/ctc_loss': Array(1763.6163, dtype=float32), 'train/wer': 0.9432716912443612, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 17325.43764925003, 'total_duration': 18766.78422665596, 'accumulated_submission_time': 17325.43764925003, 'accumulated_eval_time': 1439.7607336044312, 'accumulated_logging_time': 0.6356685161590576}
I0328 04:08:12.903489 139847883450112 logging_writer.py:48] [22476] accumulated_eval_time=1439.760734, accumulated_logging_time=0.635669, accumulated_submission_time=17325.437649, global_step=22476, preemption_count=0, score=17325.437649, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=18766.784227, train/ctc_loss=1763.6163330078125, train/wer=0.943272, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 04:08:31.604994 139847745218304 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.0, loss=1863.5672607421875
I0328 04:14:46.482887 139847883450112 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.0, loss=1826.6016845703125
I0328 04:21:20.835556 139847745218304 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.0, loss=1814.818115234375
I0328 04:27:50.621942 139847883450112 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.0, loss=1834.5865478515625
I0328 04:32:13.966913 140003960837952 spec.py:321] Evaluating on the training split.
I0328 04:32:53.163168 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 04:33:36.666022 140003960837952 spec.py:349] Evaluating on the test split.
I0328 04:33:57.714664 140003960837952 submission_runner.py:422] Time since start: 20311.64s, 	Step: 24348, 	{'train/ctc_loss': Array(1739.3485, dtype=float32), 'train/wer': 0.944685667249717, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 18766.40936422348, 'total_duration': 20311.63661837578, 'accumulated_submission_time': 18766.40936422348, 'accumulated_eval_time': 1543.5027351379395, 'accumulated_logging_time': 0.6933820247650146}
I0328 04:33:57.748414 139847576250112 logging_writer.py:48] [24348] accumulated_eval_time=1543.502735, accumulated_logging_time=0.693382, accumulated_submission_time=18766.409364, global_step=24348, preemption_count=0, score=18766.409364, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=20311.636618, train/ctc_loss=1739.3485107421875, train/wer=0.944686, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 04:35:51.672874 139847567857408 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.0, loss=1834.849609375
I0328 04:42:09.296508 139847576250112 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.0, loss=1820.75537109375
I0328 04:48:40.236173 139847567857408 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.0, loss=1823.8687744140625
I0328 04:55:18.512741 139847576250112 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.0, loss=1861.399169921875
I0328 04:57:58.326702 140003960837952 spec.py:321] Evaluating on the training split.
I0328 04:58:37.353018 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 04:59:21.010381 140003960837952 spec.py:349] Evaluating on the test split.
I0328 04:59:42.342700 140003960837952 submission_runner.py:422] Time since start: 21856.26s, 	Step: 26216, 	{'train/ctc_loss': Array(1769.4734, dtype=float32), 'train/wer': 0.9432456399645285, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 20206.899822950363, 'total_duration': 21856.264822006226, 'accumulated_submission_time': 20206.899822950363, 'accumulated_eval_time': 1647.5131170749664, 'accumulated_logging_time': 0.7416789531707764}
I0328 04:59:42.378848 139847893698304 logging_writer.py:48] [26216] accumulated_eval_time=1647.513117, accumulated_logging_time=0.741679, accumulated_submission_time=20206.899823, global_step=26216, preemption_count=0, score=20206.899823, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=21856.264822, train/ctc_loss=1769.473388671875, train/wer=0.943246, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 05:03:14.677167 139847750326016 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.0, loss=1886.22314453125
I0328 05:09:45.285765 139847893698304 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.0, loss=1851.972900390625
I0328 05:16:07.400265 139847750326016 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.0, loss=1838.1434326171875
I0328 05:22:48.378684 139847893698304 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.0, loss=1800.395263671875
I0328 05:23:43.053690 140003960837952 spec.py:321] Evaluating on the training split.
I0328 05:24:22.016120 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 05:25:05.697542 140003960837952 spec.py:349] Evaluating on the test split.
I0328 05:25:26.935289 140003960837952 submission_runner.py:422] Time since start: 23400.86s, 	Step: 28075, 	{'train/ctc_loss': Array(1736.9209, dtype=float32), 'train/wer': 0.9439109001278072, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 21647.483785390854, 'total_duration': 23400.860488414764, 'accumulated_submission_time': 21647.483785390854, 'accumulated_eval_time': 1751.3921954631805, 'accumulated_logging_time': 0.7965822219848633}
I0328 05:25:26.961882 139847965374208 logging_writer.py:48] [28075] accumulated_eval_time=1751.392195, accumulated_logging_time=0.796582, accumulated_submission_time=21647.483785, global_step=28075, preemption_count=0, score=21647.483785, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=23400.860488, train/ctc_loss=1736.9208984375, train/wer=0.943911, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 05:30:42.862147 139847956981504 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.0, loss=1857.0782470703125
I0328 05:37:26.272747 139847965374208 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.0, loss=1833.666748046875
I0328 05:43:42.933257 139847956981504 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.0, loss=1850.9017333984375
I0328 05:49:27.048068 140003960837952 spec.py:321] Evaluating on the training split.
I0328 05:50:06.251113 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 05:50:50.548683 140003960837952 spec.py:349] Evaluating on the test split.
I0328 05:51:11.434046 140003960837952 submission_runner.py:422] Time since start: 24945.36s, 	Step: 29918, 	{'train/ctc_loss': Array(1715.2468, dtype=float32), 'train/wer': 0.9450143703143059, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 23087.483112573624, 'total_duration': 24945.35572862625, 'accumulated_submission_time': 23087.483112573624, 'accumulated_eval_time': 1855.772367477417, 'accumulated_logging_time': 0.8384244441986084}
I0328 05:51:11.466110 139847699126016 logging_writer.py:48] [29918] accumulated_eval_time=1855.772367, accumulated_logging_time=0.838424, accumulated_submission_time=23087.483113, global_step=29918, preemption_count=0, score=23087.483113, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=24945.355729, train/ctc_loss=1715.246826171875, train/wer=0.945014, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 05:52:12.961820 139847690733312 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.0, loss=1775.6693115234375
I0328 05:58:24.187385 139847699126016 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.0, loss=1820.237548828125
I0328 06:05:08.524608 139847699126016 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.0, loss=1843.0396728515625
I0328 06:11:23.056008 139847690733312 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.0, loss=1824.9088134765625
I0328 06:15:11.893420 140003960837952 spec.py:321] Evaluating on the training split.
I0328 06:15:53.700056 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 06:16:37.261105 140003960837952 spec.py:349] Evaluating on the test split.
I0328 06:16:58.643164 140003960837952 submission_runner.py:422] Time since start: 26492.56s, 	Step: 31775, 	{'train/ctc_loss': Array(1783.4976, dtype=float32), 'train/wer': 0.9417576703068122, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 24527.82481122017, 'total_duration': 26492.56491136551, 'accumulated_submission_time': 24527.82481122017, 'accumulated_eval_time': 1962.516150712967, 'accumulated_logging_time': 0.882831335067749}
I0328 06:16:58.683417 139847704250112 logging_writer.py:48] [31775] accumulated_eval_time=1962.516151, accumulated_logging_time=0.882831, accumulated_submission_time=24527.824811, global_step=31775, preemption_count=0, score=24527.824811, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=26492.564911, train/ctc_loss=1783.49755859375, train/wer=0.941758, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 06:19:49.306335 139847704250112 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.0, loss=1751.613037109375
I0328 06:26:03.677430 139847695857408 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.0, loss=1776.4085693359375
I0328 06:32:59.061263 139847704250112 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.0, loss=1832.6165771484375
I0328 06:39:09.799412 139847695857408 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.0, loss=1848.362548828125
I0328 06:40:59.219550 140003960837952 spec.py:321] Evaluating on the training split.
I0328 06:41:39.131599 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 06:42:22.555898 140003960837952 spec.py:349] Evaluating on the test split.
I0328 06:42:44.110029 140003960837952 submission_runner.py:422] Time since start: 28038.03s, 	Step: 33636, 	{'train/ctc_loss': Array(1824.6915, dtype=float32), 'train/wer': 0.9416600198590335, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 25968.27384829521, 'total_duration': 28038.030095100403, 'accumulated_submission_time': 25968.27384829521, 'accumulated_eval_time': 2067.39896941185, 'accumulated_logging_time': 0.9372401237487793}
I0328 06:42:44.143736 139847842498304 logging_writer.py:48] [33636] accumulated_eval_time=2067.398969, accumulated_logging_time=0.937240, accumulated_submission_time=25968.273848, global_step=33636, preemption_count=0, score=25968.273848, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=28038.030095, train/ctc_loss=1824.6915283203125, train/wer=0.941660, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 06:47:19.099668 139847842498304 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.0, loss=1828.820068359375
I0328 06:53:29.934744 139847834105600 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.0, loss=1770.8785400390625
I0328 07:00:18.720562 139847842498304 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.0, loss=1796.3526611328125
I0328 07:06:33.421504 139847842498304 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.0, loss=1827.9058837890625
I0328 07:06:44.448513 140003960837952 spec.py:321] Evaluating on the training split.
I0328 07:07:23.358050 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 07:08:07.055599 140003960837952 spec.py:349] Evaluating on the test split.
I0328 07:08:28.664057 140003960837952 submission_runner.py:422] Time since start: 29582.59s, 	Step: 35516, 	{'train/ctc_loss': Array(1692.7736, dtype=float32), 'train/wer': 0.9447677853176417, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 27408.490597724915, 'total_duration': 29582.585765600204, 'accumulated_submission_time': 27408.490597724915, 'accumulated_eval_time': 2171.608489751816, 'accumulated_logging_time': 0.9847252368927002}
I0328 07:08:28.701342 139847842498304 logging_writer.py:48] [35516] accumulated_eval_time=2171.608490, accumulated_logging_time=0.984725, accumulated_submission_time=27408.490598, global_step=35516, preemption_count=0, score=27408.490598, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=29582.585766, train/ctc_loss=1692.7735595703125, train/wer=0.944768, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 07:14:41.919674 139847834105600 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.0, loss=1839.8607177734375
I0328 07:21:00.373636 139847514818304 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.0, loss=1784.8304443359375
I0328 07:27:43.570338 139847506425600 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.0, loss=1796.3526611328125
I0328 07:32:28.844486 140003960837952 spec.py:321] Evaluating on the training split.
I0328 07:33:07.592609 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 07:33:50.552000 140003960837952 spec.py:349] Evaluating on the test split.
I0328 07:34:11.637285 140003960837952 submission_runner.py:422] Time since start: 31125.56s, 	Step: 37373, 	{'train/ctc_loss': Array(1787.1794, dtype=float32), 'train/wer': 0.9427091658940503, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 28848.541474819183, 'total_duration': 31125.562560081482, 'accumulated_submission_time': 28848.541474819183, 'accumulated_eval_time': 2274.3990318775177, 'accumulated_logging_time': 1.0396275520324707}
I0328 07:34:11.670350 139847514818304 logging_writer.py:48] [37373] accumulated_eval_time=2274.399032, accumulated_logging_time=1.039628, accumulated_submission_time=28848.541475, global_step=37373, preemption_count=0, score=28848.541475, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=31125.562560, train/ctc_loss=1787.179443359375, train/wer=0.942709, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 07:35:46.750963 139847506425600 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.0, loss=1873.1119384765625
I0328 07:42:19.500171 139847514818304 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.0, loss=1858.561279296875
I0328 07:48:43.214996 139847842498304 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.0, loss=1804.07470703125
I0328 07:55:23.598249 139847834105600 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.0, loss=1821.7921142578125
I0328 07:58:11.685383 140003960837952 spec.py:321] Evaluating on the training split.
I0328 07:58:50.305210 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 07:59:34.136280 140003960837952 spec.py:349] Evaluating on the test split.
I0328 07:59:55.159857 140003960837952 submission_runner.py:422] Time since start: 32669.08s, 	Step: 39208, 	{'train/ctc_loss': Array(1714.328, dtype=float32), 'train/wer': 0.9448971433842748, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 30288.469445466995, 'total_duration': 32669.081364393234, 'accumulated_submission_time': 30288.469445466995, 'accumulated_eval_time': 2377.8674325942993, 'accumulated_logging_time': 1.0864319801330566}
I0328 07:59:55.193935 139847514818304 logging_writer.py:48] [39208] accumulated_eval_time=2377.867433, accumulated_logging_time=1.086432, accumulated_submission_time=30288.469445, global_step=39208, preemption_count=0, score=30288.469445, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=32669.081364, train/ctc_loss=1714.3280029296875, train/wer=0.944897, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 08:03:33.136458 139847506425600 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.0, loss=1841.5814208984375
I0328 08:10:00.160303 139847514818304 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.0, loss=1870.2384033203125
I0328 08:16:29.937924 139847514818304 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.0, loss=1783.83544921875
I0328 08:23:02.528786 139847506425600 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.0, loss=1817.7818603515625
I0328 08:23:55.195428 140003960837952 spec.py:321] Evaluating on the training split.
I0328 08:24:34.423229 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 08:25:18.279637 140003960837952 spec.py:349] Evaluating on the test split.
I0328 08:25:39.543786 140003960837952 submission_runner.py:422] Time since start: 34213.47s, 	Step: 41064, 	{'train/ctc_loss': Array(1760.6877, dtype=float32), 'train/wer': 0.9432324554919642, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 31728.385579824448, 'total_duration': 34213.46521806717, 'accumulated_submission_time': 31728.385579824448, 'accumulated_eval_time': 2482.2094728946686, 'accumulated_logging_time': 1.13423752784729}
I0328 08:25:39.576295 139847514818304 logging_writer.py:48] [41064] accumulated_eval_time=2482.209473, accumulated_logging_time=1.134238, accumulated_submission_time=31728.385580, global_step=41064, preemption_count=0, score=31728.385580, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=34213.465218, train/ctc_loss=1760.687744140625, train/wer=0.943232, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 08:31:07.236306 139847514818304 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.0, loss=1827.3839111328125
I0328 08:37:39.869726 139847506425600 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.0, loss=1801.028564453125
I0328 08:44:14.017138 139847514818304 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.0, loss=1865.0606689453125
I0328 08:49:40.112892 140003960837952 spec.py:321] Evaluating on the training split.
I0328 08:50:19.853248 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 08:51:03.161791 140003960837952 spec.py:349] Evaluating on the test split.
I0328 08:51:24.469542 140003960837952 submission_runner.py:422] Time since start: 35758.39s, 	Step: 42928, 	{'train/ctc_loss': Array(1852.6233, dtype=float32), 'train/wer': 0.941680272071945, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 33168.835923194885, 'total_duration': 35758.39141345024, 'accumulated_submission_time': 33168.835923194885, 'accumulated_eval_time': 2586.5602476596832, 'accumulated_logging_time': 1.1817538738250732}
I0328 08:51:24.515180 139847514818304 logging_writer.py:48] [42928] accumulated_eval_time=2586.560248, accumulated_logging_time=1.181754, accumulated_submission_time=33168.835923, global_step=42928, preemption_count=0, score=33168.835923, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=35758.391413, train/ctc_loss=1852.623291015625, train/wer=0.941680, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 08:52:19.002156 139847506425600 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.0, loss=1857.212890625
I0328 08:58:33.400272 139847514818304 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.0, loss=1838.5394287109375
I0328 09:04:58.101847 139847506425600 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.0, loss=1784.8304443359375
I0328 09:11:38.809433 139847514818304 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.0, loss=1837.219970703125
I0328 09:15:24.851366 140003960837952 spec.py:321] Evaluating on the training split.
I0328 09:16:03.779488 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 09:16:46.629217 140003960837952 spec.py:349] Evaluating on the test split.
I0328 09:17:07.616280 140003960837952 submission_runner.py:422] Time since start: 37301.54s, 	Step: 44806, 	{'train/ctc_loss': Array(1901.3729, dtype=float32), 'train/wer': 0.940312095793757, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 34609.07900071144, 'total_duration': 37301.53804039955, 'accumulated_submission_time': 34609.07900071144, 'accumulated_eval_time': 2689.319209098816, 'accumulated_logging_time': 1.2480909824371338}
I0328 09:17:07.648655 139847842498304 logging_writer.py:48] [44806] accumulated_eval_time=2689.319209, accumulated_logging_time=1.248091, accumulated_submission_time=34609.079001, global_step=44806, preemption_count=0, score=34609.079001, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=37301.538040, train/ctc_loss=1901.3729248046875, train/wer=0.940312, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 09:19:32.401196 139847834105600 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.0, loss=1842.2440185546875
I0328 09:26:00.966416 139847514818304 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.0, loss=1769.899169921875
I0328 09:32:21.284005 139847506425600 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.0, loss=1846.629150390625
I0328 09:39:05.693021 139847514818304 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.0, loss=1807.8970947265625
I0328 09:41:08.111262 140003960837952 spec.py:321] Evaluating on the training split.
I0328 09:41:47.321322 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 09:42:30.746637 140003960837952 spec.py:349] Evaluating on the test split.
I0328 09:42:51.727223 140003960837952 submission_runner.py:422] Time since start: 38845.65s, 	Step: 46667, 	{'train/ctc_loss': Array(1959.8695, dtype=float32), 'train/wer': 0.9371047844119075, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 36049.45591020584, 'total_duration': 38845.65246319771, 'accumulated_submission_time': 36049.45591020584, 'accumulated_eval_time': 2792.9326734542847, 'accumulated_logging_time': 1.2944605350494385}
I0328 09:42:51.757672 139847514818304 logging_writer.py:48] [46667] accumulated_eval_time=2792.932673, accumulated_logging_time=1.294461, accumulated_submission_time=36049.455910, global_step=46667, preemption_count=0, score=36049.455910, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=38845.652463, train/ctc_loss=1959.8695068359375, train/wer=0.937105, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 09:46:59.175757 139847506425600 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.0, loss=1851.5711669921875
I0328 09:53:41.550635 139847514818304 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.0, loss=1840.522216796875
I0328 09:59:54.450318 140003960837952 spec.py:321] Evaluating on the training split.
I0328 10:00:33.754060 140003960837952 spec.py:333] Evaluating on the validation split.
I0328 10:01:17.472415 140003960837952 spec.py:349] Evaluating on the test split.
I0328 10:01:38.356228 140003960837952 submission_runner.py:422] Time since start: 39972.28s, 	Step: 48000, 	{'train/ctc_loss': Array(2009.277, dtype=float32), 'train/wer': 0.9366967129626904, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 37072.079999923706, 'total_duration': 39972.278036117554, 'accumulated_submission_time': 37072.079999923706, 'accumulated_eval_time': 2896.8326518535614, 'accumulated_logging_time': 1.3415584564208984}
I0328 10:01:38.394634 139847514818304 logging_writer.py:48] [48000] accumulated_eval_time=2896.832652, accumulated_logging_time=1.341558, accumulated_submission_time=37072.080000, global_step=48000, preemption_count=0, score=37072.080000, test/ctc_loss=3189.8603515625, test/num_examples=2472, test/wer=0.899580, total_duration=39972.278036, train/ctc_loss=2009.2769775390625, train/wer=0.936697, validation/ctc_loss=3357.921875, validation/num_examples=5348, validation/wer=0.896618
I0328 10:01:38.423256 139847506425600 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=37072.080000
I0328 10:01:39.617930 140003960837952 submission_runner.py:596] Tuning trial 1/1
I0328 10:01:39.618195 140003960837952 submission_runner.py:597] Hyperparameters: Hyperparameters(learning_rate=4.131896390902391, beta1=0.9274758113254791, beta2=0.9978504782314613, warmup_steps=6999, decay_steps_factor=0.9007765761611038, end_factor=0.001, weight_decay=5.6687777311501786e-06, label_smoothing=0.2)
I0328 10:01:39.630179 140003960837952 submission_runner.py:598] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(30.895857, dtype=float32), 'train/wer': 2.8071913458134645, 'validation/ctc_loss': Array(29.951494, dtype=float32), 'validation/wer': 2.573940160460334, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.239141, dtype=float32), 'test/wer': 2.687282919992688, 'test/num_examples': 2472, 'score': 40.231555700302124, 'total_duration': 236.50531101226807, 'accumulated_submission_time': 40.231555700302124, 'accumulated_eval_time': 196.273695230484, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1889, {'train/ctc_loss': Array(1767.6814, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1480.2585592269897, 'total_duration': 1778.7481315135956, 'accumulated_submission_time': 1480.2585592269897, 'accumulated_eval_time': 298.37354612350464, 'accumulated_logging_time': 0.04358315467834473, 'global_step': 1889, 'preemption_count': 0}), (3790, {'train/ctc_loss': Array(1761.5704, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2920.4347846508026, 'total_duration': 3321.8674907684326, 'accumulated_submission_time': 2920.4347846508026, 'accumulated_eval_time': 401.18030858039856, 'accumulated_logging_time': 0.09850454330444336, 'global_step': 3790, 'preemption_count': 0}), (5687, {'train/ctc_loss': Array(1741.2977, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4360.514379024506, 'total_duration': 4865.484000682831, 'accumulated_submission_time': 4360.514379024506, 'accumulated_eval_time': 504.5643231868744, 'accumulated_logging_time': 0.16921186447143555, 'global_step': 5687, 'preemption_count': 0}), (7562, {'train/ctc_loss': Array(1724.8612, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 5800.64427113533, 'total_duration': 6408.002635240555, 'accumulated_submission_time': 5800.64427113533, 'accumulated_eval_time': 606.8254625797272, 'accumulated_logging_time': 0.21919870376586914, 'global_step': 7562, 'preemption_count': 0}), (9416, {'train/ctc_loss': Array(1832.9286, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7241.184892416, 'total_duration': 7951.773111820221, 'accumulated_submission_time': 7241.184892416, 'accumulated_eval_time': 709.9238367080688, 'accumulated_logging_time': 0.27213096618652344, 'global_step': 9416, 'preemption_count': 0}), (11280, {'train/ctc_loss': Array(1752.8002, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 8681.343116283417, 'total_duration': 9498.179074287415, 'accumulated_submission_time': 8681.343116283417, 'accumulated_eval_time': 816.0390911102295, 'accumulated_logging_time': 0.324566125869751, 'global_step': 11280, 'preemption_count': 0}), (13142, {'train/ctc_loss': Array(1746.1107, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 10121.755238056183, 'total_duration': 11042.359280586243, 'accumulated_submission_time': 10121.755238056183, 'accumulated_eval_time': 919.6746597290039, 'accumulated_logging_time': 0.37816405296325684, 'global_step': 13142, 'preemption_count': 0}), (15020, {'train/ctc_loss': Array(1733.7391, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 11561.767615318298, 'total_duration': 12587.424719333649, 'accumulated_submission_time': 11561.767615318298, 'accumulated_eval_time': 1024.5904006958008, 'accumulated_logging_time': 0.4345240592956543, 'global_step': 15020, 'preemption_count': 0}), (16891, {'train/ctc_loss': Array(1786.8646, dtype=float32), 'train/wer': 0.9427990785714666, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 13001.662180423737, 'total_duration': 14130.217215776443, 'accumulated_submission_time': 13001.662180423737, 'accumulated_eval_time': 1127.3577146530151, 'accumulated_logging_time': 0.48458337783813477, 'global_step': 16891, 'preemption_count': 0}), (18750, {'train/ctc_loss': Array(1755.9376, dtype=float32), 'train/wer': 0.9423383225986367, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14442.213552474976, 'total_duration': 15674.260643720627, 'accumulated_submission_time': 14442.213552474976, 'accumulated_eval_time': 1230.7227623462677, 'accumulated_logging_time': 0.5332691669464111, 'global_step': 18750, 'preemption_count': 0}), (20601, {'train/ctc_loss': Array(1731.249, dtype=float32), 'train/wer': 0.9431396916893625, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15885.487879753113, 'total_duration': 17222.049082756042, 'accumulated_submission_time': 15885.487879753113, 'accumulated_eval_time': 1335.1082394123077, 'accumulated_logging_time': 0.5847785472869873, 'global_step': 20601, 'preemption_count': 0}), (22476, {'train/ctc_loss': Array(1763.6163, dtype=float32), 'train/wer': 0.9432716912443612, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 17325.43764925003, 'total_duration': 18766.78422665596, 'accumulated_submission_time': 17325.43764925003, 'accumulated_eval_time': 1439.7607336044312, 'accumulated_logging_time': 0.6356685161590576, 'global_step': 22476, 'preemption_count': 0}), (24348, {'train/ctc_loss': Array(1739.3485, dtype=float32), 'train/wer': 0.944685667249717, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 18766.40936422348, 'total_duration': 20311.63661837578, 'accumulated_submission_time': 18766.40936422348, 'accumulated_eval_time': 1543.5027351379395, 'accumulated_logging_time': 0.6933820247650146, 'global_step': 24348, 'preemption_count': 0}), (26216, {'train/ctc_loss': Array(1769.4734, dtype=float32), 'train/wer': 0.9432456399645285, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 20206.899822950363, 'total_duration': 21856.264822006226, 'accumulated_submission_time': 20206.899822950363, 'accumulated_eval_time': 1647.5131170749664, 'accumulated_logging_time': 0.7416789531707764, 'global_step': 26216, 'preemption_count': 0}), (28075, {'train/ctc_loss': Array(1736.9209, dtype=float32), 'train/wer': 0.9439109001278072, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 21647.483785390854, 'total_duration': 23400.860488414764, 'accumulated_submission_time': 21647.483785390854, 'accumulated_eval_time': 1751.3921954631805, 'accumulated_logging_time': 0.7965822219848633, 'global_step': 28075, 'preemption_count': 0}), (29918, {'train/ctc_loss': Array(1715.2468, dtype=float32), 'train/wer': 0.9450143703143059, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 23087.483112573624, 'total_duration': 24945.35572862625, 'accumulated_submission_time': 23087.483112573624, 'accumulated_eval_time': 1855.772367477417, 'accumulated_logging_time': 0.8384244441986084, 'global_step': 29918, 'preemption_count': 0}), (31775, {'train/ctc_loss': Array(1783.4976, dtype=float32), 'train/wer': 0.9417576703068122, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 24527.82481122017, 'total_duration': 26492.56491136551, 'accumulated_submission_time': 24527.82481122017, 'accumulated_eval_time': 1962.516150712967, 'accumulated_logging_time': 0.882831335067749, 'global_step': 31775, 'preemption_count': 0}), (33636, {'train/ctc_loss': Array(1824.6915, dtype=float32), 'train/wer': 0.9416600198590335, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 25968.27384829521, 'total_duration': 28038.030095100403, 'accumulated_submission_time': 25968.27384829521, 'accumulated_eval_time': 2067.39896941185, 'accumulated_logging_time': 0.9372401237487793, 'global_step': 33636, 'preemption_count': 0}), (35516, {'train/ctc_loss': Array(1692.7736, dtype=float32), 'train/wer': 0.9447677853176417, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 27408.490597724915, 'total_duration': 29582.585765600204, 'accumulated_submission_time': 27408.490597724915, 'accumulated_eval_time': 2171.608489751816, 'accumulated_logging_time': 0.9847252368927002, 'global_step': 35516, 'preemption_count': 0}), (37373, {'train/ctc_loss': Array(1787.1794, dtype=float32), 'train/wer': 0.9427091658940503, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 28848.541474819183, 'total_duration': 31125.562560081482, 'accumulated_submission_time': 28848.541474819183, 'accumulated_eval_time': 2274.3990318775177, 'accumulated_logging_time': 1.0396275520324707, 'global_step': 37373, 'preemption_count': 0}), (39208, {'train/ctc_loss': Array(1714.328, dtype=float32), 'train/wer': 0.9448971433842748, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 30288.469445466995, 'total_duration': 32669.081364393234, 'accumulated_submission_time': 30288.469445466995, 'accumulated_eval_time': 2377.8674325942993, 'accumulated_logging_time': 1.0864319801330566, 'global_step': 39208, 'preemption_count': 0}), (41064, {'train/ctc_loss': Array(1760.6877, dtype=float32), 'train/wer': 0.9432324554919642, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 31728.385579824448, 'total_duration': 34213.46521806717, 'accumulated_submission_time': 31728.385579824448, 'accumulated_eval_time': 2482.2094728946686, 'accumulated_logging_time': 1.13423752784729, 'global_step': 41064, 'preemption_count': 0}), (42928, {'train/ctc_loss': Array(1852.6233, dtype=float32), 'train/wer': 0.941680272071945, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 33168.835923194885, 'total_duration': 35758.39141345024, 'accumulated_submission_time': 33168.835923194885, 'accumulated_eval_time': 2586.5602476596832, 'accumulated_logging_time': 1.1817538738250732, 'global_step': 42928, 'preemption_count': 0}), (44806, {'train/ctc_loss': Array(1901.3729, dtype=float32), 'train/wer': 0.940312095793757, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 34609.07900071144, 'total_duration': 37301.53804039955, 'accumulated_submission_time': 34609.07900071144, 'accumulated_eval_time': 2689.319209098816, 'accumulated_logging_time': 1.2480909824371338, 'global_step': 44806, 'preemption_count': 0}), (46667, {'train/ctc_loss': Array(1959.8695, dtype=float32), 'train/wer': 0.9371047844119075, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 36049.45591020584, 'total_duration': 38845.65246319771, 'accumulated_submission_time': 36049.45591020584, 'accumulated_eval_time': 2792.9326734542847, 'accumulated_logging_time': 1.2944605350494385, 'global_step': 46667, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(2009.277, dtype=float32), 'train/wer': 0.9366967129626904, 'validation/ctc_loss': Array(3357.9219, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8604, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 37072.079999923706, 'total_duration': 39972.278036117554, 'accumulated_submission_time': 37072.079999923706, 'accumulated_eval_time': 2896.8326518535614, 'accumulated_logging_time': 1.3415584564208984, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0328 10:01:39.630374 140003960837952 submission_runner.py:599] Timing: 37072.079999923706
I0328 10:01:39.630439 140003960837952 submission_runner.py:601] Total number of evals: 27
I0328 10:01:39.630495 140003960837952 submission_runner.py:602] ====================
I0328 10:01:39.633971 140003960837952 submission_runner.py:686] Final librispeech_deepspeech_norm_and_spec_aug score: 0
