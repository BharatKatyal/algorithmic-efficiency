python3 submission_runner.py --framework=jax --workload=imagenet_resnet_large_bn_init --submission_path=reference_algorithms/target_setting_algorithms/jax_momentum.py --tuning_search_space=reference_algorithms/target_setting_algorithms/imagenet_resnet_large_bn_init/tuning_search_space.json --data_dir=/data/imagenet/jax --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --num_tuning_trials=1 --rng_seed=1692305324 --max_global_steps=186666 --imagenet_v2_data_dir=/data/imagenet/jax 2>&1 | tee -a /logs/imagenet_resnet_large_bn_init_jax_02-15-2024-09-15-29.log
/usr/local/lib/python3.8/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: 

TensorFlow Addons (TFA) has ended development and introduction of new features.
TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.
Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). 

For more information see: https://github.com/tensorflow/addons/issues/2807 

  warnings.warn(
I0215 09:15:49.829264 139970484569920 logger_utils.py:61] Removing existing experiment directory /experiment_runs/variants_target_setting/study_0/imagenet_resnet_large_bn_init_jax because --overwrite was set.
I0215 09:15:49.829870 139970484569920 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/imagenet_resnet_large_bn_init_jax.
I0215 09:15:50.784833 139970484569920 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0215 09:15:50.785950 139970484569920 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0215 09:15:50.786107 139970484569920 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0215 09:15:50.791990 139970484569920 submission_runner.py:542] Using RNG seed 1692305324
I0215 09:15:51.874746 139970484569920 submission_runner.py:551] --- Tuning run 1/1 ---
I0215 09:15:51.874933 139970484569920 submission_runner.py:556] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/imagenet_resnet_large_bn_init_jax/trial_1.
I0215 09:15:51.875102 139970484569920 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/imagenet_resnet_large_bn_init_jax/trial_1/hparams.json.
I0215 09:15:52.051753 139970484569920 submission_runner.py:206] Initializing dataset.
I0215 09:15:52.067308 139970484569920 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0215 09:15:52.083019 139970484569920 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0215 09:15:52.456702 139970484569920 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0215 09:15:53.585121 139970484569920 submission_runner.py:213] Initializing model.
I0215 09:16:04.023197 139970484569920 submission_runner.py:255] Initializing optimizer.
I0215 09:16:05.526411 139970484569920 submission_runner.py:262] Initializing metrics bundle.
I0215 09:16:05.526603 139970484569920 submission_runner.py:280] Initializing checkpoint and logger.
I0215 09:16:05.527717 139970484569920 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/imagenet_resnet_large_bn_init_jax/trial_1 with prefix checkpoint_
I0215 09:16:05.527869 139970484569920 submission_runner.py:300] Saving meta data to /experiment_runs/variants_target_setting/study_0/imagenet_resnet_large_bn_init_jax/trial_1/meta_data_0.json.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0215 09:16:05.858502 139970484569920 logger_utils.py:220] Unable to record git information. Continuing without it.
I0215 09:16:06.177506 139970484569920 submission_runner.py:304] Saving flags to /experiment_runs/variants_target_setting/study_0/imagenet_resnet_large_bn_init_jax/trial_1/flags_0.json.
I0215 09:16:06.187170 139970484569920 submission_runner.py:314] Starting training loop.
I0215 09:16:49.876213 139803953706752 logging_writer.py:48] [0] global_step=0, grad_norm=111.78033447265625, loss=10.447689056396484
I0215 09:16:49.892123 139970484569920 spec.py:321] Evaluating on the training split.
I0215 09:16:50.836518 139970484569920 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0215 09:16:50.845693 139970484569920 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0215 09:16:50.930351 139970484569920 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split train, from /data/imagenet/jax/imagenet2012/5.1.0
I0215 09:17:04.251835 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 09:17:05.495365 139970484569920 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet2012/5.1.0
I0215 09:17:05.511019 139970484569920 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0215 09:17:05.567977 139970484569920 logging_logger.py:49] Constructing tf.data.Dataset imagenet2012 for split validation, from /data/imagenet/jax/imagenet2012/5.1.0
I0215 09:17:21.507752 139970484569920 spec.py:349] Evaluating on the test split.
I0215 09:17:22.285838 139970484569920 dataset_info.py:578] Load dataset info from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0215 09:17:22.293847 139970484569920 dataset_builder.py:528] Reusing dataset imagenet_v2 (/data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0)
I0215 09:17:22.330811 139970484569920 logging_logger.py:49] Constructing tf.data.Dataset imagenet_v2 for split test, from /data/imagenet/jax/imagenet_v2/matched-frequency/3.0.0
I0215 09:17:26.538624 139970484569920 submission_runner.py:408] Time since start: 80.35s, 	Step: 1, 	{'train/accuracy': 0.0007971938466653228, 'train/loss': 590.2318115234375, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 596.8076782226562, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 587.8714599609375, 'test/num_examples': 10000, 'score': 43.70478010177612, 'total_duration': 80.3514039516449, 'accumulated_submission_time': 43.70478010177612, 'accumulated_eval_time': 36.646462202072144, 'accumulated_logging_time': 0}
I0215 09:17:26.557399 139785733658368 logging_writer.py:48] [1] accumulated_eval_time=36.646462, accumulated_logging_time=0, accumulated_submission_time=43.704780, global_step=1, preemption_count=0, score=43.704780, test/accuracy=0.001000, test/loss=587.871460, test/num_examples=10000, total_duration=80.351404, train/accuracy=0.000797, train/loss=590.231812, validation/accuracy=0.001000, validation/loss=596.807678, validation/num_examples=50000
I0215 09:17:26.900544 139785725265664 logging_writer.py:48] [1] global_step=1, grad_norm=114.05445098876953, loss=10.586479187011719
I0215 09:17:27.236091 139785733658368 logging_writer.py:48] [2] global_step=2, grad_norm=92.64237976074219, loss=9.763307571411133
I0215 09:17:27.577989 139785725265664 logging_writer.py:48] [3] global_step=3, grad_norm=92.25382232666016, loss=9.130448341369629
I0215 09:17:27.917794 139785733658368 logging_writer.py:48] [4] global_step=4, grad_norm=84.08551025390625, loss=8.809321403503418
I0215 09:17:28.252479 139785725265664 logging_writer.py:48] [5] global_step=5, grad_norm=82.81585693359375, loss=8.606311798095703
I0215 09:17:28.585147 139785733658368 logging_writer.py:48] [6] global_step=6, grad_norm=64.88052368164062, loss=8.638337135314941
I0215 09:17:28.916581 139785725265664 logging_writer.py:48] [7] global_step=7, grad_norm=55.83683776855469, loss=8.826155662536621
I0215 09:17:29.259396 139785733658368 logging_writer.py:48] [8] global_step=8, grad_norm=69.22832489013672, loss=9.444746971130371
I0215 09:17:29.593471 139785725265664 logging_writer.py:48] [9] global_step=9, grad_norm=72.18461608886719, loss=10.242188453674316
I0215 09:17:29.927762 139785733658368 logging_writer.py:48] [10] global_step=10, grad_norm=43.48914337158203, loss=11.106698989868164
I0215 09:17:30.262810 139785725265664 logging_writer.py:48] [11] global_step=11, grad_norm=55.79546356201172, loss=11.012836456298828
I0215 09:17:30.595088 139785733658368 logging_writer.py:48] [12] global_step=12, grad_norm=56.46809768676758, loss=12.366975784301758
I0215 09:17:30.935834 139785725265664 logging_writer.py:48] [13] global_step=13, grad_norm=53.56230926513672, loss=12.83301830291748
I0215 09:17:31.271064 139785733658368 logging_writer.py:48] [14] global_step=14, grad_norm=50.11287307739258, loss=12.132342338562012
I0215 09:17:31.603338 139785725265664 logging_writer.py:48] [15] global_step=15, grad_norm=43.162044525146484, loss=13.369282722473145
I0215 09:17:31.933854 139785733658368 logging_writer.py:48] [16] global_step=16, grad_norm=78.99871063232422, loss=18.34375762939453
I0215 09:17:32.268264 139785725265664 logging_writer.py:48] [17] global_step=17, grad_norm=52.02387619018555, loss=13.114462852478027
I0215 09:17:32.615652 139785733658368 logging_writer.py:48] [18] global_step=18, grad_norm=99.88175201416016, loss=19.888500213623047
I0215 09:17:32.956909 139785725265664 logging_writer.py:48] [19] global_step=19, grad_norm=70.77129364013672, loss=15.281916618347168
I0215 09:17:33.297634 139785733658368 logging_writer.py:48] [20] global_step=20, grad_norm=33.19501495361328, loss=14.168302536010742
I0215 09:17:33.631856 139785725265664 logging_writer.py:48] [21] global_step=21, grad_norm=109.6395034790039, loss=10.547228813171387
I0215 09:17:33.971064 139785733658368 logging_writer.py:48] [22] global_step=22, grad_norm=58.87735366821289, loss=11.77752685546875
I0215 09:17:34.306423 139785725265664 logging_writer.py:48] [23] global_step=23, grad_norm=151.45127868652344, loss=16.332473754882812
I0215 09:17:34.643497 139785733658368 logging_writer.py:48] [24] global_step=24, grad_norm=37.72553253173828, loss=21.423580169677734
I0215 09:17:34.987047 139785725265664 logging_writer.py:48] [25] global_step=25, grad_norm=39.86387252807617, loss=13.856178283691406
I0215 09:17:35.317249 139785733658368 logging_writer.py:48] [26] global_step=26, grad_norm=25.613149642944336, loss=17.61974334716797
I0215 09:17:35.649891 139785725265664 logging_writer.py:48] [27] global_step=27, grad_norm=21.751684188842773, loss=16.92376136779785
I0215 09:17:35.992157 139785733658368 logging_writer.py:48] [28] global_step=28, grad_norm=18.425251007080078, loss=14.71125602722168
I0215 09:17:36.325689 139785725265664 logging_writer.py:48] [29] global_step=29, grad_norm=28.636810302734375, loss=12.815762519836426
I0215 09:17:36.662045 139785733658368 logging_writer.py:48] [30] global_step=30, grad_norm=29.209692001342773, loss=22.434738159179688
I0215 09:17:36.997723 139785725265664 logging_writer.py:48] [31] global_step=31, grad_norm=15.566967964172363, loss=10.462368965148926
I0215 09:17:37.332624 139785733658368 logging_writer.py:48] [32] global_step=32, grad_norm=15.205240249633789, loss=14.882925033569336
I0215 09:17:37.674011 139785725265664 logging_writer.py:48] [33] global_step=33, grad_norm=12.853875160217285, loss=14.210939407348633
I0215 09:17:38.022240 139785733658368 logging_writer.py:48] [34] global_step=34, grad_norm=17.64087677001953, loss=12.174281120300293
I0215 09:17:38.361266 139785725265664 logging_writer.py:48] [35] global_step=35, grad_norm=33.47959518432617, loss=13.37547492980957
I0215 09:17:38.693673 139785733658368 logging_writer.py:48] [36] global_step=36, grad_norm=23.698875427246094, loss=14.293834686279297
I0215 09:17:39.041452 139785725265664 logging_writer.py:48] [37] global_step=37, grad_norm=23.95499038696289, loss=15.034074783325195
I0215 09:17:39.377565 139785733658368 logging_writer.py:48] [38] global_step=38, grad_norm=18.12995147705078, loss=13.942441940307617
I0215 09:17:39.710225 139785725265664 logging_writer.py:48] [39] global_step=39, grad_norm=118.64014434814453, loss=16.634784698486328
I0215 09:17:40.045146 139785733658368 logging_writer.py:48] [40] global_step=40, grad_norm=95.77565002441406, loss=13.965788841247559
I0215 09:17:40.387594 139785725265664 logging_writer.py:48] [41] global_step=41, grad_norm=65.76103210449219, loss=15.514305114746094
I0215 09:17:40.731938 139785733658368 logging_writer.py:48] [42] global_step=42, grad_norm=10.628655433654785, loss=15.412467956542969
I0215 09:17:41.065366 139785725265664 logging_writer.py:48] [43] global_step=43, grad_norm=15.058265686035156, loss=14.497321128845215
I0215 09:17:41.406166 139785733658368 logging_writer.py:48] [44] global_step=44, grad_norm=13.090845108032227, loss=13.764581680297852
I0215 09:17:41.749057 139785725265664 logging_writer.py:48] [45] global_step=45, grad_norm=11.363570213317871, loss=13.12332534790039
I0215 09:17:42.084373 139785733658368 logging_writer.py:48] [46] global_step=46, grad_norm=13.627697944641113, loss=14.818471908569336
I0215 09:17:42.421539 139785725265664 logging_writer.py:48] [47] global_step=47, grad_norm=19.210111618041992, loss=15.196807861328125
I0215 09:17:42.759674 139785733658368 logging_writer.py:48] [48] global_step=48, grad_norm=27.154388427734375, loss=13.56631088256836
I0215 09:17:43.092995 139785725265664 logging_writer.py:48] [49] global_step=49, grad_norm=10.44544506072998, loss=13.563332557678223
I0215 09:17:43.424613 139785733658368 logging_writer.py:48] [50] global_step=50, grad_norm=8.295686721801758, loss=12.835517883300781
I0215 09:17:43.757872 139785725265664 logging_writer.py:48] [51] global_step=51, grad_norm=5.845334529876709, loss=13.116640090942383
I0215 09:17:44.092882 139785733658368 logging_writer.py:48] [52] global_step=52, grad_norm=11.812294006347656, loss=11.134761810302734
I0215 09:17:44.423054 139785725265664 logging_writer.py:48] [53] global_step=53, grad_norm=7.327727794647217, loss=11.85517692565918
I0215 09:17:44.759682 139785733658368 logging_writer.py:48] [54] global_step=54, grad_norm=8.143437385559082, loss=11.788114547729492
I0215 09:17:45.100461 139785725265664 logging_writer.py:48] [55] global_step=55, grad_norm=4.109147548675537, loss=10.73827838897705
I0215 09:17:45.445760 139785733658368 logging_writer.py:48] [56] global_step=56, grad_norm=20.351022720336914, loss=11.647359848022461
I0215 09:17:45.781405 139785725265664 logging_writer.py:48] [57] global_step=57, grad_norm=4.579626560211182, loss=10.380680084228516
I0215 09:17:46.116957 139785733658368 logging_writer.py:48] [58] global_step=58, grad_norm=4.669677257537842, loss=10.365918159484863
I0215 09:17:46.448881 139785725265664 logging_writer.py:48] [59] global_step=59, grad_norm=8.171286582946777, loss=12.467348098754883
I0215 09:17:46.786779 139785733658368 logging_writer.py:48] [60] global_step=60, grad_norm=9.278663635253906, loss=11.628952980041504
I0215 09:17:47.128180 139785725265664 logging_writer.py:48] [61] global_step=61, grad_norm=6.134442329406738, loss=10.315004348754883
I0215 09:17:47.463348 139785733658368 logging_writer.py:48] [62] global_step=62, grad_norm=11.542476654052734, loss=10.53209400177002
I0215 09:17:47.799916 139785725265664 logging_writer.py:48] [63] global_step=63, grad_norm=4.26185941696167, loss=10.609073638916016
I0215 09:17:48.132324 139785733658368 logging_writer.py:48] [64] global_step=64, grad_norm=3.7976901531219482, loss=10.816017150878906
I0215 09:17:48.474803 139785725265664 logging_writer.py:48] [65] global_step=65, grad_norm=3.672945976257324, loss=8.979869842529297
I0215 09:17:48.808808 139785733658368 logging_writer.py:48] [66] global_step=66, grad_norm=3.940974712371826, loss=9.365909576416016
I0215 09:17:49.144295 139785725265664 logging_writer.py:48] [67] global_step=67, grad_norm=5.1955132484436035, loss=10.235374450683594
I0215 09:17:49.485336 139785733658368 logging_writer.py:48] [68] global_step=68, grad_norm=6.365362167358398, loss=11.071121215820312
I0215 09:17:49.818357 139785725265664 logging_writer.py:48] [69] global_step=69, grad_norm=3.3195855617523193, loss=8.856937408447266
I0215 09:17:50.151221 139785733658368 logging_writer.py:48] [70] global_step=70, grad_norm=7.179177761077881, loss=8.925145149230957
I0215 09:17:50.495747 139785725265664 logging_writer.py:48] [71] global_step=71, grad_norm=3.193195104598999, loss=8.367709159851074
I0215 09:17:50.830190 139785733658368 logging_writer.py:48] [72] global_step=72, grad_norm=2.4275832176208496, loss=8.16705322265625
I0215 09:17:51.166620 139785725265664 logging_writer.py:48] [73] global_step=73, grad_norm=2.533259391784668, loss=7.946438312530518
I0215 09:17:51.505769 139785733658368 logging_writer.py:48] [74] global_step=74, grad_norm=3.2271766662597656, loss=7.998987674713135
I0215 09:17:51.842943 139785725265664 logging_writer.py:48] [75] global_step=75, grad_norm=14.555439949035645, loss=9.318680763244629
I0215 09:17:52.171620 139785733658368 logging_writer.py:48] [76] global_step=76, grad_norm=3.5779736042022705, loss=8.489542007446289
I0215 09:17:52.504170 139785725265664 logging_writer.py:48] [77] global_step=77, grad_norm=3.479749917984009, loss=8.707609176635742
I0215 09:17:52.836479 139785733658368 logging_writer.py:48] [78] global_step=78, grad_norm=4.3386616706848145, loss=8.729843139648438
I0215 09:17:53.174322 139785725265664 logging_writer.py:48] [79] global_step=79, grad_norm=8.260109901428223, loss=8.612167358398438
I0215 09:17:53.514953 139785733658368 logging_writer.py:48] [80] global_step=80, grad_norm=3.724477767944336, loss=9.066102981567383
I0215 09:17:53.847595 139785725265664 logging_writer.py:48] [81] global_step=81, grad_norm=6.192121982574463, loss=9.966226577758789
I0215 09:17:54.182935 139785733658368 logging_writer.py:48] [82] global_step=82, grad_norm=2.626997470855713, loss=7.64320182800293
I0215 09:17:54.515487 139785725265664 logging_writer.py:48] [83] global_step=83, grad_norm=4.044679164886475, loss=8.016871452331543
I0215 09:17:54.846800 139785733658368 logging_writer.py:48] [84] global_step=84, grad_norm=2.6323702335357666, loss=9.066094398498535
I0215 09:17:55.181946 139785725265664 logging_writer.py:48] [85] global_step=85, grad_norm=3.30635929107666, loss=8.118155479431152
I0215 09:17:55.525520 139785733658368 logging_writer.py:48] [86] global_step=86, grad_norm=2.474475145339966, loss=8.625829696655273
I0215 09:17:55.859800 139785725265664 logging_writer.py:48] [87] global_step=87, grad_norm=2.7011685371398926, loss=8.122611999511719
I0215 09:17:56.197684 139785733658368 logging_writer.py:48] [88] global_step=88, grad_norm=3.4103660583496094, loss=7.74635648727417
I0215 09:17:56.528974 139785725265664 logging_writer.py:48] [89] global_step=89, grad_norm=2.728682279586792, loss=8.5693941116333
I0215 09:17:56.878979 139785733658368 logging_writer.py:48] [90] global_step=90, grad_norm=4.089672088623047, loss=8.897969245910645
I0215 09:17:57.223938 139785725265664 logging_writer.py:48] [91] global_step=91, grad_norm=1.7907658815383911, loss=7.77662467956543
I0215 09:17:57.562039 139785733658368 logging_writer.py:48] [92] global_step=92, grad_norm=4.373659610748291, loss=7.566026210784912
I0215 09:17:57.895693 139785725265664 logging_writer.py:48] [93] global_step=93, grad_norm=1.725746750831604, loss=7.70435094833374
I0215 09:17:58.229843 139785733658368 logging_writer.py:48] [94] global_step=94, grad_norm=109.48705291748047, loss=8.481704711914062
I0215 09:17:58.570441 139785725265664 logging_writer.py:48] [95] global_step=95, grad_norm=2.161630868911743, loss=7.581794738769531
I0215 09:17:58.903750 139785733658368 logging_writer.py:48] [96] global_step=96, grad_norm=3.485244035720825, loss=9.401662826538086
I0215 09:17:59.241655 139785725265664 logging_writer.py:48] [97] global_step=97, grad_norm=5.977212429046631, loss=8.523266792297363
I0215 09:17:59.587869 139785733658368 logging_writer.py:48] [98] global_step=98, grad_norm=4.356372833251953, loss=9.645608901977539
I0215 09:17:59.920835 139785725265664 logging_writer.py:48] [99] global_step=99, grad_norm=9.863054275512695, loss=9.200064659118652
I0215 09:18:00.252711 139785733658368 logging_writer.py:48] [100] global_step=100, grad_norm=9.436226844787598, loss=9.945306777954102
I0215 09:20:14.122608 139785725265664 logging_writer.py:48] [500] global_step=500, grad_norm=0.16617968678474426, loss=6.853789806365967
I0215 09:23:01.756443 139785733658368 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.24898196756839752, loss=6.601654529571533
I0215 09:25:49.179963 139785725265664 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.16712108254432678, loss=6.627212047576904
I0215 09:25:56.665302 139970484569920 spec.py:321] Evaluating on the training split.
I0215 09:26:03.959842 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 09:26:12.054981 139970484569920 spec.py:349] Evaluating on the test split.
I0215 09:26:14.335638 139970484569920 submission_runner.py:408] Time since start: 608.15s, 	Step: 1524, 	{'train/accuracy': 0.009805484674870968, 'train/loss': 6.519259452819824, 'validation/accuracy': 0.010160000063478947, 'validation/loss': 6.551246166229248, 'validation/num_examples': 50000, 'test/accuracy': 0.00800000037997961, 'test/loss': 6.623855113983154, 'test/num_examples': 10000, 'score': 553.7564685344696, 'total_duration': 608.1484117507935, 'accumulated_submission_time': 553.7564685344696, 'accumulated_eval_time': 54.31677436828613, 'accumulated_logging_time': 0.02762579917907715}
I0215 09:26:14.352651 139785742051072 logging_writer.py:48] [1524] accumulated_eval_time=54.316774, accumulated_logging_time=0.027626, accumulated_submission_time=553.756469, global_step=1524, preemption_count=0, score=553.756469, test/accuracy=0.008000, test/loss=6.623855, test/num_examples=10000, total_duration=608.148412, train/accuracy=0.009805, train/loss=6.519259, validation/accuracy=0.010160, validation/loss=6.551246, validation/num_examples=50000
I0215 09:28:54.000541 139785750443776 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.15001820027828217, loss=6.6274003982543945
I0215 09:31:41.529945 139785742051072 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.19440391659736633, loss=6.346930503845215
I0215 09:34:29.058468 139785750443776 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.20979037880897522, loss=6.0069146156311035
I0215 09:34:44.585831 139970484569920 spec.py:321] Evaluating on the training split.
I0215 09:34:51.806969 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 09:35:00.096484 139970484569920 spec.py:349] Evaluating on the test split.
I0215 09:35:02.382465 139970484569920 submission_runner.py:408] Time since start: 1136.20s, 	Step: 3048, 	{'train/accuracy': 0.07106982916593552, 'train/loss': 5.383684158325195, 'validation/accuracy': 0.0667399987578392, 'validation/loss': 5.417434215545654, 'validation/num_examples': 50000, 'test/accuracy': 0.048500001430511475, 'test/loss': 5.687870025634766, 'test/num_examples': 10000, 'score': 1063.929992198944, 'total_duration': 1136.1952157020569, 'accumulated_submission_time': 1063.929992198944, 'accumulated_eval_time': 72.11336135864258, 'accumulated_logging_time': 0.05489993095397949}
I0215 09:35:02.404463 139807611148032 logging_writer.py:48] [3048] accumulated_eval_time=72.113361, accumulated_logging_time=0.054900, accumulated_submission_time=1063.929992, global_step=3048, preemption_count=0, score=1063.929992, test/accuracy=0.048500, test/loss=5.687870, test/num_examples=10000, total_duration=1136.195216, train/accuracy=0.071070, train/loss=5.383684, validation/accuracy=0.066740, validation/loss=5.417434, validation/num_examples=50000
I0215 09:37:34.244688 139807619540736 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.2215476632118225, loss=5.68761682510376
I0215 09:40:21.706525 139807611148032 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.22506383061408997, loss=5.458990097045898
I0215 09:43:09.212127 139807619540736 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.23349328339099884, loss=5.1966986656188965
I0215 09:43:32.407058 139970484569920 spec.py:321] Evaluating on the training split.
I0215 09:43:39.689708 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 09:43:48.135613 139970484569920 spec.py:349] Evaluating on the test split.
I0215 09:43:50.375330 139970484569920 submission_runner.py:408] Time since start: 1664.19s, 	Step: 4571, 	{'train/accuracy': 0.19214364886283875, 'train/loss': 4.239551544189453, 'validation/accuracy': 0.1824599951505661, 'validation/loss': 4.3354997634887695, 'validation/num_examples': 50000, 'test/accuracy': 0.12870000302791595, 'test/loss': 4.781617641448975, 'test/num_examples': 10000, 'score': 1573.874829530716, 'total_duration': 1664.1880810260773, 'accumulated_submission_time': 1573.874829530716, 'accumulated_eval_time': 90.0815896987915, 'accumulated_logging_time': 0.08605551719665527}
I0215 09:43:50.392458 139807695009536 logging_writer.py:48] [4571] accumulated_eval_time=90.081590, accumulated_logging_time=0.086056, accumulated_submission_time=1573.874830, global_step=4571, preemption_count=0, score=1573.874830, test/accuracy=0.128700, test/loss=4.781618, test/num_examples=10000, total_duration=1664.188081, train/accuracy=0.192144, train/loss=4.239552, validation/accuracy=0.182460, validation/loss=4.335500, validation/num_examples=50000
I0215 09:46:14.713907 139807703402240 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.2661466896533966, loss=5.01703405380249
I0215 09:49:02.426527 139807695009536 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.2151733636856079, loss=4.846457481384277
I0215 09:51:50.207768 139807703402240 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.22853709757328033, loss=4.756847858428955
I0215 09:52:20.465095 139970484569920 spec.py:321] Evaluating on the training split.
I0215 09:52:27.634137 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 09:52:35.902213 139970484569920 spec.py:349] Evaluating on the test split.
I0215 09:52:38.183410 139970484569920 submission_runner.py:408] Time since start: 2192.00s, 	Step: 6092, 	{'train/accuracy': 0.3301379084587097, 'train/loss': 3.4222092628479004, 'validation/accuracy': 0.30640000104904175, 'validation/loss': 3.514439344406128, 'validation/num_examples': 50000, 'test/accuracy': 0.23600001633167267, 'test/loss': 4.117316246032715, 'test/num_examples': 10000, 'score': 2083.8879685401917, 'total_duration': 2191.996179819107, 'accumulated_submission_time': 2083.8879685401917, 'accumulated_eval_time': 107.7998673915863, 'accumulated_logging_time': 0.11339092254638672}
I0215 09:52:38.200626 139807661438720 logging_writer.py:48] [6092] accumulated_eval_time=107.799867, accumulated_logging_time=0.113391, accumulated_submission_time=2083.887969, global_step=6092, preemption_count=0, score=2083.887969, test/accuracy=0.236000, test/loss=4.117316, test/num_examples=10000, total_duration=2191.996180, train/accuracy=0.330138, train/loss=3.422209, validation/accuracy=0.306400, validation/loss=3.514439, validation/num_examples=50000
I0215 09:54:55.300802 139807669831424 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.2217567265033722, loss=4.639840126037598
I0215 09:57:43.007161 139807661438720 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.23066681623458862, loss=4.461307525634766
I0215 10:00:30.644897 139807669831424 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.2222604751586914, loss=4.393030166625977
I0215 10:01:08.249652 139970484569920 spec.py:321] Evaluating on the training split.
I0215 10:01:15.498394 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 10:01:23.835793 139970484569920 spec.py:349] Evaluating on the test split.
I0215 10:01:26.098363 139970484569920 submission_runner.py:408] Time since start: 2719.91s, 	Step: 7614, 	{'train/accuracy': 0.37272799015045166, 'train/loss': 3.1425509452819824, 'validation/accuracy': 0.3504999876022339, 'validation/loss': 3.261110544204712, 'validation/num_examples': 50000, 'test/accuracy': 0.26740002632141113, 'test/loss': 3.8572018146514893, 'test/num_examples': 10000, 'score': 2593.8787322044373, 'total_duration': 2719.911128759384, 'accumulated_submission_time': 2593.8787322044373, 'accumulated_eval_time': 125.64854097366333, 'accumulated_logging_time': 0.139801025390625}
I0215 10:01:26.115855 139807703402240 logging_writer.py:48] [7614] accumulated_eval_time=125.648541, accumulated_logging_time=0.139801, accumulated_submission_time=2593.878732, global_step=7614, preemption_count=0, score=2593.878732, test/accuracy=0.267400, test/loss=3.857202, test/num_examples=10000, total_duration=2719.911129, train/accuracy=0.372728, train/loss=3.142551, validation/accuracy=0.350500, validation/loss=3.261111, validation/num_examples=50000
I0215 10:03:35.905480 139807711794944 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.2171851247549057, loss=4.322395324707031
I0215 10:06:23.574666 139807703402240 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.213669553399086, loss=4.171944618225098
I0215 10:09:11.261662 139807711794944 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.2185954451560974, loss=4.174562454223633
I0215 10:09:56.267855 139970484569920 spec.py:321] Evaluating on the training split.
I0215 10:10:03.526181 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 10:10:11.915271 139970484569920 spec.py:349] Evaluating on the test split.
I0215 10:10:14.185628 139970484569920 submission_runner.py:408] Time since start: 3248.00s, 	Step: 9136, 	{'train/accuracy': 0.4763631820678711, 'train/loss': 2.512876510620117, 'validation/accuracy': 0.4087199866771698, 'validation/loss': 2.858280658721924, 'validation/num_examples': 50000, 'test/accuracy': 0.3043000102043152, 'test/loss': 3.535163164138794, 'test/num_examples': 10000, 'score': 3103.9691920280457, 'total_duration': 3247.9983870983124, 'accumulated_submission_time': 3103.9691920280457, 'accumulated_eval_time': 143.56626749038696, 'accumulated_logging_time': 0.16625690460205078}
I0215 10:10:14.202906 139807678224128 logging_writer.py:48] [9136] accumulated_eval_time=143.566267, accumulated_logging_time=0.166257, accumulated_submission_time=3103.969192, global_step=9136, preemption_count=0, score=3103.969192, test/accuracy=0.304300, test/loss=3.535163, test/num_examples=10000, total_duration=3247.998387, train/accuracy=0.476363, train/loss=2.512877, validation/accuracy=0.408720, validation/loss=2.858281, validation/num_examples=50000
I0215 10:12:16.476393 139807686616832 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.2097126692533493, loss=4.101297855377197
I0215 10:15:04.015275 139807678224128 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.21350926160812378, loss=4.024308681488037
I0215 10:17:51.715216 139807686616832 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.22535964846611023, loss=4.0602521896362305
I0215 10:18:44.462342 139970484569920 spec.py:321] Evaluating on the training split.
I0215 10:18:51.776617 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 10:19:00.278314 139970484569920 spec.py:349] Evaluating on the test split.
I0215 10:19:02.535878 139970484569920 submission_runner.py:408] Time since start: 3776.35s, 	Step: 10659, 	{'train/accuracy': 0.49892377853393555, 'train/loss': 2.4701223373413086, 'validation/accuracy': 0.4554999768733978, 'validation/loss': 2.689612627029419, 'validation/num_examples': 50000, 'test/accuracy': 0.34870001673698425, 'test/loss': 3.3380837440490723, 'test/num_examples': 10000, 'score': 3614.1681060791016, 'total_duration': 3776.3486366271973, 'accumulated_submission_time': 3614.1681060791016, 'accumulated_eval_time': 161.63976645469666, 'accumulated_logging_time': 0.19428277015686035}
I0215 10:19:02.558330 139807661438720 logging_writer.py:48] [10659] accumulated_eval_time=161.639766, accumulated_logging_time=0.194283, accumulated_submission_time=3614.168106, global_step=10659, preemption_count=0, score=3614.168106, test/accuracy=0.348700, test/loss=3.338084, test/num_examples=10000, total_duration=3776.348637, train/accuracy=0.498924, train/loss=2.470122, validation/accuracy=0.455500, validation/loss=2.689613, validation/num_examples=50000
I0215 10:20:57.116379 139807669831424 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.21069926023483276, loss=3.8851637840270996
I0215 10:23:44.724674 139807661438720 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.21388942003250122, loss=3.996131181716919
I0215 10:26:32.359024 139807669831424 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.2231837958097458, loss=3.9027929306030273
I0215 10:27:32.547962 139970484569920 spec.py:321] Evaluating on the training split.
I0215 10:27:39.922336 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 10:27:48.527204 139970484569920 spec.py:349] Evaluating on the test split.
I0215 10:27:50.792058 139970484569920 submission_runner.py:408] Time since start: 4304.60s, 	Step: 12181, 	{'train/accuracy': 0.5123764276504517, 'train/loss': 2.372680902481079, 'validation/accuracy': 0.4696599841117859, 'validation/loss': 2.5708394050598145, 'validation/num_examples': 50000, 'test/accuracy': 0.3646000027656555, 'test/loss': 3.2151970863342285, 'test/num_examples': 10000, 'score': 4124.095969200134, 'total_duration': 4304.604807376862, 'accumulated_submission_time': 4124.095969200134, 'accumulated_eval_time': 179.88380932807922, 'accumulated_logging_time': 0.22716879844665527}
I0215 10:27:50.814115 139807703402240 logging_writer.py:48] [12181] accumulated_eval_time=179.883809, accumulated_logging_time=0.227169, accumulated_submission_time=4124.095969, global_step=12181, preemption_count=0, score=4124.095969, test/accuracy=0.364600, test/loss=3.215197, test/num_examples=10000, total_duration=4304.604807, train/accuracy=0.512376, train/loss=2.372681, validation/accuracy=0.469660, validation/loss=2.570839, validation/num_examples=50000
I0215 10:29:37.925714 139807711794944 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.22130997478961945, loss=3.9734272956848145
I0215 10:32:25.378263 139807703402240 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.21732401847839355, loss=3.9089577198028564
I0215 10:35:12.917355 139807711794944 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.22495462000370026, loss=3.8959779739379883
I0215 10:36:20.804641 139970484569920 spec.py:321] Evaluating on the training split.
I0215 10:36:28.379387 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 10:36:41.005016 139970484569920 spec.py:349] Evaluating on the test split.
I0215 10:36:43.110070 139970484569920 submission_runner.py:408] Time since start: 4836.92s, 	Step: 13704, 	{'train/accuracy': 0.5216039419174194, 'train/loss': 2.334874391555786, 'validation/accuracy': 0.4868199825286865, 'validation/loss': 2.5068442821502686, 'validation/num_examples': 50000, 'test/accuracy': 0.37060001492500305, 'test/loss': 3.2004566192626953, 'test/num_examples': 10000, 'score': 4634.026791334152, 'total_duration': 4836.92283821106, 'accumulated_submission_time': 4634.026791334152, 'accumulated_eval_time': 202.18920636177063, 'accumulated_logging_time': 0.2580592632293701}
I0215 10:36:43.128206 139807661438720 logging_writer.py:48] [13704] accumulated_eval_time=202.189206, accumulated_logging_time=0.258059, accumulated_submission_time=4634.026791, global_step=13704, preemption_count=0, score=4634.026791, test/accuracy=0.370600, test/loss=3.200457, test/num_examples=10000, total_duration=4836.922838, train/accuracy=0.521604, train/loss=2.334874, validation/accuracy=0.486820, validation/loss=2.506844, validation/num_examples=50000
I0215 10:38:22.576124 139807669831424 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.22420059144496918, loss=3.882298231124878
I0215 10:41:10.131328 139807661438720 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.22652393579483032, loss=3.8279237747192383
I0215 10:43:57.770385 139807669831424 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.22531624138355255, loss=3.8622000217437744
I0215 10:45:13.324867 139970484569920 spec.py:321] Evaluating on the training split.
I0215 10:45:21.437386 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 10:45:35.452226 139970484569920 spec.py:349] Evaluating on the test split.
I0215 10:45:37.575704 139970484569920 submission_runner.py:408] Time since start: 5371.39s, 	Step: 15227, 	{'train/accuracy': 0.5216039419174194, 'train/loss': 2.3197691440582275, 'validation/accuracy': 0.48603999614715576, 'validation/loss': 2.4832115173339844, 'validation/num_examples': 50000, 'test/accuracy': 0.3760000169277191, 'test/loss': 3.140584945678711, 'test/num_examples': 10000, 'score': 5144.163766860962, 'total_duration': 5371.388475894928, 'accumulated_submission_time': 5144.163766860962, 'accumulated_eval_time': 226.44001698493958, 'accumulated_logging_time': 0.2855396270751953}
I0215 10:45:37.602833 139807669831424 logging_writer.py:48] [15227] accumulated_eval_time=226.440017, accumulated_logging_time=0.285540, accumulated_submission_time=5144.163767, global_step=15227, preemption_count=0, score=5144.163767, test/accuracy=0.376000, test/loss=3.140585, test/num_examples=10000, total_duration=5371.388476, train/accuracy=0.521604, train/loss=2.319769, validation/accuracy=0.486040, validation/loss=2.483212, validation/num_examples=50000
I0215 10:47:09.447110 139807686616832 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.2245703637599945, loss=3.7172350883483887
I0215 10:49:57.156915 139807669831424 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.23318538069725037, loss=3.8578479290008545
I0215 10:52:44.691881 139807686616832 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.23974503576755524, loss=3.876976251602173
I0215 10:54:07.871922 139970484569920 spec.py:321] Evaluating on the training split.
I0215 10:54:15.533121 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 10:54:29.159939 139970484569920 spec.py:349] Evaluating on the test split.
I0215 10:54:31.275852 139970484569920 submission_runner.py:408] Time since start: 5905.09s, 	Step: 16750, 	{'train/accuracy': 0.5450813174247742, 'train/loss': 2.1684956550598145, 'validation/accuracy': 0.5137400031089783, 'validation/loss': 2.3319098949432373, 'validation/num_examples': 50000, 'test/accuracy': 0.3976000249385834, 'test/loss': 2.9867873191833496, 'test/num_examples': 10000, 'score': 5654.372869491577, 'total_duration': 5905.088617563248, 'accumulated_submission_time': 5654.372869491577, 'accumulated_eval_time': 249.84391474723816, 'accumulated_logging_time': 0.32094502449035645}
I0215 10:54:31.301839 139807653046016 logging_writer.py:48] [16750] accumulated_eval_time=249.843915, accumulated_logging_time=0.320945, accumulated_submission_time=5654.372869, global_step=16750, preemption_count=0, score=5654.372869, test/accuracy=0.397600, test/loss=2.986787, test/num_examples=10000, total_duration=5905.088618, train/accuracy=0.545081, train/loss=2.168496, validation/accuracy=0.513740, validation/loss=2.331910, validation/num_examples=50000
I0215 10:55:55.414180 139807661438720 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.22741559147834778, loss=3.7514328956604004
I0215 10:58:43.033246 139807653046016 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.22458313405513763, loss=3.775726795196533
I0215 11:01:30.537637 139807661438720 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.23361225426197052, loss=3.8084919452667236
I0215 11:03:01.563645 139970484569920 spec.py:321] Evaluating on the training split.
I0215 11:03:09.761462 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 11:03:23.657438 139970484569920 spec.py:349] Evaluating on the test split.
I0215 11:03:25.768741 139970484569920 submission_runner.py:408] Time since start: 6439.58s, 	Step: 18273, 	{'train/accuracy': 0.5945870280265808, 'train/loss': 2.0021681785583496, 'validation/accuracy': 0.5326600074768066, 'validation/loss': 2.2867698669433594, 'validation/num_examples': 50000, 'test/accuracy': 0.41120001673698425, 'test/loss': 2.9663474559783936, 'test/num_examples': 10000, 'score': 6164.574551582336, 'total_duration': 6439.581478834152, 'accumulated_submission_time': 6164.574551582336, 'accumulated_eval_time': 274.04896569252014, 'accumulated_logging_time': 0.35637998580932617}
I0215 11:03:25.791497 139807686616832 logging_writer.py:48] [18273] accumulated_eval_time=274.048966, accumulated_logging_time=0.356380, accumulated_submission_time=6164.574552, global_step=18273, preemption_count=0, score=6164.574552, test/accuracy=0.411200, test/loss=2.966347, test/num_examples=10000, total_duration=6439.581479, train/accuracy=0.594587, train/loss=2.002168, validation/accuracy=0.532660, validation/loss=2.286770, validation/num_examples=50000
I0215 11:04:42.052855 139807695009536 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.22908025979995728, loss=3.686260461807251
I0215 11:07:29.569055 139807686616832 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.24344903230667114, loss=3.7669122219085693
I0215 11:10:17.183837 139807695009536 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.2270178198814392, loss=3.699869394302368
I0215 11:11:56.026458 139970484569920 spec.py:321] Evaluating on the training split.
I0215 11:12:03.657812 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 11:12:18.043841 139970484569920 spec.py:349] Evaluating on the test split.
I0215 11:12:20.260862 139970484569920 submission_runner.py:408] Time since start: 6974.07s, 	Step: 19797, 	{'train/accuracy': 0.5412148833274841, 'train/loss': 2.2569923400878906, 'validation/accuracy': 0.4917199909687042, 'validation/loss': 2.4967575073242188, 'validation/num_examples': 50000, 'test/accuracy': 0.3806000053882599, 'test/loss': 3.1496195793151855, 'test/num_examples': 10000, 'score': 6674.7448387146, 'total_duration': 6974.072465419769, 'accumulated_submission_time': 6674.7448387146, 'accumulated_eval_time': 298.28218626976013, 'accumulated_logging_time': 0.38889336585998535}
I0215 11:12:20.279462 139807669831424 logging_writer.py:48] [19797] accumulated_eval_time=298.282186, accumulated_logging_time=0.388893, accumulated_submission_time=6674.744839, global_step=19797, preemption_count=0, score=6674.744839, test/accuracy=0.380600, test/loss=3.149620, test/num_examples=10000, total_duration=6974.072465, train/accuracy=0.541215, train/loss=2.256992, validation/accuracy=0.491720, validation/loss=2.496758, validation/num_examples=50000
I0215 11:13:28.514094 139807678224128 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.24244628846645355, loss=3.721954584121704
I0215 11:16:16.060589 139807669831424 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.24128879606723785, loss=3.7812976837158203
I0215 11:19:03.680936 139807678224128 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.23768609762191772, loss=3.757878303527832
I0215 11:20:50.287475 139970484569920 spec.py:321] Evaluating on the training split.
I0215 11:20:57.824495 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 11:21:11.491471 139970484569920 spec.py:349] Evaluating on the test split.
I0215 11:21:13.653890 139970484569920 submission_runner.py:408] Time since start: 7507.47s, 	Step: 21320, 	{'train/accuracy': 0.5771085619926453, 'train/loss': 2.0007622241973877, 'validation/accuracy': 0.5321399569511414, 'validation/loss': 2.222144603729248, 'validation/num_examples': 50000, 'test/accuracy': 0.4082000255584717, 'test/loss': 2.923381805419922, 'test/num_examples': 10000, 'score': 7184.691862821579, 'total_duration': 7507.466633558273, 'accumulated_submission_time': 7184.691862821579, 'accumulated_eval_time': 321.64855337142944, 'accumulated_logging_time': 0.41835951805114746}
I0215 11:21:13.672798 139807661438720 logging_writer.py:48] [21320] accumulated_eval_time=321.648553, accumulated_logging_time=0.418360, accumulated_submission_time=7184.691863, global_step=21320, preemption_count=0, score=7184.691863, test/accuracy=0.408200, test/loss=2.923382, test/num_examples=10000, total_duration=7507.466634, train/accuracy=0.577109, train/loss=2.000762, validation/accuracy=0.532140, validation/loss=2.222145, validation/num_examples=50000
I0215 11:22:14.240690 139807686616832 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.239913210272789, loss=3.690246105194092
I0215 11:25:01.793482 139807661438720 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.23421254754066467, loss=3.583004951477051
I0215 11:27:49.276147 139807686616832 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.23934030532836914, loss=3.649806022644043
I0215 11:29:43.776757 139970484569920 spec.py:321] Evaluating on the training split.
I0215 11:29:51.462925 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 11:30:05.370180 139970484569920 spec.py:349] Evaluating on the test split.
I0215 11:30:07.544353 139970484569920 submission_runner.py:408] Time since start: 8041.36s, 	Step: 22843, 	{'train/accuracy': 0.5935506820678711, 'train/loss': 1.934483289718628, 'validation/accuracy': 0.5549799799919128, 'validation/loss': 2.129647970199585, 'validation/num_examples': 50000, 'test/accuracy': 0.4279000163078308, 'test/loss': 2.785773277282715, 'test/num_examples': 10000, 'score': 7694.737969875336, 'total_duration': 8041.35707783699, 'accumulated_submission_time': 7694.737969875336, 'accumulated_eval_time': 345.41608119010925, 'accumulated_logging_time': 0.4467165470123291}
I0215 11:30:07.565883 139807653046016 logging_writer.py:48] [22843] accumulated_eval_time=345.416081, accumulated_logging_time=0.446717, accumulated_submission_time=7694.737970, global_step=22843, preemption_count=0, score=7694.737970, test/accuracy=0.427900, test/loss=2.785773, test/num_examples=10000, total_duration=8041.357078, train/accuracy=0.593551, train/loss=1.934483, validation/accuracy=0.554980, validation/loss=2.129648, validation/num_examples=50000
I0215 11:31:00.453181 139807661438720 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.23335154354572296, loss=3.7040696144104004
I0215 11:33:48.044952 139807653046016 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.24988727271556854, loss=3.6089704036712646
I0215 11:36:35.807908 139807661438720 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.24342364072799683, loss=3.682671546936035
I0215 11:38:37.592143 139970484569920 spec.py:321] Evaluating on the training split.
I0215 11:38:45.440486 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 11:38:59.505127 139970484569920 spec.py:349] Evaluating on the test split.
I0215 11:39:01.635847 139970484569920 submission_runner.py:408] Time since start: 8575.45s, 	Step: 24365, 	{'train/accuracy': 0.5981743931770325, 'train/loss': 1.9284611940383911, 'validation/accuracy': 0.553659975528717, 'validation/loss': 2.1154723167419434, 'validation/num_examples': 50000, 'test/accuracy': 0.42990002036094666, 'test/loss': 2.796659231185913, 'test/num_examples': 10000, 'score': 8204.703876972198, 'total_duration': 8575.448627948761, 'accumulated_submission_time': 8204.703876972198, 'accumulated_eval_time': 369.45977759361267, 'accumulated_logging_time': 0.4787428379058838}
I0215 11:39:01.653367 139807678224128 logging_writer.py:48] [24365] accumulated_eval_time=369.459778, accumulated_logging_time=0.478743, accumulated_submission_time=8204.703877, global_step=24365, preemption_count=0, score=8204.703877, test/accuracy=0.429900, test/loss=2.796659, test/num_examples=10000, total_duration=8575.448628, train/accuracy=0.598174, train/loss=1.928461, validation/accuracy=0.553660, validation/loss=2.115472, validation/num_examples=50000
I0215 11:39:47.122035 139807686616832 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.24716518819332123, loss=3.634408473968506
I0215 11:42:34.536344 139807678224128 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.2434902936220169, loss=3.642235517501831
I0215 11:45:22.185464 139807686616832 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.24088509380817413, loss=3.6054556369781494
I0215 11:47:31.842802 139970484569920 spec.py:321] Evaluating on the training split.
I0215 11:47:39.646986 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 11:47:53.342298 139970484569920 spec.py:349] Evaluating on the test split.
I0215 11:47:55.475758 139970484569920 submission_runner.py:408] Time since start: 9109.29s, 	Step: 25889, 	{'train/accuracy': 0.6305803656578064, 'train/loss': 1.8610056638717651, 'validation/accuracy': 0.5537199974060059, 'validation/loss': 2.2009079456329346, 'validation/num_examples': 50000, 'test/accuracy': 0.4328000247478485, 'test/loss': 2.8634281158447266, 'test/num_examples': 10000, 'score': 8714.832880973816, 'total_duration': 9109.288538694382, 'accumulated_submission_time': 8714.832880973816, 'accumulated_eval_time': 393.09272933006287, 'accumulated_logging_time': 0.5058033466339111}
I0215 11:47:55.493420 139807653046016 logging_writer.py:48] [25889] accumulated_eval_time=393.092729, accumulated_logging_time=0.505803, accumulated_submission_time=8714.832881, global_step=25889, preemption_count=0, score=8714.832881, test/accuracy=0.432800, test/loss=2.863428, test/num_examples=10000, total_duration=9109.288539, train/accuracy=0.630580, train/loss=1.861006, validation/accuracy=0.553720, validation/loss=2.200908, validation/num_examples=50000
I0215 11:48:32.964297 139807661438720 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.24569687247276306, loss=3.5785646438598633
I0215 11:51:20.363119 139807653046016 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.2441481500864029, loss=3.630763053894043
I0215 11:54:08.022865 139807661438720 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.2429404854774475, loss=3.6000776290893555
I0215 11:56:25.516170 139970484569920 spec.py:321] Evaluating on the training split.
I0215 11:56:33.492418 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 11:56:46.760633 139970484569920 spec.py:349] Evaluating on the test split.
I0215 11:56:48.988831 139970484569920 submission_runner.py:408] Time since start: 9642.80s, 	Step: 27412, 	{'train/accuracy': 0.5844627022743225, 'train/loss': 2.019122838973999, 'validation/accuracy': 0.5302799940109253, 'validation/loss': 2.2804887294769287, 'validation/num_examples': 50000, 'test/accuracy': 0.4180000126361847, 'test/loss': 2.9541094303131104, 'test/num_examples': 10000, 'score': 9224.797441482544, 'total_duration': 9642.80155491829, 'accumulated_submission_time': 9224.797441482544, 'accumulated_eval_time': 416.5653192996979, 'accumulated_logging_time': 0.5326685905456543}
I0215 11:56:49.010334 139807711794944 logging_writer.py:48] [27412] accumulated_eval_time=416.565319, accumulated_logging_time=0.532669, accumulated_submission_time=9224.797441, global_step=27412, preemption_count=0, score=9224.797441, test/accuracy=0.418000, test/loss=2.954109, test/num_examples=10000, total_duration=9642.801555, train/accuracy=0.584463, train/loss=2.019123, validation/accuracy=0.530280, validation/loss=2.280489, validation/num_examples=50000
I0215 11:57:18.779033 139807720187648 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.2489945888519287, loss=3.5620126724243164
I0215 12:00:06.138710 139807711794944 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.23749051988124847, loss=3.5372278690338135
I0215 12:02:53.523573 139807720187648 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.2644890248775482, loss=3.6564724445343018
I0215 12:05:19.025253 139970484569920 spec.py:321] Evaluating on the training split.
I0215 12:05:27.149371 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 12:05:40.688071 139970484569920 spec.py:349] Evaluating on the test split.
I0215 12:05:42.790253 139970484569920 submission_runner.py:408] Time since start: 10176.60s, 	Step: 28936, 	{'train/accuracy': 0.6235451102256775, 'train/loss': 1.7438464164733887, 'validation/accuracy': 0.5734800100326538, 'validation/loss': 1.9896472692489624, 'validation/num_examples': 50000, 'test/accuracy': 0.4466000199317932, 'test/loss': 2.6595518589019775, 'test/num_examples': 10000, 'score': 9734.749379396439, 'total_duration': 10176.603019237518, 'accumulated_submission_time': 9734.749379396439, 'accumulated_eval_time': 440.3303003311157, 'accumulated_logging_time': 0.5646851062774658}
I0215 12:05:42.807469 139807669831424 logging_writer.py:48] [28936] accumulated_eval_time=440.330300, accumulated_logging_time=0.564685, accumulated_submission_time=9734.749379, global_step=28936, preemption_count=0, score=9734.749379, test/accuracy=0.446600, test/loss=2.659552, test/num_examples=10000, total_duration=10176.603019, train/accuracy=0.623545, train/loss=1.743846, validation/accuracy=0.573480, validation/loss=1.989647, validation/num_examples=50000
I0215 12:06:04.565867 139807678224128 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.24147984385490417, loss=3.5653014183044434
I0215 12:08:52.077594 139807669831424 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.2656523287296295, loss=3.6322052478790283
I0215 12:11:39.527286 139807678224128 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.26057401299476624, loss=3.635003089904785
I0215 12:14:13.036696 139970484569920 spec.py:321] Evaluating on the training split.
I0215 12:14:21.265419 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 12:14:35.853952 139970484569920 spec.py:349] Evaluating on the test split.
I0215 12:14:38.004375 139970484569920 submission_runner.py:408] Time since start: 10711.82s, 	Step: 30460, 	{'train/accuracy': 0.6092952489852905, 'train/loss': 1.9225893020629883, 'validation/accuracy': 0.5622999668121338, 'validation/loss': 2.1229209899902344, 'validation/num_examples': 50000, 'test/accuracy': 0.44220003485679626, 'test/loss': 2.7977213859558105, 'test/num_examples': 10000, 'score': 10244.919599533081, 'total_duration': 10711.817137241364, 'accumulated_submission_time': 10244.919599533081, 'accumulated_eval_time': 465.29795002937317, 'accumulated_logging_time': 0.5910100936889648}
I0215 12:14:38.023594 139807703402240 logging_writer.py:48] [30460] accumulated_eval_time=465.297950, accumulated_logging_time=0.591010, accumulated_submission_time=10244.919600, global_step=30460, preemption_count=0, score=10244.919600, test/accuracy=0.442200, test/loss=2.797721, test/num_examples=10000, total_duration=10711.817137, train/accuracy=0.609295, train/loss=1.922589, validation/accuracy=0.562300, validation/loss=2.122921, validation/num_examples=50000
I0215 12:14:51.755353 139807711794944 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.2680169939994812, loss=3.5329501628875732
I0215 12:17:39.244082 139807703402240 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.261199027299881, loss=3.6260266304016113
I0215 12:20:26.830669 139807711794944 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.25344356894493103, loss=3.5533342361450195
I0215 12:23:08.324821 139970484569920 spec.py:321] Evaluating on the training split.
I0215 12:23:16.205689 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 12:23:30.277000 139970484569920 spec.py:349] Evaluating on the test split.
I0215 12:23:32.430143 139970484569920 submission_runner.py:408] Time since start: 11246.24s, 	Step: 31984, 	{'train/accuracy': 0.6049904227256775, 'train/loss': 1.8935025930404663, 'validation/accuracy': 0.5635600090026855, 'validation/loss': 2.091768741607666, 'validation/num_examples': 50000, 'test/accuracy': 0.4473000168800354, 'test/loss': 2.7491166591644287, 'test/num_examples': 10000, 'score': 10755.159065246582, 'total_duration': 11246.242913007736, 'accumulated_submission_time': 10755.159065246582, 'accumulated_eval_time': 489.4032461643219, 'accumulated_logging_time': 0.6199793815612793}
I0215 12:23:32.450597 139804322813696 logging_writer.py:48] [31984] accumulated_eval_time=489.403246, accumulated_logging_time=0.619979, accumulated_submission_time=10755.159065, global_step=31984, preemption_count=0, score=10755.159065, test/accuracy=0.447300, test/loss=2.749117, test/num_examples=10000, total_duration=11246.242913, train/accuracy=0.604990, train/loss=1.893503, validation/accuracy=0.563560, validation/loss=2.091769, validation/num_examples=50000
I0215 12:23:38.140219 139804331206400 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.2558530867099762, loss=3.5262842178344727
I0215 12:26:25.574900 139804322813696 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.25065261125564575, loss=3.512580394744873
I0215 12:29:13.119866 139804331206400 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.257151335477829, loss=3.4979746341705322
I0215 12:32:00.725387 139804322813696 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.25997453927993774, loss=3.5730690956115723
I0215 12:32:02.502314 139970484569920 spec.py:321] Evaluating on the training split.
I0215 12:32:10.489986 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 12:32:25.205346 139970484569920 spec.py:349] Evaluating on the test split.
I0215 12:32:27.377308 139970484569920 submission_runner.py:408] Time since start: 11781.19s, 	Step: 33507, 	{'train/accuracy': 0.6124641299247742, 'train/loss': 1.8301490545272827, 'validation/accuracy': 0.5707199573516846, 'validation/loss': 2.038625955581665, 'validation/num_examples': 50000, 'test/accuracy': 0.4524000287055969, 'test/loss': 2.6779978275299072, 'test/num_examples': 10000, 'score': 11265.150213241577, 'total_duration': 11781.190009593964, 'accumulated_submission_time': 11265.150213241577, 'accumulated_eval_time': 514.2781403064728, 'accumulated_logging_time': 0.6504311561584473}
I0215 12:32:27.397604 139807703402240 logging_writer.py:48] [33507] accumulated_eval_time=514.278140, accumulated_logging_time=0.650431, accumulated_submission_time=11265.150213, global_step=33507, preemption_count=0, score=11265.150213, test/accuracy=0.452400, test/loss=2.677998, test/num_examples=10000, total_duration=11781.190010, train/accuracy=0.612464, train/loss=1.830149, validation/accuracy=0.570720, validation/loss=2.038626, validation/num_examples=50000
I0215 12:35:12.815353 139807711794944 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.26094481348991394, loss=3.597590208053589
I0215 12:38:00.359021 139807703402240 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.2640497088432312, loss=3.582796573638916
I0215 12:40:47.864326 139807711794944 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.26346492767333984, loss=3.508782148361206
I0215 12:40:57.680753 139970484569920 spec.py:321] Evaluating on the training split.
I0215 12:41:05.698466 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 12:41:19.211291 139970484569920 spec.py:349] Evaluating on the test split.
I0215 12:41:21.355654 139970484569920 submission_runner.py:408] Time since start: 12315.17s, 	Step: 35031, 	{'train/accuracy': 0.6182039380073547, 'train/loss': 1.8635214567184448, 'validation/accuracy': 0.549239993095398, 'validation/loss': 2.212022542953491, 'validation/num_examples': 50000, 'test/accuracy': 0.4263000190258026, 'test/loss': 2.8801536560058594, 'test/num_examples': 10000, 'score': 11775.370599031448, 'total_duration': 12315.168415546417, 'accumulated_submission_time': 11775.370599031448, 'accumulated_eval_time': 537.9529922008514, 'accumulated_logging_time': 0.6818163394927979}
I0215 12:41:21.373595 139804331206400 logging_writer.py:48] [35031] accumulated_eval_time=537.952992, accumulated_logging_time=0.681816, accumulated_submission_time=11775.370599, global_step=35031, preemption_count=0, score=11775.370599, test/accuracy=0.426300, test/loss=2.880154, test/num_examples=10000, total_duration=12315.168416, train/accuracy=0.618204, train/loss=1.863521, validation/accuracy=0.549240, validation/loss=2.212023, validation/num_examples=50000
I0215 12:43:58.799220 139804339599104 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.2544029951095581, loss=3.4928300380706787
I0215 12:46:46.280538 139804331206400 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.2554306387901306, loss=3.486271619796753
I0215 12:49:33.740210 139804339599104 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.2697570025920868, loss=3.636282444000244
I0215 12:49:51.649369 139970484569920 spec.py:321] Evaluating on the training split.
I0215 12:49:59.636828 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 12:50:20.094475 139970484569920 spec.py:349] Evaluating on the test split.
I0215 12:50:22.227738 139970484569920 submission_runner.py:408] Time since start: 12856.04s, 	Step: 36555, 	{'train/accuracy': 0.5966796875, 'train/loss': 1.9315264225006104, 'validation/accuracy': 0.5424000024795532, 'validation/loss': 2.1991610527038574, 'validation/num_examples': 50000, 'test/accuracy': 0.42640000581741333, 'test/loss': 2.880263566970825, 'test/num_examples': 10000, 'score': 12285.586842775345, 'total_duration': 12856.040487766266, 'accumulated_submission_time': 12285.586842775345, 'accumulated_eval_time': 568.5313017368317, 'accumulated_logging_time': 0.7088274955749512}
I0215 12:50:22.245098 139807644673792 logging_writer.py:48] [36555] accumulated_eval_time=568.531302, accumulated_logging_time=0.708827, accumulated_submission_time=12285.586843, global_step=36555, preemption_count=0, score=12285.586843, test/accuracy=0.426400, test/loss=2.880264, test/num_examples=10000, total_duration=12856.040488, train/accuracy=0.596680, train/loss=1.931526, validation/accuracy=0.542400, validation/loss=2.199161, validation/num_examples=50000
I0215 12:52:51.455151 139807703402240 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.26298844814300537, loss=3.563995361328125
I0215 12:55:38.981422 139807644673792 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.2590535581111908, loss=3.493572235107422
I0215 12:58:26.434706 139807703402240 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.2562352120876312, loss=3.4664669036865234
I0215 12:58:52.361337 139970484569920 spec.py:321] Evaluating on the training split.
I0215 12:58:59.417567 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 12:59:13.138368 139970484569920 spec.py:349] Evaluating on the test split.
I0215 12:59:15.236386 139970484569920 submission_runner.py:408] Time since start: 13389.05s, 	Step: 38079, 	{'train/accuracy': 0.6463249325752258, 'train/loss': 1.78249990940094, 'validation/accuracy': 0.5925399661064148, 'validation/loss': 2.026228189468384, 'validation/num_examples': 50000, 'test/accuracy': 0.47200003266334534, 'test/loss': 2.6642837524414062, 'test/num_examples': 10000, 'score': 12795.643894195557, 'total_duration': 13389.049154758453, 'accumulated_submission_time': 12795.643894195557, 'accumulated_eval_time': 591.4063141345978, 'accumulated_logging_time': 0.7357571125030518}
I0215 12:59:15.253384 139803995662080 logging_writer.py:48] [38079] accumulated_eval_time=591.406314, accumulated_logging_time=0.735757, accumulated_submission_time=12795.643894, global_step=38079, preemption_count=0, score=12795.643894, test/accuracy=0.472000, test/loss=2.664284, test/num_examples=10000, total_duration=13389.049155, train/accuracy=0.646325, train/loss=1.782500, validation/accuracy=0.592540, validation/loss=2.026228, validation/num_examples=50000
I0215 13:01:36.672570 139804004054784 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.26131197810173035, loss=3.474386692047119
I0215 13:04:24.088587 139803995662080 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.27365922927856445, loss=3.4686524868011475
I0215 13:07:11.705542 139804004054784 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.2624155580997467, loss=3.433337450027466
I0215 13:07:45.298347 139970484569920 spec.py:321] Evaluating on the training split.
I0215 13:07:52.408735 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 13:08:05.531283 139970484569920 spec.py:349] Evaluating on the test split.
I0215 13:08:07.733372 139970484569920 submission_runner.py:408] Time since start: 13921.55s, 	Step: 39602, 	{'train/accuracy': 0.6285275816917419, 'train/loss': 1.8114537000656128, 'validation/accuracy': 0.5889999866485596, 'validation/loss': 2.0118296146392822, 'validation/num_examples': 50000, 'test/accuracy': 0.4617000222206116, 'test/loss': 2.6926677227020264, 'test/num_examples': 10000, 'score': 13305.627413749695, 'total_duration': 13921.54613494873, 'accumulated_submission_time': 13305.627413749695, 'accumulated_eval_time': 613.8412947654724, 'accumulated_logging_time': 0.7629227638244629}
I0215 13:08:07.750311 139803995662080 logging_writer.py:48] [39602] accumulated_eval_time=613.841295, accumulated_logging_time=0.762923, accumulated_submission_time=13305.627414, global_step=39602, preemption_count=0, score=13305.627414, test/accuracy=0.461700, test/loss=2.692668, test/num_examples=10000, total_duration=13921.546135, train/accuracy=0.628528, train/loss=1.811454, validation/accuracy=0.589000, validation/loss=2.011830, validation/num_examples=50000
I0215 13:10:21.205163 139804004054784 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.25388890504837036, loss=3.4852733612060547
I0215 13:13:08.668313 139803995662080 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.265471488237381, loss=3.571601152420044
I0215 13:15:56.122505 139804004054784 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.2625001072883606, loss=3.529127359390259
I0215 13:16:38.020407 139970484569920 spec.py:321] Evaluating on the training split.
I0215 13:16:45.032032 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 13:16:55.896890 139970484569920 spec.py:349] Evaluating on the test split.
I0215 13:16:58.180264 139970484569920 submission_runner.py:408] Time since start: 14451.99s, 	Step: 41127, 	{'train/accuracy': 0.6400271058082581, 'train/loss': 1.7320114374160767, 'validation/accuracy': 0.5934799909591675, 'validation/loss': 1.9413087368011475, 'validation/num_examples': 50000, 'test/accuracy': 0.46800002455711365, 'test/loss': 2.597066640853882, 'test/num_examples': 10000, 'score': 13815.838862657547, 'total_duration': 14451.993034601212, 'accumulated_submission_time': 13815.838862657547, 'accumulated_eval_time': 634.0011188983917, 'accumulated_logging_time': 0.7886664867401123}
I0215 13:16:58.198014 139803995662080 logging_writer.py:48] [41127] accumulated_eval_time=634.001119, accumulated_logging_time=0.788666, accumulated_submission_time=13815.838863, global_step=41127, preemption_count=0, score=13815.838863, test/accuracy=0.468000, test/loss=2.597067, test/num_examples=10000, total_duration=14451.993035, train/accuracy=0.640027, train/loss=1.732011, validation/accuracy=0.593480, validation/loss=1.941309, validation/num_examples=50000
I0215 13:19:03.376937 139807149778688 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.26120221614837646, loss=3.4183597564697266
I0215 13:21:50.862740 139803995662080 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.2540471851825714, loss=3.468780755996704
I0215 13:24:38.502155 139807149778688 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.2599295675754547, loss=3.5123066902160645
I0215 13:25:28.228728 139970484569920 spec.py:321] Evaluating on the training split.
I0215 13:25:35.089790 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 13:25:47.780405 139970484569920 spec.py:349] Evaluating on the test split.
I0215 13:25:50.007259 139970484569920 submission_runner.py:408] Time since start: 14983.82s, 	Step: 42650, 	{'train/accuracy': 0.6274314522743225, 'train/loss': 1.8249386548995972, 'validation/accuracy': 0.5841000080108643, 'validation/loss': 2.039947748184204, 'validation/num_examples': 50000, 'test/accuracy': 0.4499000310897827, 'test/loss': 2.7198336124420166, 'test/num_examples': 10000, 'score': 14325.808423280716, 'total_duration': 14983.820021390915, 'accumulated_submission_time': 14325.808423280716, 'accumulated_eval_time': 655.7796139717102, 'accumulated_logging_time': 0.815596342086792}
I0215 13:25:50.024523 139807644673792 logging_writer.py:48] [42650] accumulated_eval_time=655.779614, accumulated_logging_time=0.815596, accumulated_submission_time=14325.808423, global_step=42650, preemption_count=0, score=14325.808423, test/accuracy=0.449900, test/loss=2.719834, test/num_examples=10000, total_duration=14983.820021, train/accuracy=0.627431, train/loss=1.824939, validation/accuracy=0.584100, validation/loss=2.039948, validation/num_examples=50000
I0215 13:27:47.526439 139807703402240 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.2592342495918274, loss=3.406280517578125
I0215 13:30:34.940310 139807644673792 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.2625042200088501, loss=3.375822067260742
I0215 13:33:22.312523 139807703402240 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.27664855122566223, loss=3.535933494567871
I0215 13:34:20.325181 139970484569920 spec.py:321] Evaluating on the training split.
I0215 13:34:27.088211 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 13:34:35.906274 139970484569920 spec.py:349] Evaluating on the test split.
I0215 13:34:38.148281 139970484569920 submission_runner.py:408] Time since start: 15511.96s, 	Step: 44175, 	{'train/accuracy': 0.6434151530265808, 'train/loss': 1.6949825286865234, 'validation/accuracy': 0.5753600001335144, 'validation/loss': 2.0091612339019775, 'validation/num_examples': 50000, 'test/accuracy': 0.4572000205516815, 'test/loss': 2.660670042037964, 'test/num_examples': 10000, 'score': 14836.049505472183, 'total_duration': 15511.961038351059, 'accumulated_submission_time': 14836.049505472183, 'accumulated_eval_time': 673.6026911735535, 'accumulated_logging_time': 0.8420405387878418}
I0215 13:34:38.171985 139807158171392 logging_writer.py:48] [44175] accumulated_eval_time=673.602691, accumulated_logging_time=0.842041, accumulated_submission_time=14836.049505, global_step=44175, preemption_count=0, score=14836.049505, test/accuracy=0.457200, test/loss=2.660670, test/num_examples=10000, total_duration=15511.961038, train/accuracy=0.643415, train/loss=1.694983, validation/accuracy=0.575360, validation/loss=2.009161, validation/num_examples=50000
I0215 13:36:27.255367 139807720187648 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.2611546218395233, loss=3.3644590377807617
I0215 13:39:14.777224 139807158171392 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.27682408690452576, loss=3.500473976135254
I0215 13:42:02.291795 139807720187648 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.25771236419677734, loss=3.3841493129730225
I0215 13:43:08.353381 139970484569920 spec.py:321] Evaluating on the training split.
I0215 13:43:15.052208 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 13:43:23.754638 139970484569920 spec.py:349] Evaluating on the test split.
I0215 13:43:26.011048 139970484569920 submission_runner.py:408] Time since start: 16039.82s, 	Step: 45699, 	{'train/accuracy': 0.6517458558082581, 'train/loss': 1.6850515604019165, 'validation/accuracy': 0.6002599596977234, 'validation/loss': 1.921714186668396, 'validation/num_examples': 50000, 'test/accuracy': 0.4750000238418579, 'test/loss': 2.5920767784118652, 'test/num_examples': 10000, 'score': 15346.166721105576, 'total_duration': 16039.82380604744, 'accumulated_submission_time': 15346.166721105576, 'accumulated_eval_time': 691.2603213787079, 'accumulated_logging_time': 0.878584623336792}
I0215 13:43:26.031959 139807149778688 logging_writer.py:48] [45699] accumulated_eval_time=691.260321, accumulated_logging_time=0.878585, accumulated_submission_time=15346.166721, global_step=45699, preemption_count=0, score=15346.166721, test/accuracy=0.475000, test/loss=2.592077, test/num_examples=10000, total_duration=16039.823806, train/accuracy=0.651746, train/loss=1.685052, validation/accuracy=0.600260, validation/loss=1.921714, validation/num_examples=50000
I0215 13:45:07.106325 139807644673792 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.2703765630722046, loss=3.4896042346954346
I0215 13:47:54.573164 139807149778688 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.2708249092102051, loss=3.485224723815918
I0215 13:50:41.920526 139807644673792 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.2717001736164093, loss=3.4337313175201416
I0215 13:51:56.304757 139970484569920 spec.py:321] Evaluating on the training split.
I0215 13:52:02.898283 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 13:52:13.402235 139970484569920 spec.py:349] Evaluating on the test split.
I0215 13:52:15.724046 139970484569920 submission_runner.py:408] Time since start: 16569.54s, 	Step: 47224, 	{'train/accuracy': 0.6491748690605164, 'train/loss': 1.6727479696273804, 'validation/accuracy': 0.5950199961662292, 'validation/loss': 1.913076639175415, 'validation/num_examples': 50000, 'test/accuracy': 0.4792000353336334, 'test/loss': 2.5365116596221924, 'test/num_examples': 10000, 'score': 15856.377391576767, 'total_duration': 16569.536675214767, 'accumulated_submission_time': 15856.377391576767, 'accumulated_eval_time': 710.6794383525848, 'accumulated_logging_time': 0.910062313079834}
I0215 13:52:15.744774 139804004054784 logging_writer.py:48] [47224] accumulated_eval_time=710.679438, accumulated_logging_time=0.910062, accumulated_submission_time=15856.377392, global_step=47224, preemption_count=0, score=15856.377392, test/accuracy=0.479200, test/loss=2.536512, test/num_examples=10000, total_duration=16569.536675, train/accuracy=0.649175, train/loss=1.672748, validation/accuracy=0.595020, validation/loss=1.913077, validation/num_examples=50000
I0215 13:53:48.418338 139807158171392 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.27101221680641174, loss=3.43051815032959
I0215 13:56:35.988040 139804004054784 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.279607355594635, loss=3.486875057220459
I0215 13:59:23.420292 139807158171392 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.27441245317459106, loss=3.4734621047973633
I0215 14:00:46.017546 139970484569920 spec.py:321] Evaluating on the training split.
I0215 14:00:52.331767 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 14:01:03.919431 139970484569920 spec.py:349] Evaluating on the test split.
I0215 14:01:06.148289 139970484569920 submission_runner.py:408] Time since start: 17099.96s, 	Step: 48748, 	{'train/accuracy': 0.6578443646430969, 'train/loss': 1.677195429801941, 'validation/accuracy': 0.6079999804496765, 'validation/loss': 1.9055547714233398, 'validation/num_examples': 50000, 'test/accuracy': 0.4805000126361847, 'test/loss': 2.5795655250549316, 'test/num_examples': 10000, 'score': 16366.59017109871, 'total_duration': 17099.961057662964, 'accumulated_submission_time': 16366.59017109871, 'accumulated_eval_time': 730.8101537227631, 'accumulated_logging_time': 0.9403750896453857}
I0215 14:01:06.166065 139807149778688 logging_writer.py:48] [48748] accumulated_eval_time=730.810154, accumulated_logging_time=0.940375, accumulated_submission_time=16366.590171, global_step=48748, preemption_count=0, score=16366.590171, test/accuracy=0.480500, test/loss=2.579566, test/num_examples=10000, total_duration=17099.961058, train/accuracy=0.657844, train/loss=1.677195, validation/accuracy=0.608000, validation/loss=1.905555, validation/num_examples=50000
I0215 14:02:30.977566 139807644673792 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.2811838388442993, loss=3.405667543411255
I0215 14:05:18.232240 139807149778688 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.26717332005500793, loss=3.4202075004577637
I0215 14:08:05.676918 139807644673792 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.28462663292884827, loss=3.3769161701202393
I0215 14:09:36.400376 139970484569920 spec.py:321] Evaluating on the training split.
I0215 14:09:42.652756 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 14:09:53.131189 139970484569920 spec.py:349] Evaluating on the test split.
I0215 14:09:55.395343 139970484569920 submission_runner.py:408] Time since start: 17629.21s, 	Step: 50273, 	{'train/accuracy': 0.6466836333274841, 'train/loss': 1.681785225868225, 'validation/accuracy': 0.6007399559020996, 'validation/loss': 1.8856514692306519, 'validation/num_examples': 50000, 'test/accuracy': 0.48130002617836, 'test/loss': 2.5445470809936523, 'test/num_examples': 10000, 'score': 16876.49962568283, 'total_duration': 17629.207921028137, 'accumulated_submission_time': 16876.49962568283, 'accumulated_eval_time': 749.8049001693726, 'accumulated_logging_time': 1.2331180572509766}
I0215 14:09:55.418686 139807158171392 logging_writer.py:48] [50273] accumulated_eval_time=749.804900, accumulated_logging_time=1.233118, accumulated_submission_time=16876.499626, global_step=50273, preemption_count=0, score=16876.499626, test/accuracy=0.481300, test/loss=2.544547, test/num_examples=10000, total_duration=17629.207921, train/accuracy=0.646684, train/loss=1.681785, validation/accuracy=0.600740, validation/loss=1.885651, validation/num_examples=50000
I0215 14:11:11.717740 139807711794944 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.27084916830062866, loss=3.3177943229675293
I0215 14:13:59.249991 139807158171392 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.2754145562648773, loss=3.479079008102417
I0215 14:16:46.584883 139807711794944 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.2741168737411499, loss=3.367375373840332
I0215 14:18:25.448578 139970484569920 spec.py:321] Evaluating on the training split.
I0215 14:18:31.769875 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 14:18:40.401520 139970484569920 spec.py:349] Evaluating on the test split.
I0215 14:18:42.716225 139970484569920 submission_runner.py:408] Time since start: 18156.53s, 	Step: 51797, 	{'train/accuracy': 0.7037826776504517, 'train/loss': 1.4232388734817505, 'validation/accuracy': 0.6117199659347534, 'validation/loss': 1.8318618535995483, 'validation/num_examples': 50000, 'test/accuracy': 0.48910000920295715, 'test/loss': 2.4729981422424316, 'test/num_examples': 10000, 'score': 17386.470391750336, 'total_duration': 18156.528982400894, 'accumulated_submission_time': 17386.470391750336, 'accumulated_eval_time': 767.072503566742, 'accumulated_logging_time': 1.2664318084716797}
I0215 14:18:42.738828 139804004054784 logging_writer.py:48] [51797] accumulated_eval_time=767.072504, accumulated_logging_time=1.266432, accumulated_submission_time=17386.470392, global_step=51797, preemption_count=0, score=17386.470392, test/accuracy=0.489100, test/loss=2.472998, test/num_examples=10000, total_duration=18156.528982, train/accuracy=0.703783, train/loss=1.423239, validation/accuracy=0.611720, validation/loss=1.831862, validation/num_examples=50000
I0215 14:19:50.988881 139807149778688 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.2744421362876892, loss=3.425966262817383
I0215 14:22:38.438279 139804004054784 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.271894246339798, loss=3.3852016925811768
I0215 14:25:25.979305 139807149778688 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.28515490889549255, loss=3.402346611022949
I0215 14:27:12.830900 139970484569920 spec.py:321] Evaluating on the training split.
I0215 14:27:19.263204 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 14:27:27.879308 139970484569920 spec.py:349] Evaluating on the test split.
I0215 14:27:30.561721 139970484569920 submission_runner.py:408] Time since start: 18684.37s, 	Step: 53321, 	{'train/accuracy': 0.6437739133834839, 'train/loss': 1.7253005504608154, 'validation/accuracy': 0.5821200013160706, 'validation/loss': 2.015341281890869, 'validation/num_examples': 50000, 'test/accuracy': 0.4604000151157379, 'test/loss': 2.6728532314300537, 'test/num_examples': 10000, 'score': 17896.50174498558, 'total_duration': 18684.37446331978, 'accumulated_submission_time': 17896.50174498558, 'accumulated_eval_time': 784.8032908439636, 'accumulated_logging_time': 1.2994449138641357}
I0215 14:27:30.584875 139807703402240 logging_writer.py:48] [53321] accumulated_eval_time=784.803291, accumulated_logging_time=1.299445, accumulated_submission_time=17896.501745, global_step=53321, preemption_count=0, score=17896.501745, test/accuracy=0.460400, test/loss=2.672853, test/num_examples=10000, total_duration=18684.374463, train/accuracy=0.643774, train/loss=1.725301, validation/accuracy=0.582120, validation/loss=2.015341, validation/num_examples=50000
I0215 14:28:30.777976 139807711794944 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.2785894572734833, loss=3.4239680767059326
I0215 14:31:18.208280 139807703402240 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.2860119938850403, loss=3.438250780105591
I0215 14:34:05.766502 139807711794944 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.27085477113723755, loss=3.4298255443573
I0215 14:36:00.706181 139970484569920 spec.py:321] Evaluating on the training split.
I0215 14:36:06.928909 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 14:36:16.062577 139970484569920 spec.py:349] Evaluating on the test split.
I0215 14:36:18.384315 139970484569920 submission_runner.py:408] Time since start: 19212.20s, 	Step: 54845, 	{'train/accuracy': 0.6750039458274841, 'train/loss': 1.6002681255340576, 'validation/accuracy': 0.6157199740409851, 'validation/loss': 1.8629964590072632, 'validation/num_examples': 50000, 'test/accuracy': 0.49240002036094666, 'test/loss': 2.5258798599243164, 'test/num_examples': 10000, 'score': 18406.563932418823, 'total_duration': 19212.19704079628, 'accumulated_submission_time': 18406.563932418823, 'accumulated_eval_time': 802.4813590049744, 'accumulated_logging_time': 1.3321518898010254}
I0215 14:36:18.406985 139803995662080 logging_writer.py:48] [54845] accumulated_eval_time=802.481359, accumulated_logging_time=1.332152, accumulated_submission_time=18406.563932, global_step=54845, preemption_count=0, score=18406.563932, test/accuracy=0.492400, test/loss=2.525880, test/num_examples=10000, total_duration=19212.197041, train/accuracy=0.675004, train/loss=1.600268, validation/accuracy=0.615720, validation/loss=1.862996, validation/num_examples=50000
I0215 14:37:10.577810 139804004054784 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.27411147952079773, loss=3.354173183441162
I0215 14:39:57.918980 139803995662080 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.2821712791919708, loss=3.4022960662841797
I0215 14:42:45.211925 139804004054784 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.28633683919906616, loss=3.386704683303833
I0215 14:44:48.478179 139970484569920 spec.py:321] Evaluating on the training split.
I0215 14:44:54.669294 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 14:45:03.348433 139970484569920 spec.py:349] Evaluating on the test split.
I0215 14:45:05.601275 139970484569920 submission_runner.py:408] Time since start: 19739.41s, 	Step: 56370, 	{'train/accuracy': 0.6426379084587097, 'train/loss': 1.7485918998718262, 'validation/accuracy': 0.5929799675941467, 'validation/loss': 1.977038025856018, 'validation/num_examples': 50000, 'test/accuracy': 0.46720001101493835, 'test/loss': 2.6630213260650635, 'test/num_examples': 10000, 'score': 18916.572904348373, 'total_duration': 19739.414001226425, 'accumulated_submission_time': 18916.572904348373, 'accumulated_eval_time': 819.604391336441, 'accumulated_logging_time': 1.3656785488128662}
I0215 14:45:05.623214 139807703402240 logging_writer.py:48] [56370] accumulated_eval_time=819.604391, accumulated_logging_time=1.365679, accumulated_submission_time=18916.572904, global_step=56370, preemption_count=0, score=18916.572904, test/accuracy=0.467200, test/loss=2.663021, test/num_examples=10000, total_duration=19739.414001, train/accuracy=0.642638, train/loss=1.748592, validation/accuracy=0.592980, validation/loss=1.977038, validation/num_examples=50000
I0215 14:45:49.476263 139807711794944 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.2751658856868744, loss=3.3607382774353027
I0215 14:48:36.940243 139807703402240 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.279657244682312, loss=3.3746185302734375
I0215 14:51:24.468177 139807711794944 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.2794554829597473, loss=3.354438304901123
I0215 14:53:35.842314 139970484569920 spec.py:321] Evaluating on the training split.
I0215 14:53:42.099570 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 14:53:50.733231 139970484569920 spec.py:349] Evaluating on the test split.
I0215 14:53:52.981738 139970484569920 submission_runner.py:408] Time since start: 20266.79s, 	Step: 57894, 	{'train/accuracy': 0.6578443646430969, 'train/loss': 1.682417392730713, 'validation/accuracy': 0.6097800135612488, 'validation/loss': 1.9047425985336304, 'validation/num_examples': 50000, 'test/accuracy': 0.4816000163555145, 'test/loss': 2.576265811920166, 'test/num_examples': 10000, 'score': 19426.731298685074, 'total_duration': 20266.79449415207, 'accumulated_submission_time': 19426.731298685074, 'accumulated_eval_time': 836.7437827587128, 'accumulated_logging_time': 1.3981499671936035}
I0215 14:53:53.004229 139803995662080 logging_writer.py:48] [57894] accumulated_eval_time=836.743783, accumulated_logging_time=1.398150, accumulated_submission_time=19426.731299, global_step=57894, preemption_count=0, score=19426.731299, test/accuracy=0.481600, test/loss=2.576266, test/num_examples=10000, total_duration=20266.794494, train/accuracy=0.657844, train/loss=1.682417, validation/accuracy=0.609780, validation/loss=1.904743, validation/num_examples=50000
I0215 14:54:28.781229 139804004054784 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.2800354063510895, loss=3.3150696754455566
I0215 14:57:15.976948 139803995662080 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.2863041162490845, loss=3.3446052074432373
I0215 15:00:03.213441 139804004054784 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.2913646996021271, loss=3.459636688232422
I0215 15:02:23.075590 139970484569920 spec.py:321] Evaluating on the training split.
I0215 15:02:29.505810 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 15:02:37.902340 139970484569920 spec.py:349] Evaluating on the test split.
I0215 15:02:40.144340 139970484569920 submission_runner.py:408] Time since start: 20793.96s, 	Step: 59420, 	{'train/accuracy': 0.6669324040412903, 'train/loss': 1.6188238859176636, 'validation/accuracy': 0.6192799806594849, 'validation/loss': 1.8437219858169556, 'validation/num_examples': 50000, 'test/accuracy': 0.4958000183105469, 'test/loss': 2.4657139778137207, 'test/num_examples': 10000, 'score': 19936.740971326828, 'total_duration': 20793.957093954086, 'accumulated_submission_time': 19936.740971326828, 'accumulated_eval_time': 853.8124947547913, 'accumulated_logging_time': 1.431046485900879}
I0215 15:02:40.167763 139807711794944 logging_writer.py:48] [59420] accumulated_eval_time=853.812495, accumulated_logging_time=1.431046, accumulated_submission_time=19936.740971, global_step=59420, preemption_count=0, score=19936.740971, test/accuracy=0.495800, test/loss=2.465714, test/num_examples=10000, total_duration=20793.957094, train/accuracy=0.666932, train/loss=1.618824, validation/accuracy=0.619280, validation/loss=1.843722, validation/num_examples=50000
I0215 15:03:07.253443 139807720187648 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.281930148601532, loss=3.3764491081237793
I0215 15:05:54.562858 139807711794944 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.2931853234767914, loss=3.352708578109741
I0215 15:08:42.019505 139807720187648 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.281340092420578, loss=3.3331024646759033
I0215 15:11:10.326022 139970484569920 spec.py:321] Evaluating on the training split.
I0215 15:11:16.531595 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 15:11:25.140289 139970484569920 spec.py:349] Evaluating on the test split.
I0215 15:11:27.489204 139970484569920 submission_runner.py:408] Time since start: 21321.30s, 	Step: 60945, 	{'train/accuracy': 0.7052973508834839, 'train/loss': 1.4652222394943237, 'validation/accuracy': 0.6233599781990051, 'validation/loss': 1.8111732006072998, 'validation/num_examples': 50000, 'test/accuracy': 0.4952000379562378, 'test/loss': 2.464139938354492, 'test/num_examples': 10000, 'score': 20446.835471630096, 'total_duration': 21321.3019258976, 'accumulated_submission_time': 20446.835471630096, 'accumulated_eval_time': 870.975608587265, 'accumulated_logging_time': 1.4685769081115723}
I0215 15:11:27.512628 139803995662080 logging_writer.py:48] [60945] accumulated_eval_time=870.975609, accumulated_logging_time=1.468577, accumulated_submission_time=20446.835472, global_step=60945, preemption_count=0, score=20446.835472, test/accuracy=0.495200, test/loss=2.464140, test/num_examples=10000, total_duration=21321.301926, train/accuracy=0.705297, train/loss=1.465222, validation/accuracy=0.623360, validation/loss=1.811173, validation/num_examples=50000
I0215 15:11:46.235079 139804004054784 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.28385913372039795, loss=3.361443281173706
I0215 15:14:33.541046 139803995662080 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.28468525409698486, loss=3.3776803016662598
I0215 15:17:20.858069 139804004054784 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.29593029618263245, loss=3.335845470428467
I0215 15:19:57.538596 139970484569920 spec.py:321] Evaluating on the training split.
I0215 15:20:03.704265 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 15:20:12.215017 139970484569920 spec.py:349] Evaluating on the test split.
I0215 15:20:14.485730 139970484569920 submission_runner.py:408] Time since start: 21848.30s, 	Step: 62470, 	{'train/accuracy': 0.6842115521430969, 'train/loss': 1.529850721359253, 'validation/accuracy': 0.6209999918937683, 'validation/loss': 1.819158673286438, 'validation/num_examples': 50000, 'test/accuracy': 0.4983000159263611, 'test/loss': 2.457092523574829, 'test/num_examples': 10000, 'score': 20956.799617528915, 'total_duration': 21848.297857761383, 'accumulated_submission_time': 20956.799617528915, 'accumulated_eval_time': 887.9220767021179, 'accumulated_logging_time': 1.503570556640625}
I0215 15:20:14.509064 139804004054784 logging_writer.py:48] [62470] accumulated_eval_time=887.922077, accumulated_logging_time=1.503571, accumulated_submission_time=20956.799618, global_step=62470, preemption_count=0, score=20956.799618, test/accuracy=0.498300, test/loss=2.457093, test/num_examples=10000, total_duration=21848.297858, train/accuracy=0.684212, train/loss=1.529851, validation/accuracy=0.621000, validation/loss=1.819159, validation/num_examples=50000
I0215 15:20:24.899319 139807644673792 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.281834214925766, loss=3.259430408477783
I0215 15:23:12.260281 139804004054784 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.2884126901626587, loss=3.32685923576355
I0215 15:25:59.695092 139807644673792 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.2857937812805176, loss=3.3098533153533936
I0215 15:28:44.783707 139970484569920 spec.py:321] Evaluating on the training split.
I0215 15:28:50.980213 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 15:28:59.598383 139970484569920 spec.py:349] Evaluating on the test split.
I0215 15:29:01.826024 139970484569920 submission_runner.py:408] Time since start: 22375.64s, 	Step: 63995, 	{'train/accuracy': 0.6692841053009033, 'train/loss': 1.5799593925476074, 'validation/accuracy': 0.6154199838638306, 'validation/loss': 1.8372719287872314, 'validation/num_examples': 50000, 'test/accuracy': 0.49160003662109375, 'test/loss': 2.4828062057495117, 'test/num_examples': 10000, 'score': 21467.012628555298, 'total_duration': 22375.638775110245, 'accumulated_submission_time': 21467.012628555298, 'accumulated_eval_time': 904.96435379982, 'accumulated_logging_time': 1.5378267765045166}
I0215 15:29:01.851666 139807158171392 logging_writer.py:48] [63995] accumulated_eval_time=904.964354, accumulated_logging_time=1.537827, accumulated_submission_time=21467.012629, global_step=63995, preemption_count=0, score=21467.012629, test/accuracy=0.491600, test/loss=2.482806, test/num_examples=10000, total_duration=22375.638775, train/accuracy=0.669284, train/loss=1.579959, validation/accuracy=0.615420, validation/loss=1.837272, validation/num_examples=50000
I0215 15:29:03.868828 139807720187648 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.295342355966568, loss=3.376467704772949
I0215 15:31:51.096584 139807158171392 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.29318469762802124, loss=3.3207173347473145
I0215 15:34:38.386097 139807720187648 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.3076704740524292, loss=3.407773494720459
I0215 15:37:25.691799 139807158171392 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.2957729399204254, loss=3.3469231128692627
I0215 15:37:32.148177 139970484569920 spec.py:321] Evaluating on the training split.
I0215 15:37:38.264302 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 15:37:46.749056 139970484569920 spec.py:349] Evaluating on the test split.
I0215 15:37:48.990236 139970484569920 submission_runner.py:408] Time since start: 22902.80s, 	Step: 65521, 	{'train/accuracy': 0.6893733739852905, 'train/loss': 1.4961163997650146, 'validation/accuracy': 0.6338399648666382, 'validation/loss': 1.744549036026001, 'validation/num_examples': 50000, 'test/accuracy': 0.5035000443458557, 'test/loss': 2.418294668197632, 'test/num_examples': 10000, 'score': 21977.248546361923, 'total_duration': 22902.802991867065, 'accumulated_submission_time': 21977.248546361923, 'accumulated_eval_time': 921.8063566684723, 'accumulated_logging_time': 1.5741550922393799}
I0215 15:37:49.012539 139803987269376 logging_writer.py:48] [65521] accumulated_eval_time=921.806357, accumulated_logging_time=1.574155, accumulated_submission_time=21977.248546, global_step=65521, preemption_count=0, score=21977.248546, test/accuracy=0.503500, test/loss=2.418295, test/num_examples=10000, total_duration=22902.802992, train/accuracy=0.689373, train/loss=1.496116, validation/accuracy=0.633840, validation/loss=1.744549, validation/num_examples=50000
I0215 15:40:29.689241 139803995662080 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.2909606397151947, loss=3.3547449111938477
I0215 15:43:17.072273 139803987269376 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.29380619525909424, loss=3.3490798473358154
I0215 15:46:04.450967 139803995662080 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.2941429913043976, loss=3.2938170433044434
I0215 15:46:19.258475 139970484569920 spec.py:321] Evaluating on the training split.
I0215 15:46:25.562630 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 15:46:34.015396 139970484569920 spec.py:349] Evaluating on the test split.
I0215 15:46:36.272416 139970484569920 submission_runner.py:408] Time since start: 23430.09s, 	Step: 67046, 	{'train/accuracy': 0.6789301633834839, 'train/loss': 1.5319353342056274, 'validation/accuracy': 0.627519965171814, 'validation/loss': 1.7659509181976318, 'validation/num_examples': 50000, 'test/accuracy': 0.49730002880096436, 'test/loss': 2.440481424331665, 'test/num_examples': 10000, 'score': 22487.43315601349, 'total_duration': 23430.085050821304, 'accumulated_submission_time': 22487.43315601349, 'accumulated_eval_time': 938.8201231956482, 'accumulated_logging_time': 1.6068379878997803}
I0215 15:46:36.297222 139803995662080 logging_writer.py:48] [67046] accumulated_eval_time=938.820123, accumulated_logging_time=1.606838, accumulated_submission_time=22487.433156, global_step=67046, preemption_count=0, score=22487.433156, test/accuracy=0.497300, test/loss=2.440481, test/num_examples=10000, total_duration=23430.085051, train/accuracy=0.678930, train/loss=1.531935, validation/accuracy=0.627520, validation/loss=1.765951, validation/num_examples=50000
I0215 15:49:08.619930 139804004054784 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.29785364866256714, loss=3.2264156341552734
I0215 15:51:55.898918 139803995662080 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.30130255222320557, loss=3.325230598449707
I0215 15:54:43.180183 139804004054784 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.30152443051338196, loss=3.2948920726776123
I0215 15:55:06.405728 139970484569920 spec.py:321] Evaluating on the training split.
I0215 15:55:12.621803 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 15:55:21.132038 139970484569920 spec.py:349] Evaluating on the test split.
I0215 15:55:23.382162 139970484569920 submission_runner.py:408] Time since start: 23957.19s, 	Step: 68571, 	{'train/accuracy': 0.7240114808082581, 'train/loss': 1.43919837474823, 'validation/accuracy': 0.6231200098991394, 'validation/loss': 1.8756496906280518, 'validation/num_examples': 50000, 'test/accuracy': 0.501800000667572, 'test/loss': 2.520477771759033, 'test/num_examples': 10000, 'score': 22997.473987579346, 'total_duration': 23957.194925546646, 'accumulated_submission_time': 22997.473987579346, 'accumulated_eval_time': 955.7965259552002, 'accumulated_logging_time': 1.6453070640563965}
I0215 15:55:23.405877 139803995662080 logging_writer.py:48] [68571] accumulated_eval_time=955.796526, accumulated_logging_time=1.645307, accumulated_submission_time=22997.473988, global_step=68571, preemption_count=0, score=22997.473988, test/accuracy=0.501800, test/loss=2.520478, test/num_examples=10000, total_duration=23957.194926, train/accuracy=0.724011, train/loss=1.439198, validation/accuracy=0.623120, validation/loss=1.875650, validation/num_examples=50000
I0215 15:57:47.327740 139807149778688 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.291909784078598, loss=3.350374698638916
I0215 16:00:34.811028 139803995662080 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.30782678723335266, loss=3.3972761631011963
I0215 16:03:22.356952 139807149778688 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.3027394115924835, loss=3.305938959121704
I0215 16:03:53.696719 139970484569920 spec.py:321] Evaluating on the training split.
I0215 16:04:00.265140 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 16:04:08.614888 139970484569920 spec.py:349] Evaluating on the test split.
I0215 16:04:10.930270 139970484569920 submission_runner.py:408] Time since start: 24484.74s, 	Step: 70095, 	{'train/accuracy': 0.7030652165412903, 'train/loss': 1.4618088006973267, 'validation/accuracy': 0.6258999705314636, 'validation/loss': 1.7977391481399536, 'validation/num_examples': 50000, 'test/accuracy': 0.5031000375747681, 'test/loss': 2.4266278743743896, 'test/num_examples': 10000, 'score': 23507.699318885803, 'total_duration': 24484.743031024933, 'accumulated_submission_time': 23507.699318885803, 'accumulated_eval_time': 973.0300307273865, 'accumulated_logging_time': 1.6790194511413574}
I0215 16:04:10.952549 139807711794944 logging_writer.py:48] [70095] accumulated_eval_time=973.030031, accumulated_logging_time=1.679019, accumulated_submission_time=23507.699319, global_step=70095, preemption_count=0, score=23507.699319, test/accuracy=0.503100, test/loss=2.426628, test/num_examples=10000, total_duration=24484.743031, train/accuracy=0.703065, train/loss=1.461809, validation/accuracy=0.625900, validation/loss=1.797739, validation/num_examples=50000
I0215 16:06:26.796945 139807720187648 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.31025972962379456, loss=3.3395485877990723
I0215 16:09:14.155147 139807711794944 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.310106486082077, loss=3.356837034225464
I0215 16:12:01.514078 139807720187648 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.29995229840278625, loss=3.289212942123413
I0215 16:12:41.136743 139970484569920 spec.py:321] Evaluating on the training split.
I0215 16:12:47.292773 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 16:12:55.771889 139970484569920 spec.py:349] Evaluating on the test split.
I0215 16:12:58.042841 139970484569920 submission_runner.py:408] Time since start: 25011.86s, 	Step: 71620, 	{'train/accuracy': 0.7074896097183228, 'train/loss': 1.4071218967437744, 'validation/accuracy': 0.6420999765396118, 'validation/loss': 1.6956228017807007, 'validation/num_examples': 50000, 'test/accuracy': 0.5198000073432922, 'test/loss': 2.3231699466705322, 'test/num_examples': 10000, 'score': 24017.822208881378, 'total_duration': 25011.85559272766, 'accumulated_submission_time': 24017.822208881378, 'accumulated_eval_time': 989.9360868930817, 'accumulated_logging_time': 1.7123217582702637}
I0215 16:12:58.065477 139807158171392 logging_writer.py:48] [71620] accumulated_eval_time=989.936087, accumulated_logging_time=1.712322, accumulated_submission_time=24017.822209, global_step=71620, preemption_count=0, score=24017.822209, test/accuracy=0.519800, test/loss=2.323170, test/num_examples=10000, total_duration=25011.855593, train/accuracy=0.707490, train/loss=1.407122, validation/accuracy=0.642100, validation/loss=1.695623, validation/num_examples=50000
I0215 16:15:05.548431 139807644673792 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.3183557093143463, loss=3.350891590118408
I0215 16:17:52.907909 139807158171392 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.3086000978946686, loss=3.324765205383301
I0215 16:20:40.329277 139807644673792 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.2904858887195587, loss=3.28790020942688
I0215 16:21:28.256385 139970484569920 spec.py:321] Evaluating on the training split.
I0215 16:21:34.526537 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 16:21:42.978841 139970484569920 spec.py:349] Evaluating on the test split.
I0215 16:21:45.231017 139970484569920 submission_runner.py:408] Time since start: 25539.04s, 	Step: 73145, 	{'train/accuracy': 0.6999959945678711, 'train/loss': 1.4288250207901, 'validation/accuracy': 0.6443600058555603, 'validation/loss': 1.693749189376831, 'validation/num_examples': 50000, 'test/accuracy': 0.5090000033378601, 'test/loss': 2.3625693321228027, 'test/num_examples': 10000, 'score': 24527.95017337799, 'total_duration': 25539.043766975403, 'accumulated_submission_time': 24527.95017337799, 'accumulated_eval_time': 1006.910671710968, 'accumulated_logging_time': 1.7452239990234375}
I0215 16:21:45.253944 139804004054784 logging_writer.py:48] [73145] accumulated_eval_time=1006.910672, accumulated_logging_time=1.745224, accumulated_submission_time=24527.950173, global_step=73145, preemption_count=0, score=24527.950173, test/accuracy=0.509000, test/loss=2.362569, test/num_examples=10000, total_duration=25539.043767, train/accuracy=0.699996, train/loss=1.428825, validation/accuracy=0.644360, validation/loss=1.693749, validation/num_examples=50000
I0215 16:23:44.381149 139807149778688 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.29943549633026123, loss=3.2575531005859375
I0215 16:26:31.745096 139804004054784 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.3084595501422882, loss=3.233269691467285
I0215 16:29:19.024023 139807149778688 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.30780649185180664, loss=3.2219972610473633
I0215 16:30:15.331094 139970484569920 spec.py:321] Evaluating on the training split.
I0215 16:30:21.546265 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 16:30:29.868760 139970484569920 spec.py:349] Evaluating on the test split.
I0215 16:30:32.121820 139970484569920 submission_runner.py:408] Time since start: 26065.93s, 	Step: 74670, 	{'train/accuracy': 0.7061144709587097, 'train/loss': 1.4120854139328003, 'validation/accuracy': 0.6490199565887451, 'validation/loss': 1.660880446434021, 'validation/num_examples': 50000, 'test/accuracy': 0.5206000208854675, 'test/loss': 2.332211494445801, 'test/num_examples': 10000, 'score': 25037.96821832657, 'total_duration': 26065.93457007408, 'accumulated_submission_time': 25037.96821832657, 'accumulated_eval_time': 1023.7013425827026, 'accumulated_logging_time': 1.7779958248138428}
I0215 16:30:32.145216 139803995662080 logging_writer.py:48] [74670] accumulated_eval_time=1023.701343, accumulated_logging_time=1.777996, accumulated_submission_time=25037.968218, global_step=74670, preemption_count=0, score=25037.968218, test/accuracy=0.520600, test/loss=2.332211, test/num_examples=10000, total_duration=26065.934570, train/accuracy=0.706114, train/loss=1.412085, validation/accuracy=0.649020, validation/loss=1.660880, validation/num_examples=50000
I0215 16:32:22.847375 139807703402240 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.3050253689289093, loss=3.245896339416504
I0215 16:35:10.231384 139803995662080 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.29508745670318604, loss=3.2421953678131104
I0215 16:37:57.644525 139807703402240 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.3199627697467804, loss=3.288865566253662
I0215 16:39:02.428911 139970484569920 spec.py:321] Evaluating on the training split.
I0215 16:39:09.369372 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 16:39:17.818033 139970484569920 spec.py:349] Evaluating on the test split.
I0215 16:39:20.081759 139970484569920 submission_runner.py:408] Time since start: 26593.89s, 	Step: 76195, 	{'train/accuracy': 0.6880181431770325, 'train/loss': 1.4884074926376343, 'validation/accuracy': 0.6317399740219116, 'validation/loss': 1.7367355823516846, 'validation/num_examples': 50000, 'test/accuracy': 0.5103000402450562, 'test/loss': 2.3835484981536865, 'test/num_examples': 10000, 'score': 25548.186242580414, 'total_duration': 26593.894502401352, 'accumulated_submission_time': 25548.186242580414, 'accumulated_eval_time': 1041.3541376590729, 'accumulated_logging_time': 1.8116345405578613}
I0215 16:39:20.105097 139807149778688 logging_writer.py:48] [76195] accumulated_eval_time=1041.354138, accumulated_logging_time=1.811635, accumulated_submission_time=25548.186243, global_step=76195, preemption_count=0, score=25548.186243, test/accuracy=0.510300, test/loss=2.383548, test/num_examples=10000, total_duration=26593.894502, train/accuracy=0.688018, train/loss=1.488407, validation/accuracy=0.631740, validation/loss=1.736736, validation/num_examples=50000
I0215 16:41:02.433075 139807158171392 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.31292468309402466, loss=3.2558045387268066
I0215 16:43:49.712397 139807149778688 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.31000617146492004, loss=3.2174267768859863
I0215 16:46:37.072158 139807158171392 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.3104844093322754, loss=3.2385780811309814
I0215 16:47:50.107463 139970484569920 spec.py:321] Evaluating on the training split.
I0215 16:47:56.383874 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 16:48:04.960062 139970484569920 spec.py:349] Evaluating on the test split.
I0215 16:48:07.183235 139970484569920 submission_runner.py:408] Time since start: 27121.00s, 	Step: 77720, 	{'train/accuracy': 0.7248684763908386, 'train/loss': 1.3948085308074951, 'validation/accuracy': 0.6405199766159058, 'validation/loss': 1.76896071434021, 'validation/num_examples': 50000, 'test/accuracy': 0.509600043296814, 'test/loss': 2.4423303604125977, 'test/num_examples': 10000, 'score': 26058.124629497528, 'total_duration': 27120.995968818665, 'accumulated_submission_time': 26058.124629497528, 'accumulated_eval_time': 1058.4298613071442, 'accumulated_logging_time': 1.8461647033691406}
I0215 16:48:07.207169 139807149778688 logging_writer.py:48] [77720] accumulated_eval_time=1058.429861, accumulated_logging_time=1.846165, accumulated_submission_time=26058.124629, global_step=77720, preemption_count=0, score=26058.124629, test/accuracy=0.509600, test/loss=2.442330, test/num_examples=10000, total_duration=27120.995969, train/accuracy=0.724868, train/loss=1.394809, validation/accuracy=0.640520, validation/loss=1.768961, validation/num_examples=50000
I0215 16:49:41.338038 139807703402240 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.3120000660419464, loss=3.207846164703369
I0215 16:52:28.853919 139807149778688 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.3294609487056732, loss=3.3018980026245117
I0215 16:55:16.316864 139807703402240 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.2998656630516052, loss=3.107105255126953
I0215 16:56:37.422713 139970484569920 spec.py:321] Evaluating on the training split.
I0215 16:56:43.653987 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 16:56:52.205410 139970484569920 spec.py:349] Evaluating on the test split.
I0215 16:56:54.462028 139970484569920 submission_runner.py:408] Time since start: 27648.27s, 	Step: 79244, 	{'train/accuracy': 0.721699595451355, 'train/loss': 1.3454585075378418, 'validation/accuracy': 0.6500999927520752, 'validation/loss': 1.6539099216461182, 'validation/num_examples': 50000, 'test/accuracy': 0.5296000242233276, 'test/loss': 2.2836482524871826, 'test/num_examples': 10000, 'score': 26568.27812385559, 'total_duration': 27648.27478313446, 'accumulated_submission_time': 26568.27812385559, 'accumulated_eval_time': 1075.4691364765167, 'accumulated_logging_time': 1.8798434734344482}
I0215 16:56:54.485269 139803987269376 logging_writer.py:48] [79244] accumulated_eval_time=1075.469136, accumulated_logging_time=1.879843, accumulated_submission_time=26568.278124, global_step=79244, preemption_count=0, score=26568.278124, test/accuracy=0.529600, test/loss=2.283648, test/num_examples=10000, total_duration=27648.274783, train/accuracy=0.721700, train/loss=1.345459, validation/accuracy=0.650100, validation/loss=1.653910, validation/num_examples=50000
I0215 16:58:20.385501 139803995662080 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.3154034912586212, loss=3.1987428665161133
I0215 17:01:07.743089 139803987269376 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.30139368772506714, loss=3.132240056991577
I0215 17:03:54.989401 139803995662080 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.31847429275512695, loss=3.2136433124542236
I0215 17:05:24.492084 139970484569920 spec.py:321] Evaluating on the training split.
I0215 17:05:30.710756 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 17:05:39.121186 139970484569920 spec.py:349] Evaluating on the test split.
I0215 17:05:41.400543 139970484569920 submission_runner.py:408] Time since start: 28175.21s, 	Step: 80769, 	{'train/accuracy': 0.7140664458274841, 'train/loss': 1.446722149848938, 'validation/accuracy': 0.6485199928283691, 'validation/loss': 1.7378699779510498, 'validation/num_examples': 50000, 'test/accuracy': 0.5218999981880188, 'test/loss': 2.391239643096924, 'test/num_examples': 10000, 'score': 27078.22215628624, 'total_duration': 28175.21328663826, 'accumulated_submission_time': 27078.22215628624, 'accumulated_eval_time': 1092.3775401115417, 'accumulated_logging_time': 1.9127840995788574}
I0215 17:05:41.424245 139803995662080 logging_writer.py:48] [80769] accumulated_eval_time=1092.377540, accumulated_logging_time=1.912784, accumulated_submission_time=27078.222156, global_step=80769, preemption_count=0, score=27078.222156, test/accuracy=0.521900, test/loss=2.391240, test/num_examples=10000, total_duration=28175.213287, train/accuracy=0.714066, train/loss=1.446722, validation/accuracy=0.648520, validation/loss=1.737870, validation/num_examples=50000
I0215 17:06:59.034593 139804004054784 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.3185029923915863, loss=3.1534159183502197
I0215 17:09:46.580661 139803995662080 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.31479135155677795, loss=3.1641249656677246
I0215 17:12:33.977393 139804004054784 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.3178638517856598, loss=3.265944480895996
I0215 17:14:11.517573 139970484569920 spec.py:321] Evaluating on the training split.
I0215 17:14:17.766259 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 17:14:26.015915 139970484569920 spec.py:349] Evaluating on the test split.
I0215 17:14:28.282140 139970484569920 submission_runner.py:408] Time since start: 28702.09s, 	Step: 82293, 	{'train/accuracy': 0.7015106678009033, 'train/loss': 1.4795687198638916, 'validation/accuracy': 0.6381399631500244, 'validation/loss': 1.760036826133728, 'validation/num_examples': 50000, 'test/accuracy': 0.5125000476837158, 'test/loss': 2.4114387035369873, 'test/num_examples': 10000, 'score': 27588.254366397858, 'total_duration': 28702.094896793365, 'accumulated_submission_time': 27588.254366397858, 'accumulated_eval_time': 1109.1420695781708, 'accumulated_logging_time': 1.9469194412231445}
I0215 17:14:28.305861 139803995662080 logging_writer.py:48] [82293] accumulated_eval_time=1109.142070, accumulated_logging_time=1.946919, accumulated_submission_time=27588.254366, global_step=82293, preemption_count=0, score=27588.254366, test/accuracy=0.512500, test/loss=2.411439, test/num_examples=10000, total_duration=28702.094897, train/accuracy=0.701511, train/loss=1.479569, validation/accuracy=0.638140, validation/loss=1.760037, validation/num_examples=50000
I0215 17:15:37.850675 139807149778688 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.31823694705963135, loss=3.2511465549468994
I0215 17:18:25.183426 139803995662080 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.31797924637794495, loss=3.168558120727539
I0215 17:21:12.534003 139807149778688 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.33631935715675354, loss=3.193103313446045
I0215 17:22:58.346049 139970484569920 spec.py:321] Evaluating on the training split.
I0215 17:23:04.563180 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 17:23:13.020217 139970484569920 spec.py:349] Evaluating on the test split.
I0215 17:23:15.318643 139970484569920 submission_runner.py:408] Time since start: 29229.13s, 	Step: 83818, 	{'train/accuracy': 0.7146444320678711, 'train/loss': 1.4013713598251343, 'validation/accuracy': 0.6601799726486206, 'validation/loss': 1.6596468687057495, 'validation/num_examples': 50000, 'test/accuracy': 0.5307000279426575, 'test/loss': 2.2922167778015137, 'test/num_examples': 10000, 'score': 28098.231053352356, 'total_duration': 29229.13139939308, 'accumulated_submission_time': 28098.231053352356, 'accumulated_eval_time': 1126.1146397590637, 'accumulated_logging_time': 1.982010841369629}
I0215 17:23:15.342338 139803995662080 logging_writer.py:48] [83818] accumulated_eval_time=1126.114640, accumulated_logging_time=1.982011, accumulated_submission_time=28098.231053, global_step=83818, preemption_count=0, score=28098.231053, test/accuracy=0.530700, test/loss=2.292217, test/num_examples=10000, total_duration=29229.131399, train/accuracy=0.714644, train/loss=1.401371, validation/accuracy=0.660180, validation/loss=1.659647, validation/num_examples=50000
I0215 17:24:16.587506 139804004054784 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.32612743973731995, loss=3.2751615047454834
I0215 17:27:03.959891 139803995662080 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.3260062336921692, loss=3.2225465774536133
I0215 17:29:51.527203 139804004054784 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.32698798179626465, loss=3.2743213176727295
I0215 17:31:45.477667 139970484569920 spec.py:321] Evaluating on the training split.
I0215 17:31:51.706655 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 17:32:00.098765 139970484569920 spec.py:349] Evaluating on the test split.
I0215 17:32:02.405443 139970484569920 submission_runner.py:408] Time since start: 29756.22s, 	Step: 85342, 	{'train/accuracy': 0.7122927308082581, 'train/loss': 1.4106295108795166, 'validation/accuracy': 0.6302799582481384, 'validation/loss': 1.772110939025879, 'validation/num_examples': 50000, 'test/accuracy': 0.4928000271320343, 'test/loss': 2.4630188941955566, 'test/num_examples': 10000, 'score': 28608.30346250534, 'total_duration': 29756.218193531036, 'accumulated_submission_time': 28608.30346250534, 'accumulated_eval_time': 1143.042379617691, 'accumulated_logging_time': 2.0171265602111816}
I0215 17:32:02.429577 139807703402240 logging_writer.py:48] [85342] accumulated_eval_time=1143.042380, accumulated_logging_time=2.017127, accumulated_submission_time=28608.303463, global_step=85342, preemption_count=0, score=28608.303463, test/accuracy=0.492800, test/loss=2.463019, test/num_examples=10000, total_duration=29756.218194, train/accuracy=0.712293, train/loss=1.410630, validation/accuracy=0.630280, validation/loss=1.772111, validation/num_examples=50000
I0215 17:32:55.633703 139807728580352 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.3215719759464264, loss=3.167964458465576
I0215 17:35:43.030689 139807703402240 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.3290736675262451, loss=3.2324612140655518
I0215 17:38:30.307955 139807728580352 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.33405816555023193, loss=3.151176929473877
I0215 17:40:32.526667 139970484569920 spec.py:321] Evaluating on the training split.
I0215 17:40:38.756757 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 17:40:47.170015 139970484569920 spec.py:349] Evaluating on the test split.
I0215 17:40:49.448127 139970484569920 submission_runner.py:408] Time since start: 30283.26s, 	Step: 86867, 	{'train/accuracy': 0.7409717440605164, 'train/loss': 1.2722578048706055, 'validation/accuracy': 0.6576600074768066, 'validation/loss': 1.6240990161895752, 'validation/num_examples': 50000, 'test/accuracy': 0.5250000357627869, 'test/loss': 2.2928431034088135, 'test/num_examples': 10000, 'score': 29118.34032726288, 'total_duration': 30283.260884284973, 'accumulated_submission_time': 29118.34032726288, 'accumulated_eval_time': 1159.9638073444366, 'accumulated_logging_time': 2.0514938831329346}
I0215 17:40:49.471848 139803995662080 logging_writer.py:48] [86867] accumulated_eval_time=1159.963807, accumulated_logging_time=2.051494, accumulated_submission_time=29118.340327, global_step=86867, preemption_count=0, score=29118.340327, test/accuracy=0.525000, test/loss=2.292843, test/num_examples=10000, total_duration=30283.260884, train/accuracy=0.740972, train/loss=1.272258, validation/accuracy=0.657660, validation/loss=1.624099, validation/num_examples=50000
I0215 17:41:34.299376 139804004054784 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.32810136675834656, loss=3.2029192447662354
I0215 17:44:21.687216 139803995662080 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.3348129987716675, loss=3.2038121223449707
I0215 17:47:09.153297 139804004054784 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.3397068381309509, loss=3.1326568126678467
I0215 17:49:19.567568 139970484569920 spec.py:321] Evaluating on the training split.
I0215 17:49:25.880616 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 17:49:34.440121 139970484569920 spec.py:349] Evaluating on the test split.
I0215 17:49:36.726985 139970484569920 submission_runner.py:408] Time since start: 30810.54s, 	Step: 88391, 	{'train/accuracy': 0.7313057780265808, 'train/loss': 1.2895619869232178, 'validation/accuracy': 0.6612600088119507, 'validation/loss': 1.6069889068603516, 'validation/num_examples': 50000, 'test/accuracy': 0.5308000445365906, 'test/loss': 2.2620644569396973, 'test/num_examples': 10000, 'score': 29628.375321388245, 'total_duration': 30810.539741039276, 'accumulated_submission_time': 29628.375321388245, 'accumulated_eval_time': 1177.1231915950775, 'accumulated_logging_time': 2.0849714279174805}
I0215 17:49:36.752582 139803987269376 logging_writer.py:48] [88391] accumulated_eval_time=1177.123192, accumulated_logging_time=2.084971, accumulated_submission_time=29628.375321, global_step=88391, preemption_count=0, score=29628.375321, test/accuracy=0.530800, test/loss=2.262064, test/num_examples=10000, total_duration=30810.539741, train/accuracy=0.731306, train/loss=1.289562, validation/accuracy=0.661260, validation/loss=1.606989, validation/num_examples=50000
I0215 17:50:13.522547 139807644673792 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.3333793878555298, loss=3.18182110786438
I0215 17:53:00.796952 139803987269376 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.33241787552833557, loss=3.1194655895233154
I0215 17:55:48.215001 139807644673792 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.33811110258102417, loss=3.128390312194824
I0215 17:58:06.875806 139970484569920 spec.py:321] Evaluating on the training split.
I0215 17:58:13.105790 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 17:58:21.578610 139970484569920 spec.py:349] Evaluating on the test split.
I0215 17:58:23.831369 139970484569920 submission_runner.py:408] Time since start: 31337.64s, 	Step: 89916, 	{'train/accuracy': 0.7133689522743225, 'train/loss': 1.3448280096054077, 'validation/accuracy': 0.6446399688720703, 'validation/loss': 1.651045560836792, 'validation/num_examples': 50000, 'test/accuracy': 0.5157000422477722, 'test/loss': 2.332489490509033, 'test/num_examples': 10000, 'score': 30138.436474323273, 'total_duration': 31337.64408659935, 'accumulated_submission_time': 30138.436474323273, 'accumulated_eval_time': 1194.078701019287, 'accumulated_logging_time': 2.1213369369506836}
I0215 17:58:23.857290 139807149778688 logging_writer.py:48] [89916] accumulated_eval_time=1194.078701, accumulated_logging_time=2.121337, accumulated_submission_time=30138.436474, global_step=89916, preemption_count=0, score=30138.436474, test/accuracy=0.515700, test/loss=2.332489, test/num_examples=10000, total_duration=31337.644087, train/accuracy=0.713369, train/loss=1.344828, validation/accuracy=0.644640, validation/loss=1.651046, validation/num_examples=50000
I0215 17:58:52.300669 139807158171392 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.33107906579971313, loss=3.1420063972473145
I0215 18:01:39.720746 139807149778688 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.34515243768692017, loss=3.1406030654907227
I0215 18:04:27.123965 139807158171392 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.34210115671157837, loss=3.1946067810058594
I0215 18:06:53.874779 139970484569920 spec.py:321] Evaluating on the training split.
I0215 18:07:00.175578 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 18:07:08.585441 139970484569920 spec.py:349] Evaluating on the test split.
I0215 18:07:10.848809 139970484569920 submission_runner.py:408] Time since start: 31864.66s, 	Step: 91440, 	{'train/accuracy': 0.7242307066917419, 'train/loss': 1.3670257329940796, 'validation/accuracy': 0.6621800065040588, 'validation/loss': 1.6457127332687378, 'validation/num_examples': 50000, 'test/accuracy': 0.5351999998092651, 'test/loss': 2.2859864234924316, 'test/num_examples': 10000, 'score': 30648.390997886658, 'total_duration': 31864.661551475525, 'accumulated_submission_time': 30648.390997886658, 'accumulated_eval_time': 1211.0527000427246, 'accumulated_logging_time': 2.159031391143799}
I0215 18:07:10.874759 139804004054784 logging_writer.py:48] [91440] accumulated_eval_time=1211.052700, accumulated_logging_time=2.159031, accumulated_submission_time=30648.390998, global_step=91440, preemption_count=0, score=30648.390998, test/accuracy=0.535200, test/loss=2.285986, test/num_examples=10000, total_duration=31864.661551, train/accuracy=0.724231, train/loss=1.367026, validation/accuracy=0.662180, validation/loss=1.645713, validation/num_examples=50000
I0215 18:07:31.281564 139807149778688 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.3470238745212555, loss=3.143505096435547
I0215 18:10:18.515259 139804004054784 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.34014126658439636, loss=3.1981844902038574
I0215 18:13:05.807906 139807149778688 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.348000168800354, loss=3.1260452270507812
I0215 18:15:41.158535 139970484569920 spec.py:321] Evaluating on the training split.
I0215 18:15:47.340523 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 18:15:55.820721 139970484569920 spec.py:349] Evaluating on the test split.
I0215 18:15:58.096928 139970484569920 submission_runner.py:408] Time since start: 32391.91s, 	Step: 92966, 	{'train/accuracy': 0.7262237071990967, 'train/loss': 1.3768022060394287, 'validation/accuracy': 0.6603800058364868, 'validation/loss': 1.6624222993850708, 'validation/num_examples': 50000, 'test/accuracy': 0.5343000292778015, 'test/loss': 2.292142629623413, 'test/num_examples': 10000, 'score': 31158.61331629753, 'total_duration': 32391.90967822075, 'accumulated_submission_time': 31158.61331629753, 'accumulated_eval_time': 1227.9910509586334, 'accumulated_logging_time': 2.1950061321258545}
I0215 18:15:58.121636 139803995662080 logging_writer.py:48] [92966] accumulated_eval_time=1227.991051, accumulated_logging_time=2.195006, accumulated_submission_time=31158.613316, global_step=92966, preemption_count=0, score=31158.613316, test/accuracy=0.534300, test/loss=2.292143, test/num_examples=10000, total_duration=32391.909678, train/accuracy=0.726224, train/loss=1.376802, validation/accuracy=0.660380, validation/loss=1.662422, validation/num_examples=50000
I0215 18:16:09.822895 139804004054784 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.35314100980758667, loss=3.163142681121826
I0215 18:18:57.181957 139803995662080 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.3474639654159546, loss=3.1397271156311035
I0215 18:21:44.467050 139804004054784 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.34960561990737915, loss=3.153747797012329
I0215 18:24:28.164765 139970484569920 spec.py:321] Evaluating on the training split.
I0215 18:24:34.555708 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 18:24:42.933227 139970484569920 spec.py:349] Evaluating on the test split.
I0215 18:24:45.227119 139970484569920 submission_runner.py:408] Time since start: 32919.04s, 	Step: 94491, 	{'train/accuracy': 0.7694714665412903, 'train/loss': 1.1644514799118042, 'validation/accuracy': 0.6688199639320374, 'validation/loss': 1.597221851348877, 'validation/num_examples': 50000, 'test/accuracy': 0.5424000024795532, 'test/loss': 2.2404980659484863, 'test/num_examples': 10000, 'score': 31668.59368467331, 'total_duration': 32919.03987455368, 'accumulated_submission_time': 31668.59368467331, 'accumulated_eval_time': 1245.053385257721, 'accumulated_logging_time': 2.2303876876831055}
I0215 18:24:45.253014 139803995662080 logging_writer.py:48] [94491] accumulated_eval_time=1245.053385, accumulated_logging_time=2.230388, accumulated_submission_time=31668.593685, global_step=94491, preemption_count=0, score=31668.593685, test/accuracy=0.542400, test/loss=2.240498, test/num_examples=10000, total_duration=32919.039875, train/accuracy=0.769471, train/loss=1.164451, validation/accuracy=0.668820, validation/loss=1.597222, validation/num_examples=50000
I0215 18:24:48.588288 139807644673792 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.3478134572505951, loss=3.123330593109131
I0215 18:27:35.800754 139803995662080 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.3487017750740051, loss=3.0844597816467285
I0215 18:30:23.038398 139807644673792 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.3477344214916229, loss=3.123007297515869
I0215 18:33:10.239281 139803995662080 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.3533236086368561, loss=3.1002860069274902
I0215 18:33:15.360570 139970484569920 spec.py:321] Evaluating on the training split.
I0215 18:33:21.561660 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 18:33:30.079140 139970484569920 spec.py:349] Evaluating on the test split.
I0215 18:33:32.378812 139970484569920 submission_runner.py:408] Time since start: 33446.19s, 	Step: 96017, 	{'train/accuracy': 0.7730787396430969, 'train/loss': 1.1428571939468384, 'validation/accuracy': 0.6869999766349792, 'validation/loss': 1.5054179430007935, 'validation/num_examples': 50000, 'test/accuracy': 0.5630000233650208, 'test/loss': 2.131014347076416, 'test/num_examples': 10000, 'score': 32178.638496160507, 'total_duration': 33446.19157075882, 'accumulated_submission_time': 32178.638496160507, 'accumulated_eval_time': 1262.0715773105621, 'accumulated_logging_time': 2.268350601196289}
I0215 18:33:32.407174 139807158171392 logging_writer.py:48] [96017] accumulated_eval_time=1262.071577, accumulated_logging_time=2.268351, accumulated_submission_time=32178.638496, global_step=96017, preemption_count=0, score=32178.638496, test/accuracy=0.563000, test/loss=2.131014, test/num_examples=10000, total_duration=33446.191571, train/accuracy=0.773079, train/loss=1.142857, validation/accuracy=0.687000, validation/loss=1.505418, validation/num_examples=50000
I0215 18:36:14.373315 139807703402240 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.36901700496673584, loss=3.1312997341156006
I0215 18:39:01.744025 139807158171392 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.3652787506580353, loss=3.153701066970825
I0215 18:41:49.164027 139807703402240 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.3743073344230652, loss=3.1118879318237305
I0215 18:42:02.633647 139970484569920 spec.py:321] Evaluating on the training split.
I0215 18:42:08.950713 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 18:42:17.299526 139970484569920 spec.py:349] Evaluating on the test split.
I0215 18:42:19.610669 139970484569920 submission_runner.py:408] Time since start: 33973.42s, 	Step: 97542, 	{'train/accuracy': 0.7545639276504517, 'train/loss': 1.2203161716461182, 'validation/accuracy': 0.6776599884033203, 'validation/loss': 1.5536553859710693, 'validation/num_examples': 50000, 'test/accuracy': 0.5504000186920166, 'test/loss': 2.1981472969055176, 'test/num_examples': 10000, 'score': 32688.80292248726, 'total_duration': 33973.42340660095, 'accumulated_submission_time': 32688.80292248726, 'accumulated_eval_time': 1279.048544883728, 'accumulated_logging_time': 2.307842493057251}
I0215 18:42:19.636608 139804004054784 logging_writer.py:48] [97542] accumulated_eval_time=1279.048545, accumulated_logging_time=2.307842, accumulated_submission_time=32688.802922, global_step=97542, preemption_count=0, score=32688.802922, test/accuracy=0.550400, test/loss=2.198147, test/num_examples=10000, total_duration=33973.423407, train/accuracy=0.754564, train/loss=1.220316, validation/accuracy=0.677660, validation/loss=1.553655, validation/num_examples=50000
I0215 18:44:53.226538 139807149778688 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.36563780903816223, loss=3.098550796508789
I0215 18:47:40.445049 139804004054784 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.3632560968399048, loss=3.0844292640686035
I0215 18:50:27.725726 139807149778688 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.3625405728816986, loss=3.1076102256774902
I0215 18:50:49.875363 139970484569920 spec.py:321] Evaluating on the training split.
I0215 18:50:56.174992 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 18:51:04.580940 139970484569920 spec.py:349] Evaluating on the test split.
I0215 18:51:06.797989 139970484569920 submission_runner.py:408] Time since start: 34500.61s, 	Step: 99068, 	{'train/accuracy': 0.7618184089660645, 'train/loss': 1.1829177141189575, 'validation/accuracy': 0.6860199570655823, 'validation/loss': 1.5084346532821655, 'validation/num_examples': 50000, 'test/accuracy': 0.5588000416755676, 'test/loss': 2.1485660076141357, 'test/num_examples': 10000, 'score': 33198.97890162468, 'total_duration': 34500.6107442379, 'accumulated_submission_time': 33198.97890162468, 'accumulated_eval_time': 1295.9711184501648, 'accumulated_logging_time': 2.3439955711364746}
I0215 18:51:06.822285 139807711794944 logging_writer.py:48] [99068] accumulated_eval_time=1295.971118, accumulated_logging_time=2.343996, accumulated_submission_time=33198.978902, global_step=99068, preemption_count=0, score=33198.978902, test/accuracy=0.558800, test/loss=2.148566, test/num_examples=10000, total_duration=34500.610744, train/accuracy=0.761818, train/loss=1.182918, validation/accuracy=0.686020, validation/loss=1.508435, validation/num_examples=50000
I0215 18:53:31.776527 139807720187648 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.3546060025691986, loss=3.0274322032928467
I0215 18:56:19.108825 139807711794944 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.36536937952041626, loss=3.0378899574279785
I0215 18:59:06.468449 139807720187648 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.3793266713619232, loss=3.027406692504883
I0215 18:59:37.031836 139970484569920 spec.py:321] Evaluating on the training split.
I0215 18:59:43.202950 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 18:59:51.656413 139970484569920 spec.py:349] Evaluating on the test split.
I0215 18:59:53.886372 139970484569920 submission_runner.py:408] Time since start: 35027.70s, 	Step: 100593, 	{'train/accuracy': 0.7625557780265808, 'train/loss': 1.164181113243103, 'validation/accuracy': 0.6901400089263916, 'validation/loss': 1.4772294759750366, 'validation/num_examples': 50000, 'test/accuracy': 0.5692000389099121, 'test/loss': 2.101778984069824, 'test/num_examples': 10000, 'score': 33709.128729104996, 'total_duration': 35027.699124097824, 'accumulated_submission_time': 33709.128729104996, 'accumulated_eval_time': 1312.825609445572, 'accumulated_logging_time': 2.3782050609588623}
I0215 18:59:53.912349 139807703402240 logging_writer.py:48] [100593] accumulated_eval_time=1312.825609, accumulated_logging_time=2.378205, accumulated_submission_time=33709.128729, global_step=100593, preemption_count=0, score=33709.128729, test/accuracy=0.569200, test/loss=2.101779, test/num_examples=10000, total_duration=35027.699124, train/accuracy=0.762556, train/loss=1.164181, validation/accuracy=0.690140, validation/loss=1.477229, validation/num_examples=50000
I0215 19:02:10.404665 139807720187648 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.38033539056777954, loss=3.097500801086426
I0215 19:04:57.645856 139807703402240 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.3788430988788605, loss=3.0315370559692383
I0215 19:07:44.922786 139807720187648 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.3781963586807251, loss=3.0365941524505615
I0215 19:08:24.181455 139970484569920 spec.py:321] Evaluating on the training split.
I0215 19:08:30.455882 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 19:08:38.832249 139970484569920 spec.py:349] Evaluating on the test split.
I0215 19:08:41.109343 139970484569920 submission_runner.py:408] Time since start: 35554.92s, 	Step: 102119, 	{'train/accuracy': 0.7662627100944519, 'train/loss': 1.1389541625976562, 'validation/accuracy': 0.6900599598884583, 'validation/loss': 1.4855501651763916, 'validation/num_examples': 50000, 'test/accuracy': 0.5582000017166138, 'test/loss': 2.1185102462768555, 'test/num_examples': 10000, 'score': 34219.33787107468, 'total_duration': 35554.92210316658, 'accumulated_submission_time': 34219.33787107468, 'accumulated_eval_time': 1329.753449678421, 'accumulated_logging_time': 2.4143362045288086}
I0215 19:08:41.134742 139807149778688 logging_writer.py:48] [102119] accumulated_eval_time=1329.753450, accumulated_logging_time=2.414336, accumulated_submission_time=34219.337871, global_step=102119, preemption_count=0, score=34219.337871, test/accuracy=0.558200, test/loss=2.118510, test/num_examples=10000, total_duration=35554.922103, train/accuracy=0.766263, train/loss=1.138954, validation/accuracy=0.690060, validation/loss=1.485550, validation/num_examples=50000
I0215 19:10:48.988423 139807158171392 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.37954798340797424, loss=3.0530483722686768
I0215 19:13:36.407143 139807149778688 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.3739565312862396, loss=3.078962802886963
I0215 19:16:23.967575 139807158171392 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.3911250829696655, loss=3.045313835144043
I0215 19:17:11.321523 139970484569920 spec.py:321] Evaluating on the training split.
I0215 19:17:17.546267 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 19:17:26.026991 139970484569920 spec.py:349] Evaluating on the test split.
I0215 19:17:28.343813 139970484569920 submission_runner.py:408] Time since start: 36082.16s, 	Step: 103643, 	{'train/accuracy': 0.7879862785339355, 'train/loss': 1.0843526124954224, 'validation/accuracy': 0.6917399764060974, 'validation/loss': 1.4955836534500122, 'validation/num_examples': 50000, 'test/accuracy': 0.5681000351905823, 'test/loss': 2.132781982421875, 'test/num_examples': 10000, 'score': 34729.46434569359, 'total_duration': 36082.156563043594, 'accumulated_submission_time': 34729.46434569359, 'accumulated_eval_time': 1346.775689125061, 'accumulated_logging_time': 2.449850559234619}
I0215 19:17:28.368930 139804004054784 logging_writer.py:48] [103643] accumulated_eval_time=1346.775689, accumulated_logging_time=2.449851, accumulated_submission_time=34729.464346, global_step=103643, preemption_count=0, score=34729.464346, test/accuracy=0.568100, test/loss=2.132782, test/num_examples=10000, total_duration=36082.156563, train/accuracy=0.787986, train/loss=1.084353, validation/accuracy=0.691740, validation/loss=1.495584, validation/num_examples=50000
I0215 19:19:28.156018 139807711794944 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.37942588329315186, loss=2.966979742050171
I0215 19:22:15.465572 139804004054784 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.3726612627506256, loss=2.989781141281128
I0215 19:25:02.823307 139807711794944 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.3853174149990082, loss=3.04805326461792
I0215 19:25:58.436144 139970484569920 spec.py:321] Evaluating on the training split.
I0215 19:26:04.686147 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 19:26:13.022569 139970484569920 spec.py:349] Evaluating on the test split.
I0215 19:26:15.270794 139970484569920 submission_runner.py:408] Time since start: 36609.08s, 	Step: 105168, 	{'train/accuracy': 0.7840401530265808, 'train/loss': 1.0800116062164307, 'validation/accuracy': 0.6991599798202515, 'validation/loss': 1.454992651939392, 'validation/num_examples': 50000, 'test/accuracy': 0.5693000555038452, 'test/loss': 2.1008152961730957, 'test/num_examples': 10000, 'score': 35239.47196364403, 'total_duration': 36609.083545684814, 'accumulated_submission_time': 35239.47196364403, 'accumulated_eval_time': 1363.6102871894836, 'accumulated_logging_time': 2.4847590923309326}
I0215 19:26:15.297234 139803995662080 logging_writer.py:48] [105168] accumulated_eval_time=1363.610287, accumulated_logging_time=2.484759, accumulated_submission_time=35239.471964, global_step=105168, preemption_count=0, score=35239.471964, test/accuracy=0.569300, test/loss=2.100815, test/num_examples=10000, total_duration=36609.083546, train/accuracy=0.784040, train/loss=1.080012, validation/accuracy=0.699160, validation/loss=1.454993, validation/num_examples=50000
I0215 19:28:06.776914 139804004054784 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.39124223589897156, loss=3.0121145248413086
I0215 19:30:54.194568 139803995662080 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.40090256929397583, loss=3.043217658996582
I0215 19:33:41.541491 139804004054784 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.39793631434440613, loss=3.0312204360961914
I0215 19:34:45.319297 139970484569920 spec.py:321] Evaluating on the training split.
I0215 19:34:51.573022 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 19:34:59.951308 139970484569920 spec.py:349] Evaluating on the test split.
I0215 19:35:02.242041 139970484569920 submission_runner.py:408] Time since start: 37136.05s, 	Step: 106692, 	{'train/accuracy': 0.7885442972183228, 'train/loss': 1.041917324066162, 'validation/accuracy': 0.706059992313385, 'validation/loss': 1.4018383026123047, 'validation/num_examples': 50000, 'test/accuracy': 0.5731000304222107, 'test/loss': 2.0337016582489014, 'test/num_examples': 10000, 'score': 35749.43216061592, 'total_duration': 37136.05479979515, 'accumulated_submission_time': 35749.43216061592, 'accumulated_eval_time': 1380.5329904556274, 'accumulated_logging_time': 2.521979570388794}
I0215 19:35:02.278772 139807644673792 logging_writer.py:48] [106692] accumulated_eval_time=1380.532990, accumulated_logging_time=2.521980, accumulated_submission_time=35749.432161, global_step=106692, preemption_count=0, score=35749.432161, test/accuracy=0.573100, test/loss=2.033702, test/num_examples=10000, total_duration=37136.054800, train/accuracy=0.788544, train/loss=1.041917, validation/accuracy=0.706060, validation/loss=1.401838, validation/num_examples=50000
I0215 19:36:45.640005 139807703402240 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.40622678399086, loss=3.0413074493408203
I0215 19:39:32.979998 139807644673792 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.3966551125049591, loss=3.0087382793426514
I0215 19:42:20.289763 139807703402240 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.4017193913459778, loss=2.987030267715454
I0215 19:43:32.330242 139970484569920 spec.py:321] Evaluating on the training split.
I0215 19:43:38.538505 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 19:43:46.784126 139970484569920 spec.py:349] Evaluating on the test split.
I0215 19:43:49.043803 139970484569920 submission_runner.py:408] Time since start: 37662.86s, 	Step: 108217, 	{'train/accuracy': 0.7863320708274841, 'train/loss': 1.0653631687164307, 'validation/accuracy': 0.7044000029563904, 'validation/loss': 1.4175564050674438, 'validation/num_examples': 50000, 'test/accuracy': 0.5789000391960144, 'test/loss': 2.0408992767333984, 'test/num_examples': 10000, 'score': 36259.42400288582, 'total_duration': 37662.85647940636, 'accumulated_submission_time': 36259.42400288582, 'accumulated_eval_time': 1397.246426820755, 'accumulated_logging_time': 2.568589448928833}
I0215 19:43:49.069601 139804004054784 logging_writer.py:48] [108217] accumulated_eval_time=1397.246427, accumulated_logging_time=2.568589, accumulated_submission_time=36259.424003, global_step=108217, preemption_count=0, score=36259.424003, test/accuracy=0.578900, test/loss=2.040899, test/num_examples=10000, total_duration=37662.856479, train/accuracy=0.786332, train/loss=1.065363, validation/accuracy=0.704400, validation/loss=1.417556, validation/num_examples=50000
I0215 19:45:24.111392 139807149778688 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.417697012424469, loss=2.9975640773773193
I0215 19:48:11.496002 139804004054784 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.41367852687835693, loss=2.998710870742798
I0215 19:50:58.946228 139807149778688 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.4163492023944855, loss=3.058483600616455
I0215 19:52:19.099133 139970484569920 spec.py:321] Evaluating on the training split.
I0215 19:52:25.430962 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 19:52:33.744630 139970484569920 spec.py:349] Evaluating on the test split.
I0215 19:52:36.032883 139970484569920 submission_runner.py:408] Time since start: 38189.85s, 	Step: 109741, 	{'train/accuracy': 0.7848373651504517, 'train/loss': 1.0459448099136353, 'validation/accuracy': 0.7018399834632874, 'validation/loss': 1.3989624977111816, 'validation/num_examples': 50000, 'test/accuracy': 0.5725000500679016, 'test/loss': 2.038954496383667, 'test/num_examples': 10000, 'score': 36769.386798620224, 'total_duration': 38189.84563994408, 'accumulated_submission_time': 36769.386798620224, 'accumulated_eval_time': 1414.1801376342773, 'accumulated_logging_time': 2.605180501937866}
I0215 19:52:36.057447 139803995662080 logging_writer.py:48] [109741] accumulated_eval_time=1414.180138, accumulated_logging_time=2.605181, accumulated_submission_time=36769.386799, global_step=109741, preemption_count=0, score=36769.386799, test/accuracy=0.572500, test/loss=2.038954, test/num_examples=10000, total_duration=38189.845640, train/accuracy=0.784837, train/loss=1.045945, validation/accuracy=0.701840, validation/loss=1.398962, validation/num_examples=50000
I0215 19:54:03.008151 139804004054784 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.39035817980766296, loss=2.91434383392334
I0215 19:56:50.254401 139803995662080 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.427174836397171, loss=2.9948692321777344
I0215 19:59:37.522309 139804004054784 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.4216831922531128, loss=2.9908599853515625
I0215 20:01:06.303801 139970484569920 spec.py:321] Evaluating on the training split.
I0215 20:01:12.566895 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 20:01:20.969480 139970484569920 spec.py:349] Evaluating on the test split.
I0215 20:01:23.229611 139970484569920 submission_runner.py:408] Time since start: 38717.04s, 	Step: 111267, 	{'train/accuracy': 0.840262234210968, 'train/loss': 0.8568135499954224, 'validation/accuracy': 0.7115600109100342, 'validation/loss': 1.3735299110412598, 'validation/num_examples': 50000, 'test/accuracy': 0.5875000357627869, 'test/loss': 2.0031495094299316, 'test/num_examples': 10000, 'score': 37279.57219338417, 'total_duration': 38717.04235768318, 'accumulated_submission_time': 37279.57219338417, 'accumulated_eval_time': 1431.1059007644653, 'accumulated_logging_time': 2.6398744583129883}
I0215 20:01:23.254580 139807158171392 logging_writer.py:48] [111267] accumulated_eval_time=1431.105901, accumulated_logging_time=2.639874, accumulated_submission_time=37279.572193, global_step=111267, preemption_count=0, score=37279.572193, test/accuracy=0.587500, test/loss=2.003150, test/num_examples=10000, total_duration=38717.042358, train/accuracy=0.840262, train/loss=0.856814, validation/accuracy=0.711560, validation/loss=1.373530, validation/num_examples=50000
I0215 20:02:41.539381 139807644673792 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.4326639473438263, loss=3.021728038787842
I0215 20:05:28.899129 139807158171392 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.4273742437362671, loss=2.987417221069336
I0215 20:08:16.184810 139807644673792 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.41749680042266846, loss=2.9256508350372314
I0215 20:09:53.415150 139970484569920 spec.py:321] Evaluating on the training split.
I0215 20:09:59.642164 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 20:10:08.010950 139970484569920 spec.py:349] Evaluating on the test split.
I0215 20:10:10.265776 139970484569920 submission_runner.py:408] Time since start: 39244.08s, 	Step: 112792, 	{'train/accuracy': 0.8233418464660645, 'train/loss': 0.9245814085006714, 'validation/accuracy': 0.7210999727249146, 'validation/loss': 1.3503004312515259, 'validation/num_examples': 50000, 'test/accuracy': 0.5892000198364258, 'test/loss': 1.9957637786865234, 'test/num_examples': 10000, 'score': 37789.67009830475, 'total_duration': 39244.07853126526, 'accumulated_submission_time': 37789.67009830475, 'accumulated_eval_time': 1447.956482887268, 'accumulated_logging_time': 2.675337553024292}
I0215 20:10:10.290983 139803987269376 logging_writer.py:48] [112792] accumulated_eval_time=1447.956483, accumulated_logging_time=2.675338, accumulated_submission_time=37789.670098, global_step=112792, preemption_count=0, score=37789.670098, test/accuracy=0.589200, test/loss=1.995764, test/num_examples=10000, total_duration=39244.078531, train/accuracy=0.823342, train/loss=0.924581, validation/accuracy=0.721100, validation/loss=1.350300, validation/num_examples=50000
I0215 20:11:20.188821 139803995662080 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.4305354654788971, loss=2.944307804107666
I0215 20:14:07.500409 139803987269376 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.4440462291240692, loss=2.9867119789123535
I0215 20:16:54.777266 139803995662080 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.4381423890590668, loss=2.873322010040283
I0215 20:18:40.556913 139970484569920 spec.py:321] Evaluating on the training split.
I0215 20:18:46.846242 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 20:18:55.207437 139970484569920 spec.py:349] Evaluating on the test split.
I0215 20:18:57.542111 139970484569920 submission_runner.py:408] Time since start: 39771.35s, 	Step: 114318, 	{'train/accuracy': 0.8214883208274841, 'train/loss': 0.9070454835891724, 'validation/accuracy': 0.7214799523353577, 'validation/loss': 1.3157671689987183, 'validation/num_examples': 50000, 'test/accuracy': 0.5967000126838684, 'test/loss': 1.93389892578125, 'test/num_examples': 10000, 'score': 38299.87454175949, 'total_duration': 39771.35484480858, 'accumulated_submission_time': 38299.87454175949, 'accumulated_eval_time': 1464.9416210651398, 'accumulated_logging_time': 2.710414171218872}
I0215 20:18:57.567191 139803995662080 logging_writer.py:48] [114318] accumulated_eval_time=1464.941621, accumulated_logging_time=2.710414, accumulated_submission_time=38299.874542, global_step=114318, preemption_count=0, score=38299.874542, test/accuracy=0.596700, test/loss=1.933899, test/num_examples=10000, total_duration=39771.354845, train/accuracy=0.821488, train/loss=0.907045, validation/accuracy=0.721480, validation/loss=1.315767, validation/num_examples=50000
I0215 20:19:58.767876 139807703402240 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.44040974974632263, loss=2.9364099502563477
I0215 20:22:46.182406 139803995662080 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.4391080439090729, loss=2.9071695804595947
I0215 20:25:33.526233 139807703402240 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.4584912061691284, loss=2.9047515392303467
I0215 20:27:27.794597 139970484569920 spec.py:321] Evaluating on the training split.
I0215 20:27:34.059056 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 20:27:42.390044 139970484569920 spec.py:349] Evaluating on the test split.
I0215 20:27:44.682349 139970484569920 submission_runner.py:408] Time since start: 40298.50s, 	Step: 115843, 	{'train/accuracy': 0.8283242583274841, 'train/loss': 0.8629956841468811, 'validation/accuracy': 0.7298600077629089, 'validation/loss': 1.2824699878692627, 'validation/num_examples': 50000, 'test/accuracy': 0.5997000336647034, 'test/loss': 1.9135383367538452, 'test/num_examples': 10000, 'score': 38810.03670358658, 'total_duration': 40298.495109796524, 'accumulated_submission_time': 38810.03670358658, 'accumulated_eval_time': 1481.8293414115906, 'accumulated_logging_time': 2.7465732097625732}
I0215 20:27:44.707245 139804004054784 logging_writer.py:48] [115843] accumulated_eval_time=1481.829341, accumulated_logging_time=2.746573, accumulated_submission_time=38810.036704, global_step=115843, preemption_count=0, score=38810.036704, test/accuracy=0.599700, test/loss=1.913538, test/num_examples=10000, total_duration=40298.495110, train/accuracy=0.828324, train/loss=0.862996, validation/accuracy=0.729860, validation/loss=1.282470, validation/num_examples=50000
I0215 20:28:37.540234 139807149778688 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.42465510964393616, loss=2.8412342071533203
I0215 20:31:24.790128 139804004054784 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.4583808481693268, loss=2.865940570831299
I0215 20:34:12.040784 139807149778688 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.4629206955432892, loss=2.8921611309051514
I0215 20:36:14.835877 139970484569920 spec.py:321] Evaluating on the training split.
I0215 20:36:21.102263 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 20:36:29.398687 139970484569920 spec.py:349] Evaluating on the test split.
I0215 20:36:31.674714 139970484569920 submission_runner.py:408] Time since start: 40825.49s, 	Step: 117369, 	{'train/accuracy': 0.8292211294174194, 'train/loss': 0.8857026100158691, 'validation/accuracy': 0.7333599925041199, 'validation/loss': 1.2927523851394653, 'validation/num_examples': 50000, 'test/accuracy': 0.6086000204086304, 'test/loss': 1.9028667211532593, 'test/num_examples': 10000, 'score': 39320.10192465782, 'total_duration': 40825.48747396469, 'accumulated_submission_time': 39320.10192465782, 'accumulated_eval_time': 1498.6681442260742, 'accumulated_logging_time': 2.7819528579711914}
I0215 20:36:31.703377 139807644673792 logging_writer.py:48] [117369] accumulated_eval_time=1498.668144, accumulated_logging_time=2.781953, accumulated_submission_time=39320.101925, global_step=117369, preemption_count=0, score=39320.101925, test/accuracy=0.608600, test/loss=1.902867, test/num_examples=10000, total_duration=40825.487474, train/accuracy=0.829221, train/loss=0.885703, validation/accuracy=0.733360, validation/loss=1.292752, validation/num_examples=50000
I0215 20:37:15.815521 139807703402240 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.4566599130630493, loss=2.842985153198242
I0215 20:40:03.171796 139807644673792 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.46148622035980225, loss=2.815253496170044
I0215 20:42:50.648061 139807703402240 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.4498159885406494, loss=2.808157205581665
I0215 20:45:01.973724 139970484569920 spec.py:321] Evaluating on the training split.
I0215 20:45:08.268476 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 20:45:16.650271 139970484569920 spec.py:349] Evaluating on the test split.
I0215 20:45:18.936600 139970484569920 submission_runner.py:408] Time since start: 41352.75s, 	Step: 118894, 	{'train/accuracy': 0.8380500674247742, 'train/loss': 0.8556788563728333, 'validation/accuracy': 0.7351599931716919, 'validation/loss': 1.2787137031555176, 'validation/num_examples': 50000, 'test/accuracy': 0.6147000193595886, 'test/loss': 1.881813406944275, 'test/num_examples': 10000, 'score': 39830.31067419052, 'total_duration': 41352.74931931496, 'accumulated_submission_time': 39830.31067419052, 'accumulated_eval_time': 1515.6309685707092, 'accumulated_logging_time': 2.820580244064331}
I0215 20:45:18.970391 139803987269376 logging_writer.py:48] [118894] accumulated_eval_time=1515.630969, accumulated_logging_time=2.820580, accumulated_submission_time=39830.310674, global_step=118894, preemption_count=0, score=39830.310674, test/accuracy=0.614700, test/loss=1.881813, test/num_examples=10000, total_duration=41352.749319, train/accuracy=0.838050, train/loss=0.855679, validation/accuracy=0.735160, validation/loss=1.278714, validation/num_examples=50000
I0215 20:45:54.739223 139803995662080 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.47369545698165894, loss=2.8228936195373535
I0215 20:48:42.022306 139803987269376 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.46226054430007935, loss=2.7953126430511475
I0215 20:51:29.215615 139803995662080 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.4757440984249115, loss=2.8236441612243652
I0215 20:53:49.186381 139970484569920 spec.py:321] Evaluating on the training split.
I0215 20:53:55.398610 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 20:54:03.681443 139970484569920 spec.py:349] Evaluating on the test split.
I0215 20:54:05.999273 139970484569920 submission_runner.py:408] Time since start: 41879.81s, 	Step: 120420, 	{'train/accuracy': 0.8646364808082581, 'train/loss': 0.7449840307235718, 'validation/accuracy': 0.7392199635505676, 'validation/loss': 1.2548187971115112, 'validation/num_examples': 50000, 'test/accuracy': 0.6165000200271606, 'test/loss': 1.8725852966308594, 'test/num_examples': 10000, 'score': 40340.46223068237, 'total_duration': 41879.81203150749, 'accumulated_submission_time': 40340.46223068237, 'accumulated_eval_time': 1532.4438333511353, 'accumulated_logging_time': 2.86673641204834}
I0215 20:54:06.028282 139804004054784 logging_writer.py:48] [120420] accumulated_eval_time=1532.443833, accumulated_logging_time=2.866736, accumulated_submission_time=40340.462231, global_step=120420, preemption_count=0, score=40340.462231, test/accuracy=0.616500, test/loss=1.872585, test/num_examples=10000, total_duration=41879.812032, train/accuracy=0.864636, train/loss=0.744984, validation/accuracy=0.739220, validation/loss=1.254819, validation/num_examples=50000
I0215 20:54:33.105939 139807711794944 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.48098766803741455, loss=2.85455060005188
I0215 20:57:20.422581 139804004054784 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.46968430280685425, loss=2.755655527114868
I0215 21:00:07.834088 139807711794944 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.4674313962459564, loss=2.7564878463745117
I0215 21:02:36.332603 139970484569920 spec.py:321] Evaluating on the training split.
I0215 21:02:42.648423 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 21:02:50.952759 139970484569920 spec.py:349] Evaluating on the test split.
I0215 21:02:53.194411 139970484569920 submission_runner.py:408] Time since start: 42407.01s, 	Step: 121945, 	{'train/accuracy': 0.8652941584587097, 'train/loss': 0.75306636095047, 'validation/accuracy': 0.7446399927139282, 'validation/loss': 1.23543381690979, 'validation/num_examples': 50000, 'test/accuracy': 0.6249000430107117, 'test/loss': 1.8201731443405151, 'test/num_examples': 10000, 'score': 40850.701830387115, 'total_duration': 42407.00716710091, 'accumulated_submission_time': 40850.701830387115, 'accumulated_eval_time': 1549.3056252002716, 'accumulated_logging_time': 2.9067347049713135}
I0215 21:02:53.222110 139803995662080 logging_writer.py:48] [121945] accumulated_eval_time=1549.305625, accumulated_logging_time=2.906735, accumulated_submission_time=40850.701830, global_step=121945, preemption_count=0, score=40850.701830, test/accuracy=0.624900, test/loss=1.820173, test/num_examples=10000, total_duration=42407.007167, train/accuracy=0.865294, train/loss=0.753066, validation/accuracy=0.744640, validation/loss=1.235434, validation/num_examples=50000
I0215 21:03:11.955781 139804004054784 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.47934505343437195, loss=2.787203311920166
I0215 21:05:59.354752 139803995662080 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.5001408457756042, loss=2.839038848876953
I0215 21:08:46.650019 139804004054784 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.49481382966041565, loss=2.71504807472229
I0215 21:11:23.478238 139970484569920 spec.py:321] Evaluating on the training split.
I0215 21:11:29.687034 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 21:11:37.966569 139970484569920 spec.py:349] Evaluating on the test split.
I0215 21:11:40.224889 139970484569920 submission_runner.py:408] Time since start: 42934.04s, 	Step: 123470, 	{'train/accuracy': 0.8700175285339355, 'train/loss': 0.7411680221557617, 'validation/accuracy': 0.753059983253479, 'validation/loss': 1.2147120237350464, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8131251335144043, 'test/num_examples': 10000, 'score': 41360.89746046066, 'total_duration': 42934.03763651848, 'accumulated_submission_time': 41360.89746046066, 'accumulated_eval_time': 1566.0522344112396, 'accumulated_logging_time': 2.9459853172302246}
I0215 21:11:40.252074 139807644673792 logging_writer.py:48] [123470] accumulated_eval_time=1566.052234, accumulated_logging_time=2.945985, accumulated_submission_time=41360.897460, global_step=123470, preemption_count=0, score=41360.897460, test/accuracy=0.629900, test/loss=1.813125, test/num_examples=10000, total_duration=42934.037637, train/accuracy=0.870018, train/loss=0.741168, validation/accuracy=0.753060, validation/loss=1.214712, validation/num_examples=50000
I0215 21:11:50.620734 139807703402240 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.5078614950180054, loss=2.7701730728149414
I0215 21:14:37.966075 139807644673792 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.49921077489852905, loss=2.685154438018799
I0215 21:17:25.417612 139807703402240 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.48014476895332336, loss=2.711656093597412
I0215 21:20:10.558206 139970484569920 spec.py:321] Evaluating on the training split.
I0215 21:20:16.983762 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 21:20:25.384842 139970484569920 spec.py:349] Evaluating on the test split.
I0215 21:20:27.659443 139970484569920 submission_runner.py:408] Time since start: 43461.47s, 	Step: 124995, 	{'train/accuracy': 0.8798429369926453, 'train/loss': 0.6981543302536011, 'validation/accuracy': 0.7577599883079529, 'validation/loss': 1.1877187490463257, 'validation/num_examples': 50000, 'test/accuracy': 0.6325000524520874, 'test/loss': 1.7903801202774048, 'test/num_examples': 10000, 'score': 41871.141345500946, 'total_duration': 43461.47218251228, 'accumulated_submission_time': 41871.141345500946, 'accumulated_eval_time': 1583.1534242630005, 'accumulated_logging_time': 2.983980655670166}
I0215 21:20:27.695421 139807149778688 logging_writer.py:48] [124995] accumulated_eval_time=1583.153424, accumulated_logging_time=2.983981, accumulated_submission_time=41871.141346, global_step=124995, preemption_count=0, score=41871.141346, test/accuracy=0.632500, test/loss=1.790380, test/num_examples=10000, total_duration=43461.472183, train/accuracy=0.879843, train/loss=0.698154, validation/accuracy=0.757760, validation/loss=1.187719, validation/num_examples=50000
I0215 21:20:29.703844 139807158171392 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.49725988507270813, loss=2.659527063369751
I0215 21:23:17.005975 139807149778688 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.5053149461746216, loss=2.6395347118377686
I0215 21:26:04.290029 139807158171392 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.4978010356426239, loss=2.646498918533325
I0215 21:28:51.637255 139807149778688 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.5149869918823242, loss=2.665454387664795
I0215 21:28:57.769089 139970484569920 spec.py:321] Evaluating on the training split.
I0215 21:29:03.995929 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 21:29:12.515602 139970484569920 spec.py:349] Evaluating on the test split.
I0215 21:29:14.777342 139970484569920 submission_runner.py:408] Time since start: 43988.59s, 	Step: 126520, 	{'train/accuracy': 0.890625, 'train/loss': 0.6434340476989746, 'validation/accuracy': 0.764519989490509, 'validation/loss': 1.151633858680725, 'validation/num_examples': 50000, 'test/accuracy': 0.6461000442504883, 'test/loss': 1.7440989017486572, 'test/num_examples': 10000, 'score': 42381.152169942856, 'total_duration': 43988.59010863304, 'accumulated_submission_time': 42381.152169942856, 'accumulated_eval_time': 1600.1616296768188, 'accumulated_logging_time': 3.0353474617004395}
I0215 21:29:14.802648 139803995662080 logging_writer.py:48] [126520] accumulated_eval_time=1600.161630, accumulated_logging_time=3.035347, accumulated_submission_time=42381.152170, global_step=126520, preemption_count=0, score=42381.152170, test/accuracy=0.646100, test/loss=1.744099, test/num_examples=10000, total_duration=43988.590109, train/accuracy=0.890625, train/loss=0.643434, validation/accuracy=0.764520, validation/loss=1.151634, validation/num_examples=50000
I0215 21:31:55.812190 139804004054784 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.5188159942626953, loss=2.677518367767334
I0215 21:34:43.201566 139803995662080 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.48725399374961853, loss=2.607818603515625
I0215 21:37:30.675629 139804004054784 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.517983615398407, loss=2.734743356704712
I0215 21:37:44.818229 139970484569920 spec.py:321] Evaluating on the training split.
I0215 21:37:51.066474 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 21:37:59.462157 139970484569920 spec.py:349] Evaluating on the test split.
I0215 21:38:01.762693 139970484569920 submission_runner.py:408] Time since start: 44515.58s, 	Step: 128044, 	{'train/accuracy': 0.9036591053009033, 'train/loss': 0.6054961085319519, 'validation/accuracy': 0.7681399583816528, 'validation/loss': 1.1449393033981323, 'validation/num_examples': 50000, 'test/accuracy': 0.6472000479698181, 'test/loss': 1.733769178390503, 'test/num_examples': 10000, 'score': 42891.09298849106, 'total_duration': 44515.57524704933, 'accumulated_submission_time': 42891.09298849106, 'accumulated_eval_time': 1617.1058330535889, 'accumulated_logging_time': 3.0884687900543213}
I0215 21:38:01.789092 139804004054784 logging_writer.py:48] [128044] accumulated_eval_time=1617.105833, accumulated_logging_time=3.088469, accumulated_submission_time=42891.092988, global_step=128044, preemption_count=0, score=42891.092988, test/accuracy=0.647200, test/loss=1.733769, test/num_examples=10000, total_duration=44515.575247, train/accuracy=0.903659, train/loss=0.605496, validation/accuracy=0.768140, validation/loss=1.144939, validation/num_examples=50000
I0215 21:40:34.730624 139807149778688 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.5112217664718628, loss=2.6245603561401367
I0215 21:43:22.160344 139804004054784 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.5227283239364624, loss=2.6631245613098145
I0215 21:46:09.553158 139807149778688 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.5106344819068909, loss=2.6217501163482666
I0215 21:46:32.072664 139970484569920 spec.py:321] Evaluating on the training split.
I0215 21:46:38.300828 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 21:46:46.602186 139970484569920 spec.py:349] Evaluating on the test split.
I0215 21:46:48.881695 139970484569920 submission_runner.py:408] Time since start: 45042.69s, 	Step: 129569, 	{'train/accuracy': 0.8992147445678711, 'train/loss': 0.6113567352294922, 'validation/accuracy': 0.7674799561500549, 'validation/loss': 1.1436293125152588, 'validation/num_examples': 50000, 'test/accuracy': 0.6494000554084778, 'test/loss': 1.7323858737945557, 'test/num_examples': 10000, 'score': 43401.31970334053, 'total_duration': 45042.69445633888, 'accumulated_submission_time': 43401.31970334053, 'accumulated_eval_time': 1633.914809703827, 'accumulated_logging_time': 3.1249136924743652}
I0215 21:46:48.910198 139803995662080 logging_writer.py:48] [129569] accumulated_eval_time=1633.914810, accumulated_logging_time=3.124914, accumulated_submission_time=43401.319703, global_step=129569, preemption_count=0, score=43401.319703, test/accuracy=0.649400, test/loss=1.732386, test/num_examples=10000, total_duration=45042.694456, train/accuracy=0.899215, train/loss=0.611357, validation/accuracy=0.767480, validation/loss=1.143629, validation/num_examples=50000
I0215 21:49:13.500683 139804004054784 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.5336790084838867, loss=2.6941630840301514
I0215 21:52:00.902521 139803995662080 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.515926718711853, loss=2.6420845985412598
I0215 21:54:48.401051 139804004054784 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.543244481086731, loss=2.6650846004486084
I0215 21:55:18.937968 139970484569920 spec.py:321] Evaluating on the training split.
I0215 21:55:25.240281 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 21:55:33.567908 139970484569920 spec.py:349] Evaluating on the test split.
I0215 21:55:35.858399 139970484569920 submission_runner.py:408] Time since start: 45569.67s, 	Step: 131093, 	{'train/accuracy': 0.9001116156578064, 'train/loss': 0.6123691201210022, 'validation/accuracy': 0.7674199938774109, 'validation/loss': 1.1417090892791748, 'validation/num_examples': 50000, 'test/accuracy': 0.6477000117301941, 'test/loss': 1.7300938367843628, 'test/num_examples': 10000, 'score': 43911.290818452835, 'total_duration': 45569.67114830017, 'accumulated_submission_time': 43911.290818452835, 'accumulated_eval_time': 1650.835176229477, 'accumulated_logging_time': 3.163182258605957}
I0215 21:55:35.884548 139807149778688 logging_writer.py:48] [131093] accumulated_eval_time=1650.835176, accumulated_logging_time=3.163182, accumulated_submission_time=43911.290818, global_step=131093, preemption_count=0, score=43911.290818, test/accuracy=0.647700, test/loss=1.730094, test/num_examples=10000, total_duration=45569.671148, train/accuracy=0.900112, train/loss=0.612369, validation/accuracy=0.767420, validation/loss=1.141709, validation/num_examples=50000
I0215 21:57:52.475423 139807644673792 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.5249295234680176, loss=2.619516372680664
I0215 22:00:39.777671 139807149778688 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.5473125576972961, loss=2.6020100116729736
I0215 22:03:27.244784 139807644673792 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.5344460606575012, loss=2.6929850578308105
I0215 22:04:05.869995 139970484569920 spec.py:321] Evaluating on the training split.
I0215 22:04:12.138304 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 22:04:20.481145 139970484569920 spec.py:349] Evaluating on the test split.
I0215 22:04:23.021981 139970484569920 submission_runner.py:408] Time since start: 46096.83s, 	Step: 132617, 	{'train/accuracy': 0.899812638759613, 'train/loss': 0.6107229590415955, 'validation/accuracy': 0.7678999900817871, 'validation/loss': 1.1452364921569824, 'validation/num_examples': 50000, 'test/accuracy': 0.6480000019073486, 'test/loss': 1.733577013015747, 'test/num_examples': 10000, 'score': 44421.219329595566, 'total_duration': 46096.83474469185, 'accumulated_submission_time': 44421.219329595566, 'accumulated_eval_time': 1667.9871294498444, 'accumulated_logging_time': 3.1995527744293213}
I0215 22:04:23.048431 139804004054784 logging_writer.py:48] [132617] accumulated_eval_time=1667.987129, accumulated_logging_time=3.199553, accumulated_submission_time=44421.219330, global_step=132617, preemption_count=0, score=44421.219330, test/accuracy=0.648000, test/loss=1.733577, test/num_examples=10000, total_duration=46096.834745, train/accuracy=0.899813, train/loss=0.610723, validation/accuracy=0.767900, validation/loss=1.145236, validation/num_examples=50000
I0215 22:06:31.613409 139807158171392 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.5458588600158691, loss=2.6340579986572266
I0215 22:09:19.163672 139804004054784 logging_writer.py:48] [133500] global_step=133500, grad_norm=0.5248041749000549, loss=2.694316864013672
I0215 22:12:06.670295 139807158171392 logging_writer.py:48] [134000] global_step=134000, grad_norm=0.5380388498306274, loss=2.6192808151245117
I0215 22:12:53.285837 139970484569920 spec.py:321] Evaluating on the training split.
I0215 22:12:59.523480 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 22:13:07.854053 139970484569920 spec.py:349] Evaluating on the test split.
I0215 22:13:10.109856 139970484569920 submission_runner.py:408] Time since start: 46623.92s, 	Step: 134141, 	{'train/accuracy': 0.9026227593421936, 'train/loss': 0.6078991889953613, 'validation/accuracy': 0.7683799862861633, 'validation/loss': 1.1395996809005737, 'validation/num_examples': 50000, 'test/accuracy': 0.6462000012397766, 'test/loss': 1.7313485145568848, 'test/num_examples': 10000, 'score': 44931.39663743973, 'total_duration': 46623.92261862755, 'accumulated_submission_time': 44931.39663743973, 'accumulated_eval_time': 1684.811104297638, 'accumulated_logging_time': 3.2384746074676514}
I0215 22:13:10.137167 139807149778688 logging_writer.py:48] [134141] accumulated_eval_time=1684.811104, accumulated_logging_time=3.238475, accumulated_submission_time=44931.396637, global_step=134141, preemption_count=0, score=44931.396637, test/accuracy=0.646200, test/loss=1.731349, test/num_examples=10000, total_duration=46623.922619, train/accuracy=0.902623, train/loss=0.607899, validation/accuracy=0.768380, validation/loss=1.139600, validation/num_examples=50000
I0215 22:15:10.625688 139807644673792 logging_writer.py:48] [134500] global_step=134500, grad_norm=0.5690649747848511, loss=2.695617198944092
I0215 22:17:57.984871 139807149778688 logging_writer.py:48] [135000] global_step=135000, grad_norm=0.5490217208862305, loss=2.6228506565093994
I0215 22:20:45.346380 139807644673792 logging_writer.py:48] [135500] global_step=135500, grad_norm=0.5317066311836243, loss=2.607844352722168
I0215 22:21:40.312077 139970484569920 spec.py:321] Evaluating on the training split.
I0215 22:21:46.516910 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 22:21:54.916266 139970484569920 spec.py:349] Evaluating on the test split.
I0215 22:21:57.179095 139970484569920 submission_runner.py:408] Time since start: 47150.99s, 	Step: 135666, 	{'train/accuracy': 0.9016063213348389, 'train/loss': 0.608150064945221, 'validation/accuracy': 0.7674399614334106, 'validation/loss': 1.142611026763916, 'validation/num_examples': 50000, 'test/accuracy': 0.6473000049591064, 'test/loss': 1.7349656820297241, 'test/num_examples': 10000, 'score': 45441.513060092926, 'total_duration': 47150.9918525219, 'accumulated_submission_time': 45441.513060092926, 'accumulated_eval_time': 1701.6780714988708, 'accumulated_logging_time': 3.2771096229553223}
I0215 22:21:57.205649 139803995662080 logging_writer.py:48] [135666] accumulated_eval_time=1701.678071, accumulated_logging_time=3.277110, accumulated_submission_time=45441.513060, global_step=135666, preemption_count=0, score=45441.513060, test/accuracy=0.647300, test/loss=1.734966, test/num_examples=10000, total_duration=47150.991853, train/accuracy=0.901606, train/loss=0.608150, validation/accuracy=0.767440, validation/loss=1.142611, validation/num_examples=50000
I0215 22:23:49.408503 139804004054784 logging_writer.py:48] [136000] global_step=136000, grad_norm=0.5119238495826721, loss=2.5682482719421387
I0215 22:26:37.059938 139803995662080 logging_writer.py:48] [136500] global_step=136500, grad_norm=0.5353772640228271, loss=2.6376986503601074
I0215 22:29:24.522961 139804004054784 logging_writer.py:48] [137000] global_step=137000, grad_norm=0.545653760433197, loss=2.587848663330078
I0215 22:30:27.232467 139970484569920 spec.py:321] Evaluating on the training split.
I0215 22:30:33.463679 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 22:30:41.759947 139970484569920 spec.py:349] Evaluating on the test split.
I0215 22:30:44.049244 139970484569920 submission_runner.py:408] Time since start: 47677.86s, 	Step: 137189, 	{'train/accuracy': 0.9120894074440002, 'train/loss': 0.5788108110427856, 'validation/accuracy': 0.76801997423172, 'validation/loss': 1.1506139039993286, 'validation/num_examples': 50000, 'test/accuracy': 0.6458000540733337, 'test/loss': 1.7417221069335938, 'test/num_examples': 10000, 'score': 45951.4829518795, 'total_duration': 47677.86198878288, 'accumulated_submission_time': 45951.4829518795, 'accumulated_eval_time': 1718.4947848320007, 'accumulated_logging_time': 3.3133702278137207}
I0215 22:30:44.079925 139803987269376 logging_writer.py:48] [137189] accumulated_eval_time=1718.494785, accumulated_logging_time=3.313370, accumulated_submission_time=45951.482952, global_step=137189, preemption_count=0, score=45951.482952, test/accuracy=0.645800, test/loss=1.741722, test/num_examples=10000, total_duration=47677.861989, train/accuracy=0.912089, train/loss=0.578811, validation/accuracy=0.768020, validation/loss=1.150614, validation/num_examples=50000
I0215 22:32:28.469637 139803995662080 logging_writer.py:48] [137500] global_step=137500, grad_norm=0.5419521927833557, loss=2.6275553703308105
I0215 22:35:15.967887 139803987269376 logging_writer.py:48] [138000] global_step=138000, grad_norm=0.5707600116729736, loss=2.646462917327881
I0215 22:38:03.355624 139803995662080 logging_writer.py:48] [138500] global_step=138500, grad_norm=0.555891752243042, loss=2.6336417198181152
I0215 22:39:14.062210 139970484569920 spec.py:321] Evaluating on the training split.
I0215 22:39:20.391438 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 22:39:28.723479 139970484569920 spec.py:349] Evaluating on the test split.
I0215 22:39:31.404027 139970484569920 submission_runner.py:408] Time since start: 48205.22s, 	Step: 138713, 	{'train/accuracy': 0.9090401530265808, 'train/loss': 0.5813366770744324, 'validation/accuracy': 0.7679799795150757, 'validation/loss': 1.1458003520965576, 'validation/num_examples': 50000, 'test/accuracy': 0.648900032043457, 'test/loss': 1.739550232887268, 'test/num_examples': 10000, 'score': 46461.408250808716, 'total_duration': 48205.216795921326, 'accumulated_submission_time': 46461.408250808716, 'accumulated_eval_time': 1735.8365664482117, 'accumulated_logging_time': 3.3539419174194336}
I0215 22:39:31.430227 139807149778688 logging_writer.py:48] [138713] accumulated_eval_time=1735.836566, accumulated_logging_time=3.353942, accumulated_submission_time=46461.408251, global_step=138713, preemption_count=0, score=46461.408251, test/accuracy=0.648900, test/loss=1.739550, test/num_examples=10000, total_duration=48205.216796, train/accuracy=0.909040, train/loss=0.581337, validation/accuracy=0.767980, validation/loss=1.145800, validation/num_examples=50000
I0215 22:41:07.831423 139807158171392 logging_writer.py:48] [139000] global_step=139000, grad_norm=0.5571683049201965, loss=2.603271722793579
I0215 22:43:55.268661 139807149778688 logging_writer.py:48] [139500] global_step=139500, grad_norm=0.5443758368492126, loss=2.6050240993499756
I0215 22:46:42.948193 139807158171392 logging_writer.py:48] [140000] global_step=140000, grad_norm=0.5276647210121155, loss=2.5507640838623047
I0215 22:48:01.415373 139970484569920 spec.py:321] Evaluating on the training split.
I0215 22:48:07.698721 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 22:48:15.965551 139970484569920 spec.py:349] Evaluating on the test split.
I0215 22:48:18.267537 139970484569920 submission_runner.py:408] Time since start: 48732.08s, 	Step: 140236, 	{'train/accuracy': 0.9064293503761292, 'train/loss': 0.5960589051246643, 'validation/accuracy': 0.7668799757957458, 'validation/loss': 1.152432918548584, 'validation/num_examples': 50000, 'test/accuracy': 0.6484000086784363, 'test/loss': 1.7396743297576904, 'test/num_examples': 10000, 'score': 46971.337717056274, 'total_duration': 48732.08028960228, 'accumulated_submission_time': 46971.337717056274, 'accumulated_eval_time': 1752.688690662384, 'accumulated_logging_time': 3.3888731002807617}
I0215 22:48:18.295711 139803987269376 logging_writer.py:48] [140236] accumulated_eval_time=1752.688691, accumulated_logging_time=3.388873, accumulated_submission_time=46971.337717, global_step=140236, preemption_count=0, score=46971.337717, test/accuracy=0.648400, test/loss=1.739674, test/num_examples=10000, total_duration=48732.080290, train/accuracy=0.906429, train/loss=0.596059, validation/accuracy=0.766880, validation/loss=1.152433, validation/num_examples=50000
I0215 22:49:46.994935 139804004054784 logging_writer.py:48] [140500] global_step=140500, grad_norm=0.5712434649467468, loss=2.633312463760376
I0215 22:52:34.328811 139803987269376 logging_writer.py:48] [141000] global_step=141000, grad_norm=0.552729606628418, loss=2.5906684398651123
I0215 22:55:21.710428 139804004054784 logging_writer.py:48] [141500] global_step=141500, grad_norm=0.5898760557174683, loss=2.5582275390625
I0215 22:56:48.443683 139970484569920 spec.py:321] Evaluating on the training split.
I0215 22:56:54.715044 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 22:57:03.064038 139970484569920 spec.py:349] Evaluating on the test split.
I0215 22:57:05.328668 139970484569920 submission_runner.py:408] Time since start: 49259.14s, 	Step: 141761, 	{'train/accuracy': 0.9070870280265808, 'train/loss': 0.5918859839439392, 'validation/accuracy': 0.7676999568939209, 'validation/loss': 1.150712490081787, 'validation/num_examples': 50000, 'test/accuracy': 0.6475000381469727, 'test/loss': 1.7458745241165161, 'test/num_examples': 10000, 'score': 47481.42756533623, 'total_duration': 49259.141426324844, 'accumulated_submission_time': 47481.42756533623, 'accumulated_eval_time': 1769.573629617691, 'accumulated_logging_time': 3.4280009269714355}
I0215 22:57:05.356121 139804004054784 logging_writer.py:48] [141761] accumulated_eval_time=1769.573630, accumulated_logging_time=3.428001, accumulated_submission_time=47481.427565, global_step=141761, preemption_count=0, score=47481.427565, test/accuracy=0.647500, test/loss=1.745875, test/num_examples=10000, total_duration=49259.141426, train/accuracy=0.907087, train/loss=0.591886, validation/accuracy=0.767700, validation/loss=1.150712, validation/num_examples=50000
I0215 22:58:25.691813 139807149778688 logging_writer.py:48] [142000] global_step=142000, grad_norm=0.585085391998291, loss=2.611464023590088
I0215 23:01:13.113854 139804004054784 logging_writer.py:48] [142500] global_step=142500, grad_norm=0.6069295406341553, loss=2.6829466819763184
I0215 23:04:00.537795 139807149778688 logging_writer.py:48] [143000] global_step=143000, grad_norm=0.5510545969009399, loss=2.5918211936950684
I0215 23:05:35.384905 139970484569920 spec.py:321] Evaluating on the training split.
I0215 23:05:41.643672 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 23:05:50.016746 139970484569920 spec.py:349] Evaluating on the test split.
I0215 23:05:52.419751 139970484569920 submission_runner.py:408] Time since start: 49786.23s, 	Step: 143285, 	{'train/accuracy': 0.9057118892669678, 'train/loss': 0.5928652286529541, 'validation/accuracy': 0.7670199871063232, 'validation/loss': 1.1489965915679932, 'validation/num_examples': 50000, 'test/accuracy': 0.6485000252723694, 'test/loss': 1.7452924251556396, 'test/num_examples': 10000, 'score': 47991.39926171303, 'total_duration': 49786.23249578476, 'accumulated_submission_time': 47991.39926171303, 'accumulated_eval_time': 1786.6084179878235, 'accumulated_logging_time': 3.465696334838867}
I0215 23:05:52.447194 139803995662080 logging_writer.py:48] [143285] accumulated_eval_time=1786.608418, accumulated_logging_time=3.465696, accumulated_submission_time=47991.399262, global_step=143285, preemption_count=0, score=47991.399262, test/accuracy=0.648500, test/loss=1.745292, test/num_examples=10000, total_duration=49786.232496, train/accuracy=0.905712, train/loss=0.592865, validation/accuracy=0.767020, validation/loss=1.148997, validation/num_examples=50000
I0215 23:07:04.679661 139804004054784 logging_writer.py:48] [143500] global_step=143500, grad_norm=0.6072457432746887, loss=2.6424779891967773
I0215 23:09:52.064703 139803995662080 logging_writer.py:48] [144000] global_step=144000, grad_norm=0.5841591954231262, loss=2.602112054824829
I0215 23:12:39.489341 139804004054784 logging_writer.py:48] [144500] global_step=144500, grad_norm=0.5691404342651367, loss=2.598479747772217
I0215 23:14:22.645261 139970484569920 spec.py:321] Evaluating on the training split.
I0215 23:14:28.920970 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 23:14:37.166361 139970484569920 spec.py:349] Evaluating on the test split.
I0215 23:14:39.418007 139970484569920 submission_runner.py:408] Time since start: 50313.23s, 	Step: 144810, 	{'train/accuracy': 0.91214919090271, 'train/loss': 0.569913387298584, 'validation/accuracy': 0.7676599621772766, 'validation/loss': 1.1494959592819214, 'validation/num_examples': 50000, 'test/accuracy': 0.6438000202178955, 'test/loss': 1.7543665170669556, 'test/num_examples': 10000, 'score': 48501.540533304214, 'total_duration': 50313.23071908951, 'accumulated_submission_time': 48501.540533304214, 'accumulated_eval_time': 1803.3810710906982, 'accumulated_logging_time': 3.5027754306793213}
I0215 23:14:39.447073 139807711794944 logging_writer.py:48] [144810] accumulated_eval_time=1803.381071, accumulated_logging_time=3.502775, accumulated_submission_time=48501.540533, global_step=144810, preemption_count=0, score=48501.540533, test/accuracy=0.643800, test/loss=1.754367, test/num_examples=10000, total_duration=50313.230719, train/accuracy=0.912149, train/loss=0.569913, validation/accuracy=0.767660, validation/loss=1.149496, validation/num_examples=50000
I0215 23:15:43.423870 139807720187648 logging_writer.py:48] [145000] global_step=145000, grad_norm=0.5685009956359863, loss=2.5451626777648926
I0215 23:18:31.043565 139807711794944 logging_writer.py:48] [145500] global_step=145500, grad_norm=0.5584359765052795, loss=2.5575673580169678
I0215 23:21:18.483392 139807720187648 logging_writer.py:48] [146000] global_step=146000, grad_norm=0.5846180319786072, loss=2.6156911849975586
I0215 23:23:09.460042 139970484569920 spec.py:321] Evaluating on the training split.
I0215 23:23:15.689054 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 23:23:23.988682 139970484569920 spec.py:349] Evaluating on the test split.
I0215 23:23:26.205230 139970484569920 submission_runner.py:408] Time since start: 50840.02s, 	Step: 146333, 	{'train/accuracy': 0.9154974222183228, 'train/loss': 0.5673818588256836, 'validation/accuracy': 0.767300009727478, 'validation/loss': 1.1546895503997803, 'validation/num_examples': 50000, 'test/accuracy': 0.6459000110626221, 'test/loss': 1.7476420402526855, 'test/num_examples': 10000, 'score': 49011.49378705025, 'total_duration': 50840.01796603203, 'accumulated_submission_time': 49011.49378705025, 'accumulated_eval_time': 1820.126195192337, 'accumulated_logging_time': 3.544480562210083}
I0215 23:23:26.232524 139804004054784 logging_writer.py:48] [146333] accumulated_eval_time=1820.126195, accumulated_logging_time=3.544481, accumulated_submission_time=49011.493787, global_step=146333, preemption_count=0, score=49011.493787, test/accuracy=0.645900, test/loss=1.747642, test/num_examples=10000, total_duration=50840.017966, train/accuracy=0.915497, train/loss=0.567382, validation/accuracy=0.767300, validation/loss=1.154690, validation/num_examples=50000
I0215 23:24:22.486557 139807149778688 logging_writer.py:48] [146500] global_step=146500, grad_norm=0.594323992729187, loss=2.630772590637207
I0215 23:27:09.777796 139804004054784 logging_writer.py:48] [147000] global_step=147000, grad_norm=0.5944937467575073, loss=2.6228957176208496
I0215 23:29:57.237855 139807149778688 logging_writer.py:48] [147500] global_step=147500, grad_norm=0.5974146127700806, loss=2.642120599746704
I0215 23:31:56.490214 139970484569920 spec.py:321] Evaluating on the training split.
I0215 23:32:02.753013 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 23:32:11.003533 139970484569920 spec.py:349] Evaluating on the test split.
I0215 23:32:13.247249 139970484569920 submission_runner.py:408] Time since start: 51367.06s, 	Step: 147858, 	{'train/accuracy': 0.9136439561843872, 'train/loss': 0.554057776927948, 'validation/accuracy': 0.7684999704360962, 'validation/loss': 1.1387571096420288, 'validation/num_examples': 50000, 'test/accuracy': 0.6473000049591064, 'test/loss': 1.7397738695144653, 'test/num_examples': 10000, 'score': 49521.69353747368, 'total_duration': 51367.05997681618, 'accumulated_submission_time': 49521.69353747368, 'accumulated_eval_time': 1836.8831577301025, 'accumulated_logging_time': 3.5825765132904053}
I0215 23:32:13.276950 139807703402240 logging_writer.py:48] [147858] accumulated_eval_time=1836.883158, accumulated_logging_time=3.582577, accumulated_submission_time=49521.693537, global_step=147858, preemption_count=0, score=49521.693537, test/accuracy=0.647300, test/loss=1.739774, test/num_examples=10000, total_duration=51367.059977, train/accuracy=0.913644, train/loss=0.554058, validation/accuracy=0.768500, validation/loss=1.138757, validation/num_examples=50000
I0215 23:33:01.127804 139807711794944 logging_writer.py:48] [148000] global_step=148000, grad_norm=0.5875089168548584, loss=2.591200590133667
I0215 23:35:48.591094 139807703402240 logging_writer.py:48] [148500] global_step=148500, grad_norm=0.6081187725067139, loss=2.6166818141937256
I0215 23:38:36.098995 139807711794944 logging_writer.py:48] [149000] global_step=149000, grad_norm=0.5883081555366516, loss=2.6235663890838623
I0215 23:40:43.260297 139970484569920 spec.py:321] Evaluating on the training split.
I0215 23:40:49.502853 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 23:40:57.921123 139970484569920 spec.py:349] Evaluating on the test split.
I0215 23:41:00.212444 139970484569920 submission_runner.py:408] Time since start: 51894.03s, 	Step: 149381, 	{'train/accuracy': 0.9146205186843872, 'train/loss': 0.5675554275512695, 'validation/accuracy': 0.7678999900817871, 'validation/loss': 1.1539218425750732, 'validation/num_examples': 50000, 'test/accuracy': 0.6495000123977661, 'test/loss': 1.747617483139038, 'test/num_examples': 10000, 'score': 50031.62016391754, 'total_duration': 51894.02519488335, 'accumulated_submission_time': 50031.62016391754, 'accumulated_eval_time': 1853.835256099701, 'accumulated_logging_time': 3.621910810470581}
I0215 23:41:00.246307 139804004054784 logging_writer.py:48] [149381] accumulated_eval_time=1853.835256, accumulated_logging_time=3.621911, accumulated_submission_time=50031.620164, global_step=149381, preemption_count=0, score=50031.620164, test/accuracy=0.649500, test/loss=1.747617, test/num_examples=10000, total_duration=51894.025195, train/accuracy=0.914621, train/loss=0.567555, validation/accuracy=0.767900, validation/loss=1.153922, validation/num_examples=50000
I0215 23:41:40.410197 139807149778688 logging_writer.py:48] [149500] global_step=149500, grad_norm=0.5897590517997742, loss=2.5637242794036865
I0215 23:44:27.763352 139804004054784 logging_writer.py:48] [150000] global_step=150000, grad_norm=0.5788090229034424, loss=2.52925443649292
I0215 23:47:15.183383 139807149778688 logging_writer.py:48] [150500] global_step=150500, grad_norm=0.6203551888465881, loss=2.5855185985565186
I0215 23:49:30.230467 139970484569920 spec.py:321] Evaluating on the training split.
I0215 23:49:36.394026 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 23:49:44.717521 139970484569920 spec.py:349] Evaluating on the test split.
I0215 23:49:47.077204 139970484569920 submission_runner.py:408] Time since start: 52420.89s, 	Step: 150905, 	{'train/accuracy': 0.9134247303009033, 'train/loss': 0.5734334588050842, 'validation/accuracy': 0.7669999599456787, 'validation/loss': 1.1549052000045776, 'validation/num_examples': 50000, 'test/accuracy': 0.6456000208854675, 'test/loss': 1.7588087320327759, 'test/num_examples': 10000, 'score': 50541.54686450958, 'total_duration': 52420.88992810249, 'accumulated_submission_time': 50541.54686450958, 'accumulated_eval_time': 1870.6819188594818, 'accumulated_logging_time': 3.666167736053467}
I0215 23:49:47.106102 139807644673792 logging_writer.py:48] [150905] accumulated_eval_time=1870.681919, accumulated_logging_time=3.666168, accumulated_submission_time=50541.546865, global_step=150905, preemption_count=0, score=50541.546865, test/accuracy=0.645600, test/loss=1.758809, test/num_examples=10000, total_duration=52420.889928, train/accuracy=0.913425, train/loss=0.573433, validation/accuracy=0.767000, validation/loss=1.154905, validation/num_examples=50000
I0215 23:50:19.188239 139807703402240 logging_writer.py:48] [151000] global_step=151000, grad_norm=0.625629186630249, loss=2.6684110164642334
I0215 23:53:06.663845 139807644673792 logging_writer.py:48] [151500] global_step=151500, grad_norm=0.6264185309410095, loss=2.6208837032318115
I0215 23:55:54.179174 139807703402240 logging_writer.py:48] [152000] global_step=152000, grad_norm=0.574744701385498, loss=2.5837883949279785
I0215 23:58:17.263273 139970484569920 spec.py:321] Evaluating on the training split.
I0215 23:58:23.506883 139970484569920 spec.py:333] Evaluating on the validation split.
I0215 23:58:31.794166 139970484569920 spec.py:349] Evaluating on the test split.
I0215 23:58:34.122769 139970484569920 submission_runner.py:408] Time since start: 52947.94s, 	Step: 152429, 	{'train/accuracy': 0.9149593114852905, 'train/loss': 0.562910258769989, 'validation/accuracy': 0.7675999999046326, 'validation/loss': 1.152549147605896, 'validation/num_examples': 50000, 'test/accuracy': 0.6487000584602356, 'test/loss': 1.7565561532974243, 'test/num_examples': 10000, 'score': 51051.64455986023, 'total_duration': 52947.93546438217, 'accumulated_submission_time': 51051.64455986023, 'accumulated_eval_time': 1887.541310787201, 'accumulated_logging_time': 3.707859992980957}
I0215 23:58:34.152320 139803995662080 logging_writer.py:48] [152429] accumulated_eval_time=1887.541311, accumulated_logging_time=3.707860, accumulated_submission_time=51051.644560, global_step=152429, preemption_count=0, score=51051.644560, test/accuracy=0.648700, test/loss=1.756556, test/num_examples=10000, total_duration=52947.935464, train/accuracy=0.914959, train/loss=0.562910, validation/accuracy=0.767600, validation/loss=1.152549, validation/num_examples=50000
I0215 23:58:58.205992 139804004054784 logging_writer.py:48] [152500] global_step=152500, grad_norm=0.5985612273216248, loss=2.5823819637298584
I0216 00:01:45.559479 139803995662080 logging_writer.py:48] [153000] global_step=153000, grad_norm=0.6069949865341187, loss=2.5624001026153564
I0216 00:04:32.890363 139804004054784 logging_writer.py:48] [153500] global_step=153500, grad_norm=0.6100573539733887, loss=2.586726427078247
I0216 00:07:04.243891 139970484569920 spec.py:321] Evaluating on the training split.
I0216 00:07:11.125569 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 00:07:19.391891 139970484569920 spec.py:349] Evaluating on the test split.
I0216 00:07:21.704213 139970484569920 submission_runner.py:408] Time since start: 53475.52s, 	Step: 153954, 	{'train/accuracy': 0.9214763641357422, 'train/loss': 0.5323159098625183, 'validation/accuracy': 0.7672199606895447, 'validation/loss': 1.148197054862976, 'validation/num_examples': 50000, 'test/accuracy': 0.6482000350952148, 'test/loss': 1.7559324502944946, 'test/num_examples': 10000, 'score': 51561.67814707756, 'total_duration': 53475.51697063446, 'accumulated_submission_time': 51561.67814707756, 'accumulated_eval_time': 1905.0015995502472, 'accumulated_logging_time': 3.7483904361724854}
I0216 00:07:21.734723 139807644673792 logging_writer.py:48] [153954] accumulated_eval_time=1905.001600, accumulated_logging_time=3.748390, accumulated_submission_time=51561.678147, global_step=153954, preemption_count=0, score=51561.678147, test/accuracy=0.648200, test/loss=1.755932, test/num_examples=10000, total_duration=53475.516971, train/accuracy=0.921476, train/loss=0.532316, validation/accuracy=0.767220, validation/loss=1.148197, validation/num_examples=50000
I0216 00:07:37.492921 139807703402240 logging_writer.py:48] [154000] global_step=154000, grad_norm=0.6348639130592346, loss=2.6333653926849365
I0216 00:10:25.008140 139807644673792 logging_writer.py:48] [154500] global_step=154500, grad_norm=0.6150370836257935, loss=2.594963550567627
I0216 00:13:12.420415 139807703402240 logging_writer.py:48] [155000] global_step=155000, grad_norm=0.619627058506012, loss=2.5811784267425537
I0216 00:15:51.973065 139970484569920 spec.py:321] Evaluating on the training split.
I0216 00:15:58.265659 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 00:16:06.540840 139970484569920 spec.py:349] Evaluating on the test split.
I0216 00:16:08.808390 139970484569920 submission_runner.py:408] Time since start: 54002.62s, 	Step: 155478, 	{'train/accuracy': 0.918965220451355, 'train/loss': 0.5517738461494446, 'validation/accuracy': 0.7668399810791016, 'validation/loss': 1.1611448526382446, 'validation/num_examples': 50000, 'test/accuracy': 0.6460000276565552, 'test/loss': 1.7580969333648682, 'test/num_examples': 10000, 'score': 52071.85892367363, 'total_duration': 54002.62115073204, 'accumulated_submission_time': 52071.85892367363, 'accumulated_eval_time': 1921.8368973731995, 'accumulated_logging_time': 3.78936767578125}
I0216 00:16:08.837216 139807149778688 logging_writer.py:48] [155478] accumulated_eval_time=1921.836897, accumulated_logging_time=3.789368, accumulated_submission_time=52071.858924, global_step=155478, preemption_count=0, score=52071.858924, test/accuracy=0.646000, test/loss=1.758097, test/num_examples=10000, total_duration=54002.621151, train/accuracy=0.918965, train/loss=0.551774, validation/accuracy=0.766840, validation/loss=1.161145, validation/num_examples=50000
I0216 00:16:16.550234 139807158171392 logging_writer.py:48] [155500] global_step=155500, grad_norm=0.5906288027763367, loss=2.5407614707946777
I0216 00:19:03.875009 139807149778688 logging_writer.py:48] [156000] global_step=156000, grad_norm=0.6121621131896973, loss=2.5828864574432373
I0216 00:21:51.232417 139807158171392 logging_writer.py:48] [156500] global_step=156500, grad_norm=0.62464839220047, loss=2.611829996109009
I0216 00:24:38.568969 139807149778688 logging_writer.py:48] [157000] global_step=157000, grad_norm=0.6414180994033813, loss=2.5999064445495605
I0216 00:24:38.997585 139970484569920 spec.py:321] Evaluating on the training split.
I0216 00:24:45.202931 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 00:24:53.529798 139970484569920 spec.py:349] Evaluating on the test split.
I0216 00:24:55.784131 139970484569920 submission_runner.py:408] Time since start: 54529.60s, 	Step: 157003, 	{'train/accuracy': 0.9165537357330322, 'train/loss': 0.5551013350486755, 'validation/accuracy': 0.7649399638175964, 'validation/loss': 1.1580182313919067, 'validation/num_examples': 50000, 'test/accuracy': 0.6455000042915344, 'test/loss': 1.7641746997833252, 'test/num_examples': 10000, 'score': 52581.95855307579, 'total_duration': 54529.59687829018, 'accumulated_submission_time': 52581.95855307579, 'accumulated_eval_time': 1938.623378276825, 'accumulated_logging_time': 3.8317160606384277}
I0216 00:24:55.812527 139803995662080 logging_writer.py:48] [157003] accumulated_eval_time=1938.623378, accumulated_logging_time=3.831716, accumulated_submission_time=52581.958553, global_step=157003, preemption_count=0, score=52581.958553, test/accuracy=0.645500, test/loss=1.764175, test/num_examples=10000, total_duration=54529.596878, train/accuracy=0.916554, train/loss=0.555101, validation/accuracy=0.764940, validation/loss=1.158018, validation/num_examples=50000
I0216 00:27:42.543906 139804004054784 logging_writer.py:48] [157500] global_step=157500, grad_norm=0.6196207404136658, loss=2.512622833251953
I0216 00:30:30.018789 139803995662080 logging_writer.py:48] [158000] global_step=158000, grad_norm=0.6559468507766724, loss=2.5996029376983643
I0216 00:33:17.522353 139804004054784 logging_writer.py:48] [158500] global_step=158500, grad_norm=0.6422014832496643, loss=2.5712473392486572
I0216 00:33:26.005290 139970484569920 spec.py:321] Evaluating on the training split.
I0216 00:33:32.155345 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 00:33:40.418735 139970484569920 spec.py:349] Evaluating on the test split.
I0216 00:33:42.694679 139970484569920 submission_runner.py:408] Time since start: 55056.51s, 	Step: 158527, 	{'train/accuracy': 0.9178292155265808, 'train/loss': 0.5524814128875732, 'validation/accuracy': 0.7671599984169006, 'validation/loss': 1.1583116054534912, 'validation/num_examples': 50000, 'test/accuracy': 0.6462000012397766, 'test/loss': 1.7575604915618896, 'test/num_examples': 10000, 'score': 53092.09315085411, 'total_duration': 55056.50742435455, 'accumulated_submission_time': 53092.09315085411, 'accumulated_eval_time': 1955.3126981258392, 'accumulated_logging_time': 3.871208429336548}
I0216 00:33:42.725752 139803995662080 logging_writer.py:48] [158527] accumulated_eval_time=1955.312698, accumulated_logging_time=3.871208, accumulated_submission_time=53092.093151, global_step=158527, preemption_count=0, score=53092.093151, test/accuracy=0.646200, test/loss=1.757560, test/num_examples=10000, total_duration=55056.507424, train/accuracy=0.917829, train/loss=0.552481, validation/accuracy=0.767160, validation/loss=1.158312, validation/num_examples=50000
I0216 00:36:21.362137 139807149778688 logging_writer.py:48] [159000] global_step=159000, grad_norm=0.6163888573646545, loss=2.580937147140503
I0216 00:39:08.751330 139803995662080 logging_writer.py:48] [159500] global_step=159500, grad_norm=0.6312947273254395, loss=2.616661787033081
I0216 00:41:56.215682 139807149778688 logging_writer.py:48] [160000] global_step=160000, grad_norm=0.6785688400268555, loss=2.677257537841797
I0216 00:42:12.718227 139970484569920 spec.py:321] Evaluating on the training split.
I0216 00:42:19.015429 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 00:42:27.281295 139970484569920 spec.py:349] Evaluating on the test split.
I0216 00:42:29.566920 139970484569920 submission_runner.py:408] Time since start: 55583.38s, 	Step: 160051, 	{'train/accuracy': 0.9171914458274841, 'train/loss': 0.5453962683677673, 'validation/accuracy': 0.7662599682807922, 'validation/loss': 1.1476150751113892, 'validation/num_examples': 50000, 'test/accuracy': 0.6456000208854675, 'test/loss': 1.7531681060791016, 'test/num_examples': 10000, 'score': 53602.02709078789, 'total_duration': 55583.37965941429, 'accumulated_submission_time': 53602.02709078789, 'accumulated_eval_time': 1972.1613173484802, 'accumulated_logging_time': 3.913592576980591}
I0216 00:42:29.595424 139803995662080 logging_writer.py:48] [160051] accumulated_eval_time=1972.161317, accumulated_logging_time=3.913593, accumulated_submission_time=53602.027091, global_step=160051, preemption_count=0, score=53602.027091, test/accuracy=0.645600, test/loss=1.753168, test/num_examples=10000, total_duration=55583.379659, train/accuracy=0.917191, train/loss=0.545396, validation/accuracy=0.766260, validation/loss=1.147615, validation/num_examples=50000
I0216 00:45:00.317337 139804004054784 logging_writer.py:48] [160500] global_step=160500, grad_norm=0.6719957590103149, loss=2.6213603019714355
I0216 00:47:47.888458 139803995662080 logging_writer.py:48] [161000] global_step=161000, grad_norm=0.6316034197807312, loss=2.6120126247406006
I0216 00:50:35.283397 139804004054784 logging_writer.py:48] [161500] global_step=161500, grad_norm=0.6407223343849182, loss=2.5992770195007324
I0216 00:50:59.829293 139970484569920 spec.py:321] Evaluating on the training split.
I0216 00:51:06.070760 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 00:51:14.404492 139970484569920 spec.py:349] Evaluating on the test split.
I0216 00:51:16.669790 139970484569920 submission_runner.py:408] Time since start: 56110.48s, 	Step: 161575, 	{'train/accuracy': 0.918387234210968, 'train/loss': 0.554888904094696, 'validation/accuracy': 0.76555997133255, 'validation/loss': 1.158990502357483, 'validation/num_examples': 50000, 'test/accuracy': 0.6438000202178955, 'test/loss': 1.7632352113723755, 'test/num_examples': 10000, 'score': 54112.200365543365, 'total_duration': 56110.482533454895, 'accumulated_submission_time': 54112.200365543365, 'accumulated_eval_time': 1989.001749753952, 'accumulated_logging_time': 3.955530881881714}
I0216 00:51:16.702355 139807158171392 logging_writer.py:48] [161575] accumulated_eval_time=1989.001750, accumulated_logging_time=3.955531, accumulated_submission_time=54112.200366, global_step=161575, preemption_count=0, score=54112.200366, test/accuracy=0.643800, test/loss=1.763235, test/num_examples=10000, total_duration=56110.482533, train/accuracy=0.918387, train/loss=0.554889, validation/accuracy=0.765560, validation/loss=1.158991, validation/num_examples=50000
I0216 00:53:39.266287 139807644673792 logging_writer.py:48] [162000] global_step=162000, grad_norm=0.6398304104804993, loss=2.5702428817749023
I0216 00:56:26.705760 139807158171392 logging_writer.py:48] [162500] global_step=162500, grad_norm=0.6302692890167236, loss=2.5553693771362305
I0216 00:59:14.063648 139807644673792 logging_writer.py:48] [163000] global_step=163000, grad_norm=0.667407214641571, loss=2.613400936126709
I0216 00:59:46.719593 139970484569920 spec.py:321] Evaluating on the training split.
I0216 00:59:52.983529 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 01:00:01.165540 139970484569920 spec.py:349] Evaluating on the test split.
I0216 01:00:03.454983 139970484569920 submission_runner.py:408] Time since start: 56637.27s, 	Step: 163099, 	{'train/accuracy': 0.9237084984779358, 'train/loss': 0.5277928113937378, 'validation/accuracy': 0.765500009059906, 'validation/loss': 1.1601675748825073, 'validation/num_examples': 50000, 'test/accuracy': 0.6462000012397766, 'test/loss': 1.761299729347229, 'test/num_examples': 10000, 'score': 54622.15748357773, 'total_duration': 56637.26773715019, 'accumulated_submission_time': 54622.15748357773, 'accumulated_eval_time': 2005.7370827198029, 'accumulated_logging_time': 4.000424385070801}
I0216 01:00:03.483972 139807703402240 logging_writer.py:48] [163099] accumulated_eval_time=2005.737083, accumulated_logging_time=4.000424, accumulated_submission_time=54622.157484, global_step=163099, preemption_count=0, score=54622.157484, test/accuracy=0.646200, test/loss=1.761300, test/num_examples=10000, total_duration=56637.267737, train/accuracy=0.923708, train/loss=0.527793, validation/accuracy=0.765500, validation/loss=1.160168, validation/num_examples=50000
I0216 01:02:18.109418 139807711794944 logging_writer.py:48] [163500] global_step=163500, grad_norm=0.6390695571899414, loss=2.518378257751465
I0216 01:05:05.606812 139807703402240 logging_writer.py:48] [164000] global_step=164000, grad_norm=0.6437197923660278, loss=2.6029486656188965
I0216 01:07:53.099668 139807711794944 logging_writer.py:48] [164500] global_step=164500, grad_norm=0.6380800604820251, loss=2.5708212852478027
I0216 01:08:33.756364 139970484569920 spec.py:321] Evaluating on the training split.
I0216 01:08:39.971815 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 01:08:48.221605 139970484569920 spec.py:349] Evaluating on the test split.
I0216 01:08:50.493802 139970484569920 submission_runner.py:408] Time since start: 57164.31s, 	Step: 164623, 	{'train/accuracy': 0.9222536683082581, 'train/loss': 0.5271082520484924, 'validation/accuracy': 0.7665799856185913, 'validation/loss': 1.1513084173202515, 'validation/num_examples': 50000, 'test/accuracy': 0.6422000527381897, 'test/loss': 1.760626196861267, 'test/num_examples': 10000, 'score': 55132.37274169922, 'total_duration': 57164.306557655334, 'accumulated_submission_time': 55132.37274169922, 'accumulated_eval_time': 2022.4744703769684, 'accumulated_logging_time': 4.0391623973846436}
I0216 01:08:50.522211 139804004054784 logging_writer.py:48] [164623] accumulated_eval_time=2022.474470, accumulated_logging_time=4.039162, accumulated_submission_time=55132.372742, global_step=164623, preemption_count=0, score=55132.372742, test/accuracy=0.642200, test/loss=1.760626, test/num_examples=10000, total_duration=57164.306558, train/accuracy=0.922254, train/loss=0.527108, validation/accuracy=0.766580, validation/loss=1.151308, validation/num_examples=50000
I0216 01:10:57.008253 139807149778688 logging_writer.py:48] [165000] global_step=165000, grad_norm=0.6671765446662903, loss=2.574124813079834
I0216 01:13:44.316946 139804004054784 logging_writer.py:48] [165500] global_step=165500, grad_norm=0.6681689023971558, loss=2.638352394104004
I0216 01:16:31.696844 139807149778688 logging_writer.py:48] [166000] global_step=166000, grad_norm=0.67567378282547, loss=2.555407762527466
I0216 01:17:20.600363 139970484569920 spec.py:321] Evaluating on the training split.
I0216 01:17:26.855971 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 01:17:35.065760 139970484569920 spec.py:349] Evaluating on the test split.
I0216 01:17:37.771235 139970484569920 submission_runner.py:408] Time since start: 57691.58s, 	Step: 166148, 	{'train/accuracy': 0.923828125, 'train/loss': 0.535024106502533, 'validation/accuracy': 0.767799973487854, 'validation/loss': 1.164089322090149, 'validation/num_examples': 50000, 'test/accuracy': 0.6450000405311584, 'test/loss': 1.772684097290039, 'test/num_examples': 10000, 'score': 55642.393181324005, 'total_duration': 57691.58399581909, 'accumulated_submission_time': 55642.393181324005, 'accumulated_eval_time': 2039.645334482193, 'accumulated_logging_time': 4.0777857303619385}
I0216 01:17:37.799724 139807711794944 logging_writer.py:48] [166148] accumulated_eval_time=2039.645334, accumulated_logging_time=4.077786, accumulated_submission_time=55642.393181, global_step=166148, preemption_count=0, score=55642.393181, test/accuracy=0.645000, test/loss=1.772684, test/num_examples=10000, total_duration=57691.583996, train/accuracy=0.923828, train/loss=0.535024, validation/accuracy=0.767800, validation/loss=1.164089, validation/num_examples=50000
I0216 01:19:35.933266 139807720187648 logging_writer.py:48] [166500] global_step=166500, grad_norm=0.6718380451202393, loss=2.604790449142456
I0216 01:22:23.395085 139807711794944 logging_writer.py:48] [167000] global_step=167000, grad_norm=0.6566430330276489, loss=2.624751329421997
I0216 01:25:10.825609 139807720187648 logging_writer.py:48] [167500] global_step=167500, grad_norm=0.6755151152610779, loss=2.6009411811828613
I0216 01:26:07.772726 139970484569920 spec.py:321] Evaluating on the training split.
I0216 01:26:14.056616 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 01:26:22.313430 139970484569920 spec.py:349] Evaluating on the test split.
I0216 01:26:24.608867 139970484569920 submission_runner.py:408] Time since start: 58218.42s, 	Step: 167672, 	{'train/accuracy': 0.9208585619926453, 'train/loss': 0.5523911118507385, 'validation/accuracy': 0.766979992389679, 'validation/loss': 1.1723792552947998, 'validation/num_examples': 50000, 'test/accuracy': 0.6449000239372253, 'test/loss': 1.777124285697937, 'test/num_examples': 10000, 'score': 56152.30905199051, 'total_duration': 58218.42157268524, 'accumulated_submission_time': 56152.30905199051, 'accumulated_eval_time': 2056.481376647949, 'accumulated_logging_time': 4.116064071655273}
I0216 01:26:24.639287 139804004054784 logging_writer.py:48] [167672] accumulated_eval_time=2056.481377, accumulated_logging_time=4.116064, accumulated_submission_time=56152.309052, global_step=167672, preemption_count=0, score=56152.309052, test/accuracy=0.644900, test/loss=1.777124, test/num_examples=10000, total_duration=58218.421573, train/accuracy=0.920859, train/loss=0.552391, validation/accuracy=0.766980, validation/loss=1.172379, validation/num_examples=50000
I0216 01:28:14.724992 139807149778688 logging_writer.py:48] [168000] global_step=168000, grad_norm=0.6856420040130615, loss=2.6012399196624756
I0216 01:31:02.230711 139804004054784 logging_writer.py:48] [168500] global_step=168500, grad_norm=0.6669543385505676, loss=2.5701451301574707
I0216 01:33:49.683954 139807149778688 logging_writer.py:48] [169000] global_step=169000, grad_norm=0.6491947770118713, loss=2.521812915802002
I0216 01:34:54.693433 139970484569920 spec.py:321] Evaluating on the training split.
I0216 01:35:01.066555 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 01:35:09.287609 139970484569920 spec.py:349] Evaluating on the test split.
I0216 01:35:11.566109 139970484569920 submission_runner.py:408] Time since start: 58745.38s, 	Step: 169196, 	{'train/accuracy': 0.9205197691917419, 'train/loss': 0.5553680062294006, 'validation/accuracy': 0.7642799615859985, 'validation/loss': 1.1756649017333984, 'validation/num_examples': 50000, 'test/accuracy': 0.6465000510215759, 'test/loss': 1.7762415409088135, 'test/num_examples': 10000, 'score': 56662.3060798645, 'total_duration': 58745.37886977196, 'accumulated_submission_time': 56662.3060798645, 'accumulated_eval_time': 2073.3540070056915, 'accumulated_logging_time': 4.155987501144409}
I0216 01:35:11.597255 139807644673792 logging_writer.py:48] [169196] accumulated_eval_time=2073.354007, accumulated_logging_time=4.155988, accumulated_submission_time=56662.306080, global_step=169196, preemption_count=0, score=56662.306080, test/accuracy=0.646500, test/loss=1.776242, test/num_examples=10000, total_duration=58745.378870, train/accuracy=0.920520, train/loss=0.555368, validation/accuracy=0.764280, validation/loss=1.175665, validation/num_examples=50000
I0216 01:36:53.670124 139807703402240 logging_writer.py:48] [169500] global_step=169500, grad_norm=0.7033116817474365, loss=2.597888469696045
I0216 01:39:41.112964 139807644673792 logging_writer.py:48] [170000] global_step=170000, grad_norm=0.6695699691772461, loss=2.595167636871338
I0216 01:42:28.495972 139807703402240 logging_writer.py:48] [170500] global_step=170500, grad_norm=0.6674548983573914, loss=2.5307774543762207
I0216 01:43:41.735621 139970484569920 spec.py:321] Evaluating on the training split.
I0216 01:43:47.957682 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 01:43:56.187120 139970484569920 spec.py:349] Evaluating on the test split.
I0216 01:43:58.461172 139970484569920 submission_runner.py:408] Time since start: 59272.27s, 	Step: 170720, 	{'train/accuracy': 0.9301259517669678, 'train/loss': 0.49675142765045166, 'validation/accuracy': 0.7644400000572205, 'validation/loss': 1.155943751335144, 'validation/num_examples': 50000, 'test/accuracy': 0.6454000473022461, 'test/loss': 1.7625746726989746, 'test/num_examples': 10000, 'score': 57172.38616704941, 'total_duration': 59272.27391242981, 'accumulated_submission_time': 57172.38616704941, 'accumulated_eval_time': 2090.0794911384583, 'accumulated_logging_time': 4.198121786117554}
I0216 01:43:58.492202 139803987269376 logging_writer.py:48] [170720] accumulated_eval_time=2090.079491, accumulated_logging_time=4.198122, accumulated_submission_time=57172.386167, global_step=170720, preemption_count=0, score=57172.386167, test/accuracy=0.645400, test/loss=1.762575, test/num_examples=10000, total_duration=59272.273912, train/accuracy=0.930126, train/loss=0.496751, validation/accuracy=0.764440, validation/loss=1.155944, validation/num_examples=50000
I0216 01:45:32.486268 139803995662080 logging_writer.py:48] [171000] global_step=171000, grad_norm=0.6784460544586182, loss=2.642815589904785
I0216 01:48:19.813119 139803987269376 logging_writer.py:48] [171500] global_step=171500, grad_norm=0.6733757853507996, loss=2.5324478149414062
I0216 01:51:07.147320 139803995662080 logging_writer.py:48] [172000] global_step=172000, grad_norm=0.7148277163505554, loss=2.576789617538452
I0216 01:52:28.594516 139970484569920 spec.py:321] Evaluating on the training split.
I0216 01:52:34.869944 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 01:52:43.088316 139970484569920 spec.py:349] Evaluating on the test split.
I0216 01:52:45.381672 139970484569920 submission_runner.py:408] Time since start: 59799.19s, 	Step: 172245, 	{'train/accuracy': 0.9285714030265808, 'train/loss': 0.5104222893714905, 'validation/accuracy': 0.7637400031089783, 'validation/loss': 1.159540057182312, 'validation/num_examples': 50000, 'test/accuracy': 0.6429000496864319, 'test/loss': 1.7697840929031372, 'test/num_examples': 10000, 'score': 57682.431550741196, 'total_duration': 59799.19443702698, 'accumulated_submission_time': 57682.431550741196, 'accumulated_eval_time': 2106.8666026592255, 'accumulated_logging_time': 4.238800764083862}
I0216 01:52:45.410917 139803995662080 logging_writer.py:48] [172245] accumulated_eval_time=2106.866603, accumulated_logging_time=4.238801, accumulated_submission_time=57682.431551, global_step=172245, preemption_count=0, score=57682.431551, test/accuracy=0.642900, test/loss=1.769784, test/num_examples=10000, total_duration=59799.194437, train/accuracy=0.928571, train/loss=0.510422, validation/accuracy=0.763740, validation/loss=1.159540, validation/num_examples=50000
I0216 01:54:11.198771 139804004054784 logging_writer.py:48] [172500] global_step=172500, grad_norm=0.6652176976203918, loss=2.555797576904297
I0216 01:56:58.604028 139803995662080 logging_writer.py:48] [173000] global_step=173000, grad_norm=0.6978990435600281, loss=2.6069676876068115
I0216 01:59:46.062469 139804004054784 logging_writer.py:48] [173500] global_step=173500, grad_norm=0.7110866904258728, loss=2.6171817779541016
I0216 02:01:15.565268 139970484569920 spec.py:321] Evaluating on the training split.
I0216 02:01:21.838127 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 02:01:30.264305 139970484569920 spec.py:349] Evaluating on the test split.
I0216 02:01:32.510062 139970484569920 submission_runner.py:408] Time since start: 60326.32s, 	Step: 173769, 	{'train/accuracy': 0.9262595176696777, 'train/loss': 0.5297057628631592, 'validation/accuracy': 0.7644000053405762, 'validation/loss': 1.170536756515503, 'validation/num_examples': 50000, 'test/accuracy': 0.6426000595092773, 'test/loss': 1.7723007202148438, 'test/num_examples': 10000, 'score': 58192.52931523323, 'total_duration': 60326.32282280922, 'accumulated_submission_time': 58192.52931523323, 'accumulated_eval_time': 2123.8113532066345, 'accumulated_logging_time': 4.277429819107056}
I0216 02:01:32.539881 139807149778688 logging_writer.py:48] [173769] accumulated_eval_time=2123.811353, accumulated_logging_time=4.277430, accumulated_submission_time=58192.529315, global_step=173769, preemption_count=0, score=58192.529315, test/accuracy=0.642600, test/loss=1.772301, test/num_examples=10000, total_duration=60326.322823, train/accuracy=0.926260, train/loss=0.529706, validation/accuracy=0.764400, validation/loss=1.170537, validation/num_examples=50000
I0216 02:02:50.167409 139807158171392 logging_writer.py:48] [174000] global_step=174000, grad_norm=0.7071146368980408, loss=2.5871036052703857
I0216 02:05:37.431452 139807149778688 logging_writer.py:48] [174500] global_step=174500, grad_norm=0.6759148836135864, loss=2.54665470123291
I0216 02:08:24.842070 139807158171392 logging_writer.py:48] [175000] global_step=175000, grad_norm=0.7136572003364563, loss=2.5822601318359375
I0216 02:10:02.932397 139970484569920 spec.py:321] Evaluating on the training split.
I0216 02:10:10.535288 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 02:10:18.886768 139970484569920 spec.py:349] Evaluating on the test split.
I0216 02:10:21.172144 139970484569920 submission_runner.py:408] Time since start: 60854.98s, 	Step: 175267, 	{'train/accuracy': 0.9266581535339355, 'train/loss': 0.5227275490760803, 'validation/accuracy': 0.7650600075721741, 'validation/loss': 1.1675528287887573, 'validation/num_examples': 50000, 'test/accuracy': 0.6401000022888184, 'test/loss': 1.7792713642120361, 'test/num_examples': 10000, 'score': 58702.86290168762, 'total_duration': 60854.98487496376, 'accumulated_submission_time': 58702.86290168762, 'accumulated_eval_time': 2142.0510466098785, 'accumulated_logging_time': 4.31864070892334}
I0216 02:10:21.200407 139807644673792 logging_writer.py:48] [175267] accumulated_eval_time=2142.051047, accumulated_logging_time=4.318641, accumulated_submission_time=58702.862902, global_step=175267, preemption_count=0, score=58702.862902, test/accuracy=0.640100, test/loss=1.779271, test/num_examples=10000, total_duration=60854.984875, train/accuracy=0.926658, train/loss=0.522728, validation/accuracy=0.765060, validation/loss=1.167553, validation/num_examples=50000
I0216 02:11:40.454314 139807703402240 logging_writer.py:48] [175500] global_step=175500, grad_norm=0.7133869528770447, loss=2.5831832885742188
I0216 02:14:28.018773 139807644673792 logging_writer.py:48] [176000] global_step=176000, grad_norm=0.6905618906021118, loss=2.5793259143829346
I0216 02:17:15.468479 139807703402240 logging_writer.py:48] [176500] global_step=176500, grad_norm=0.6843637228012085, loss=2.5577540397644043
I0216 02:18:51.484870 139970484569920 spec.py:321] Evaluating on the training split.
I0216 02:18:58.361620 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 02:19:06.965067 139970484569920 spec.py:349] Evaluating on the test split.
I0216 02:19:09.255451 139970484569920 submission_runner.py:408] Time since start: 61383.07s, 	Step: 176788, 	{'train/accuracy': 0.9225127100944519, 'train/loss': 0.5303520560264587, 'validation/accuracy': 0.7651399970054626, 'validation/loss': 1.1630430221557617, 'validation/num_examples': 50000, 'test/accuracy': 0.6421000361442566, 'test/loss': 1.777036190032959, 'test/num_examples': 10000, 'score': 59212.194242239, 'total_duration': 61383.06815433502, 'accumulated_submission_time': 59212.194242239, 'accumulated_eval_time': 2159.8215250968933, 'accumulated_logging_time': 5.252411365509033}
I0216 02:19:09.285448 139804004054784 logging_writer.py:48] [176788] accumulated_eval_time=2159.821525, accumulated_logging_time=5.252411, accumulated_submission_time=59212.194242, global_step=176788, preemption_count=0, score=59212.194242, test/accuracy=0.642100, test/loss=1.777036, test/num_examples=10000, total_duration=61383.068154, train/accuracy=0.922513, train/loss=0.530352, validation/accuracy=0.765140, validation/loss=1.163043, validation/num_examples=50000
I0216 02:20:20.572051 139807149778688 logging_writer.py:48] [177000] global_step=177000, grad_norm=0.7036139965057373, loss=2.516206741333008
I0216 02:23:07.974909 139804004054784 logging_writer.py:48] [177500] global_step=177500, grad_norm=0.7379900813102722, loss=2.5995359420776367
I0216 02:25:55.367390 139807149778688 logging_writer.py:48] [178000] global_step=178000, grad_norm=0.7151333093643188, loss=2.603783130645752
I0216 02:27:39.279262 139970484569920 spec.py:321] Evaluating on the training split.
I0216 02:27:46.118510 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 02:27:54.392390 139970484569920 spec.py:349] Evaluating on the test split.
I0216 02:27:56.830171 139970484569920 submission_runner.py:408] Time since start: 61910.64s, 	Step: 178312, 	{'train/accuracy': 0.9236288070678711, 'train/loss': 0.5388383865356445, 'validation/accuracy': 0.7633799910545349, 'validation/loss': 1.18033766746521, 'validation/num_examples': 50000, 'test/accuracy': 0.6434000134468079, 'test/loss': 1.7938323020935059, 'test/num_examples': 10000, 'score': 59722.13116836548, 'total_duration': 61910.64293670654, 'accumulated_submission_time': 59722.13116836548, 'accumulated_eval_time': 2177.372397184372, 'accumulated_logging_time': 5.292085886001587}
I0216 02:27:56.860620 139803987269376 logging_writer.py:48] [178312] accumulated_eval_time=2177.372397, accumulated_logging_time=5.292086, accumulated_submission_time=59722.131168, global_step=178312, preemption_count=0, score=59722.131168, test/accuracy=0.643400, test/loss=1.793832, test/num_examples=10000, total_duration=61910.642937, train/accuracy=0.923629, train/loss=0.538838, validation/accuracy=0.763380, validation/loss=1.180338, validation/num_examples=50000
I0216 02:29:00.157410 139803995662080 logging_writer.py:48] [178500] global_step=178500, grad_norm=0.732068657875061, loss=2.6130075454711914
I0216 02:31:47.708300 139803987269376 logging_writer.py:48] [179000] global_step=179000, grad_norm=0.6990178823471069, loss=2.5670506954193115
I0216 02:34:35.333245 139803995662080 logging_writer.py:48] [179500] global_step=179500, grad_norm=0.6736969351768494, loss=2.4988927841186523
I0216 02:36:26.915414 139970484569920 spec.py:321] Evaluating on the training split.
I0216 02:36:33.592185 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 02:36:42.026790 139970484569920 spec.py:349] Evaluating on the test split.
I0216 02:36:44.362653 139970484569920 submission_runner.py:408] Time since start: 62438.18s, 	Step: 179835, 	{'train/accuracy': 0.9350884556770325, 'train/loss': 0.4926590323448181, 'validation/accuracy': 0.7642399668693542, 'validation/loss': 1.173576831817627, 'validation/num_examples': 50000, 'test/accuracy': 0.642300009727478, 'test/loss': 1.782651424407959, 'test/num_examples': 10000, 'score': 60232.12835073471, 'total_duration': 62438.17541408539, 'accumulated_submission_time': 60232.12835073471, 'accumulated_eval_time': 2194.8195939064026, 'accumulated_logging_time': 5.332860708236694}
I0216 02:36:44.395007 139803987269376 logging_writer.py:48] [179835] accumulated_eval_time=2194.819594, accumulated_logging_time=5.332861, accumulated_submission_time=60232.128351, global_step=179835, preemption_count=0, score=60232.128351, test/accuracy=0.642300, test/loss=1.782651, test/num_examples=10000, total_duration=62438.175414, train/accuracy=0.935088, train/loss=0.492659, validation/accuracy=0.764240, validation/loss=1.173577, validation/num_examples=50000
I0216 02:37:39.904225 139803995662080 logging_writer.py:48] [180000] global_step=180000, grad_norm=0.6762701869010925, loss=2.5242955684661865
I0216 02:40:27.252886 139803987269376 logging_writer.py:48] [180500] global_step=180500, grad_norm=0.6954881548881531, loss=2.505617380142212
I0216 02:43:14.723229 139803995662080 logging_writer.py:48] [181000] global_step=181000, grad_norm=0.7117872834205627, loss=2.561429023742676
I0216 02:45:14.636048 139970484569920 spec.py:321] Evaluating on the training split.
I0216 02:45:20.944969 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 02:45:29.632223 139970484569920 spec.py:349] Evaluating on the test split.
I0216 02:45:31.916823 139970484569920 submission_runner.py:408] Time since start: 62965.73s, 	Step: 181360, 	{'train/accuracy': 0.9288902878761292, 'train/loss': 0.5021556615829468, 'validation/accuracy': 0.7616599798202515, 'validation/loss': 1.1682405471801758, 'validation/num_examples': 50000, 'test/accuracy': 0.6452000141143799, 'test/loss': 1.7737975120544434, 'test/num_examples': 10000, 'score': 60742.311823129654, 'total_duration': 62965.72957491875, 'accumulated_submission_time': 60742.311823129654, 'accumulated_eval_time': 2212.100319623947, 'accumulated_logging_time': 5.375080347061157}
I0216 02:45:31.946861 139803987269376 logging_writer.py:48] [181360] accumulated_eval_time=2212.100320, accumulated_logging_time=5.375080, accumulated_submission_time=60742.311823, global_step=181360, preemption_count=0, score=60742.311823, test/accuracy=0.645200, test/loss=1.773798, test/num_examples=10000, total_duration=62965.729575, train/accuracy=0.928890, train/loss=0.502156, validation/accuracy=0.761660, validation/loss=1.168241, validation/num_examples=50000
I0216 02:46:19.109660 139803995662080 logging_writer.py:48] [181500] global_step=181500, grad_norm=0.725456178188324, loss=2.5674285888671875
I0216 02:49:06.756417 139803987269376 logging_writer.py:48] [182000] global_step=182000, grad_norm=0.712369441986084, loss=2.5496575832366943
I0216 02:51:54.172310 139803995662080 logging_writer.py:48] [182500] global_step=182500, grad_norm=0.712476372718811, loss=2.538569450378418
I0216 02:54:02.153760 139970484569920 spec.py:321] Evaluating on the training split.
I0216 02:54:08.364084 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 02:54:17.086889 139970484569920 spec.py:349] Evaluating on the test split.
I0216 02:54:19.367164 139970484569920 submission_runner.py:408] Time since start: 63493.18s, 	Step: 182884, 	{'train/accuracy': 0.9288105964660645, 'train/loss': 0.5267924666404724, 'validation/accuracy': 0.7643799781799316, 'validation/loss': 1.1856269836425781, 'validation/num_examples': 50000, 'test/accuracy': 0.6434000134468079, 'test/loss': 1.8020799160003662, 'test/num_examples': 10000, 'score': 61252.4593539238, 'total_duration': 63493.179923295975, 'accumulated_submission_time': 61252.4593539238, 'accumulated_eval_time': 2229.313698530197, 'accumulated_logging_time': 5.416741371154785}
I0216 02:54:19.397207 139803987269376 logging_writer.py:48] [182884] accumulated_eval_time=2229.313699, accumulated_logging_time=5.416741, accumulated_submission_time=61252.459354, global_step=182884, preemption_count=0, score=61252.459354, test/accuracy=0.643400, test/loss=1.802080, test/num_examples=10000, total_duration=63493.179923, train/accuracy=0.928811, train/loss=0.526792, validation/accuracy=0.764380, validation/loss=1.185627, validation/num_examples=50000
I0216 02:54:58.506945 139807149778688 logging_writer.py:48] [183000] global_step=183000, grad_norm=0.7286673784255981, loss=2.5747599601745605
I0216 02:57:45.896659 139803987269376 logging_writer.py:48] [183500] global_step=183500, grad_norm=0.7571953535079956, loss=2.596525192260742
I0216 03:00:33.239879 139807149778688 logging_writer.py:48] [184000] global_step=184000, grad_norm=0.7138574123382568, loss=2.5688395500183105
I0216 03:02:49.509998 139970484569920 spec.py:321] Evaluating on the training split.
I0216 03:02:55.777006 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 03:03:04.313878 139970484569920 spec.py:349] Evaluating on the test split.
I0216 03:03:06.608180 139970484569920 submission_runner.py:408] Time since start: 64020.42s, 	Step: 184409, 	{'train/accuracy': 0.9275749325752258, 'train/loss': 0.5183637738227844, 'validation/accuracy': 0.7660399675369263, 'validation/loss': 1.1716256141662598, 'validation/num_examples': 50000, 'test/accuracy': 0.6447000503540039, 'test/loss': 1.7825127840042114, 'test/num_examples': 10000, 'score': 61762.51425027847, 'total_duration': 64020.42093586922, 'accumulated_submission_time': 61762.51425027847, 'accumulated_eval_time': 2246.4118337631226, 'accumulated_logging_time': 5.4571852684021}
I0216 03:03:06.642872 139803987269376 logging_writer.py:48] [184409] accumulated_eval_time=2246.411834, accumulated_logging_time=5.457185, accumulated_submission_time=61762.514250, global_step=184409, preemption_count=0, score=61762.514250, test/accuracy=0.644700, test/loss=1.782513, test/num_examples=10000, total_duration=64020.420936, train/accuracy=0.927575, train/loss=0.518364, validation/accuracy=0.766040, validation/loss=1.171626, validation/num_examples=50000
I0216 03:03:37.417789 139803995662080 logging_writer.py:48] [184500] global_step=184500, grad_norm=0.6892725229263306, loss=2.513916492462158
I0216 03:06:24.838189 139803987269376 logging_writer.py:48] [185000] global_step=185000, grad_norm=0.7164985537528992, loss=2.524610996246338
I0216 03:09:12.252977 139803995662080 logging_writer.py:48] [185500] global_step=185500, grad_norm=0.7156591415405273, loss=2.540325403213501
I0216 03:11:36.656097 139970484569920 spec.py:321] Evaluating on the training split.
I0216 03:11:42.988409 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 03:11:51.533727 139970484569920 spec.py:349] Evaluating on the test split.
I0216 03:11:53.810765 139970484569920 submission_runner.py:408] Time since start: 64547.62s, 	Step: 185933, 	{'train/accuracy': 0.9296077489852905, 'train/loss': 0.5146064162254333, 'validation/accuracy': 0.76419997215271, 'validation/loss': 1.1747709512710571, 'validation/num_examples': 50000, 'test/accuracy': 0.6410000324249268, 'test/loss': 1.7878018617630005, 'test/num_examples': 10000, 'score': 62272.46965265274, 'total_duration': 64547.62351298332, 'accumulated_submission_time': 62272.46965265274, 'accumulated_eval_time': 2263.56644654274, 'accumulated_logging_time': 5.502199172973633}
I0216 03:11:53.842585 139803987269376 logging_writer.py:48] [185933] accumulated_eval_time=2263.566447, accumulated_logging_time=5.502199, accumulated_submission_time=62272.469653, global_step=185933, preemption_count=0, score=62272.469653, test/accuracy=0.641000, test/loss=1.787802, test/num_examples=10000, total_duration=64547.623513, train/accuracy=0.929608, train/loss=0.514606, validation/accuracy=0.764200, validation/loss=1.174771, validation/num_examples=50000
I0216 03:12:16.536925 139803995662080 logging_writer.py:48] [186000] global_step=186000, grad_norm=0.7304546236991882, loss=2.5408082008361816
I0216 03:15:03.834093 139803987269376 logging_writer.py:48] [186500] global_step=186500, grad_norm=0.7518646717071533, loss=2.5538058280944824
I0216 03:15:58.858795 139970484569920 spec.py:321] Evaluating on the training split.
I0216 03:16:05.102717 139970484569920 spec.py:333] Evaluating on the validation split.
I0216 03:16:13.607989 139970484569920 spec.py:349] Evaluating on the test split.
I0216 03:16:15.919645 139970484569920 submission_runner.py:408] Time since start: 64809.73s, 	Step: 186666, 	{'train/accuracy': 0.9305046200752258, 'train/loss': 0.5085150599479675, 'validation/accuracy': 0.7631399631500244, 'validation/loss': 1.1819463968276978, 'validation/num_examples': 50000, 'test/accuracy': 0.641700029373169, 'test/loss': 1.794360637664795, 'test/num_examples': 10000, 'score': 62517.452444791794, 'total_duration': 64809.73242044449, 'accumulated_submission_time': 62517.452444791794, 'accumulated_eval_time': 2280.6272785663605, 'accumulated_logging_time': 5.544475078582764}
I0216 03:16:15.948756 139807149778688 logging_writer.py:48] [186666] accumulated_eval_time=2280.627279, accumulated_logging_time=5.544475, accumulated_submission_time=62517.452445, global_step=186666, preemption_count=0, score=62517.452445, test/accuracy=0.641700, test/loss=1.794361, test/num_examples=10000, total_duration=64809.732420, train/accuracy=0.930505, train/loss=0.508515, validation/accuracy=0.763140, validation/loss=1.181946, validation/num_examples=50000
I0216 03:16:15.973104 139807158171392 logging_writer.py:48] [186666] global_step=186666, preemption_count=0, score=62517.452445
I0216 03:16:16.166245 139970484569920 checkpoints.py:490] Saving checkpoint at step: 186666
I0216 03:16:17.009670 139970484569920 checkpoints.py:422] Saved checkpoint at /experiment_runs/variants_target_setting/study_0/imagenet_resnet_large_bn_init_jax/trial_1/checkpoint_186666
I0216 03:16:17.028880 139970484569920 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/variants_target_setting/study_0/imagenet_resnet_large_bn_init_jax/trial_1/checkpoint_186666.
I0216 03:16:17.748450 139970484569920 submission_runner.py:583] Tuning trial 1/1
I0216 03:16:17.749693 139970484569920 submission_runner.py:584] Hyperparameters: Hyperparameters(learning_rate=4.131896390902391, beta1=0.9274758113254791, warmup_steps=6999, decay_steps_factor=0.9007765761611038, end_factor=0.01, weight_decay=5.6687777311501786e-06, label_smoothing=0.2)
I0216 03:16:17.754522 139970484569920 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0007971938466653228, 'train/loss': 590.2318115234375, 'validation/accuracy': 0.0009999999310821295, 'validation/loss': 596.8076782226562, 'validation/num_examples': 50000, 'test/accuracy': 0.0010000000474974513, 'test/loss': 587.8714599609375, 'test/num_examples': 10000, 'score': 43.70478010177612, 'total_duration': 80.3514039516449, 'accumulated_submission_time': 43.70478010177612, 'accumulated_eval_time': 36.646462202072144, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1524, {'train/accuracy': 0.009805484674870968, 'train/loss': 6.519259452819824, 'validation/accuracy': 0.010160000063478947, 'validation/loss': 6.551246166229248, 'validation/num_examples': 50000, 'test/accuracy': 0.00800000037997961, 'test/loss': 6.623855113983154, 'test/num_examples': 10000, 'score': 553.7564685344696, 'total_duration': 608.1484117507935, 'accumulated_submission_time': 553.7564685344696, 'accumulated_eval_time': 54.31677436828613, 'accumulated_logging_time': 0.02762579917907715, 'global_step': 1524, 'preemption_count': 0}), (3048, {'train/accuracy': 0.07106982916593552, 'train/loss': 5.383684158325195, 'validation/accuracy': 0.0667399987578392, 'validation/loss': 5.417434215545654, 'validation/num_examples': 50000, 'test/accuracy': 0.048500001430511475, 'test/loss': 5.687870025634766, 'test/num_examples': 10000, 'score': 1063.929992198944, 'total_duration': 1136.1952157020569, 'accumulated_submission_time': 1063.929992198944, 'accumulated_eval_time': 72.11336135864258, 'accumulated_logging_time': 0.05489993095397949, 'global_step': 3048, 'preemption_count': 0}), (4571, {'train/accuracy': 0.19214364886283875, 'train/loss': 4.239551544189453, 'validation/accuracy': 0.1824599951505661, 'validation/loss': 4.3354997634887695, 'validation/num_examples': 50000, 'test/accuracy': 0.12870000302791595, 'test/loss': 4.781617641448975, 'test/num_examples': 10000, 'score': 1573.874829530716, 'total_duration': 1664.1880810260773, 'accumulated_submission_time': 1573.874829530716, 'accumulated_eval_time': 90.0815896987915, 'accumulated_logging_time': 0.08605551719665527, 'global_step': 4571, 'preemption_count': 0}), (6092, {'train/accuracy': 0.3301379084587097, 'train/loss': 3.4222092628479004, 'validation/accuracy': 0.30640000104904175, 'validation/loss': 3.514439344406128, 'validation/num_examples': 50000, 'test/accuracy': 0.23600001633167267, 'test/loss': 4.117316246032715, 'test/num_examples': 10000, 'score': 2083.8879685401917, 'total_duration': 2191.996179819107, 'accumulated_submission_time': 2083.8879685401917, 'accumulated_eval_time': 107.7998673915863, 'accumulated_logging_time': 0.11339092254638672, 'global_step': 6092, 'preemption_count': 0}), (7614, {'train/accuracy': 0.37272799015045166, 'train/loss': 3.1425509452819824, 'validation/accuracy': 0.3504999876022339, 'validation/loss': 3.261110544204712, 'validation/num_examples': 50000, 'test/accuracy': 0.26740002632141113, 'test/loss': 3.8572018146514893, 'test/num_examples': 10000, 'score': 2593.8787322044373, 'total_duration': 2719.911128759384, 'accumulated_submission_time': 2593.8787322044373, 'accumulated_eval_time': 125.64854097366333, 'accumulated_logging_time': 0.139801025390625, 'global_step': 7614, 'preemption_count': 0}), (9136, {'train/accuracy': 0.4763631820678711, 'train/loss': 2.512876510620117, 'validation/accuracy': 0.4087199866771698, 'validation/loss': 2.858280658721924, 'validation/num_examples': 50000, 'test/accuracy': 0.3043000102043152, 'test/loss': 3.535163164138794, 'test/num_examples': 10000, 'score': 3103.9691920280457, 'total_duration': 3247.9983870983124, 'accumulated_submission_time': 3103.9691920280457, 'accumulated_eval_time': 143.56626749038696, 'accumulated_logging_time': 0.16625690460205078, 'global_step': 9136, 'preemption_count': 0}), (10659, {'train/accuracy': 0.49892377853393555, 'train/loss': 2.4701223373413086, 'validation/accuracy': 0.4554999768733978, 'validation/loss': 2.689612627029419, 'validation/num_examples': 50000, 'test/accuracy': 0.34870001673698425, 'test/loss': 3.3380837440490723, 'test/num_examples': 10000, 'score': 3614.1681060791016, 'total_duration': 3776.3486366271973, 'accumulated_submission_time': 3614.1681060791016, 'accumulated_eval_time': 161.63976645469666, 'accumulated_logging_time': 0.19428277015686035, 'global_step': 10659, 'preemption_count': 0}), (12181, {'train/accuracy': 0.5123764276504517, 'train/loss': 2.372680902481079, 'validation/accuracy': 0.4696599841117859, 'validation/loss': 2.5708394050598145, 'validation/num_examples': 50000, 'test/accuracy': 0.3646000027656555, 'test/loss': 3.2151970863342285, 'test/num_examples': 10000, 'score': 4124.095969200134, 'total_duration': 4304.604807376862, 'accumulated_submission_time': 4124.095969200134, 'accumulated_eval_time': 179.88380932807922, 'accumulated_logging_time': 0.22716879844665527, 'global_step': 12181, 'preemption_count': 0}), (13704, {'train/accuracy': 0.5216039419174194, 'train/loss': 2.334874391555786, 'validation/accuracy': 0.4868199825286865, 'validation/loss': 2.5068442821502686, 'validation/num_examples': 50000, 'test/accuracy': 0.37060001492500305, 'test/loss': 3.2004566192626953, 'test/num_examples': 10000, 'score': 4634.026791334152, 'total_duration': 4836.92283821106, 'accumulated_submission_time': 4634.026791334152, 'accumulated_eval_time': 202.18920636177063, 'accumulated_logging_time': 0.2580592632293701, 'global_step': 13704, 'preemption_count': 0}), (15227, {'train/accuracy': 0.5216039419174194, 'train/loss': 2.3197691440582275, 'validation/accuracy': 0.48603999614715576, 'validation/loss': 2.4832115173339844, 'validation/num_examples': 50000, 'test/accuracy': 0.3760000169277191, 'test/loss': 3.140584945678711, 'test/num_examples': 10000, 'score': 5144.163766860962, 'total_duration': 5371.388475894928, 'accumulated_submission_time': 5144.163766860962, 'accumulated_eval_time': 226.44001698493958, 'accumulated_logging_time': 0.2855396270751953, 'global_step': 15227, 'preemption_count': 0}), (16750, {'train/accuracy': 0.5450813174247742, 'train/loss': 2.1684956550598145, 'validation/accuracy': 0.5137400031089783, 'validation/loss': 2.3319098949432373, 'validation/num_examples': 50000, 'test/accuracy': 0.3976000249385834, 'test/loss': 2.9867873191833496, 'test/num_examples': 10000, 'score': 5654.372869491577, 'total_duration': 5905.088617563248, 'accumulated_submission_time': 5654.372869491577, 'accumulated_eval_time': 249.84391474723816, 'accumulated_logging_time': 0.32094502449035645, 'global_step': 16750, 'preemption_count': 0}), (18273, {'train/accuracy': 0.5945870280265808, 'train/loss': 2.0021681785583496, 'validation/accuracy': 0.5326600074768066, 'validation/loss': 2.2867698669433594, 'validation/num_examples': 50000, 'test/accuracy': 0.41120001673698425, 'test/loss': 2.9663474559783936, 'test/num_examples': 10000, 'score': 6164.574551582336, 'total_duration': 6439.581478834152, 'accumulated_submission_time': 6164.574551582336, 'accumulated_eval_time': 274.04896569252014, 'accumulated_logging_time': 0.35637998580932617, 'global_step': 18273, 'preemption_count': 0}), (19797, {'train/accuracy': 0.5412148833274841, 'train/loss': 2.2569923400878906, 'validation/accuracy': 0.4917199909687042, 'validation/loss': 2.4967575073242188, 'validation/num_examples': 50000, 'test/accuracy': 0.3806000053882599, 'test/loss': 3.1496195793151855, 'test/num_examples': 10000, 'score': 6674.7448387146, 'total_duration': 6974.072465419769, 'accumulated_submission_time': 6674.7448387146, 'accumulated_eval_time': 298.28218626976013, 'accumulated_logging_time': 0.38889336585998535, 'global_step': 19797, 'preemption_count': 0}), (21320, {'train/accuracy': 0.5771085619926453, 'train/loss': 2.0007622241973877, 'validation/accuracy': 0.5321399569511414, 'validation/loss': 2.222144603729248, 'validation/num_examples': 50000, 'test/accuracy': 0.4082000255584717, 'test/loss': 2.923381805419922, 'test/num_examples': 10000, 'score': 7184.691862821579, 'total_duration': 7507.466633558273, 'accumulated_submission_time': 7184.691862821579, 'accumulated_eval_time': 321.64855337142944, 'accumulated_logging_time': 0.41835951805114746, 'global_step': 21320, 'preemption_count': 0}), (22843, {'train/accuracy': 0.5935506820678711, 'train/loss': 1.934483289718628, 'validation/accuracy': 0.5549799799919128, 'validation/loss': 2.129647970199585, 'validation/num_examples': 50000, 'test/accuracy': 0.4279000163078308, 'test/loss': 2.785773277282715, 'test/num_examples': 10000, 'score': 7694.737969875336, 'total_duration': 8041.35707783699, 'accumulated_submission_time': 7694.737969875336, 'accumulated_eval_time': 345.41608119010925, 'accumulated_logging_time': 0.4467165470123291, 'global_step': 22843, 'preemption_count': 0}), (24365, {'train/accuracy': 0.5981743931770325, 'train/loss': 1.9284611940383911, 'validation/accuracy': 0.553659975528717, 'validation/loss': 2.1154723167419434, 'validation/num_examples': 50000, 'test/accuracy': 0.42990002036094666, 'test/loss': 2.796659231185913, 'test/num_examples': 10000, 'score': 8204.703876972198, 'total_duration': 8575.448627948761, 'accumulated_submission_time': 8204.703876972198, 'accumulated_eval_time': 369.45977759361267, 'accumulated_logging_time': 0.4787428379058838, 'global_step': 24365, 'preemption_count': 0}), (25889, {'train/accuracy': 0.6305803656578064, 'train/loss': 1.8610056638717651, 'validation/accuracy': 0.5537199974060059, 'validation/loss': 2.2009079456329346, 'validation/num_examples': 50000, 'test/accuracy': 0.4328000247478485, 'test/loss': 2.8634281158447266, 'test/num_examples': 10000, 'score': 8714.832880973816, 'total_duration': 9109.288538694382, 'accumulated_submission_time': 8714.832880973816, 'accumulated_eval_time': 393.09272933006287, 'accumulated_logging_time': 0.5058033466339111, 'global_step': 25889, 'preemption_count': 0}), (27412, {'train/accuracy': 0.5844627022743225, 'train/loss': 2.019122838973999, 'validation/accuracy': 0.5302799940109253, 'validation/loss': 2.2804887294769287, 'validation/num_examples': 50000, 'test/accuracy': 0.4180000126361847, 'test/loss': 2.9541094303131104, 'test/num_examples': 10000, 'score': 9224.797441482544, 'total_duration': 9642.80155491829, 'accumulated_submission_time': 9224.797441482544, 'accumulated_eval_time': 416.5653192996979, 'accumulated_logging_time': 0.5326685905456543, 'global_step': 27412, 'preemption_count': 0}), (28936, {'train/accuracy': 0.6235451102256775, 'train/loss': 1.7438464164733887, 'validation/accuracy': 0.5734800100326538, 'validation/loss': 1.9896472692489624, 'validation/num_examples': 50000, 'test/accuracy': 0.4466000199317932, 'test/loss': 2.6595518589019775, 'test/num_examples': 10000, 'score': 9734.749379396439, 'total_duration': 10176.603019237518, 'accumulated_submission_time': 9734.749379396439, 'accumulated_eval_time': 440.3303003311157, 'accumulated_logging_time': 0.5646851062774658, 'global_step': 28936, 'preemption_count': 0}), (30460, {'train/accuracy': 0.6092952489852905, 'train/loss': 1.9225893020629883, 'validation/accuracy': 0.5622999668121338, 'validation/loss': 2.1229209899902344, 'validation/num_examples': 50000, 'test/accuracy': 0.44220003485679626, 'test/loss': 2.7977213859558105, 'test/num_examples': 10000, 'score': 10244.919599533081, 'total_duration': 10711.817137241364, 'accumulated_submission_time': 10244.919599533081, 'accumulated_eval_time': 465.29795002937317, 'accumulated_logging_time': 0.5910100936889648, 'global_step': 30460, 'preemption_count': 0}), (31984, {'train/accuracy': 0.6049904227256775, 'train/loss': 1.8935025930404663, 'validation/accuracy': 0.5635600090026855, 'validation/loss': 2.091768741607666, 'validation/num_examples': 50000, 'test/accuracy': 0.4473000168800354, 'test/loss': 2.7491166591644287, 'test/num_examples': 10000, 'score': 10755.159065246582, 'total_duration': 11246.242913007736, 'accumulated_submission_time': 10755.159065246582, 'accumulated_eval_time': 489.4032461643219, 'accumulated_logging_time': 0.6199793815612793, 'global_step': 31984, 'preemption_count': 0}), (33507, {'train/accuracy': 0.6124641299247742, 'train/loss': 1.8301490545272827, 'validation/accuracy': 0.5707199573516846, 'validation/loss': 2.038625955581665, 'validation/num_examples': 50000, 'test/accuracy': 0.4524000287055969, 'test/loss': 2.6779978275299072, 'test/num_examples': 10000, 'score': 11265.150213241577, 'total_duration': 11781.190009593964, 'accumulated_submission_time': 11265.150213241577, 'accumulated_eval_time': 514.2781403064728, 'accumulated_logging_time': 0.6504311561584473, 'global_step': 33507, 'preemption_count': 0}), (35031, {'train/accuracy': 0.6182039380073547, 'train/loss': 1.8635214567184448, 'validation/accuracy': 0.549239993095398, 'validation/loss': 2.212022542953491, 'validation/num_examples': 50000, 'test/accuracy': 0.4263000190258026, 'test/loss': 2.8801536560058594, 'test/num_examples': 10000, 'score': 11775.370599031448, 'total_duration': 12315.168415546417, 'accumulated_submission_time': 11775.370599031448, 'accumulated_eval_time': 537.9529922008514, 'accumulated_logging_time': 0.6818163394927979, 'global_step': 35031, 'preemption_count': 0}), (36555, {'train/accuracy': 0.5966796875, 'train/loss': 1.9315264225006104, 'validation/accuracy': 0.5424000024795532, 'validation/loss': 2.1991610527038574, 'validation/num_examples': 50000, 'test/accuracy': 0.42640000581741333, 'test/loss': 2.880263566970825, 'test/num_examples': 10000, 'score': 12285.586842775345, 'total_duration': 12856.040487766266, 'accumulated_submission_time': 12285.586842775345, 'accumulated_eval_time': 568.5313017368317, 'accumulated_logging_time': 0.7088274955749512, 'global_step': 36555, 'preemption_count': 0}), (38079, {'train/accuracy': 0.6463249325752258, 'train/loss': 1.78249990940094, 'validation/accuracy': 0.5925399661064148, 'validation/loss': 2.026228189468384, 'validation/num_examples': 50000, 'test/accuracy': 0.47200003266334534, 'test/loss': 2.6642837524414062, 'test/num_examples': 10000, 'score': 12795.643894195557, 'total_duration': 13389.049154758453, 'accumulated_submission_time': 12795.643894195557, 'accumulated_eval_time': 591.4063141345978, 'accumulated_logging_time': 0.7357571125030518, 'global_step': 38079, 'preemption_count': 0}), (39602, {'train/accuracy': 0.6285275816917419, 'train/loss': 1.8114537000656128, 'validation/accuracy': 0.5889999866485596, 'validation/loss': 2.0118296146392822, 'validation/num_examples': 50000, 'test/accuracy': 0.4617000222206116, 'test/loss': 2.6926677227020264, 'test/num_examples': 10000, 'score': 13305.627413749695, 'total_duration': 13921.54613494873, 'accumulated_submission_time': 13305.627413749695, 'accumulated_eval_time': 613.8412947654724, 'accumulated_logging_time': 0.7629227638244629, 'global_step': 39602, 'preemption_count': 0}), (41127, {'train/accuracy': 0.6400271058082581, 'train/loss': 1.7320114374160767, 'validation/accuracy': 0.5934799909591675, 'validation/loss': 1.9413087368011475, 'validation/num_examples': 50000, 'test/accuracy': 0.46800002455711365, 'test/loss': 2.597066640853882, 'test/num_examples': 10000, 'score': 13815.838862657547, 'total_duration': 14451.993034601212, 'accumulated_submission_time': 13815.838862657547, 'accumulated_eval_time': 634.0011188983917, 'accumulated_logging_time': 0.7886664867401123, 'global_step': 41127, 'preemption_count': 0}), (42650, {'train/accuracy': 0.6274314522743225, 'train/loss': 1.8249386548995972, 'validation/accuracy': 0.5841000080108643, 'validation/loss': 2.039947748184204, 'validation/num_examples': 50000, 'test/accuracy': 0.4499000310897827, 'test/loss': 2.7198336124420166, 'test/num_examples': 10000, 'score': 14325.808423280716, 'total_duration': 14983.820021390915, 'accumulated_submission_time': 14325.808423280716, 'accumulated_eval_time': 655.7796139717102, 'accumulated_logging_time': 0.815596342086792, 'global_step': 42650, 'preemption_count': 0}), (44175, {'train/accuracy': 0.6434151530265808, 'train/loss': 1.6949825286865234, 'validation/accuracy': 0.5753600001335144, 'validation/loss': 2.0091612339019775, 'validation/num_examples': 50000, 'test/accuracy': 0.4572000205516815, 'test/loss': 2.660670042037964, 'test/num_examples': 10000, 'score': 14836.049505472183, 'total_duration': 15511.961038351059, 'accumulated_submission_time': 14836.049505472183, 'accumulated_eval_time': 673.6026911735535, 'accumulated_logging_time': 0.8420405387878418, 'global_step': 44175, 'preemption_count': 0}), (45699, {'train/accuracy': 0.6517458558082581, 'train/loss': 1.6850515604019165, 'validation/accuracy': 0.6002599596977234, 'validation/loss': 1.921714186668396, 'validation/num_examples': 50000, 'test/accuracy': 0.4750000238418579, 'test/loss': 2.5920767784118652, 'test/num_examples': 10000, 'score': 15346.166721105576, 'total_duration': 16039.82380604744, 'accumulated_submission_time': 15346.166721105576, 'accumulated_eval_time': 691.2603213787079, 'accumulated_logging_time': 0.878584623336792, 'global_step': 45699, 'preemption_count': 0}), (47224, {'train/accuracy': 0.6491748690605164, 'train/loss': 1.6727479696273804, 'validation/accuracy': 0.5950199961662292, 'validation/loss': 1.913076639175415, 'validation/num_examples': 50000, 'test/accuracy': 0.4792000353336334, 'test/loss': 2.5365116596221924, 'test/num_examples': 10000, 'score': 15856.377391576767, 'total_duration': 16569.536675214767, 'accumulated_submission_time': 15856.377391576767, 'accumulated_eval_time': 710.6794383525848, 'accumulated_logging_time': 0.910062313079834, 'global_step': 47224, 'preemption_count': 0}), (48748, {'train/accuracy': 0.6578443646430969, 'train/loss': 1.677195429801941, 'validation/accuracy': 0.6079999804496765, 'validation/loss': 1.9055547714233398, 'validation/num_examples': 50000, 'test/accuracy': 0.4805000126361847, 'test/loss': 2.5795655250549316, 'test/num_examples': 10000, 'score': 16366.59017109871, 'total_duration': 17099.961057662964, 'accumulated_submission_time': 16366.59017109871, 'accumulated_eval_time': 730.8101537227631, 'accumulated_logging_time': 0.9403750896453857, 'global_step': 48748, 'preemption_count': 0}), (50273, {'train/accuracy': 0.6466836333274841, 'train/loss': 1.681785225868225, 'validation/accuracy': 0.6007399559020996, 'validation/loss': 1.8856514692306519, 'validation/num_examples': 50000, 'test/accuracy': 0.48130002617836, 'test/loss': 2.5445470809936523, 'test/num_examples': 10000, 'score': 16876.49962568283, 'total_duration': 17629.207921028137, 'accumulated_submission_time': 16876.49962568283, 'accumulated_eval_time': 749.8049001693726, 'accumulated_logging_time': 1.2331180572509766, 'global_step': 50273, 'preemption_count': 0}), (51797, {'train/accuracy': 0.7037826776504517, 'train/loss': 1.4232388734817505, 'validation/accuracy': 0.6117199659347534, 'validation/loss': 1.8318618535995483, 'validation/num_examples': 50000, 'test/accuracy': 0.48910000920295715, 'test/loss': 2.4729981422424316, 'test/num_examples': 10000, 'score': 17386.470391750336, 'total_duration': 18156.528982400894, 'accumulated_submission_time': 17386.470391750336, 'accumulated_eval_time': 767.072503566742, 'accumulated_logging_time': 1.2664318084716797, 'global_step': 51797, 'preemption_count': 0}), (53321, {'train/accuracy': 0.6437739133834839, 'train/loss': 1.7253005504608154, 'validation/accuracy': 0.5821200013160706, 'validation/loss': 2.015341281890869, 'validation/num_examples': 50000, 'test/accuracy': 0.4604000151157379, 'test/loss': 2.6728532314300537, 'test/num_examples': 10000, 'score': 17896.50174498558, 'total_duration': 18684.37446331978, 'accumulated_submission_time': 17896.50174498558, 'accumulated_eval_time': 784.8032908439636, 'accumulated_logging_time': 1.2994449138641357, 'global_step': 53321, 'preemption_count': 0}), (54845, {'train/accuracy': 0.6750039458274841, 'train/loss': 1.6002681255340576, 'validation/accuracy': 0.6157199740409851, 'validation/loss': 1.8629964590072632, 'validation/num_examples': 50000, 'test/accuracy': 0.49240002036094666, 'test/loss': 2.5258798599243164, 'test/num_examples': 10000, 'score': 18406.563932418823, 'total_duration': 19212.19704079628, 'accumulated_submission_time': 18406.563932418823, 'accumulated_eval_time': 802.4813590049744, 'accumulated_logging_time': 1.3321518898010254, 'global_step': 54845, 'preemption_count': 0}), (56370, {'train/accuracy': 0.6426379084587097, 'train/loss': 1.7485918998718262, 'validation/accuracy': 0.5929799675941467, 'validation/loss': 1.977038025856018, 'validation/num_examples': 50000, 'test/accuracy': 0.46720001101493835, 'test/loss': 2.6630213260650635, 'test/num_examples': 10000, 'score': 18916.572904348373, 'total_duration': 19739.414001226425, 'accumulated_submission_time': 18916.572904348373, 'accumulated_eval_time': 819.604391336441, 'accumulated_logging_time': 1.3656785488128662, 'global_step': 56370, 'preemption_count': 0}), (57894, {'train/accuracy': 0.6578443646430969, 'train/loss': 1.682417392730713, 'validation/accuracy': 0.6097800135612488, 'validation/loss': 1.9047425985336304, 'validation/num_examples': 50000, 'test/accuracy': 0.4816000163555145, 'test/loss': 2.576265811920166, 'test/num_examples': 10000, 'score': 19426.731298685074, 'total_duration': 20266.79449415207, 'accumulated_submission_time': 19426.731298685074, 'accumulated_eval_time': 836.7437827587128, 'accumulated_logging_time': 1.3981499671936035, 'global_step': 57894, 'preemption_count': 0}), (59420, {'train/accuracy': 0.6669324040412903, 'train/loss': 1.6188238859176636, 'validation/accuracy': 0.6192799806594849, 'validation/loss': 1.8437219858169556, 'validation/num_examples': 50000, 'test/accuracy': 0.4958000183105469, 'test/loss': 2.4657139778137207, 'test/num_examples': 10000, 'score': 19936.740971326828, 'total_duration': 20793.957093954086, 'accumulated_submission_time': 19936.740971326828, 'accumulated_eval_time': 853.8124947547913, 'accumulated_logging_time': 1.431046485900879, 'global_step': 59420, 'preemption_count': 0}), (60945, {'train/accuracy': 0.7052973508834839, 'train/loss': 1.4652222394943237, 'validation/accuracy': 0.6233599781990051, 'validation/loss': 1.8111732006072998, 'validation/num_examples': 50000, 'test/accuracy': 0.4952000379562378, 'test/loss': 2.464139938354492, 'test/num_examples': 10000, 'score': 20446.835471630096, 'total_duration': 21321.3019258976, 'accumulated_submission_time': 20446.835471630096, 'accumulated_eval_time': 870.975608587265, 'accumulated_logging_time': 1.4685769081115723, 'global_step': 60945, 'preemption_count': 0}), (62470, {'train/accuracy': 0.6842115521430969, 'train/loss': 1.529850721359253, 'validation/accuracy': 0.6209999918937683, 'validation/loss': 1.819158673286438, 'validation/num_examples': 50000, 'test/accuracy': 0.4983000159263611, 'test/loss': 2.457092523574829, 'test/num_examples': 10000, 'score': 20956.799617528915, 'total_duration': 21848.297857761383, 'accumulated_submission_time': 20956.799617528915, 'accumulated_eval_time': 887.9220767021179, 'accumulated_logging_time': 1.503570556640625, 'global_step': 62470, 'preemption_count': 0}), (63995, {'train/accuracy': 0.6692841053009033, 'train/loss': 1.5799593925476074, 'validation/accuracy': 0.6154199838638306, 'validation/loss': 1.8372719287872314, 'validation/num_examples': 50000, 'test/accuracy': 0.49160003662109375, 'test/loss': 2.4828062057495117, 'test/num_examples': 10000, 'score': 21467.012628555298, 'total_duration': 22375.638775110245, 'accumulated_submission_time': 21467.012628555298, 'accumulated_eval_time': 904.96435379982, 'accumulated_logging_time': 1.5378267765045166, 'global_step': 63995, 'preemption_count': 0}), (65521, {'train/accuracy': 0.6893733739852905, 'train/loss': 1.4961163997650146, 'validation/accuracy': 0.6338399648666382, 'validation/loss': 1.744549036026001, 'validation/num_examples': 50000, 'test/accuracy': 0.5035000443458557, 'test/loss': 2.418294668197632, 'test/num_examples': 10000, 'score': 21977.248546361923, 'total_duration': 22902.802991867065, 'accumulated_submission_time': 21977.248546361923, 'accumulated_eval_time': 921.8063566684723, 'accumulated_logging_time': 1.5741550922393799, 'global_step': 65521, 'preemption_count': 0}), (67046, {'train/accuracy': 0.6789301633834839, 'train/loss': 1.5319353342056274, 'validation/accuracy': 0.627519965171814, 'validation/loss': 1.7659509181976318, 'validation/num_examples': 50000, 'test/accuracy': 0.49730002880096436, 'test/loss': 2.440481424331665, 'test/num_examples': 10000, 'score': 22487.43315601349, 'total_duration': 23430.085050821304, 'accumulated_submission_time': 22487.43315601349, 'accumulated_eval_time': 938.8201231956482, 'accumulated_logging_time': 1.6068379878997803, 'global_step': 67046, 'preemption_count': 0}), (68571, {'train/accuracy': 0.7240114808082581, 'train/loss': 1.43919837474823, 'validation/accuracy': 0.6231200098991394, 'validation/loss': 1.8756496906280518, 'validation/num_examples': 50000, 'test/accuracy': 0.501800000667572, 'test/loss': 2.520477771759033, 'test/num_examples': 10000, 'score': 22997.473987579346, 'total_duration': 23957.194925546646, 'accumulated_submission_time': 22997.473987579346, 'accumulated_eval_time': 955.7965259552002, 'accumulated_logging_time': 1.6453070640563965, 'global_step': 68571, 'preemption_count': 0}), (70095, {'train/accuracy': 0.7030652165412903, 'train/loss': 1.4618088006973267, 'validation/accuracy': 0.6258999705314636, 'validation/loss': 1.7977391481399536, 'validation/num_examples': 50000, 'test/accuracy': 0.5031000375747681, 'test/loss': 2.4266278743743896, 'test/num_examples': 10000, 'score': 23507.699318885803, 'total_duration': 24484.743031024933, 'accumulated_submission_time': 23507.699318885803, 'accumulated_eval_time': 973.0300307273865, 'accumulated_logging_time': 1.6790194511413574, 'global_step': 70095, 'preemption_count': 0}), (71620, {'train/accuracy': 0.7074896097183228, 'train/loss': 1.4071218967437744, 'validation/accuracy': 0.6420999765396118, 'validation/loss': 1.6956228017807007, 'validation/num_examples': 50000, 'test/accuracy': 0.5198000073432922, 'test/loss': 2.3231699466705322, 'test/num_examples': 10000, 'score': 24017.822208881378, 'total_duration': 25011.85559272766, 'accumulated_submission_time': 24017.822208881378, 'accumulated_eval_time': 989.9360868930817, 'accumulated_logging_time': 1.7123217582702637, 'global_step': 71620, 'preemption_count': 0}), (73145, {'train/accuracy': 0.6999959945678711, 'train/loss': 1.4288250207901, 'validation/accuracy': 0.6443600058555603, 'validation/loss': 1.693749189376831, 'validation/num_examples': 50000, 'test/accuracy': 0.5090000033378601, 'test/loss': 2.3625693321228027, 'test/num_examples': 10000, 'score': 24527.95017337799, 'total_duration': 25539.043766975403, 'accumulated_submission_time': 24527.95017337799, 'accumulated_eval_time': 1006.910671710968, 'accumulated_logging_time': 1.7452239990234375, 'global_step': 73145, 'preemption_count': 0}), (74670, {'train/accuracy': 0.7061144709587097, 'train/loss': 1.4120854139328003, 'validation/accuracy': 0.6490199565887451, 'validation/loss': 1.660880446434021, 'validation/num_examples': 50000, 'test/accuracy': 0.5206000208854675, 'test/loss': 2.332211494445801, 'test/num_examples': 10000, 'score': 25037.96821832657, 'total_duration': 26065.93457007408, 'accumulated_submission_time': 25037.96821832657, 'accumulated_eval_time': 1023.7013425827026, 'accumulated_logging_time': 1.7779958248138428, 'global_step': 74670, 'preemption_count': 0}), (76195, {'train/accuracy': 0.6880181431770325, 'train/loss': 1.4884074926376343, 'validation/accuracy': 0.6317399740219116, 'validation/loss': 1.7367355823516846, 'validation/num_examples': 50000, 'test/accuracy': 0.5103000402450562, 'test/loss': 2.3835484981536865, 'test/num_examples': 10000, 'score': 25548.186242580414, 'total_duration': 26593.894502401352, 'accumulated_submission_time': 25548.186242580414, 'accumulated_eval_time': 1041.3541376590729, 'accumulated_logging_time': 1.8116345405578613, 'global_step': 76195, 'preemption_count': 0}), (77720, {'train/accuracy': 0.7248684763908386, 'train/loss': 1.3948085308074951, 'validation/accuracy': 0.6405199766159058, 'validation/loss': 1.76896071434021, 'validation/num_examples': 50000, 'test/accuracy': 0.509600043296814, 'test/loss': 2.4423303604125977, 'test/num_examples': 10000, 'score': 26058.124629497528, 'total_duration': 27120.995968818665, 'accumulated_submission_time': 26058.124629497528, 'accumulated_eval_time': 1058.4298613071442, 'accumulated_logging_time': 1.8461647033691406, 'global_step': 77720, 'preemption_count': 0}), (79244, {'train/accuracy': 0.721699595451355, 'train/loss': 1.3454585075378418, 'validation/accuracy': 0.6500999927520752, 'validation/loss': 1.6539099216461182, 'validation/num_examples': 50000, 'test/accuracy': 0.5296000242233276, 'test/loss': 2.2836482524871826, 'test/num_examples': 10000, 'score': 26568.27812385559, 'total_duration': 27648.27478313446, 'accumulated_submission_time': 26568.27812385559, 'accumulated_eval_time': 1075.4691364765167, 'accumulated_logging_time': 1.8798434734344482, 'global_step': 79244, 'preemption_count': 0}), (80769, {'train/accuracy': 0.7140664458274841, 'train/loss': 1.446722149848938, 'validation/accuracy': 0.6485199928283691, 'validation/loss': 1.7378699779510498, 'validation/num_examples': 50000, 'test/accuracy': 0.5218999981880188, 'test/loss': 2.391239643096924, 'test/num_examples': 10000, 'score': 27078.22215628624, 'total_duration': 28175.21328663826, 'accumulated_submission_time': 27078.22215628624, 'accumulated_eval_time': 1092.3775401115417, 'accumulated_logging_time': 1.9127840995788574, 'global_step': 80769, 'preemption_count': 0}), (82293, {'train/accuracy': 0.7015106678009033, 'train/loss': 1.4795687198638916, 'validation/accuracy': 0.6381399631500244, 'validation/loss': 1.760036826133728, 'validation/num_examples': 50000, 'test/accuracy': 0.5125000476837158, 'test/loss': 2.4114387035369873, 'test/num_examples': 10000, 'score': 27588.254366397858, 'total_duration': 28702.094896793365, 'accumulated_submission_time': 27588.254366397858, 'accumulated_eval_time': 1109.1420695781708, 'accumulated_logging_time': 1.9469194412231445, 'global_step': 82293, 'preemption_count': 0}), (83818, {'train/accuracy': 0.7146444320678711, 'train/loss': 1.4013713598251343, 'validation/accuracy': 0.6601799726486206, 'validation/loss': 1.6596468687057495, 'validation/num_examples': 50000, 'test/accuracy': 0.5307000279426575, 'test/loss': 2.2922167778015137, 'test/num_examples': 10000, 'score': 28098.231053352356, 'total_duration': 29229.13139939308, 'accumulated_submission_time': 28098.231053352356, 'accumulated_eval_time': 1126.1146397590637, 'accumulated_logging_time': 1.982010841369629, 'global_step': 83818, 'preemption_count': 0}), (85342, {'train/accuracy': 0.7122927308082581, 'train/loss': 1.4106295108795166, 'validation/accuracy': 0.6302799582481384, 'validation/loss': 1.772110939025879, 'validation/num_examples': 50000, 'test/accuracy': 0.4928000271320343, 'test/loss': 2.4630188941955566, 'test/num_examples': 10000, 'score': 28608.30346250534, 'total_duration': 29756.218193531036, 'accumulated_submission_time': 28608.30346250534, 'accumulated_eval_time': 1143.042379617691, 'accumulated_logging_time': 2.0171265602111816, 'global_step': 85342, 'preemption_count': 0}), (86867, {'train/accuracy': 0.7409717440605164, 'train/loss': 1.2722578048706055, 'validation/accuracy': 0.6576600074768066, 'validation/loss': 1.6240990161895752, 'validation/num_examples': 50000, 'test/accuracy': 0.5250000357627869, 'test/loss': 2.2928431034088135, 'test/num_examples': 10000, 'score': 29118.34032726288, 'total_duration': 30283.260884284973, 'accumulated_submission_time': 29118.34032726288, 'accumulated_eval_time': 1159.9638073444366, 'accumulated_logging_time': 2.0514938831329346, 'global_step': 86867, 'preemption_count': 0}), (88391, {'train/accuracy': 0.7313057780265808, 'train/loss': 1.2895619869232178, 'validation/accuracy': 0.6612600088119507, 'validation/loss': 1.6069889068603516, 'validation/num_examples': 50000, 'test/accuracy': 0.5308000445365906, 'test/loss': 2.2620644569396973, 'test/num_examples': 10000, 'score': 29628.375321388245, 'total_duration': 30810.539741039276, 'accumulated_submission_time': 29628.375321388245, 'accumulated_eval_time': 1177.1231915950775, 'accumulated_logging_time': 2.0849714279174805, 'global_step': 88391, 'preemption_count': 0}), (89916, {'train/accuracy': 0.7133689522743225, 'train/loss': 1.3448280096054077, 'validation/accuracy': 0.6446399688720703, 'validation/loss': 1.651045560836792, 'validation/num_examples': 50000, 'test/accuracy': 0.5157000422477722, 'test/loss': 2.332489490509033, 'test/num_examples': 10000, 'score': 30138.436474323273, 'total_duration': 31337.64408659935, 'accumulated_submission_time': 30138.436474323273, 'accumulated_eval_time': 1194.078701019287, 'accumulated_logging_time': 2.1213369369506836, 'global_step': 89916, 'preemption_count': 0}), (91440, {'train/accuracy': 0.7242307066917419, 'train/loss': 1.3670257329940796, 'validation/accuracy': 0.6621800065040588, 'validation/loss': 1.6457127332687378, 'validation/num_examples': 50000, 'test/accuracy': 0.5351999998092651, 'test/loss': 2.2859864234924316, 'test/num_examples': 10000, 'score': 30648.390997886658, 'total_duration': 31864.661551475525, 'accumulated_submission_time': 30648.390997886658, 'accumulated_eval_time': 1211.0527000427246, 'accumulated_logging_time': 2.159031391143799, 'global_step': 91440, 'preemption_count': 0}), (92966, {'train/accuracy': 0.7262237071990967, 'train/loss': 1.3768022060394287, 'validation/accuracy': 0.6603800058364868, 'validation/loss': 1.6624222993850708, 'validation/num_examples': 50000, 'test/accuracy': 0.5343000292778015, 'test/loss': 2.292142629623413, 'test/num_examples': 10000, 'score': 31158.61331629753, 'total_duration': 32391.90967822075, 'accumulated_submission_time': 31158.61331629753, 'accumulated_eval_time': 1227.9910509586334, 'accumulated_logging_time': 2.1950061321258545, 'global_step': 92966, 'preemption_count': 0}), (94491, {'train/accuracy': 0.7694714665412903, 'train/loss': 1.1644514799118042, 'validation/accuracy': 0.6688199639320374, 'validation/loss': 1.597221851348877, 'validation/num_examples': 50000, 'test/accuracy': 0.5424000024795532, 'test/loss': 2.2404980659484863, 'test/num_examples': 10000, 'score': 31668.59368467331, 'total_duration': 32919.03987455368, 'accumulated_submission_time': 31668.59368467331, 'accumulated_eval_time': 1245.053385257721, 'accumulated_logging_time': 2.2303876876831055, 'global_step': 94491, 'preemption_count': 0}), (96017, {'train/accuracy': 0.7730787396430969, 'train/loss': 1.1428571939468384, 'validation/accuracy': 0.6869999766349792, 'validation/loss': 1.5054179430007935, 'validation/num_examples': 50000, 'test/accuracy': 0.5630000233650208, 'test/loss': 2.131014347076416, 'test/num_examples': 10000, 'score': 32178.638496160507, 'total_duration': 33446.19157075882, 'accumulated_submission_time': 32178.638496160507, 'accumulated_eval_time': 1262.0715773105621, 'accumulated_logging_time': 2.268350601196289, 'global_step': 96017, 'preemption_count': 0}), (97542, {'train/accuracy': 0.7545639276504517, 'train/loss': 1.2203161716461182, 'validation/accuracy': 0.6776599884033203, 'validation/loss': 1.5536553859710693, 'validation/num_examples': 50000, 'test/accuracy': 0.5504000186920166, 'test/loss': 2.1981472969055176, 'test/num_examples': 10000, 'score': 32688.80292248726, 'total_duration': 33973.42340660095, 'accumulated_submission_time': 32688.80292248726, 'accumulated_eval_time': 1279.048544883728, 'accumulated_logging_time': 2.307842493057251, 'global_step': 97542, 'preemption_count': 0}), (99068, {'train/accuracy': 0.7618184089660645, 'train/loss': 1.1829177141189575, 'validation/accuracy': 0.6860199570655823, 'validation/loss': 1.5084346532821655, 'validation/num_examples': 50000, 'test/accuracy': 0.5588000416755676, 'test/loss': 2.1485660076141357, 'test/num_examples': 10000, 'score': 33198.97890162468, 'total_duration': 34500.6107442379, 'accumulated_submission_time': 33198.97890162468, 'accumulated_eval_time': 1295.9711184501648, 'accumulated_logging_time': 2.3439955711364746, 'global_step': 99068, 'preemption_count': 0}), (100593, {'train/accuracy': 0.7625557780265808, 'train/loss': 1.164181113243103, 'validation/accuracy': 0.6901400089263916, 'validation/loss': 1.4772294759750366, 'validation/num_examples': 50000, 'test/accuracy': 0.5692000389099121, 'test/loss': 2.101778984069824, 'test/num_examples': 10000, 'score': 33709.128729104996, 'total_duration': 35027.699124097824, 'accumulated_submission_time': 33709.128729104996, 'accumulated_eval_time': 1312.825609445572, 'accumulated_logging_time': 2.3782050609588623, 'global_step': 100593, 'preemption_count': 0}), (102119, {'train/accuracy': 0.7662627100944519, 'train/loss': 1.1389541625976562, 'validation/accuracy': 0.6900599598884583, 'validation/loss': 1.4855501651763916, 'validation/num_examples': 50000, 'test/accuracy': 0.5582000017166138, 'test/loss': 2.1185102462768555, 'test/num_examples': 10000, 'score': 34219.33787107468, 'total_duration': 35554.92210316658, 'accumulated_submission_time': 34219.33787107468, 'accumulated_eval_time': 1329.753449678421, 'accumulated_logging_time': 2.4143362045288086, 'global_step': 102119, 'preemption_count': 0}), (103643, {'train/accuracy': 0.7879862785339355, 'train/loss': 1.0843526124954224, 'validation/accuracy': 0.6917399764060974, 'validation/loss': 1.4955836534500122, 'validation/num_examples': 50000, 'test/accuracy': 0.5681000351905823, 'test/loss': 2.132781982421875, 'test/num_examples': 10000, 'score': 34729.46434569359, 'total_duration': 36082.156563043594, 'accumulated_submission_time': 34729.46434569359, 'accumulated_eval_time': 1346.775689125061, 'accumulated_logging_time': 2.449850559234619, 'global_step': 103643, 'preemption_count': 0}), (105168, {'train/accuracy': 0.7840401530265808, 'train/loss': 1.0800116062164307, 'validation/accuracy': 0.6991599798202515, 'validation/loss': 1.454992651939392, 'validation/num_examples': 50000, 'test/accuracy': 0.5693000555038452, 'test/loss': 2.1008152961730957, 'test/num_examples': 10000, 'score': 35239.47196364403, 'total_duration': 36609.083545684814, 'accumulated_submission_time': 35239.47196364403, 'accumulated_eval_time': 1363.6102871894836, 'accumulated_logging_time': 2.4847590923309326, 'global_step': 105168, 'preemption_count': 0}), (106692, {'train/accuracy': 0.7885442972183228, 'train/loss': 1.041917324066162, 'validation/accuracy': 0.706059992313385, 'validation/loss': 1.4018383026123047, 'validation/num_examples': 50000, 'test/accuracy': 0.5731000304222107, 'test/loss': 2.0337016582489014, 'test/num_examples': 10000, 'score': 35749.43216061592, 'total_duration': 37136.05479979515, 'accumulated_submission_time': 35749.43216061592, 'accumulated_eval_time': 1380.5329904556274, 'accumulated_logging_time': 2.521979570388794, 'global_step': 106692, 'preemption_count': 0}), (108217, {'train/accuracy': 0.7863320708274841, 'train/loss': 1.0653631687164307, 'validation/accuracy': 0.7044000029563904, 'validation/loss': 1.4175564050674438, 'validation/num_examples': 50000, 'test/accuracy': 0.5789000391960144, 'test/loss': 2.0408992767333984, 'test/num_examples': 10000, 'score': 36259.42400288582, 'total_duration': 37662.85647940636, 'accumulated_submission_time': 36259.42400288582, 'accumulated_eval_time': 1397.246426820755, 'accumulated_logging_time': 2.568589448928833, 'global_step': 108217, 'preemption_count': 0}), (109741, {'train/accuracy': 0.7848373651504517, 'train/loss': 1.0459448099136353, 'validation/accuracy': 0.7018399834632874, 'validation/loss': 1.3989624977111816, 'validation/num_examples': 50000, 'test/accuracy': 0.5725000500679016, 'test/loss': 2.038954496383667, 'test/num_examples': 10000, 'score': 36769.386798620224, 'total_duration': 38189.84563994408, 'accumulated_submission_time': 36769.386798620224, 'accumulated_eval_time': 1414.1801376342773, 'accumulated_logging_time': 2.605180501937866, 'global_step': 109741, 'preemption_count': 0}), (111267, {'train/accuracy': 0.840262234210968, 'train/loss': 0.8568135499954224, 'validation/accuracy': 0.7115600109100342, 'validation/loss': 1.3735299110412598, 'validation/num_examples': 50000, 'test/accuracy': 0.5875000357627869, 'test/loss': 2.0031495094299316, 'test/num_examples': 10000, 'score': 37279.57219338417, 'total_duration': 38717.04235768318, 'accumulated_submission_time': 37279.57219338417, 'accumulated_eval_time': 1431.1059007644653, 'accumulated_logging_time': 2.6398744583129883, 'global_step': 111267, 'preemption_count': 0}), (112792, {'train/accuracy': 0.8233418464660645, 'train/loss': 0.9245814085006714, 'validation/accuracy': 0.7210999727249146, 'validation/loss': 1.3503004312515259, 'validation/num_examples': 50000, 'test/accuracy': 0.5892000198364258, 'test/loss': 1.9957637786865234, 'test/num_examples': 10000, 'score': 37789.67009830475, 'total_duration': 39244.07853126526, 'accumulated_submission_time': 37789.67009830475, 'accumulated_eval_time': 1447.956482887268, 'accumulated_logging_time': 2.675337553024292, 'global_step': 112792, 'preemption_count': 0}), (114318, {'train/accuracy': 0.8214883208274841, 'train/loss': 0.9070454835891724, 'validation/accuracy': 0.7214799523353577, 'validation/loss': 1.3157671689987183, 'validation/num_examples': 50000, 'test/accuracy': 0.5967000126838684, 'test/loss': 1.93389892578125, 'test/num_examples': 10000, 'score': 38299.87454175949, 'total_duration': 39771.35484480858, 'accumulated_submission_time': 38299.87454175949, 'accumulated_eval_time': 1464.9416210651398, 'accumulated_logging_time': 2.710414171218872, 'global_step': 114318, 'preemption_count': 0}), (115843, {'train/accuracy': 0.8283242583274841, 'train/loss': 0.8629956841468811, 'validation/accuracy': 0.7298600077629089, 'validation/loss': 1.2824699878692627, 'validation/num_examples': 50000, 'test/accuracy': 0.5997000336647034, 'test/loss': 1.9135383367538452, 'test/num_examples': 10000, 'score': 38810.03670358658, 'total_duration': 40298.495109796524, 'accumulated_submission_time': 38810.03670358658, 'accumulated_eval_time': 1481.8293414115906, 'accumulated_logging_time': 2.7465732097625732, 'global_step': 115843, 'preemption_count': 0}), (117369, {'train/accuracy': 0.8292211294174194, 'train/loss': 0.8857026100158691, 'validation/accuracy': 0.7333599925041199, 'validation/loss': 1.2927523851394653, 'validation/num_examples': 50000, 'test/accuracy': 0.6086000204086304, 'test/loss': 1.9028667211532593, 'test/num_examples': 10000, 'score': 39320.10192465782, 'total_duration': 40825.48747396469, 'accumulated_submission_time': 39320.10192465782, 'accumulated_eval_time': 1498.6681442260742, 'accumulated_logging_time': 2.7819528579711914, 'global_step': 117369, 'preemption_count': 0}), (118894, {'train/accuracy': 0.8380500674247742, 'train/loss': 0.8556788563728333, 'validation/accuracy': 0.7351599931716919, 'validation/loss': 1.2787137031555176, 'validation/num_examples': 50000, 'test/accuracy': 0.6147000193595886, 'test/loss': 1.881813406944275, 'test/num_examples': 10000, 'score': 39830.31067419052, 'total_duration': 41352.74931931496, 'accumulated_submission_time': 39830.31067419052, 'accumulated_eval_time': 1515.6309685707092, 'accumulated_logging_time': 2.820580244064331, 'global_step': 118894, 'preemption_count': 0}), (120420, {'train/accuracy': 0.8646364808082581, 'train/loss': 0.7449840307235718, 'validation/accuracy': 0.7392199635505676, 'validation/loss': 1.2548187971115112, 'validation/num_examples': 50000, 'test/accuracy': 0.6165000200271606, 'test/loss': 1.8725852966308594, 'test/num_examples': 10000, 'score': 40340.46223068237, 'total_duration': 41879.81203150749, 'accumulated_submission_time': 40340.46223068237, 'accumulated_eval_time': 1532.4438333511353, 'accumulated_logging_time': 2.86673641204834, 'global_step': 120420, 'preemption_count': 0}), (121945, {'train/accuracy': 0.8652941584587097, 'train/loss': 0.75306636095047, 'validation/accuracy': 0.7446399927139282, 'validation/loss': 1.23543381690979, 'validation/num_examples': 50000, 'test/accuracy': 0.6249000430107117, 'test/loss': 1.8201731443405151, 'test/num_examples': 10000, 'score': 40850.701830387115, 'total_duration': 42407.00716710091, 'accumulated_submission_time': 40850.701830387115, 'accumulated_eval_time': 1549.3056252002716, 'accumulated_logging_time': 2.9067347049713135, 'global_step': 121945, 'preemption_count': 0}), (123470, {'train/accuracy': 0.8700175285339355, 'train/loss': 0.7411680221557617, 'validation/accuracy': 0.753059983253479, 'validation/loss': 1.2147120237350464, 'validation/num_examples': 50000, 'test/accuracy': 0.6299000382423401, 'test/loss': 1.8131251335144043, 'test/num_examples': 10000, 'score': 41360.89746046066, 'total_duration': 42934.03763651848, 'accumulated_submission_time': 41360.89746046066, 'accumulated_eval_time': 1566.0522344112396, 'accumulated_logging_time': 2.9459853172302246, 'global_step': 123470, 'preemption_count': 0}), (124995, {'train/accuracy': 0.8798429369926453, 'train/loss': 0.6981543302536011, 'validation/accuracy': 0.7577599883079529, 'validation/loss': 1.1877187490463257, 'validation/num_examples': 50000, 'test/accuracy': 0.6325000524520874, 'test/loss': 1.7903801202774048, 'test/num_examples': 10000, 'score': 41871.141345500946, 'total_duration': 43461.47218251228, 'accumulated_submission_time': 41871.141345500946, 'accumulated_eval_time': 1583.1534242630005, 'accumulated_logging_time': 2.983980655670166, 'global_step': 124995, 'preemption_count': 0}), (126520, {'train/accuracy': 0.890625, 'train/loss': 0.6434340476989746, 'validation/accuracy': 0.764519989490509, 'validation/loss': 1.151633858680725, 'validation/num_examples': 50000, 'test/accuracy': 0.6461000442504883, 'test/loss': 1.7440989017486572, 'test/num_examples': 10000, 'score': 42381.152169942856, 'total_duration': 43988.59010863304, 'accumulated_submission_time': 42381.152169942856, 'accumulated_eval_time': 1600.1616296768188, 'accumulated_logging_time': 3.0353474617004395, 'global_step': 126520, 'preemption_count': 0}), (128044, {'train/accuracy': 0.9036591053009033, 'train/loss': 0.6054961085319519, 'validation/accuracy': 0.7681399583816528, 'validation/loss': 1.1449393033981323, 'validation/num_examples': 50000, 'test/accuracy': 0.6472000479698181, 'test/loss': 1.733769178390503, 'test/num_examples': 10000, 'score': 42891.09298849106, 'total_duration': 44515.57524704933, 'accumulated_submission_time': 42891.09298849106, 'accumulated_eval_time': 1617.1058330535889, 'accumulated_logging_time': 3.0884687900543213, 'global_step': 128044, 'preemption_count': 0}), (129569, {'train/accuracy': 0.8992147445678711, 'train/loss': 0.6113567352294922, 'validation/accuracy': 0.7674799561500549, 'validation/loss': 1.1436293125152588, 'validation/num_examples': 50000, 'test/accuracy': 0.6494000554084778, 'test/loss': 1.7323858737945557, 'test/num_examples': 10000, 'score': 43401.31970334053, 'total_duration': 45042.69445633888, 'accumulated_submission_time': 43401.31970334053, 'accumulated_eval_time': 1633.914809703827, 'accumulated_logging_time': 3.1249136924743652, 'global_step': 129569, 'preemption_count': 0}), (131093, {'train/accuracy': 0.9001116156578064, 'train/loss': 0.6123691201210022, 'validation/accuracy': 0.7674199938774109, 'validation/loss': 1.1417090892791748, 'validation/num_examples': 50000, 'test/accuracy': 0.6477000117301941, 'test/loss': 1.7300938367843628, 'test/num_examples': 10000, 'score': 43911.290818452835, 'total_duration': 45569.67114830017, 'accumulated_submission_time': 43911.290818452835, 'accumulated_eval_time': 1650.835176229477, 'accumulated_logging_time': 3.163182258605957, 'global_step': 131093, 'preemption_count': 0}), (132617, {'train/accuracy': 0.899812638759613, 'train/loss': 0.6107229590415955, 'validation/accuracy': 0.7678999900817871, 'validation/loss': 1.1452364921569824, 'validation/num_examples': 50000, 'test/accuracy': 0.6480000019073486, 'test/loss': 1.733577013015747, 'test/num_examples': 10000, 'score': 44421.219329595566, 'total_duration': 46096.83474469185, 'accumulated_submission_time': 44421.219329595566, 'accumulated_eval_time': 1667.9871294498444, 'accumulated_logging_time': 3.1995527744293213, 'global_step': 132617, 'preemption_count': 0}), (134141, {'train/accuracy': 0.9026227593421936, 'train/loss': 0.6078991889953613, 'validation/accuracy': 0.7683799862861633, 'validation/loss': 1.1395996809005737, 'validation/num_examples': 50000, 'test/accuracy': 0.6462000012397766, 'test/loss': 1.7313485145568848, 'test/num_examples': 10000, 'score': 44931.39663743973, 'total_duration': 46623.92261862755, 'accumulated_submission_time': 44931.39663743973, 'accumulated_eval_time': 1684.811104297638, 'accumulated_logging_time': 3.2384746074676514, 'global_step': 134141, 'preemption_count': 0}), (135666, {'train/accuracy': 0.9016063213348389, 'train/loss': 0.608150064945221, 'validation/accuracy': 0.7674399614334106, 'validation/loss': 1.142611026763916, 'validation/num_examples': 50000, 'test/accuracy': 0.6473000049591064, 'test/loss': 1.7349656820297241, 'test/num_examples': 10000, 'score': 45441.513060092926, 'total_duration': 47150.9918525219, 'accumulated_submission_time': 45441.513060092926, 'accumulated_eval_time': 1701.6780714988708, 'accumulated_logging_time': 3.2771096229553223, 'global_step': 135666, 'preemption_count': 0}), (137189, {'train/accuracy': 0.9120894074440002, 'train/loss': 0.5788108110427856, 'validation/accuracy': 0.76801997423172, 'validation/loss': 1.1506139039993286, 'validation/num_examples': 50000, 'test/accuracy': 0.6458000540733337, 'test/loss': 1.7417221069335938, 'test/num_examples': 10000, 'score': 45951.4829518795, 'total_duration': 47677.86198878288, 'accumulated_submission_time': 45951.4829518795, 'accumulated_eval_time': 1718.4947848320007, 'accumulated_logging_time': 3.3133702278137207, 'global_step': 137189, 'preemption_count': 0}), (138713, {'train/accuracy': 0.9090401530265808, 'train/loss': 0.5813366770744324, 'validation/accuracy': 0.7679799795150757, 'validation/loss': 1.1458003520965576, 'validation/num_examples': 50000, 'test/accuracy': 0.648900032043457, 'test/loss': 1.739550232887268, 'test/num_examples': 10000, 'score': 46461.408250808716, 'total_duration': 48205.216795921326, 'accumulated_submission_time': 46461.408250808716, 'accumulated_eval_time': 1735.8365664482117, 'accumulated_logging_time': 3.3539419174194336, 'global_step': 138713, 'preemption_count': 0}), (140236, {'train/accuracy': 0.9064293503761292, 'train/loss': 0.5960589051246643, 'validation/accuracy': 0.7668799757957458, 'validation/loss': 1.152432918548584, 'validation/num_examples': 50000, 'test/accuracy': 0.6484000086784363, 'test/loss': 1.7396743297576904, 'test/num_examples': 10000, 'score': 46971.337717056274, 'total_duration': 48732.08028960228, 'accumulated_submission_time': 46971.337717056274, 'accumulated_eval_time': 1752.688690662384, 'accumulated_logging_time': 3.3888731002807617, 'global_step': 140236, 'preemption_count': 0}), (141761, {'train/accuracy': 0.9070870280265808, 'train/loss': 0.5918859839439392, 'validation/accuracy': 0.7676999568939209, 'validation/loss': 1.150712490081787, 'validation/num_examples': 50000, 'test/accuracy': 0.6475000381469727, 'test/loss': 1.7458745241165161, 'test/num_examples': 10000, 'score': 47481.42756533623, 'total_duration': 49259.141426324844, 'accumulated_submission_time': 47481.42756533623, 'accumulated_eval_time': 1769.573629617691, 'accumulated_logging_time': 3.4280009269714355, 'global_step': 141761, 'preemption_count': 0}), (143285, {'train/accuracy': 0.9057118892669678, 'train/loss': 0.5928652286529541, 'validation/accuracy': 0.7670199871063232, 'validation/loss': 1.1489965915679932, 'validation/num_examples': 50000, 'test/accuracy': 0.6485000252723694, 'test/loss': 1.7452924251556396, 'test/num_examples': 10000, 'score': 47991.39926171303, 'total_duration': 49786.23249578476, 'accumulated_submission_time': 47991.39926171303, 'accumulated_eval_time': 1786.6084179878235, 'accumulated_logging_time': 3.465696334838867, 'global_step': 143285, 'preemption_count': 0}), (144810, {'train/accuracy': 0.91214919090271, 'train/loss': 0.569913387298584, 'validation/accuracy': 0.7676599621772766, 'validation/loss': 1.1494959592819214, 'validation/num_examples': 50000, 'test/accuracy': 0.6438000202178955, 'test/loss': 1.7543665170669556, 'test/num_examples': 10000, 'score': 48501.540533304214, 'total_duration': 50313.23071908951, 'accumulated_submission_time': 48501.540533304214, 'accumulated_eval_time': 1803.3810710906982, 'accumulated_logging_time': 3.5027754306793213, 'global_step': 144810, 'preemption_count': 0}), (146333, {'train/accuracy': 0.9154974222183228, 'train/loss': 0.5673818588256836, 'validation/accuracy': 0.767300009727478, 'validation/loss': 1.1546895503997803, 'validation/num_examples': 50000, 'test/accuracy': 0.6459000110626221, 'test/loss': 1.7476420402526855, 'test/num_examples': 10000, 'score': 49011.49378705025, 'total_duration': 50840.01796603203, 'accumulated_submission_time': 49011.49378705025, 'accumulated_eval_time': 1820.126195192337, 'accumulated_logging_time': 3.544480562210083, 'global_step': 146333, 'preemption_count': 0}), (147858, {'train/accuracy': 0.9136439561843872, 'train/loss': 0.554057776927948, 'validation/accuracy': 0.7684999704360962, 'validation/loss': 1.1387571096420288, 'validation/num_examples': 50000, 'test/accuracy': 0.6473000049591064, 'test/loss': 1.7397738695144653, 'test/num_examples': 10000, 'score': 49521.69353747368, 'total_duration': 51367.05997681618, 'accumulated_submission_time': 49521.69353747368, 'accumulated_eval_time': 1836.8831577301025, 'accumulated_logging_time': 3.5825765132904053, 'global_step': 147858, 'preemption_count': 0}), (149381, {'train/accuracy': 0.9146205186843872, 'train/loss': 0.5675554275512695, 'validation/accuracy': 0.7678999900817871, 'validation/loss': 1.1539218425750732, 'validation/num_examples': 50000, 'test/accuracy': 0.6495000123977661, 'test/loss': 1.747617483139038, 'test/num_examples': 10000, 'score': 50031.62016391754, 'total_duration': 51894.02519488335, 'accumulated_submission_time': 50031.62016391754, 'accumulated_eval_time': 1853.835256099701, 'accumulated_logging_time': 3.621910810470581, 'global_step': 149381, 'preemption_count': 0}), (150905, {'train/accuracy': 0.9134247303009033, 'train/loss': 0.5734334588050842, 'validation/accuracy': 0.7669999599456787, 'validation/loss': 1.1549052000045776, 'validation/num_examples': 50000, 'test/accuracy': 0.6456000208854675, 'test/loss': 1.7588087320327759, 'test/num_examples': 10000, 'score': 50541.54686450958, 'total_duration': 52420.88992810249, 'accumulated_submission_time': 50541.54686450958, 'accumulated_eval_time': 1870.6819188594818, 'accumulated_logging_time': 3.666167736053467, 'global_step': 150905, 'preemption_count': 0}), (152429, {'train/accuracy': 0.9149593114852905, 'train/loss': 0.562910258769989, 'validation/accuracy': 0.7675999999046326, 'validation/loss': 1.152549147605896, 'validation/num_examples': 50000, 'test/accuracy': 0.6487000584602356, 'test/loss': 1.7565561532974243, 'test/num_examples': 10000, 'score': 51051.64455986023, 'total_duration': 52947.93546438217, 'accumulated_submission_time': 51051.64455986023, 'accumulated_eval_time': 1887.541310787201, 'accumulated_logging_time': 3.707859992980957, 'global_step': 152429, 'preemption_count': 0}), (153954, {'train/accuracy': 0.9214763641357422, 'train/loss': 0.5323159098625183, 'validation/accuracy': 0.7672199606895447, 'validation/loss': 1.148197054862976, 'validation/num_examples': 50000, 'test/accuracy': 0.6482000350952148, 'test/loss': 1.7559324502944946, 'test/num_examples': 10000, 'score': 51561.67814707756, 'total_duration': 53475.51697063446, 'accumulated_submission_time': 51561.67814707756, 'accumulated_eval_time': 1905.0015995502472, 'accumulated_logging_time': 3.7483904361724854, 'global_step': 153954, 'preemption_count': 0}), (155478, {'train/accuracy': 0.918965220451355, 'train/loss': 0.5517738461494446, 'validation/accuracy': 0.7668399810791016, 'validation/loss': 1.1611448526382446, 'validation/num_examples': 50000, 'test/accuracy': 0.6460000276565552, 'test/loss': 1.7580969333648682, 'test/num_examples': 10000, 'score': 52071.85892367363, 'total_duration': 54002.62115073204, 'accumulated_submission_time': 52071.85892367363, 'accumulated_eval_time': 1921.8368973731995, 'accumulated_logging_time': 3.78936767578125, 'global_step': 155478, 'preemption_count': 0}), (157003, {'train/accuracy': 0.9165537357330322, 'train/loss': 0.5551013350486755, 'validation/accuracy': 0.7649399638175964, 'validation/loss': 1.1580182313919067, 'validation/num_examples': 50000, 'test/accuracy': 0.6455000042915344, 'test/loss': 1.7641746997833252, 'test/num_examples': 10000, 'score': 52581.95855307579, 'total_duration': 54529.59687829018, 'accumulated_submission_time': 52581.95855307579, 'accumulated_eval_time': 1938.623378276825, 'accumulated_logging_time': 3.8317160606384277, 'global_step': 157003, 'preemption_count': 0}), (158527, {'train/accuracy': 0.9178292155265808, 'train/loss': 0.5524814128875732, 'validation/accuracy': 0.7671599984169006, 'validation/loss': 1.1583116054534912, 'validation/num_examples': 50000, 'test/accuracy': 0.6462000012397766, 'test/loss': 1.7575604915618896, 'test/num_examples': 10000, 'score': 53092.09315085411, 'total_duration': 55056.50742435455, 'accumulated_submission_time': 53092.09315085411, 'accumulated_eval_time': 1955.3126981258392, 'accumulated_logging_time': 3.871208429336548, 'global_step': 158527, 'preemption_count': 0}), (160051, {'train/accuracy': 0.9171914458274841, 'train/loss': 0.5453962683677673, 'validation/accuracy': 0.7662599682807922, 'validation/loss': 1.1476150751113892, 'validation/num_examples': 50000, 'test/accuracy': 0.6456000208854675, 'test/loss': 1.7531681060791016, 'test/num_examples': 10000, 'score': 53602.02709078789, 'total_duration': 55583.37965941429, 'accumulated_submission_time': 53602.02709078789, 'accumulated_eval_time': 1972.1613173484802, 'accumulated_logging_time': 3.913592576980591, 'global_step': 160051, 'preemption_count': 0}), (161575, {'train/accuracy': 0.918387234210968, 'train/loss': 0.554888904094696, 'validation/accuracy': 0.76555997133255, 'validation/loss': 1.158990502357483, 'validation/num_examples': 50000, 'test/accuracy': 0.6438000202178955, 'test/loss': 1.7632352113723755, 'test/num_examples': 10000, 'score': 54112.200365543365, 'total_duration': 56110.482533454895, 'accumulated_submission_time': 54112.200365543365, 'accumulated_eval_time': 1989.001749753952, 'accumulated_logging_time': 3.955530881881714, 'global_step': 161575, 'preemption_count': 0}), (163099, {'train/accuracy': 0.9237084984779358, 'train/loss': 0.5277928113937378, 'validation/accuracy': 0.765500009059906, 'validation/loss': 1.1601675748825073, 'validation/num_examples': 50000, 'test/accuracy': 0.6462000012397766, 'test/loss': 1.761299729347229, 'test/num_examples': 10000, 'score': 54622.15748357773, 'total_duration': 56637.26773715019, 'accumulated_submission_time': 54622.15748357773, 'accumulated_eval_time': 2005.7370827198029, 'accumulated_logging_time': 4.000424385070801, 'global_step': 163099, 'preemption_count': 0}), (164623, {'train/accuracy': 0.9222536683082581, 'train/loss': 0.5271082520484924, 'validation/accuracy': 0.7665799856185913, 'validation/loss': 1.1513084173202515, 'validation/num_examples': 50000, 'test/accuracy': 0.6422000527381897, 'test/loss': 1.760626196861267, 'test/num_examples': 10000, 'score': 55132.37274169922, 'total_duration': 57164.306557655334, 'accumulated_submission_time': 55132.37274169922, 'accumulated_eval_time': 2022.4744703769684, 'accumulated_logging_time': 4.0391623973846436, 'global_step': 164623, 'preemption_count': 0}), (166148, {'train/accuracy': 0.923828125, 'train/loss': 0.535024106502533, 'validation/accuracy': 0.767799973487854, 'validation/loss': 1.164089322090149, 'validation/num_examples': 50000, 'test/accuracy': 0.6450000405311584, 'test/loss': 1.772684097290039, 'test/num_examples': 10000, 'score': 55642.393181324005, 'total_duration': 57691.58399581909, 'accumulated_submission_time': 55642.393181324005, 'accumulated_eval_time': 2039.645334482193, 'accumulated_logging_time': 4.0777857303619385, 'global_step': 166148, 'preemption_count': 0}), (167672, {'train/accuracy': 0.9208585619926453, 'train/loss': 0.5523911118507385, 'validation/accuracy': 0.766979992389679, 'validation/loss': 1.1723792552947998, 'validation/num_examples': 50000, 'test/accuracy': 0.6449000239372253, 'test/loss': 1.777124285697937, 'test/num_examples': 10000, 'score': 56152.30905199051, 'total_duration': 58218.42157268524, 'accumulated_submission_time': 56152.30905199051, 'accumulated_eval_time': 2056.481376647949, 'accumulated_logging_time': 4.116064071655273, 'global_step': 167672, 'preemption_count': 0}), (169196, {'train/accuracy': 0.9205197691917419, 'train/loss': 0.5553680062294006, 'validation/accuracy': 0.7642799615859985, 'validation/loss': 1.1756649017333984, 'validation/num_examples': 50000, 'test/accuracy': 0.6465000510215759, 'test/loss': 1.7762415409088135, 'test/num_examples': 10000, 'score': 56662.3060798645, 'total_duration': 58745.37886977196, 'accumulated_submission_time': 56662.3060798645, 'accumulated_eval_time': 2073.3540070056915, 'accumulated_logging_time': 4.155987501144409, 'global_step': 169196, 'preemption_count': 0}), (170720, {'train/accuracy': 0.9301259517669678, 'train/loss': 0.49675142765045166, 'validation/accuracy': 0.7644400000572205, 'validation/loss': 1.155943751335144, 'validation/num_examples': 50000, 'test/accuracy': 0.6454000473022461, 'test/loss': 1.7625746726989746, 'test/num_examples': 10000, 'score': 57172.38616704941, 'total_duration': 59272.27391242981, 'accumulated_submission_time': 57172.38616704941, 'accumulated_eval_time': 2090.0794911384583, 'accumulated_logging_time': 4.198121786117554, 'global_step': 170720, 'preemption_count': 0}), (172245, {'train/accuracy': 0.9285714030265808, 'train/loss': 0.5104222893714905, 'validation/accuracy': 0.7637400031089783, 'validation/loss': 1.159540057182312, 'validation/num_examples': 50000, 'test/accuracy': 0.6429000496864319, 'test/loss': 1.7697840929031372, 'test/num_examples': 10000, 'score': 57682.431550741196, 'total_duration': 59799.19443702698, 'accumulated_submission_time': 57682.431550741196, 'accumulated_eval_time': 2106.8666026592255, 'accumulated_logging_time': 4.238800764083862, 'global_step': 172245, 'preemption_count': 0}), (173769, {'train/accuracy': 0.9262595176696777, 'train/loss': 0.5297057628631592, 'validation/accuracy': 0.7644000053405762, 'validation/loss': 1.170536756515503, 'validation/num_examples': 50000, 'test/accuracy': 0.6426000595092773, 'test/loss': 1.7723007202148438, 'test/num_examples': 10000, 'score': 58192.52931523323, 'total_duration': 60326.32282280922, 'accumulated_submission_time': 58192.52931523323, 'accumulated_eval_time': 2123.8113532066345, 'accumulated_logging_time': 4.277429819107056, 'global_step': 173769, 'preemption_count': 0}), (175267, {'train/accuracy': 0.9266581535339355, 'train/loss': 0.5227275490760803, 'validation/accuracy': 0.7650600075721741, 'validation/loss': 1.1675528287887573, 'validation/num_examples': 50000, 'test/accuracy': 0.6401000022888184, 'test/loss': 1.7792713642120361, 'test/num_examples': 10000, 'score': 58702.86290168762, 'total_duration': 60854.98487496376, 'accumulated_submission_time': 58702.86290168762, 'accumulated_eval_time': 2142.0510466098785, 'accumulated_logging_time': 4.31864070892334, 'global_step': 175267, 'preemption_count': 0}), (176788, {'train/accuracy': 0.9225127100944519, 'train/loss': 0.5303520560264587, 'validation/accuracy': 0.7651399970054626, 'validation/loss': 1.1630430221557617, 'validation/num_examples': 50000, 'test/accuracy': 0.6421000361442566, 'test/loss': 1.777036190032959, 'test/num_examples': 10000, 'score': 59212.194242239, 'total_duration': 61383.06815433502, 'accumulated_submission_time': 59212.194242239, 'accumulated_eval_time': 2159.8215250968933, 'accumulated_logging_time': 5.252411365509033, 'global_step': 176788, 'preemption_count': 0}), (178312, {'train/accuracy': 0.9236288070678711, 'train/loss': 0.5388383865356445, 'validation/accuracy': 0.7633799910545349, 'validation/loss': 1.18033766746521, 'validation/num_examples': 50000, 'test/accuracy': 0.6434000134468079, 'test/loss': 1.7938323020935059, 'test/num_examples': 10000, 'score': 59722.13116836548, 'total_duration': 61910.64293670654, 'accumulated_submission_time': 59722.13116836548, 'accumulated_eval_time': 2177.372397184372, 'accumulated_logging_time': 5.292085886001587, 'global_step': 178312, 'preemption_count': 0}), (179835, {'train/accuracy': 0.9350884556770325, 'train/loss': 0.4926590323448181, 'validation/accuracy': 0.7642399668693542, 'validation/loss': 1.173576831817627, 'validation/num_examples': 50000, 'test/accuracy': 0.642300009727478, 'test/loss': 1.782651424407959, 'test/num_examples': 10000, 'score': 60232.12835073471, 'total_duration': 62438.17541408539, 'accumulated_submission_time': 60232.12835073471, 'accumulated_eval_time': 2194.8195939064026, 'accumulated_logging_time': 5.332860708236694, 'global_step': 179835, 'preemption_count': 0}), (181360, {'train/accuracy': 0.9288902878761292, 'train/loss': 0.5021556615829468, 'validation/accuracy': 0.7616599798202515, 'validation/loss': 1.1682405471801758, 'validation/num_examples': 50000, 'test/accuracy': 0.6452000141143799, 'test/loss': 1.7737975120544434, 'test/num_examples': 10000, 'score': 60742.311823129654, 'total_duration': 62965.72957491875, 'accumulated_submission_time': 60742.311823129654, 'accumulated_eval_time': 2212.100319623947, 'accumulated_logging_time': 5.375080347061157, 'global_step': 181360, 'preemption_count': 0}), (182884, {'train/accuracy': 0.9288105964660645, 'train/loss': 0.5267924666404724, 'validation/accuracy': 0.7643799781799316, 'validation/loss': 1.1856269836425781, 'validation/num_examples': 50000, 'test/accuracy': 0.6434000134468079, 'test/loss': 1.8020799160003662, 'test/num_examples': 10000, 'score': 61252.4593539238, 'total_duration': 63493.179923295975, 'accumulated_submission_time': 61252.4593539238, 'accumulated_eval_time': 2229.313698530197, 'accumulated_logging_time': 5.416741371154785, 'global_step': 182884, 'preemption_count': 0}), (184409, {'train/accuracy': 0.9275749325752258, 'train/loss': 0.5183637738227844, 'validation/accuracy': 0.7660399675369263, 'validation/loss': 1.1716256141662598, 'validation/num_examples': 50000, 'test/accuracy': 0.6447000503540039, 'test/loss': 1.7825127840042114, 'test/num_examples': 10000, 'score': 61762.51425027847, 'total_duration': 64020.42093586922, 'accumulated_submission_time': 61762.51425027847, 'accumulated_eval_time': 2246.4118337631226, 'accumulated_logging_time': 5.4571852684021, 'global_step': 184409, 'preemption_count': 0}), (185933, {'train/accuracy': 0.9296077489852905, 'train/loss': 0.5146064162254333, 'validation/accuracy': 0.76419997215271, 'validation/loss': 1.1747709512710571, 'validation/num_examples': 50000, 'test/accuracy': 0.6410000324249268, 'test/loss': 1.7878018617630005, 'test/num_examples': 10000, 'score': 62272.46965265274, 'total_duration': 64547.62351298332, 'accumulated_submission_time': 62272.46965265274, 'accumulated_eval_time': 2263.56644654274, 'accumulated_logging_time': 5.502199172973633, 'global_step': 185933, 'preemption_count': 0}), (186666, {'train/accuracy': 0.9305046200752258, 'train/loss': 0.5085150599479675, 'validation/accuracy': 0.7631399631500244, 'validation/loss': 1.1819463968276978, 'validation/num_examples': 50000, 'test/accuracy': 0.641700029373169, 'test/loss': 1.794360637664795, 'test/num_examples': 10000, 'score': 62517.452444791794, 'total_duration': 64809.73242044449, 'accumulated_submission_time': 62517.452444791794, 'accumulated_eval_time': 2280.6272785663605, 'accumulated_logging_time': 5.544475078582764, 'global_step': 186666, 'preemption_count': 0})], 'global_step': 186666}
I0216 03:16:17.754917 139970484569920 submission_runner.py:586] Timing: 62517.452444791794
I0216 03:16:17.755020 139970484569920 submission_runner.py:588] Total number of evals: 124
I0216 03:16:17.755089 139970484569920 submission_runner.py:589] ====================
I0216 03:16:17.755425 139970484569920 submission_runner.py:673] Final imagenet_resnet_large_bn_init score: 62517.452444791794
