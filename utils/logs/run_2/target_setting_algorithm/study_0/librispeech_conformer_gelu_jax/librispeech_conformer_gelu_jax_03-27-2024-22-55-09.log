python3 submission_runner.py --framework=jax --workload=librispeech_conformer_gelu --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --rng_seed=3996696240 --max_global_steps=80000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab --tuning_ruleset=external --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_conformer_gelu/tuning_search_space.json --num_tuning_trials=1 2>&1 | tee -a /logs/librispeech_conformer_gelu_jax_03-27-2024-22-55-09.log
I0327 22:55:30.021256 139830572304192 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/librispeech_conformer_gelu_jax.
I0327 22:55:31.094296 139830572304192 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0327 22:55:31.095125 139830572304192 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0327 22:55:31.095283 139830572304192 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0327 22:55:31.100857 139830572304192 submission_runner.py:557] Using RNG seed 3996696240
I0327 22:55:32.316743 139830572304192 submission_runner.py:566] --- Tuning run 1/1 ---
I0327 22:55:32.316947 139830572304192 submission_runner.py:571] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/librispeech_conformer_gelu_jax/trial_1.
I0327 22:55:32.317355 139830572304192 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_gelu_jax/trial_1/hparams.json.
I0327 22:55:32.500186 139830572304192 submission_runner.py:211] Initializing dataset.
I0327 22:55:32.500412 139830572304192 submission_runner.py:222] Initializing model.
I0327 22:55:37.468149 139830572304192 submission_runner.py:264] Initializing optimizer.
I0327 22:55:38.697904 139830572304192 submission_runner.py:271] Initializing metrics bundle.
I0327 22:55:38.698126 139830572304192 submission_runner.py:289] Initializing checkpoint and logger.
I0327 22:55:38.699430 139830572304192 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/librispeech_conformer_gelu_jax/trial_1 with prefix checkpoint_
I0327 22:55:38.699574 139830572304192 submission_runner.py:309] Saving meta data to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_gelu_jax/trial_1/meta_data_0.json.
I0327 22:55:38.699785 139830572304192 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0327 22:55:38.699852 139830572304192 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0327 22:55:38.973861 139830572304192 logger_utils.py:220] Unable to record git information. Continuing without it.
I0327 22:55:39.222521 139830572304192 submission_runner.py:313] Saving flags to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_gelu_jax/trial_1/flags_0.json.
I0327 22:55:39.237705 139830572304192 submission_runner.py:323] Starting training loop.
I0327 22:55:39.556672 139830572304192 input_pipeline.py:20] Loading split = train-clean-100
I0327 22:55:39.595620 139830572304192 input_pipeline.py:20] Loading split = train-clean-360
I0327 22:55:39.997073 139830572304192 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0327 22:56:40.760012 139654461888256 logging_writer.py:48] [0] global_step=0, grad_norm=78.74433135986328, loss=31.30824089050293
I0327 22:56:40.797868 139830572304192 spec.py:321] Evaluating on the training split.
I0327 22:56:40.966493 139830572304192 input_pipeline.py:20] Loading split = train-clean-100
I0327 22:56:41.001958 139830572304192 input_pipeline.py:20] Loading split = train-clean-360
I0327 22:56:41.393221 139830572304192 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0327 22:57:48.488304 139830572304192 spec.py:333] Evaluating on the validation split.
I0327 22:57:48.610601 139830572304192 input_pipeline.py:20] Loading split = dev-clean
I0327 22:57:48.615652 139830572304192 input_pipeline.py:20] Loading split = dev-other
I0327 22:58:53.013781 139830572304192 spec.py:349] Evaluating on the test split.
I0327 22:58:53.134905 139830572304192 input_pipeline.py:20] Loading split = test-clean
I0327 22:59:29.948662 139830572304192 submission_runner.py:422] Time since start: 230.71s, 	Step: 1, 	{'train/ctc_loss': Array(29.724083, dtype=float32), 'train/wer': 1.0418653561834954, 'validation/ctc_loss': Array(29.371702, dtype=float32), 'validation/wer': 1.245691611071956, 'validation/num_examples': 5348, 'test/ctc_loss': Array(29.384619, dtype=float32), 'test/wer': 1.243881136635996, 'test/num_examples': 2472, 'score': 61.5601065158844, 'total_duration': 230.70839548110962, 'accumulated_submission_time': 61.5601065158844, 'accumulated_eval_time': 169.1482331752777, 'accumulated_logging_time': 0}
I0327 22:59:29.983735 139647097595648 logging_writer.py:48] [1] accumulated_eval_time=169.148233, accumulated_logging_time=0, accumulated_submission_time=61.560107, global_step=1, preemption_count=0, score=61.560107, test/ctc_loss=29.384618759155273, test/num_examples=2472, test/wer=1.243881, total_duration=230.708395, train/ctc_loss=29.724082946777344, train/wer=1.041865, validation/ctc_loss=29.371702194213867, validation/num_examples=5348, validation/wer=1.245692
I0327 22:59:54.481812 139660410959616 logging_writer.py:48] [1] global_step=1, grad_norm=93.67100524902344, loss=31.814626693725586
I0327 22:59:55.409172 139660419352320 logging_writer.py:48] [2] global_step=2, grad_norm=87.06180572509766, loss=31.042436599731445
I0327 22:59:56.318629 139660410959616 logging_writer.py:48] [3] global_step=3, grad_norm=90.71673583984375, loss=31.336233139038086
I0327 22:59:57.214806 139660419352320 logging_writer.py:48] [4] global_step=4, grad_norm=87.81141662597656, loss=31.016141891479492
I0327 22:59:58.110746 139660410959616 logging_writer.py:48] [5] global_step=5, grad_norm=77.13430786132812, loss=31.801448822021484
I0327 22:59:59.005825 139660419352320 logging_writer.py:48] [6] global_step=6, grad_norm=95.46174621582031, loss=31.565954208374023
I0327 22:59:59.901387 139660410959616 logging_writer.py:48] [7] global_step=7, grad_norm=94.2545166015625, loss=30.959680557250977
I0327 23:00:00.798738 139660419352320 logging_writer.py:48] [8] global_step=8, grad_norm=93.88470458984375, loss=31.243200302124023
I0327 23:00:01.751460 139660410959616 logging_writer.py:48] [9] global_step=9, grad_norm=98.87254333496094, loss=31.26786994934082
I0327 23:00:02.662198 139660419352320 logging_writer.py:48] [10] global_step=10, grad_norm=109.25639343261719, loss=30.542110443115234
I0327 23:00:03.570844 139660410959616 logging_writer.py:48] [11] global_step=11, grad_norm=102.34039306640625, loss=30.85202407836914
I0327 23:00:04.485590 139660419352320 logging_writer.py:48] [12] global_step=12, grad_norm=112.59085083007812, loss=30.68131446838379
I0327 23:00:05.400967 139660410959616 logging_writer.py:48] [13] global_step=13, grad_norm=116.325927734375, loss=30.26146697998047
I0327 23:00:06.316545 139660419352320 logging_writer.py:48] [14] global_step=14, grad_norm=124.77831268310547, loss=30.051599502563477
I0327 23:00:07.221303 139660410959616 logging_writer.py:48] [15] global_step=15, grad_norm=124.75286865234375, loss=30.057279586791992
I0327 23:00:08.130373 139660419352320 logging_writer.py:48] [16] global_step=16, grad_norm=129.76243591308594, loss=29.977262496948242
I0327 23:00:09.038130 139660410959616 logging_writer.py:48] [17] global_step=17, grad_norm=142.06053161621094, loss=29.749040603637695
I0327 23:00:09.956224 139660419352320 logging_writer.py:48] [18] global_step=18, grad_norm=133.14610290527344, loss=29.47006607055664
I0327 23:00:10.857790 139660410959616 logging_writer.py:48] [19] global_step=19, grad_norm=135.9239501953125, loss=28.516939163208008
I0327 23:00:11.773507 139660419352320 logging_writer.py:48] [20] global_step=20, grad_norm=126.64293670654297, loss=29.218507766723633
I0327 23:00:12.685314 139660410959616 logging_writer.py:48] [21] global_step=21, grad_norm=138.560302734375, loss=28.196809768676758
I0327 23:00:13.590848 139660419352320 logging_writer.py:48] [22] global_step=22, grad_norm=133.79991149902344, loss=27.570425033569336
I0327 23:00:14.496951 139660410959616 logging_writer.py:48] [23] global_step=23, grad_norm=134.46533203125, loss=27.789840698242188
I0327 23:00:15.410084 139660419352320 logging_writer.py:48] [24] global_step=24, grad_norm=144.34010314941406, loss=27.324424743652344
I0327 23:00:16.318370 139660410959616 logging_writer.py:48] [25] global_step=25, grad_norm=137.02801513671875, loss=26.520112991333008
I0327 23:00:17.222104 139660419352320 logging_writer.py:48] [26] global_step=26, grad_norm=126.55525970458984, loss=27.259817123413086
I0327 23:00:18.116508 139660410959616 logging_writer.py:48] [27] global_step=27, grad_norm=140.54051208496094, loss=26.034603118896484
I0327 23:00:19.022745 139660419352320 logging_writer.py:48] [28] global_step=28, grad_norm=138.43142700195312, loss=25.807540893554688
I0327 23:00:19.923709 139660410959616 logging_writer.py:48] [29] global_step=29, grad_norm=124.21992492675781, loss=25.09288215637207
I0327 23:00:20.833923 139660419352320 logging_writer.py:48] [30] global_step=30, grad_norm=131.17877197265625, loss=24.883499145507812
I0327 23:00:21.742814 139660410959616 logging_writer.py:48] [31] global_step=31, grad_norm=120.64924621582031, loss=24.03354263305664
I0327 23:00:22.665982 139660419352320 logging_writer.py:48] [32] global_step=32, grad_norm=120.50570678710938, loss=24.27634620666504
I0327 23:00:23.580671 139660410959616 logging_writer.py:48] [33] global_step=33, grad_norm=118.72149658203125, loss=23.538969039916992
I0327 23:00:24.485472 139660419352320 logging_writer.py:48] [34] global_step=34, grad_norm=119.15721130371094, loss=22.590497970581055
I0327 23:00:25.385678 139660410959616 logging_writer.py:48] [35] global_step=35, grad_norm=108.5201187133789, loss=22.33323860168457
I0327 23:00:26.305272 139660419352320 logging_writer.py:48] [36] global_step=36, grad_norm=110.61378479003906, loss=21.06568145751953
I0327 23:00:27.217141 139660410959616 logging_writer.py:48] [37] global_step=37, grad_norm=99.2374267578125, loss=21.186038970947266
I0327 23:00:28.121134 139660419352320 logging_writer.py:48] [38] global_step=38, grad_norm=100.00007629394531, loss=20.557819366455078
I0327 23:00:29.021834 139660410959616 logging_writer.py:48] [39] global_step=39, grad_norm=90.59281158447266, loss=20.499889373779297
I0327 23:00:29.939640 139660419352320 logging_writer.py:48] [40] global_step=40, grad_norm=79.09313201904297, loss=20.45289421081543
I0327 23:00:30.851234 139660410959616 logging_writer.py:48] [41] global_step=41, grad_norm=69.62940979003906, loss=20.037540435791016
I0327 23:00:31.757593 139660419352320 logging_writer.py:48] [42] global_step=42, grad_norm=56.69398880004883, loss=19.97612190246582
I0327 23:00:32.666313 139660410959616 logging_writer.py:48] [43] global_step=43, grad_norm=54.02206802368164, loss=18.904685974121094
I0327 23:00:33.587160 139660419352320 logging_writer.py:48] [44] global_step=44, grad_norm=39.11474609375, loss=19.589717864990234
I0327 23:00:34.503543 139660410959616 logging_writer.py:48] [45] global_step=45, grad_norm=31.54407501220703, loss=19.2227783203125
I0327 23:00:35.413033 139660419352320 logging_writer.py:48] [46] global_step=46, grad_norm=20.316410064697266, loss=18.170326232910156
I0327 23:00:36.328735 139660410959616 logging_writer.py:48] [47] global_step=47, grad_norm=23.476123809814453, loss=18.17389678955078
I0327 23:00:37.245356 139660419352320 logging_writer.py:48] [48] global_step=48, grad_norm=32.449729919433594, loss=18.08683967590332
I0327 23:00:38.153506 139660410959616 logging_writer.py:48] [49] global_step=49, grad_norm=38.24641036987305, loss=18.03462791442871
I0327 23:00:39.062138 139660419352320 logging_writer.py:48] [50] global_step=50, grad_norm=32.554405212402344, loss=16.237565994262695
I0327 23:00:39.979629 139660410959616 logging_writer.py:48] [51] global_step=51, grad_norm=45.60055160522461, loss=17.351133346557617
I0327 23:00:40.896564 139660419352320 logging_writer.py:48] [52] global_step=52, grad_norm=45.43497085571289, loss=16.97971534729004
I0327 23:00:41.810659 139660410959616 logging_writer.py:48] [53] global_step=53, grad_norm=34.06060791015625, loss=15.712889671325684
I0327 23:00:42.718152 139660419352320 logging_writer.py:48] [54] global_step=54, grad_norm=37.79057312011719, loss=16.692535400390625
I0327 23:00:43.634423 139660410959616 logging_writer.py:48] [55] global_step=55, grad_norm=34.84282302856445, loss=16.077011108398438
I0327 23:00:44.552395 139660419352320 logging_writer.py:48] [56] global_step=56, grad_norm=42.75889587402344, loss=16.567121505737305
I0327 23:00:45.467576 139660410959616 logging_writer.py:48] [57] global_step=57, grad_norm=28.79339599609375, loss=15.695183753967285
I0327 23:00:46.375970 139660419352320 logging_writer.py:48] [58] global_step=58, grad_norm=32.03135681152344, loss=16.29664421081543
I0327 23:00:47.289005 139660410959616 logging_writer.py:48] [59] global_step=59, grad_norm=28.731597900390625, loss=16.593603134155273
I0327 23:00:48.199958 139660419352320 logging_writer.py:48] [60] global_step=60, grad_norm=26.0711612701416, loss=15.197480201721191
I0327 23:00:49.114579 139660410959616 logging_writer.py:48] [61] global_step=61, grad_norm=30.114547729492188, loss=14.634026527404785
I0327 23:00:50.024156 139660419352320 logging_writer.py:48] [62] global_step=62, grad_norm=31.221179962158203, loss=15.934972763061523
I0327 23:00:50.940698 139660410959616 logging_writer.py:48] [63] global_step=63, grad_norm=34.701026916503906, loss=15.308046340942383
I0327 23:00:51.852995 139660419352320 logging_writer.py:48] [64] global_step=64, grad_norm=35.15087890625, loss=14.991693496704102
I0327 23:00:52.770650 139660410959616 logging_writer.py:48] [65] global_step=65, grad_norm=43.463478088378906, loss=14.397014617919922
I0327 23:00:53.684280 139660419352320 logging_writer.py:48] [66] global_step=66, grad_norm=26.504077911376953, loss=15.413823127746582
I0327 23:00:54.597287 139660410959616 logging_writer.py:48] [67] global_step=67, grad_norm=24.626489639282227, loss=15.096381187438965
I0327 23:00:55.512176 139660419352320 logging_writer.py:48] [68] global_step=68, grad_norm=20.004728317260742, loss=14.879350662231445
I0327 23:00:56.427704 139660410959616 logging_writer.py:48] [69] global_step=69, grad_norm=14.612202644348145, loss=14.236649513244629
I0327 23:00:57.338787 139660419352320 logging_writer.py:48] [70] global_step=70, grad_norm=20.081560134887695, loss=14.33501148223877
I0327 23:00:58.257827 139660410959616 logging_writer.py:48] [71] global_step=71, grad_norm=14.116790771484375, loss=14.315510749816895
I0327 23:00:59.166055 139660419352320 logging_writer.py:48] [72] global_step=72, grad_norm=15.697107315063477, loss=13.987715721130371
I0327 23:01:00.086674 139660410959616 logging_writer.py:48] [73] global_step=73, grad_norm=23.040874481201172, loss=14.397920608520508
I0327 23:01:00.999635 139660419352320 logging_writer.py:48] [74] global_step=74, grad_norm=19.980833053588867, loss=14.289398193359375
I0327 23:01:01.938937 139660410959616 logging_writer.py:48] [75] global_step=75, grad_norm=16.215871810913086, loss=14.514498710632324
I0327 23:01:02.845578 139660419352320 logging_writer.py:48] [76] global_step=76, grad_norm=9.690832138061523, loss=13.963744163513184
I0327 23:01:03.769086 139660410959616 logging_writer.py:48] [77] global_step=77, grad_norm=23.21376609802246, loss=13.670784950256348
I0327 23:01:04.665910 139660419352320 logging_writer.py:48] [78] global_step=78, grad_norm=18.6062068939209, loss=13.20963191986084
I0327 23:01:05.583432 139660410959616 logging_writer.py:48] [79] global_step=79, grad_norm=13.997209548950195, loss=13.840429306030273
I0327 23:01:06.487084 139660419352320 logging_writer.py:48] [80] global_step=80, grad_norm=9.610503196716309, loss=13.857528686523438
I0327 23:01:07.396228 139660410959616 logging_writer.py:48] [81] global_step=81, grad_norm=7.9827494621276855, loss=13.615434646606445
I0327 23:01:08.294520 139660419352320 logging_writer.py:48] [82] global_step=82, grad_norm=22.567203521728516, loss=14.497868537902832
I0327 23:01:09.202556 139660410959616 logging_writer.py:48] [83] global_step=83, grad_norm=21.05522346496582, loss=14.10488510131836
I0327 23:01:10.105696 139660419352320 logging_writer.py:48] [84] global_step=84, grad_norm=12.99457836151123, loss=14.624382972717285
I0327 23:01:11.010768 139660410959616 logging_writer.py:48] [85] global_step=85, grad_norm=7.490337371826172, loss=14.03860855102539
I0327 23:01:11.906348 139660419352320 logging_writer.py:48] [86] global_step=86, grad_norm=22.928409576416016, loss=14.092964172363281
I0327 23:01:12.811933 139660410959616 logging_writer.py:48] [87] global_step=87, grad_norm=19.496749877929688, loss=13.644166946411133
I0327 23:01:13.714529 139660419352320 logging_writer.py:48] [88] global_step=88, grad_norm=13.859223365783691, loss=13.430745124816895
I0327 23:01:14.618680 139660410959616 logging_writer.py:48] [89] global_step=89, grad_norm=11.716197967529297, loss=13.547188758850098
I0327 23:01:15.518913 139660419352320 logging_writer.py:48] [90] global_step=90, grad_norm=7.7685956954956055, loss=13.169808387756348
I0327 23:01:16.421422 139660410959616 logging_writer.py:48] [91] global_step=91, grad_norm=20.136150360107422, loss=14.552375793457031
I0327 23:01:17.323523 139660419352320 logging_writer.py:48] [92] global_step=92, grad_norm=13.771768569946289, loss=13.420867919921875
I0327 23:01:18.227923 139660410959616 logging_writer.py:48] [93] global_step=93, grad_norm=5.676396369934082, loss=13.241496086120605
I0327 23:01:19.126933 139660419352320 logging_writer.py:48] [94] global_step=94, grad_norm=9.843690872192383, loss=12.719128608703613
I0327 23:01:20.038671 139660410959616 logging_writer.py:48] [95] global_step=95, grad_norm=10.723893165588379, loss=13.318680763244629
I0327 23:01:20.972875 139660419352320 logging_writer.py:48] [96] global_step=96, grad_norm=10.246420860290527, loss=13.096624374389648
I0327 23:01:21.873630 139660410959616 logging_writer.py:48] [97] global_step=97, grad_norm=9.014445304870605, loss=12.582462310791016
I0327 23:01:22.775799 139660419352320 logging_writer.py:48] [98] global_step=98, grad_norm=4.832730293273926, loss=12.560542106628418
I0327 23:01:23.699319 139660410959616 logging_writer.py:48] [99] global_step=99, grad_norm=4.90983247756958, loss=12.449219703674316
I0327 23:01:24.607827 139660419352320 logging_writer.py:48] [100] global_step=100, grad_norm=18.862112045288086, loss=13.435603141784668
I0327 23:06:38.726136 139660410959616 logging_writer.py:48] [500] global_step=500, grad_norm=0.9663858413696289, loss=5.798056602478027
I0327 23:13:58.910855 139660419352320 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.0371536016464233, loss=5.526208877563477
I0327 23:20:37.429159 139660461315840 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.4648431539535522, loss=5.3397111892700195
I0327 23:23:30.483465 139830572304192 spec.py:321] Evaluating on the training split.
I0327 23:24:14.210904 139830572304192 spec.py:333] Evaluating on the validation split.
I0327 23:25:02.380163 139830572304192 spec.py:349] Evaluating on the test split.
I0327 23:25:27.554662 139830572304192 submission_runner.py:422] Time since start: 1788.31s, 	Step: 1696, 	{'train/ctc_loss': Array(6.410073, dtype=float32), 'train/wer': 0.8939620193198904, 'validation/ctc_loss': Array(6.336784, dtype=float32), 'validation/wer': 0.8628459986290392, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.272086, dtype=float32), 'test/wer': 0.8588548331403734, 'test/num_examples': 2472, 'score': 1501.9804787635803, 'total_duration': 1788.3104214668274, 'accumulated_submission_time': 1501.9804787635803, 'accumulated_eval_time': 286.21294832229614, 'accumulated_logging_time': 0.05088329315185547}
I0327 23:25:27.592735 139660461315840 logging_writer.py:48] [1696] accumulated_eval_time=286.212948, accumulated_logging_time=0.050883, accumulated_submission_time=1501.980479, global_step=1696, preemption_count=0, score=1501.980479, test/ctc_loss=6.272086143493652, test/num_examples=2472, test/wer=0.858855, total_duration=1788.310421, train/ctc_loss=6.4100728034973145, train/wer=0.893962, validation/ctc_loss=6.3367838859558105, validation/num_examples=5348, validation/wer=0.862846
I0327 23:29:26.471509 139660452923136 logging_writer.py:48] [2000] global_step=2000, grad_norm=1.482446312904358, loss=3.811957597732544
I0327 23:36:11.218075 139660461315840 logging_writer.py:48] [2500] global_step=2500, grad_norm=1.2691164016723633, loss=3.20310115814209
I0327 23:43:42.978291 139660452923136 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.9792738556861877, loss=2.8346545696258545
I0327 23:49:28.169683 139830572304192 spec.py:321] Evaluating on the training split.
I0327 23:50:21.305931 139830572304192 spec.py:333] Evaluating on the validation split.
I0327 23:51:13.773401 139830572304192 spec.py:349] Evaluating on the test split.
I0327 23:51:40.055832 139830572304192 submission_runner.py:422] Time since start: 3360.81s, 	Step: 3425, 	{'train/ctc_loss': Array(2.6952374, dtype=float32), 'train/wer': 0.6571664491383213, 'validation/ctc_loss': Array(2.9338875, dtype=float32), 'validation/wer': 0.6544406576749664, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.6769202, dtype=float32), 'test/wer': 0.6290089980297767, 'test/num_examples': 2472, 'score': 2942.470513343811, 'total_duration': 3360.8145723342896, 'accumulated_submission_time': 2942.470513343811, 'accumulated_eval_time': 418.0957956314087, 'accumulated_logging_time': 0.10571146011352539}
I0327 23:51:40.090755 139660461315840 logging_writer.py:48] [3425] accumulated_eval_time=418.095796, accumulated_logging_time=0.105711, accumulated_submission_time=2942.470513, global_step=3425, preemption_count=0, score=2942.470513, test/ctc_loss=2.676920175552368, test/num_examples=2472, test/wer=0.629009, total_duration=3360.814572, train/ctc_loss=2.695237398147583, train/wer=0.657166, validation/ctc_loss=2.933887481689453, validation/num_examples=5348, validation/wer=0.654441
I0327 23:52:38.839702 139660452923136 logging_writer.py:48] [3500] global_step=3500, grad_norm=1.0152121782302856, loss=2.5967350006103516
I0327 23:59:49.125627 139660461315840 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.9965134263038635, loss=2.4045825004577637
I0328 00:06:44.005836 139660461315840 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.029732346534729, loss=2.2845821380615234
I0328 00:14:23.645718 139660452923136 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.5047937631607056, loss=2.1288230419158936
I0328 00:15:41.516079 139830572304192 spec.py:321] Evaluating on the training split.
I0328 00:16:38.331226 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 00:17:30.231606 139830572304192 spec.py:349] Evaluating on the test split.
I0328 00:17:56.829033 139830572304192 submission_runner.py:422] Time since start: 4937.59s, 	Step: 5086, 	{'train/ctc_loss': Array(0.8473239, dtype=float32), 'train/wer': 0.287279071851395, 'validation/ctc_loss': Array(1.2079897, dtype=float32), 'validation/wer': 0.3485329754675266, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.91311014, dtype=float32), 'test/wer': 0.2880791339142445, 'test/num_examples': 2472, 'score': 4383.815589189529, 'total_duration': 4937.585582733154, 'accumulated_submission_time': 4383.815589189529, 'accumulated_eval_time': 553.4030690193176, 'accumulated_logging_time': 0.1536262035369873}
I0328 00:17:56.862374 139660461315840 logging_writer.py:48] [5086] accumulated_eval_time=553.403069, accumulated_logging_time=0.153626, accumulated_submission_time=4383.815589, global_step=5086, preemption_count=0, score=4383.815589, test/ctc_loss=0.9131101369857788, test/num_examples=2472, test/wer=0.288079, total_duration=4937.585583, train/ctc_loss=0.8473238945007324, train/wer=0.287279, validation/ctc_loss=1.2079896926879883, validation/num_examples=5348, validation/wer=0.348533
I0328 00:23:30.579591 139660461315840 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.6911646127700806, loss=2.01188588142395
I0328 00:31:07.810508 139660452923136 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.9815291166305542, loss=1.9583250284194946
I0328 00:38:14.350749 139660461315840 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.8634225726127625, loss=1.891937255859375
I0328 00:41:57.263362 139830572304192 spec.py:321] Evaluating on the training split.
I0328 00:42:51.499702 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 00:43:44.006013 139830572304192 spec.py:349] Evaluating on the test split.
I0328 00:44:11.151159 139830572304192 submission_runner.py:422] Time since start: 6511.91s, 	Step: 6753, 	{'train/ctc_loss': Array(0.578787, dtype=float32), 'train/wer': 0.19471876341777586, 'validation/ctc_loss': Array(0.9009755, dtype=float32), 'validation/wer': 0.26286723886577135, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6343018, dtype=float32), 'test/wer': 0.20654845327321106, 'test/num_examples': 2472, 'score': 5824.133357048035, 'total_duration': 6511.907742738724, 'accumulated_submission_time': 5824.133357048035, 'accumulated_eval_time': 687.2851898670197, 'accumulated_logging_time': 0.20379209518432617}
I0328 00:44:11.189675 139660461315840 logging_writer.py:48] [6753] accumulated_eval_time=687.285190, accumulated_logging_time=0.203792, accumulated_submission_time=5824.133357, global_step=6753, preemption_count=0, score=5824.133357, test/ctc_loss=0.6343017816543579, test/num_examples=2472, test/wer=0.206548, total_duration=6511.907743, train/ctc_loss=0.5787870287895203, train/wer=0.194719, validation/ctc_loss=0.9009755253791809, validation/num_examples=5348, validation/wer=0.262867
I0328 00:47:26.380726 139660452923136 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.8938352465629578, loss=1.8307931423187256
I0328 00:54:28.620083 139660461315840 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.8904273509979248, loss=1.8652527332305908
I0328 01:02:03.365883 139660452923136 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.9340442419052124, loss=1.8126674890518188
I0328 01:08:11.667165 139830572304192 spec.py:321] Evaluating on the training split.
I0328 01:09:04.787260 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 01:09:56.289925 139830572304192 spec.py:349] Evaluating on the test split.
I0328 01:10:23.269425 139830572304192 submission_runner.py:422] Time since start: 8084.03s, 	Step: 8422, 	{'train/ctc_loss': Array(0.4766765, dtype=float32), 'train/wer': 0.16644197525495227, 'validation/ctc_loss': Array(0.7910166, dtype=float32), 'validation/wer': 0.2347915077671684, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.52758694, dtype=float32), 'test/wer': 0.17581703329067902, 'test/num_examples': 2472, 'score': 7264.523436784744, 'total_duration': 8084.025124788284, 'accumulated_submission_time': 7264.523436784744, 'accumulated_eval_time': 818.8811542987823, 'accumulated_logging_time': 0.2602109909057617}
I0328 01:10:23.311694 139660461315840 logging_writer.py:48] [8422] accumulated_eval_time=818.881154, accumulated_logging_time=0.260211, accumulated_submission_time=7264.523437, global_step=8422, preemption_count=0, score=7264.523437, test/ctc_loss=0.5275869369506836, test/num_examples=2472, test/wer=0.175817, total_duration=8084.025125, train/ctc_loss=0.47667649388313293, train/wer=0.166442, validation/ctc_loss=0.7910165786743164, validation/num_examples=5348, validation/wer=0.234792
I0328 01:11:24.113289 139660452923136 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.6591472029685974, loss=1.8026385307312012
I0328 01:18:45.679310 139660461315840 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6864529252052307, loss=1.7184354066848755
I0328 01:26:06.356176 139660461315840 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.8479019999504089, loss=1.7532488107681274
I0328 01:33:34.829292 139660452923136 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.913636326789856, loss=1.6495832204818726
I0328 01:34:23.905328 139830572304192 spec.py:321] Evaluating on the training split.
I0328 01:35:17.416920 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 01:36:09.937788 139830572304192 spec.py:349] Evaluating on the test split.
I0328 01:36:36.061024 139830572304192 submission_runner.py:422] Time since start: 9656.82s, 	Step: 10054, 	{'train/ctc_loss': Array(0.44431105, dtype=float32), 'train/wer': 0.1535921387455922, 'validation/ctc_loss': Array(0.71349037, dtype=float32), 'validation/wer': 0.21470982940228042, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47078082, dtype=float32), 'test/wer': 0.1561148010480775, 'test/num_examples': 2472, 'score': 8705.031720638275, 'total_duration': 9656.81776881218, 'accumulated_submission_time': 8705.031720638275, 'accumulated_eval_time': 951.0313429832458, 'accumulated_logging_time': 0.32175588607788086}
I0328 01:36:36.097500 139660461315840 logging_writer.py:48] [10054] accumulated_eval_time=951.031343, accumulated_logging_time=0.321756, accumulated_submission_time=8705.031721, global_step=10054, preemption_count=0, score=8705.031721, test/ctc_loss=0.4707808196544647, test/num_examples=2472, test/wer=0.156115, total_duration=9656.817769, train/ctc_loss=0.4443110525608063, train/wer=0.153592, validation/ctc_loss=0.71349036693573, validation/num_examples=5348, validation/wer=0.214710
I0328 01:42:33.757280 139660461315840 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.6954727172851562, loss=1.649773120880127
I0328 01:50:02.570101 139660452923136 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.5870663523674011, loss=1.6240622997283936
I0328 01:57:32.607115 139660461315840 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6277055144309998, loss=1.5905721187591553
I0328 02:00:36.738540 139830572304192 spec.py:321] Evaluating on the training split.
I0328 02:01:29.920031 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 02:02:22.303723 139830572304192 spec.py:349] Evaluating on the test split.
I0328 02:02:48.966581 139830572304192 submission_runner.py:422] Time since start: 11229.72s, 	Step: 11727, 	{'train/ctc_loss': Array(0.3836643, dtype=float32), 'train/wer': 0.13669623295684172, 'validation/ctc_loss': Array(0.67822397, dtype=float32), 'validation/wer': 0.202506347934387, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4395763, dtype=float32), 'test/wer': 0.14764487234172202, 'test/num_examples': 2472, 'score': 10145.590273141861, 'total_duration': 11229.721556901932, 'accumulated_submission_time': 10145.590273141861, 'accumulated_eval_time': 1083.252146244049, 'accumulated_logging_time': 0.37372469902038574}
I0328 02:02:49.000719 139660461315840 logging_writer.py:48] [11727] accumulated_eval_time=1083.252146, accumulated_logging_time=0.373725, accumulated_submission_time=10145.590273, global_step=11727, preemption_count=0, score=10145.590273, test/ctc_loss=0.43957629799842834, test/num_examples=2472, test/wer=0.147645, total_duration=11229.721557, train/ctc_loss=0.3836643099784851, train/wer=0.136696, validation/ctc_loss=0.6782239675521851, validation/num_examples=5348, validation/wer=0.202506
I0328 02:06:33.105854 139660452923136 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.691405177116394, loss=1.5939652919769287
I0328 02:14:06.394193 139660461315840 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.8160220980644226, loss=1.5499448776245117
I0328 02:21:23.120412 139660452923136 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.6206692457199097, loss=1.5896649360656738
I0328 02:26:50.232583 139830572304192 spec.py:321] Evaluating on the training split.
I0328 02:27:44.610670 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 02:28:37.511723 139830572304192 spec.py:349] Evaluating on the test split.
I0328 02:29:03.799848 139830572304192 submission_runner.py:422] Time since start: 12804.56s, 	Step: 13357, 	{'train/ctc_loss': Array(0.3055984, dtype=float32), 'train/wer': 0.1128578486180088, 'validation/ctc_loss': Array(0.61342394, dtype=float32), 'validation/wer': 0.18492522471204997, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39000106, dtype=float32), 'test/wer': 0.13125342757906283, 'test/num_examples': 2472, 'score': 11586.739322423935, 'total_duration': 12804.55618262291, 'accumulated_submission_time': 11586.739322423935, 'accumulated_eval_time': 1216.8135154247284, 'accumulated_logging_time': 0.42533135414123535}
I0328 02:29:03.834367 139660461315840 logging_writer.py:48] [13357] accumulated_eval_time=1216.813515, accumulated_logging_time=0.425331, accumulated_submission_time=11586.739322, global_step=13357, preemption_count=0, score=11586.739322, test/ctc_loss=0.3900010585784912, test/num_examples=2472, test/wer=0.131253, total_duration=12804.556183, train/ctc_loss=0.3055984079837799, train/wer=0.112858, validation/ctc_loss=0.6134239435195923, validation/num_examples=5348, validation/wer=0.184925
I0328 02:30:59.204456 139660461315840 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.5994521975517273, loss=1.4888191223144531
I0328 02:38:14.796578 139660452923136 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.676763653755188, loss=1.5699045658111572
I0328 02:45:50.233332 139660461315840 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.8528773188591003, loss=1.4849905967712402
I0328 02:52:55.639827 139660452923136 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.5999657511711121, loss=1.455186367034912
I0328 02:53:04.196961 139830572304192 spec.py:321] Evaluating on the training split.
I0328 02:53:58.042548 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 02:54:49.990615 139830572304192 spec.py:349] Evaluating on the test split.
I0328 02:55:16.592857 139830572304192 submission_runner.py:422] Time since start: 14377.35s, 	Step: 15011, 	{'train/ctc_loss': Array(0.28416067, dtype=float32), 'train/wer': 0.10289994626773563, 'validation/ctc_loss': Array(0.5857413, dtype=float32), 'validation/wer': 0.17584985083561022, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3622261, dtype=float32), 'test/wer': 0.12172729673186684, 'test/num_examples': 2472, 'score': 13027.020214796066, 'total_duration': 14377.346850395203, 'accumulated_submission_time': 13027.020214796066, 'accumulated_eval_time': 1349.2012033462524, 'accumulated_logging_time': 0.474001407623291}
I0328 02:55:16.628109 139660461315840 logging_writer.py:48] [15011] accumulated_eval_time=1349.201203, accumulated_logging_time=0.474001, accumulated_submission_time=13027.020215, global_step=15011, preemption_count=0, score=13027.020215, test/ctc_loss=0.36222609877586365, test/num_examples=2472, test/wer=0.121727, total_duration=14377.346850, train/ctc_loss=0.28416067361831665, train/wer=0.102900, validation/ctc_loss=0.5857412815093994, validation/num_examples=5348, validation/wer=0.175850
I0328 03:02:12.822746 139660461315840 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6339918375015259, loss=1.4200881719589233
I0328 03:09:10.894636 139660452923136 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.5814478397369385, loss=1.425463080406189
I0328 03:16:57.970474 139660461315840 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.5791611671447754, loss=1.4393267631530762
I0328 03:19:17.135774 139830572304192 spec.py:321] Evaluating on the training split.
I0328 03:20:10.947752 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 03:21:02.596600 139830572304192 spec.py:349] Evaluating on the test split.
I0328 03:21:28.627725 139830572304192 submission_runner.py:422] Time since start: 15949.38s, 	Step: 16682, 	{'train/ctc_loss': Array(0.27041918, dtype=float32), 'train/wer': 0.10149785216687135, 'validation/ctc_loss': Array(0.5640194, dtype=float32), 'validation/wer': 0.16954536238740262, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34598127, dtype=float32), 'test/wer': 0.11721812605366319, 'test/num_examples': 2472, 'score': 14467.445353746414, 'total_duration': 15949.384074926376, 'accumulated_submission_time': 14467.445353746414, 'accumulated_eval_time': 1480.6872804164886, 'accumulated_logging_time': 0.5242908000946045}
I0328 03:21:28.664301 139660461315840 logging_writer.py:48] [16682] accumulated_eval_time=1480.687280, accumulated_logging_time=0.524291, accumulated_submission_time=14467.445354, global_step=16682, preemption_count=0, score=14467.445354, test/ctc_loss=0.34598127007484436, test/num_examples=2472, test/wer=0.117218, total_duration=15949.384075, train/ctc_loss=0.270419180393219, train/wer=0.101498, validation/ctc_loss=0.5640193819999695, validation/num_examples=5348, validation/wer=0.169545
I0328 03:25:42.543639 139660452923136 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.5870776176452637, loss=1.457401156425476
I0328 03:33:27.146052 139660461315840 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.5622258186340332, loss=1.4385199546813965
I0328 03:40:29.438119 139660461315840 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.6032423377037048, loss=1.4399913549423218
I0328 03:45:30.211046 139830572304192 spec.py:321] Evaluating on the training split.
I0328 03:46:24.836539 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 03:47:16.582682 139830572304192 spec.py:349] Evaluating on the test split.
I0328 03:47:42.503709 139830572304192 submission_runner.py:422] Time since start: 17523.26s, 	Step: 18324, 	{'train/ctc_loss': Array(0.27392852, dtype=float32), 'train/wer': 0.0977127898675269, 'validation/ctc_loss': Array(0.5390841, dtype=float32), 'validation/wer': 0.1617830213271286, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3257008, dtype=float32), 'test/wer': 0.11067779741230475, 'test/num_examples': 2472, 'score': 15908.906670331955, 'total_duration': 17523.260184288025, 'accumulated_submission_time': 15908.906670331955, 'accumulated_eval_time': 1612.9742171764374, 'accumulated_logging_time': 0.5788280963897705}
I0328 03:47:42.540240 139660461315840 logging_writer.py:48] [18324] accumulated_eval_time=1612.974217, accumulated_logging_time=0.578828, accumulated_submission_time=15908.906670, global_step=18324, preemption_count=0, score=15908.906670, test/ctc_loss=0.3257007896900177, test/num_examples=2472, test/wer=0.110678, total_duration=17523.260184, train/ctc_loss=0.27392852306365967, train/wer=0.097713, validation/ctc_loss=0.5390840768814087, validation/num_examples=5348, validation/wer=0.161783
I0328 03:49:59.203086 139660452923136 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.5447486639022827, loss=1.3726186752319336
I0328 03:56:48.900966 139660461315840 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.9788918495178223, loss=1.4180828332901
I0328 04:04:35.115366 139660452923136 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.7448723316192627, loss=1.395076036453247
I0328 04:11:38.547855 139660461315840 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.7709578275680542, loss=1.3807885646820068
I0328 04:11:42.700999 139830572304192 spec.py:321] Evaluating on the training split.
I0328 04:12:36.766130 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 04:13:29.680097 139830572304192 spec.py:349] Evaluating on the test split.
I0328 04:13:56.877891 139830572304192 submission_runner.py:422] Time since start: 19097.63s, 	Step: 20006, 	{'train/ctc_loss': Array(0.2641379, dtype=float32), 'train/wer': 0.09598754847389664, 'validation/ctc_loss': Array(0.5339303, dtype=float32), 'validation/wer': 0.1605761896946233, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32188797, dtype=float32), 'test/wer': 0.1081997846972559, 'test/num_examples': 2472, 'score': 17348.98264670372, 'total_duration': 19097.63163137436, 'accumulated_submission_time': 17348.98264670372, 'accumulated_eval_time': 1747.1426239013672, 'accumulated_logging_time': 0.6316156387329102}
I0328 04:13:56.919244 139660461315840 logging_writer.py:48] [20006] accumulated_eval_time=1747.142624, accumulated_logging_time=0.631616, accumulated_submission_time=17348.982647, global_step=20006, preemption_count=0, score=17348.982647, test/ctc_loss=0.3218879699707031, test/num_examples=2472, test/wer=0.108200, total_duration=19097.631631, train/ctc_loss=0.2641378939151764, train/wer=0.095988, validation/ctc_loss=0.5339303016662598, validation/num_examples=5348, validation/wer=0.160576
I0328 04:21:03.880861 139660452923136 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.9382143020629883, loss=1.3590068817138672
I0328 04:27:58.995979 139660461315840 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.7586607933044434, loss=1.3292005062103271
I0328 04:35:44.367440 139660452923136 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.7860080599784851, loss=1.3549436330795288
I0328 04:37:57.298998 139830572304192 spec.py:321] Evaluating on the training split.
I0328 04:38:52.004296 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 04:39:44.561503 139830572304192 spec.py:349] Evaluating on the test split.
I0328 04:40:11.343665 139830572304192 submission_runner.py:422] Time since start: 20672.10s, 	Step: 21646, 	{'train/ctc_loss': Array(0.24032316, dtype=float32), 'train/wer': 0.09030764481942587, 'validation/ctc_loss': Array(0.51044405, dtype=float32), 'validation/wer': 0.15583575504214256, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30741346, dtype=float32), 'test/wer': 0.10627018463225885, 'test/num_examples': 2472, 'score': 18789.276383399963, 'total_duration': 20672.09979915619, 'accumulated_submission_time': 18789.276383399963, 'accumulated_eval_time': 1881.1813807487488, 'accumulated_logging_time': 0.6918046474456787}
I0328 04:40:11.378843 139660461315840 logging_writer.py:48] [21646] accumulated_eval_time=1881.181381, accumulated_logging_time=0.691805, accumulated_submission_time=18789.276383, global_step=21646, preemption_count=0, score=18789.276383, test/ctc_loss=0.3074134588241577, test/num_examples=2472, test/wer=0.106270, total_duration=20672.099799, train/ctc_loss=0.24032315611839294, train/wer=0.090308, validation/ctc_loss=0.5104440450668335, validation/num_examples=5348, validation/wer=0.155836
I0328 04:44:45.254606 139660452923136 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.5562446713447571, loss=1.3234111070632935
I0328 04:52:20.740826 139660461315840 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.6883074641227722, loss=1.3420848846435547
I0328 04:59:26.569839 139660461315840 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.5971941351890564, loss=1.3571584224700928
I0328 05:04:11.925021 139830572304192 spec.py:321] Evaluating on the training split.
I0328 05:05:06.334838 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 05:05:58.607354 139830572304192 spec.py:349] Evaluating on the test split.
I0328 05:06:24.911828 139830572304192 submission_runner.py:422] Time since start: 22245.67s, 	Step: 23306, 	{'train/ctc_loss': Array(0.22086065, dtype=float32), 'train/wer': 0.08272454710527427, 'validation/ctc_loss': Array(0.501632, dtype=float32), 'validation/wer': 0.15032294814485841, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29966813, dtype=float32), 'test/wer': 0.1018828834318445, 'test/num_examples': 2472, 'score': 20229.73876142502, 'total_duration': 22245.6676902771, 'accumulated_submission_time': 20229.73876142502, 'accumulated_eval_time': 2014.161872625351, 'accumulated_logging_time': 0.7436320781707764}
I0328 05:06:24.953939 139660461315840 logging_writer.py:48] [23306] accumulated_eval_time=2014.161873, accumulated_logging_time=0.743632, accumulated_submission_time=20229.738761, global_step=23306, preemption_count=0, score=20229.738761, test/ctc_loss=0.2996681332588196, test/num_examples=2472, test/wer=0.101883, total_duration=22245.667690, train/ctc_loss=0.22086064517498016, train/wer=0.082725, validation/ctc_loss=0.5016319751739502, validation/num_examples=5348, validation/wer=0.150323
I0328 05:08:55.674729 139660452923136 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.6483432054519653, loss=1.3311020135879517
I0328 05:16:02.272650 139660461315840 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.6115933060646057, loss=1.3072178363800049
I0328 05:23:37.685956 139660452923136 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.6416068077087402, loss=1.318056583404541
I0328 05:30:25.842488 139830572304192 spec.py:321] Evaluating on the training split.
I0328 05:31:19.895370 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 05:32:13.136189 139830572304192 spec.py:349] Evaluating on the test split.
I0328 05:32:39.743679 139830572304192 submission_runner.py:422] Time since start: 23820.50s, 	Step: 24979, 	{'train/ctc_loss': Array(0.20263907, dtype=float32), 'train/wer': 0.07717467852578315, 'validation/ctc_loss': Array(0.48382592, dtype=float32), 'validation/wer': 0.14499357965571508, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28374016, dtype=float32), 'test/wer': 0.09554567058680154, 'test/num_examples': 2472, 'score': 21670.540892362595, 'total_duration': 23820.499814510345, 'accumulated_submission_time': 21670.540892362595, 'accumulated_eval_time': 2148.05703496933, 'accumulated_logging_time': 0.8052024841308594}
I0328 05:32:39.780509 139660461315840 logging_writer.py:48] [24979] accumulated_eval_time=2148.057035, accumulated_logging_time=0.805202, accumulated_submission_time=21670.540892, global_step=24979, preemption_count=0, score=21670.540892, test/ctc_loss=0.28374016284942627, test/num_examples=2472, test/wer=0.095546, total_duration=23820.499815, train/ctc_loss=0.20263907313346863, train/wer=0.077175, validation/ctc_loss=0.4838259220123291, validation/num_examples=5348, validation/wer=0.144994
I0328 05:32:56.752567 139660452923136 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.5994676351547241, loss=1.279463291168213
I0328 05:39:59.550409 139660461315840 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.6888344287872314, loss=1.3827497959136963
I0328 05:47:12.123226 139660461315840 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.7745823264122009, loss=1.2991048097610474
I0328 05:54:46.083940 139660452923136 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.6045498847961426, loss=1.3085657358169556
I0328 05:56:40.337079 139830572304192 spec.py:321] Evaluating on the training split.
I0328 05:57:35.517884 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 05:58:27.736536 139830572304192 spec.py:349] Evaluating on the test split.
I0328 05:58:54.200573 139830572304192 submission_runner.py:422] Time since start: 25394.96s, 	Step: 26623, 	{'train/ctc_loss': Array(0.19714585, dtype=float32), 'train/wer': 0.07220443689846683, 'validation/ctc_loss': Array(0.47429636, dtype=float32), 'validation/wer': 0.14053312994197553, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2735317, dtype=float32), 'test/wer': 0.0924379989031747, 'test/num_examples': 2472, 'score': 23111.013807058334, 'total_duration': 25394.956656455994, 'accumulated_submission_time': 23111.013807058334, 'accumulated_eval_time': 2281.9143664836884, 'accumulated_logging_time': 0.8588449954986572}
I0328 05:58:54.235952 139660461315840 logging_writer.py:48] [26623] accumulated_eval_time=2281.914366, accumulated_logging_time=0.858845, accumulated_submission_time=23111.013807, global_step=26623, preemption_count=0, score=23111.013807, test/ctc_loss=0.2735317051410675, test/num_examples=2472, test/wer=0.092438, total_duration=25394.956656, train/ctc_loss=0.19714584946632385, train/wer=0.072204, validation/ctc_loss=0.47429636120796204, validation/num_examples=5348, validation/wer=0.140533
I0328 06:03:52.718860 139660461315840 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.6732746362686157, loss=1.2863999605178833
I0328 06:11:12.453154 139660452923136 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.7789961695671082, loss=1.2618756294250488
I0328 06:18:29.090607 139660461315840 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.5530169010162354, loss=1.3214055299758911
I0328 06:22:54.698896 139830572304192 spec.py:321] Evaluating on the training split.
I0328 06:23:48.297552 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 06:24:41.875915 139830572304192 spec.py:349] Evaluating on the test split.
I0328 06:25:08.221326 139830572304192 submission_runner.py:422] Time since start: 26968.98s, 	Step: 28312, 	{'train/ctc_loss': Array(0.20399657, dtype=float32), 'train/wer': 0.07608554342173687, 'validation/ctc_loss': Array(0.45795652, dtype=float32), 'validation/wer': 0.1373277851260415, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26537573, dtype=float32), 'test/wer': 0.09101618832896634, 'test/num_examples': 2472, 'score': 24551.39372587204, 'total_duration': 26968.977268457413, 'accumulated_submission_time': 24551.39372587204, 'accumulated_eval_time': 2415.4304938316345, 'accumulated_logging_time': 0.909921407699585}
I0328 06:25:08.267375 139660461315840 logging_writer.py:48] [28312] accumulated_eval_time=2415.430494, accumulated_logging_time=0.909921, accumulated_submission_time=24551.393726, global_step=28312, preemption_count=0, score=24551.393726, test/ctc_loss=0.2653757333755493, test/num_examples=2472, test/wer=0.091016, total_duration=26968.977268, train/ctc_loss=0.20399656891822815, train/wer=0.076086, validation/ctc_loss=0.4579565227031708, validation/num_examples=5348, validation/wer=0.137328
I0328 06:27:34.946985 139660452923136 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.6789758801460266, loss=1.2846245765686035
I0328 06:35:01.149878 139660461315840 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.6081745028495789, loss=1.2270170450210571
I0328 06:42:30.724273 139660452923136 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.8001906871795654, loss=1.2293636798858643
I0328 06:49:08.578613 139830572304192 spec.py:321] Evaluating on the training split.
I0328 06:50:01.941536 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 06:50:54.566832 139830572304192 spec.py:349] Evaluating on the test split.
I0328 06:51:21.540365 139830572304192 submission_runner.py:422] Time since start: 28542.30s, 	Step: 29935, 	{'train/ctc_loss': Array(0.18769386, dtype=float32), 'train/wer': 0.06888805690752527, 'validation/ctc_loss': Array(0.44672412, dtype=float32), 'validation/wer': 0.1347693020651303, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25843588, dtype=float32), 'test/wer': 0.08772571242865558, 'test/num_examples': 2472, 'score': 25991.61189365387, 'total_duration': 28542.296139240265, 'accumulated_submission_time': 25991.61189365387, 'accumulated_eval_time': 2548.385992050171, 'accumulated_logging_time': 0.9827554225921631}
I0328 06:51:21.578056 139660461315840 logging_writer.py:48] [29935] accumulated_eval_time=2548.385992, accumulated_logging_time=0.982755, accumulated_submission_time=25991.611894, global_step=29935, preemption_count=0, score=25991.611894, test/ctc_loss=0.2584358751773834, test/num_examples=2472, test/wer=0.087726, total_duration=28542.296139, train/ctc_loss=0.18769386410713196, train/wer=0.068888, validation/ctc_loss=0.4467241168022156, validation/num_examples=5348, validation/wer=0.134769
I0328 06:52:12.683914 139660452923136 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.784225583076477, loss=1.2575206756591797
I0328 06:59:12.633740 139660461315840 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.6711167097091675, loss=1.1851767301559448
I0328 07:06:54.090649 139660461315840 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.7378185391426086, loss=1.2064306735992432
I0328 07:14:04.661691 139660452923136 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.6965311169624329, loss=1.2098981142044067
I0328 07:15:22.673548 139830572304192 spec.py:321] Evaluating on the training split.
I0328 07:16:17.906982 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 07:17:10.842655 139830572304192 spec.py:349] Evaluating on the test split.
I0328 07:17:37.293313 139830572304192 submission_runner.py:422] Time since start: 30118.05s, 	Step: 31582, 	{'train/ctc_loss': Array(0.19159754, dtype=float32), 'train/wer': 0.07030908876092239, 'validation/ctc_loss': Array(0.43406945, dtype=float32), 'validation/wer': 0.1298164650453286, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2523516, dtype=float32), 'test/wer': 0.08528832287286982, 'test/num_examples': 2472, 'score': 27432.624835014343, 'total_duration': 30118.049394369125, 'accumulated_submission_time': 27432.624835014343, 'accumulated_eval_time': 2682.999590396881, 'accumulated_logging_time': 1.0350024700164795}
I0328 07:17:37.331691 139660461315840 logging_writer.py:48] [31582] accumulated_eval_time=2682.999590, accumulated_logging_time=1.035002, accumulated_submission_time=27432.624835, global_step=31582, preemption_count=0, score=27432.624835, test/ctc_loss=0.2523516118526459, test/num_examples=2472, test/wer=0.085288, total_duration=30118.049394, train/ctc_loss=0.19159753620624542, train/wer=0.070309, validation/ctc_loss=0.4340694546699524, validation/num_examples=5348, validation/wer=0.129816
I0328 07:23:28.362207 139660461315840 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.5816916823387146, loss=1.2020503282546997
I0328 07:30:41.735933 139660452923136 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.6737870573997498, loss=1.241336703300476
I0328 07:38:31.720103 139660461315840 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.8081759214401245, loss=1.2449556589126587
I0328 07:41:37.597119 139830572304192 spec.py:321] Evaluating on the training split.
I0328 07:42:31.198041 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 07:43:24.198605 139830572304192 spec.py:349] Evaluating on the test split.
I0328 07:43:50.767690 139830572304192 submission_runner.py:422] Time since start: 31691.53s, 	Step: 33237, 	{'train/ctc_loss': Array(0.18278466, dtype=float32), 'train/wer': 0.06397263339620832, 'validation/ctc_loss': Array(0.42616615, dtype=float32), 'validation/wer': 0.1277407146374195, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24534538, dtype=float32), 'test/wer': 0.08037292060203521, 'test/num_examples': 2472, 'score': 28872.809269428253, 'total_duration': 31691.52712202072, 'accumulated_submission_time': 28872.809269428253, 'accumulated_eval_time': 2816.167342185974, 'accumulated_logging_time': 1.0879781246185303}
I0328 07:43:50.802142 139660461315840 logging_writer.py:48] [33237] accumulated_eval_time=2816.167342, accumulated_logging_time=1.087978, accumulated_submission_time=28872.809269, global_step=33237, preemption_count=0, score=28872.809269, test/ctc_loss=0.24534538388252258, test/num_examples=2472, test/wer=0.080373, total_duration=31691.527122, train/ctc_loss=0.18278466165065765, train/wer=0.063973, validation/ctc_loss=0.4261661469936371, validation/num_examples=5348, validation/wer=0.127741
I0328 07:47:22.600086 139660452923136 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.7235015630722046, loss=1.183103322982788
I0328 07:55:13.633662 139660461315840 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.7197080850601196, loss=1.1987636089324951
I0328 08:02:11.307862 139660452923136 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.6879205107688904, loss=1.199332356452942
I0328 08:07:51.080954 139830572304192 spec.py:321] Evaluating on the training split.
I0328 08:08:46.797707 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 08:09:40.627536 139830572304192 spec.py:349] Evaluating on the test split.
I0328 08:10:07.283917 139830572304192 submission_runner.py:422] Time since start: 33268.04s, 	Step: 34860, 	{'train/ctc_loss': Array(0.13542211, dtype=float32), 'train/wer': 0.0516230538216332, 'validation/ctc_loss': Array(0.41311878, dtype=float32), 'validation/wer': 0.12366645104608166, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23395407, dtype=float32), 'test/wer': 0.07787459630735483, 'test/num_examples': 2472, 'score': 30313.009445905685, 'total_duration': 33268.0389714241, 'accumulated_submission_time': 30313.009445905685, 'accumulated_eval_time': 2952.363121509552, 'accumulated_logging_time': 1.135054588317871}
I0328 08:10:07.326202 139660461315840 logging_writer.py:48] [34860] accumulated_eval_time=2952.363122, accumulated_logging_time=1.135055, accumulated_submission_time=30313.009446, global_step=34860, preemption_count=0, score=30313.009446, test/ctc_loss=0.2339540719985962, test/num_examples=2472, test/wer=0.077875, total_duration=33268.038971, train/ctc_loss=0.13542211055755615, train/wer=0.051623, validation/ctc_loss=0.41311877965927124, validation/num_examples=5348, validation/wer=0.123666
I0328 08:11:56.185339 139660452923136 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.6632118821144104, loss=1.1839451789855957
I0328 08:19:04.769307 139660461315840 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.7644464373588562, loss=1.197114109992981
I0328 08:26:51.220884 139660452923136 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.6227802038192749, loss=1.198266625404358
I0328 08:33:44.314038 139660461315840 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.8048165440559387, loss=1.197754979133606
I0328 08:34:07.429267 139830572304192 spec.py:321] Evaluating on the training split.
I0328 08:35:01.699215 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 08:35:53.224032 139830572304192 spec.py:349] Evaluating on the test split.
I0328 08:36:19.272361 139830572304192 submission_runner.py:422] Time since start: 34840.03s, 	Step: 36527, 	{'train/ctc_loss': Array(0.1507603, dtype=float32), 'train/wer': 0.0563451669447319, 'validation/ctc_loss': Array(0.40173313, dtype=float32), 'validation/wer': 0.12027766782200681, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22683525, dtype=float32), 'test/wer': 0.07344667194767737, 'test/num_examples': 2472, 'score': 31753.026800870895, 'total_duration': 34840.02878165245, 'accumulated_submission_time': 31753.026800870895, 'accumulated_eval_time': 3084.2003774642944, 'accumulated_logging_time': 1.1955199241638184}
I0328 08:36:19.310306 139660461315840 logging_writer.py:48] [36527] accumulated_eval_time=3084.200377, accumulated_logging_time=1.195520, accumulated_submission_time=31753.026801, global_step=36527, preemption_count=0, score=31753.026801, test/ctc_loss=0.2268352508544922, test/num_examples=2472, test/wer=0.073447, total_duration=34840.028782, train/ctc_loss=0.15076029300689697, train/wer=0.056345, validation/ctc_loss=0.4017331302165985, validation/num_examples=5348, validation/wer=0.120278
I0328 08:43:02.614209 139660452923136 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.7316736578941345, loss=1.1698869466781616
I0328 08:50:06.305030 139660461315840 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.7879729866981506, loss=1.15603506565094
I0328 08:57:48.344349 139660452923136 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.8194803595542908, loss=1.1618376970291138
I0328 09:00:19.428972 139830572304192 spec.py:321] Evaluating on the training split.
I0328 09:01:12.846803 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 09:02:04.741628 139830572304192 spec.py:349] Evaluating on the test split.
I0328 09:02:31.390056 139830572304192 submission_runner.py:422] Time since start: 36412.15s, 	Step: 38168, 	{'train/ctc_loss': Array(0.1802866, dtype=float32), 'train/wer': 0.0659622592385674, 'validation/ctc_loss': Array(0.39695808, dtype=float32), 'validation/wer': 0.11656062639389053, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22254269, dtype=float32), 'test/wer': 0.07407633091625536, 'test/num_examples': 2472, 'score': 33193.06160712242, 'total_duration': 36412.148929834366, 'accumulated_submission_time': 33193.06160712242, 'accumulated_eval_time': 3216.1582770347595, 'accumulated_logging_time': 1.2505128383636475}
I0328 09:02:31.420790 139660461315840 logging_writer.py:48] [38168] accumulated_eval_time=3216.158277, accumulated_logging_time=1.250513, accumulated_submission_time=33193.061607, global_step=38168, preemption_count=0, score=33193.061607, test/ctc_loss=0.2225426882505417, test/num_examples=2472, test/wer=0.074076, total_duration=36412.148930, train/ctc_loss=0.18028660118579865, train/wer=0.065962, validation/ctc_loss=0.3969580829143524, validation/num_examples=5348, validation/wer=0.116561
I0328 09:06:48.093166 139660452923136 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.7130223512649536, loss=1.1393415927886963
I0328 09:14:31.480933 139660461315840 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.6896810531616211, loss=1.1379057168960571
I0328 09:21:36.762043 139660461315840 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.71230548620224, loss=1.1558427810668945
I0328 09:26:31.945306 139830572304192 spec.py:321] Evaluating on the training split.
I0328 09:27:24.973309 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 09:28:17.576472 139830572304192 spec.py:349] Evaluating on the test split.
I0328 09:28:43.642552 139830572304192 submission_runner.py:422] Time since start: 37984.40s, 	Step: 39824, 	{'train/ctc_loss': Array(0.17747895, dtype=float32), 'train/wer': 0.06445094566912961, 'validation/ctc_loss': Array(0.38370577, dtype=float32), 'validation/wer': 0.11201328480261062, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21735923, dtype=float32), 'test/wer': 0.07090772449373388, 'test/num_examples': 2472, 'score': 34633.50564241409, 'total_duration': 37984.39910316467, 'accumulated_submission_time': 34633.50564241409, 'accumulated_eval_time': 3347.8498339653015, 'accumulated_logging_time': 1.2941467761993408}
I0328 09:28:43.686891 139660461315840 logging_writer.py:48] [39824] accumulated_eval_time=3347.849834, accumulated_logging_time=1.294147, accumulated_submission_time=34633.505642, global_step=39824, preemption_count=0, score=34633.505642, test/ctc_loss=0.21735922992229462, test/num_examples=2472, test/wer=0.070908, total_duration=37984.399103, train/ctc_loss=0.17747895419597626, train/wer=0.064451, validation/ctc_loss=0.3837057650089264, validation/num_examples=5348, validation/wer=0.112013
I0328 09:31:02.446690 139660452923136 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.7069605588912964, loss=1.0970549583435059
I0328 09:38:04.053249 139660461315840 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.0361683368682861, loss=1.1658354997634888
I0328 09:45:47.143164 139660452923136 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.6868339776992798, loss=1.1112439632415771
I0328 09:52:44.712890 139830572304192 spec.py:321] Evaluating on the training split.
I0328 09:53:35.992565 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 09:54:27.894109 139830572304192 spec.py:349] Evaluating on the test split.
I0328 09:54:54.439008 139830572304192 submission_runner.py:422] Time since start: 39555.20s, 	Step: 41487, 	{'train/ctc_loss': Array(0.19878934, dtype=float32), 'train/wer': 0.07492205368789887, 'validation/ctc_loss': Array(0.37435403, dtype=float32), 'validation/wer': 0.10956100292535988, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21286099, dtype=float32), 'test/wer': 0.06930310970284159, 'test/num_examples': 2472, 'score': 36074.44389986992, 'total_duration': 39555.19847750664, 'accumulated_submission_time': 36074.44389986992, 'accumulated_eval_time': 3477.573362827301, 'accumulated_logging_time': 1.3564932346343994}
I0328 09:54:54.471894 139660461315840 logging_writer.py:48] [41487] accumulated_eval_time=3477.573363, accumulated_logging_time=1.356493, accumulated_submission_time=36074.443900, global_step=41487, preemption_count=0, score=36074.443900, test/ctc_loss=0.21286098659038544, test/num_examples=2472, test/wer=0.069303, total_duration=39555.198478, train/ctc_loss=0.1987893432378769, train/wer=0.074922, validation/ctc_loss=0.3743540346622467, validation/num_examples=5348, validation/wer=0.109561
I0328 09:55:05.321920 139660452923136 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.6722778677940369, loss=1.0815218687057495
I0328 10:02:21.829350 139660461315840 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.6372054815292358, loss=1.064835548400879
I0328 10:09:38.322695 139660461315840 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.9655597805976868, loss=1.0790095329284668
I0328 10:17:23.658708 139660452923136 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.6109561324119568, loss=1.1160475015640259
I0328 10:18:55.823354 139830572304192 spec.py:321] Evaluating on the training split.
I0328 10:19:47.736289 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 10:20:40.119745 139830572304192 spec.py:349] Evaluating on the test split.
I0328 10:21:07.414733 139830572304192 submission_runner.py:422] Time since start: 41128.17s, 	Step: 43100, 	{'train/ctc_loss': Array(0.17164013, dtype=float32), 'train/wer': 0.06192559273236816, 'validation/ctc_loss': Array(0.36618376, dtype=float32), 'validation/wer': 0.10800660378269307, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20301427, dtype=float32), 'test/wer': 0.06686572014705584, 'test/num_examples': 2472, 'score': 37515.71615982056, 'total_duration': 41128.17096161842, 'accumulated_submission_time': 37515.71615982056, 'accumulated_eval_time': 3609.1587464809418, 'accumulated_logging_time': 1.403339147567749}
I0328 10:21:07.451626 139660461315840 logging_writer.py:48] [43100] accumulated_eval_time=3609.158746, accumulated_logging_time=1.403339, accumulated_submission_time=37515.716160, global_step=43100, preemption_count=0, score=37515.716160, test/ctc_loss=0.20301426947116852, test/num_examples=2472, test/wer=0.066866, total_duration=41128.170962, train/ctc_loss=0.17164012789726257, train/wer=0.061926, validation/ctc_loss=0.3661837577819824, validation/num_examples=5348, validation/wer=0.108007
I0328 10:26:25.106281 139660461315840 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.9193975925445557, loss=1.0883182287216187
I0328 10:33:57.059104 139660452923136 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.7682289481163025, loss=1.0840089321136475
I0328 10:41:17.096210 139660461315840 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.6947020292282104, loss=1.0504138469696045
I0328 10:45:07.986476 139830572304192 spec.py:321] Evaluating on the training split.
I0328 10:46:00.582582 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 10:46:53.648804 139830572304192 spec.py:349] Evaluating on the test split.
I0328 10:47:21.067364 139830572304192 submission_runner.py:422] Time since start: 42701.82s, 	Step: 44769, 	{'train/ctc_loss': Array(0.15248455, dtype=float32), 'train/wer': 0.05740260318662109, 'validation/ctc_loss': Array(0.3566359, dtype=float32), 'validation/wer': 0.10574741496664318, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19947329, dtype=float32), 'test/wer': 0.06461113480795401, 'test/num_examples': 2472, 'score': 38956.16731572151, 'total_duration': 42701.82305312157, 'accumulated_submission_time': 38956.16731572151, 'accumulated_eval_time': 3742.2331025600433, 'accumulated_logging_time': 1.4563360214233398}
I0328 10:47:21.107264 139660461315840 logging_writer.py:48] [44769] accumulated_eval_time=3742.233103, accumulated_logging_time=1.456336, accumulated_submission_time=38956.167316, global_step=44769, preemption_count=0, score=38956.167316, test/ctc_loss=0.1994732916355133, test/num_examples=2472, test/wer=0.064611, total_duration=42701.823053, train/ctc_loss=0.15248455107212067, train/wer=0.057403, validation/ctc_loss=0.3566358983516693, validation/num_examples=5348, validation/wer=0.105747
I0328 10:50:27.012105 139660452923136 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.867955207824707, loss=1.0667996406555176
I0328 10:57:53.529077 139660461315840 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.7805885672569275, loss=1.032362699508667
I0328 11:05:28.509404 139660452923136 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.8637064099311829, loss=1.063272476196289
I0328 11:11:21.565665 139830572304192 spec.py:321] Evaluating on the training split.
I0328 11:12:15.240997 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 11:13:08.079653 139830572304192 spec.py:349] Evaluating on the test split.
I0328 11:13:34.619297 139830572304192 submission_runner.py:422] Time since start: 44275.38s, 	Step: 46372, 	{'train/ctc_loss': Array(0.12579452, dtype=float32), 'train/wer': 0.047471968156260536, 'validation/ctc_loss': Array(0.3557792, dtype=float32), 'validation/wer': 0.10374890178321441, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19177322, dtype=float32), 'test/wer': 0.0627018463225885, 'test/num_examples': 2472, 'score': 40396.53971219063, 'total_duration': 44275.375703811646, 'accumulated_submission_time': 40396.53971219063, 'accumulated_eval_time': 3875.2810966968536, 'accumulated_logging_time': 1.516535997390747}
I0328 11:13:34.655827 139660461315840 logging_writer.py:48] [46372] accumulated_eval_time=3875.281097, accumulated_logging_time=1.516536, accumulated_submission_time=40396.539712, global_step=46372, preemption_count=0, score=40396.539712, test/ctc_loss=0.1917732208967209, test/num_examples=2472, test/wer=0.062702, total_duration=44275.375704, train/ctc_loss=0.12579451501369476, train/wer=0.047472, validation/ctc_loss=0.3557792007923126, validation/num_examples=5348, validation/wer=0.103749
I0328 11:15:14.063053 139660452923136 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.7561455368995667, loss=1.0399068593978882
I0328 11:22:21.636896 139660461315840 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.6227839589118958, loss=0.9819564819335938
I0328 11:29:57.512266 139660461315840 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.829849898815155, loss=1.0330708026885986
I0328 11:37:14.074856 139660452923136 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.7370032668113708, loss=1.0227309465408325
I0328 11:37:34.734531 139830572304192 spec.py:321] Evaluating on the training split.
I0328 11:38:28.433212 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 11:39:20.233224 139830572304192 spec.py:349] Evaluating on the test split.
I0328 11:39:46.695216 139830572304192 submission_runner.py:422] Time since start: 45847.45s, 	Step: 48023, 	{'train/ctc_loss': Array(0.13559163, dtype=float32), 'train/wer': 0.05152965073020253, 'validation/ctc_loss': Array(0.34571385, dtype=float32), 'validation/wer': 0.1004083918244398, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18671605, dtype=float32), 'test/wer': 0.06073162309832836, 'test/num_examples': 2472, 'score': 41836.53591775894, 'total_duration': 45847.44951367378, 'accumulated_submission_time': 41836.53591775894, 'accumulated_eval_time': 4007.2338552474976, 'accumulated_logging_time': 1.5684778690338135}
I0328 11:39:46.739769 139660461315840 logging_writer.py:48] [48023] accumulated_eval_time=4007.233855, accumulated_logging_time=1.568478, accumulated_submission_time=41836.535918, global_step=48023, preemption_count=0, score=41836.535918, test/ctc_loss=0.18671604990959167, test/num_examples=2472, test/wer=0.060732, total_duration=45847.449514, train/ctc_loss=0.13559162616729736, train/wer=0.051530, validation/ctc_loss=0.34571385383605957, validation/num_examples=5348, validation/wer=0.100408
I0328 11:46:28.640774 139660461315840 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.1044892072677612, loss=1.0289132595062256
I0328 11:53:48.719570 139660452923136 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.9723376035690308, loss=1.07421875
I0328 12:01:37.367792 139660461315840 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.6665417551994324, loss=0.9887092709541321
I0328 12:03:47.405509 139830572304192 spec.py:321] Evaluating on the training split.
I0328 12:04:40.990909 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 12:05:34.085121 139830572304192 spec.py:349] Evaluating on the test split.
I0328 12:06:01.483155 139830572304192 submission_runner.py:422] Time since start: 47422.24s, 	Step: 49661, 	{'train/ctc_loss': Array(0.11946654, dtype=float32), 'train/wer': 0.04475712935921663, 'validation/ctc_loss': Array(0.34036654, dtype=float32), 'validation/wer': 0.09740579472276664, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18593042, dtype=float32), 'test/wer': 0.059167631466699164, 'test/num_examples': 2472, 'score': 43277.1169128418, 'total_duration': 47422.242717027664, 'accumulated_submission_time': 43277.1169128418, 'accumulated_eval_time': 4141.308844566345, 'accumulated_logging_time': 1.6302440166473389}
I0328 12:06:01.515466 139660461315840 logging_writer.py:48] [49661] accumulated_eval_time=4141.308845, accumulated_logging_time=1.630244, accumulated_submission_time=43277.116913, global_step=49661, preemption_count=0, score=43277.116913, test/ctc_loss=0.18593041598796844, test/num_examples=2472, test/wer=0.059168, total_duration=47422.242717, train/ctc_loss=0.11946653574705124, train/wer=0.044757, validation/ctc_loss=0.34036654233932495, validation/num_examples=5348, validation/wer=0.097406
I0328 12:10:44.020104 139660452923136 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.8103296756744385, loss=1.0187008380889893
I0328 12:18:37.632512 139660461315840 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.71805340051651, loss=1.0322421789169312
I0328 12:25:43.108747 139660452923136 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.7477239966392517, loss=1.03187894821167
I0328 12:30:01.734178 139830572304192 spec.py:321] Evaluating on the training split.
I0328 12:30:56.055693 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 12:31:50.320607 139830572304192 spec.py:349] Evaluating on the test split.
I0328 12:32:17.054081 139830572304192 submission_runner.py:422] Time since start: 48997.81s, 	Step: 51278, 	{'train/ctc_loss': Array(0.11783698, dtype=float32), 'train/wer': 0.04436476854815088, 'validation/ctc_loss': Array(0.33751705, dtype=float32), 'validation/wer': 0.09714511909014549, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18185702, dtype=float32), 'test/wer': 0.05943168200190929, 'test/num_examples': 2472, 'score': 44717.258138895035, 'total_duration': 48997.80965971947, 'accumulated_submission_time': 44717.258138895035, 'accumulated_eval_time': 4276.622076034546, 'accumulated_logging_time': 1.6737875938415527}
I0328 12:32:17.099309 139660461315840 logging_writer.py:48] [51278] accumulated_eval_time=4276.622076, accumulated_logging_time=1.673788, accumulated_submission_time=44717.258139, global_step=51278, preemption_count=0, score=44717.258139, test/ctc_loss=0.18185701966285706, test/num_examples=2472, test/wer=0.059432, total_duration=48997.809660, train/ctc_loss=0.11783698201179504, train/wer=0.044365, validation/ctc_loss=0.33751705288887024, validation/num_examples=5348, validation/wer=0.097145
I0328 12:35:15.340309 139660461315840 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.824131190776825, loss=1.0633056163787842
I0328 12:42:11.361814 139660452923136 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.7480672597885132, loss=1.0109325647354126
I0328 12:49:57.661294 139660461315840 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.8008458018302917, loss=1.0055705308914185
I0328 12:56:17.882678 139830572304192 spec.py:321] Evaluating on the training split.
I0328 12:57:11.071774 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 12:58:04.656550 139830572304192 spec.py:349] Evaluating on the test split.
I0328 12:58:31.020212 139830572304192 submission_runner.py:422] Time since start: 50571.78s, 	Step: 52957, 	{'train/ctc_loss': Array(0.11180287, dtype=float32), 'train/wer': 0.04214216701173223, 'validation/ctc_loss': Array(0.3338917, dtype=float32), 'validation/wer': 0.09554244668217847, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1787828, dtype=float32), 'test/wer': 0.05742083561838604, 'test/num_examples': 2472, 'score': 46157.956984996796, 'total_duration': 50571.77580285072, 'accumulated_submission_time': 46157.956984996796, 'accumulated_eval_time': 4409.753199100494, 'accumulated_logging_time': 1.733916997909546}
I0328 12:58:31.070716 139660461315840 logging_writer.py:48] [52957] accumulated_eval_time=4409.753199, accumulated_logging_time=1.733917, accumulated_submission_time=46157.956985, global_step=52957, preemption_count=0, score=46157.956985, test/ctc_loss=0.17878280580043793, test/num_examples=2472, test/wer=0.057421, total_duration=50571.775803, train/ctc_loss=0.11180286854505539, train/wer=0.042142, validation/ctc_loss=0.33389168977737427, validation/num_examples=5348, validation/wer=0.095542
I0328 12:59:05.255084 139660452923136 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.7671288251876831, loss=0.9698283672332764
I0328 13:06:33.137542 139660461315840 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.7768926024436951, loss=0.959316074848175
I0328 13:13:36.421464 139660461315840 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.9540071487426758, loss=0.9448221325874329
I0328 13:21:28.333114 139660452923136 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.9272425174713135, loss=1.0065689086914062
I0328 13:22:31.722718 139830572304192 spec.py:321] Evaluating on the training split.
I0328 13:23:23.616839 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 13:24:17.134737 139830572304192 spec.py:349] Evaluating on the test split.
I0328 13:24:43.665227 139830572304192 submission_runner.py:422] Time since start: 52144.42s, 	Step: 54568, 	{'train/ctc_loss': Array(0.11568967, dtype=float32), 'train/wer': 0.043542460767143985, 'validation/ctc_loss': Array(0.3303629, dtype=float32), 'validation/wer': 0.0955714106413586, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17728579, dtype=float32), 'test/wer': 0.05764426299433307, 'test/num_examples': 2472, 'score': 47598.514798402786, 'total_duration': 52144.421548604965, 'accumulated_submission_time': 47598.514798402786, 'accumulated_eval_time': 4541.6897876262665, 'accumulated_logging_time': 1.8134727478027344}
I0328 13:24:43.701131 139660461315840 logging_writer.py:48] [54568] accumulated_eval_time=4541.689788, accumulated_logging_time=1.813473, accumulated_submission_time=47598.514798, global_step=54568, preemption_count=0, score=47598.514798, test/ctc_loss=0.1772857904434204, test/num_examples=2472, test/wer=0.057644, total_duration=52144.421549, train/ctc_loss=0.11568966507911682, train/wer=0.043542, validation/ctc_loss=0.33036288619041443, validation/num_examples=5348, validation/wer=0.095571
I0328 13:30:41.180378 139660461315840 logging_writer.py:48] [55000] global_step=55000, grad_norm=1.0565153360366821, loss=1.0013244152069092
I0328 13:38:34.451796 139660452923136 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.0711456537246704, loss=0.9707751274108887
I0328 13:45:46.682282 139660461315840 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.3716907501220703, loss=1.0136520862579346
I0328 13:48:43.950655 139830572304192 spec.py:321] Evaluating on the training split.
I0328 13:49:37.878753 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 13:50:29.771906 139830572304192 spec.py:349] Evaluating on the test split.
I0328 13:50:56.281089 139830572304192 submission_runner.py:422] Time since start: 53717.04s, 	Step: 56192, 	{'train/ctc_loss': Array(0.12559758, dtype=float32), 'train/wer': 0.04641740816170945, 'validation/ctc_loss': Array(0.3269986, dtype=float32), 'validation/wer': 0.09448043484557382, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17591009, dtype=float32), 'test/wer': 0.056689618751650316, 'test/num_examples': 2472, 'score': 49038.68143892288, 'total_duration': 53717.037764787674, 'accumulated_submission_time': 49038.68143892288, 'accumulated_eval_time': 4674.014678001404, 'accumulated_logging_time': 1.8659427165985107}
I0328 13:50:56.319615 139660461315840 logging_writer.py:48] [56192] accumulated_eval_time=4674.014678, accumulated_logging_time=1.865943, accumulated_submission_time=49038.681439, global_step=56192, preemption_count=0, score=49038.681439, test/ctc_loss=0.175910085439682, test/num_examples=2472, test/wer=0.056690, total_duration=53717.037765, train/ctc_loss=0.12559758126735687, train/wer=0.046417, validation/ctc_loss=0.32699859142303467, validation/num_examples=5348, validation/wer=0.094480
I0328 13:55:11.896863 139660452923136 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.9315374493598938, loss=0.9376745820045471
I0328 14:02:22.261823 139660461315840 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.8777207732200623, loss=0.9651268124580383
I0328 14:10:16.918732 139660452923136 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.8500605821609497, loss=0.9774832725524902
I0328 14:14:56.566360 139830572304192 spec.py:321] Evaluating on the training split.
I0328 14:15:49.274108 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 14:16:42.962620 139830572304192 spec.py:349] Evaluating on the test split.
I0328 14:17:10.409238 139830572304192 submission_runner.py:422] Time since start: 55291.17s, 	Step: 57818, 	{'train/ctc_loss': Array(0.10578808, dtype=float32), 'train/wer': 0.04010581298899276, 'validation/ctc_loss': Array(0.3257869, dtype=float32), 'validation/wer': 0.09434526970273323, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17495489, dtype=float32), 'test/wer': 0.056811488229439606, 'test/num_examples': 2472, 'score': 50478.84465932846, 'total_duration': 55291.16542124748, 'accumulated_submission_time': 50478.84465932846, 'accumulated_eval_time': 4807.851681232452, 'accumulated_logging_time': 1.9215850830078125}
I0328 14:17:10.447640 139660461315840 logging_writer.py:48] [57818] accumulated_eval_time=4807.851681, accumulated_logging_time=1.921585, accumulated_submission_time=50478.844659, global_step=57818, preemption_count=0, score=50478.844659, test/ctc_loss=0.17495489120483398, test/num_examples=2472, test/wer=0.056811, total_duration=55291.165421, train/ctc_loss=0.10578808188438416, train/wer=0.040106, validation/ctc_loss=0.32578688859939575, validation/num_examples=5348, validation/wer=0.094345
I0328 14:19:32.530896 139660452923136 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.7133698463439941, loss=0.9935770630836487
I0328 14:27:11.044075 139660461315840 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.8623507618904114, loss=0.9608191847801208
I0328 14:34:25.701904 139660461315840 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.6761469841003418, loss=0.9652498364448547
I0328 14:41:10.742364 139830572304192 spec.py:321] Evaluating on the training split.
I0328 14:42:03.513680 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 14:42:56.740266 139830572304192 spec.py:349] Evaluating on the test split.
I0328 14:43:23.802652 139830572304192 submission_runner.py:422] Time since start: 56864.56s, 	Step: 59447, 	{'train/ctc_loss': Array(0.11182193, dtype=float32), 'train/wer': 0.04132627447400849, 'validation/ctc_loss': Array(0.32557967, dtype=float32), 'validation/wer': 0.09415217664153239, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17495407, dtype=float32), 'test/wer': 0.05662868401275567, 'test/num_examples': 2472, 'score': 51919.05823588371, 'total_duration': 56864.5586206913, 'accumulated_submission_time': 51919.05823588371, 'accumulated_eval_time': 4940.905741930008, 'accumulated_logging_time': 1.9755945205688477}
I0328 14:43:23.842721 139660461315840 logging_writer.py:48] [59447] accumulated_eval_time=4940.905742, accumulated_logging_time=1.975595, accumulated_submission_time=51919.058236, global_step=59447, preemption_count=0, score=51919.058236, test/ctc_loss=0.17495407164096832, test/num_examples=2472, test/wer=0.056629, total_duration=56864.558621, train/ctc_loss=0.11182193458080292, train/wer=0.041326, validation/ctc_loss=0.3255796730518341, validation/num_examples=5348, validation/wer=0.094152
I0328 14:44:05.420571 139660452923136 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.8890528678894043, loss=0.9848926663398743
I0328 14:51:03.319429 139660461315840 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.8604637384414673, loss=0.993407666683197
I0328 14:58:31.374370 139660452923136 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.6999542713165283, loss=0.9980889558792114
I0328 15:05:41.697890 139660461315840 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.7379418611526489, loss=0.9374591112136841
I0328 15:07:24.173769 139830572304192 spec.py:321] Evaluating on the training split.
I0328 15:08:16.951641 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 15:09:10.394187 139830572304192 spec.py:349] Evaluating on the test split.
I0328 15:09:36.876797 139830572304192 submission_runner.py:422] Time since start: 58437.64s, 	Step: 61125, 	{'train/ctc_loss': Array(0.10868546, dtype=float32), 'train/wer': 0.04067535597910994, 'validation/ctc_loss': Array(0.32558396, dtype=float32), 'validation/wer': 0.0941328673354123, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17496096, dtype=float32), 'test/wer': 0.05666930717201877, 'test/num_examples': 2472, 'score': 53359.30217576027, 'total_duration': 58437.636414051056, 'accumulated_submission_time': 53359.30217576027, 'accumulated_eval_time': 5073.60614490509, 'accumulated_logging_time': 2.0332281589508057}
I0328 15:09:36.910757 139660461315840 logging_writer.py:48] [61125] accumulated_eval_time=5073.606145, accumulated_logging_time=2.033228, accumulated_submission_time=53359.302176, global_step=61125, preemption_count=0, score=53359.302176, test/ctc_loss=0.17496095597743988, test/num_examples=2472, test/wer=0.056669, total_duration=58437.636414, train/ctc_loss=0.1086854636669159, train/wer=0.040675, validation/ctc_loss=0.32558396458625793, validation/num_examples=5348, validation/wer=0.094133
I0328 15:14:53.167218 139660452923136 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.7473440170288086, loss=0.9558125138282776
I0328 15:22:10.427839 139660461315840 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.8187441825866699, loss=0.9846770167350769
I0328 15:29:33.717212 139660452923136 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.154747486114502, loss=0.9089284539222717
I0328 15:33:37.378923 139830572304192 spec.py:321] Evaluating on the training split.
I0328 15:34:29.778444 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 15:35:23.502021 139830572304192 spec.py:349] Evaluating on the test split.
I0328 15:35:50.631969 139830572304192 submission_runner.py:422] Time since start: 60011.39s, 	Step: 62761, 	{'train/ctc_loss': Array(0.1148664, dtype=float32), 'train/wer': 0.042690321515398234, 'validation/ctc_loss': Array(0.32558548, dtype=float32), 'validation/wer': 0.09414252198847234, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17495435, dtype=float32), 'test/wer': 0.05664899559238722, 'test/num_examples': 2472, 'score': 54799.69149470329, 'total_duration': 60011.38698935509, 'accumulated_submission_time': 54799.69149470329, 'accumulated_eval_time': 5206.851982355118, 'accumulated_logging_time': 2.0803353786468506}
I0328 15:35:50.674558 139660461315840 logging_writer.py:48] [62761] accumulated_eval_time=5206.851982, accumulated_logging_time=2.080335, accumulated_submission_time=54799.691495, global_step=62761, preemption_count=0, score=54799.691495, test/ctc_loss=0.174954354763031, test/num_examples=2472, test/wer=0.056649, total_duration=60011.386989, train/ctc_loss=0.11486639827489853, train/wer=0.042690, validation/ctc_loss=0.3255854845046997, validation/num_examples=5348, validation/wer=0.094143
I0328 15:39:00.488493 139660461315840 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.7288156151771545, loss=0.9745436310768127
I0328 15:46:25.102970 139660452923136 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.7894469499588013, loss=0.955530047416687
I0328 15:53:50.922338 139660461315840 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.0783859491348267, loss=0.9897096753120422
I0328 15:59:50.756265 139830572304192 spec.py:321] Evaluating on the training split.
I0328 16:00:43.995880 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 16:01:36.903350 139830572304192 spec.py:349] Evaluating on the test split.
I0328 16:02:04.403268 139830572304192 submission_runner.py:422] Time since start: 61585.16s, 	Step: 64422, 	{'train/ctc_loss': Array(0.11367623, dtype=float32), 'train/wer': 0.042284385411775226, 'validation/ctc_loss': Array(0.3255867, dtype=float32), 'validation/wer': 0.09414252198847234, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17495622, dtype=float32), 'test/wer': 0.05664899559238722, 'test/num_examples': 2472, 'score': 56239.69022512436, 'total_duration': 61585.15878009796, 'accumulated_submission_time': 56239.69022512436, 'accumulated_eval_time': 5340.49228644371, 'accumulated_logging_time': 2.139230489730835}
I0328 16:02:04.458133 139660461315840 logging_writer.py:48] [64422] accumulated_eval_time=5340.492286, accumulated_logging_time=2.139230, accumulated_submission_time=56239.690225, global_step=64422, preemption_count=0, score=56239.690225, test/ctc_loss=0.17495621740818024, test/num_examples=2472, test/wer=0.056649, total_duration=61585.158780, train/ctc_loss=0.11367622762918472, train/wer=0.042284, validation/ctc_loss=0.3255867063999176, validation/num_examples=5348, validation/wer=0.094143
I0328 16:03:05.880672 139660452923136 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.8151850700378418, loss=0.9793145060539246
I0328 16:10:20.833711 139660461315840 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.9527367353439331, loss=0.9451651573181152
I0328 16:17:39.008783 139660452923136 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.9629155993461609, loss=1.0035159587860107
I0328 16:25:13.839079 139660461315840 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.883293628692627, loss=0.9958545565605164
I0328 16:26:04.785640 139830572304192 spec.py:321] Evaluating on the training split.
I0328 16:26:57.808232 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 16:27:50.950844 139830572304192 spec.py:349] Evaluating on the test split.
I0328 16:28:17.972398 139830572304192 submission_runner.py:422] Time since start: 63158.73s, 	Step: 66067, 	{'train/ctc_loss': Array(0.11457115, dtype=float32), 'train/wer': 0.04306365739459615, 'validation/ctc_loss': Array(0.32557157, dtype=float32), 'validation/wer': 0.09412321268235226, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17495905, dtype=float32), 'test/wer': 0.05664899559238722, 'test/num_examples': 2472, 'score': 57679.919651031494, 'total_duration': 63158.72807574272, 'accumulated_submission_time': 57679.919651031494, 'accumulated_eval_time': 5473.672495365143, 'accumulated_logging_time': 2.223335027694702}
I0328 16:28:18.011291 139660461315840 logging_writer.py:48] [66067] accumulated_eval_time=5473.672495, accumulated_logging_time=2.223335, accumulated_submission_time=57679.919651, global_step=66067, preemption_count=0, score=57679.919651, test/ctc_loss=0.17495904862880707, test/num_examples=2472, test/wer=0.056649, total_duration=63158.728076, train/ctc_loss=0.11457115411758423, train/wer=0.043064, validation/ctc_loss=0.32557156682014465, validation/num_examples=5348, validation/wer=0.094123
I0328 16:34:12.995889 139660452923136 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.7195574641227722, loss=0.9724903106689453
I0328 16:41:54.345188 139660461315840 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.7679446935653687, loss=0.9576243758201599
I0328 16:48:56.508934 139660452923136 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.8778637051582336, loss=0.9501112699508667
I0328 16:52:18.351385 139830572304192 spec.py:321] Evaluating on the training split.
I0328 16:53:13.440073 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 16:54:05.019128 139830572304192 spec.py:349] Evaluating on the test split.
I0328 16:54:31.507671 139830572304192 submission_runner.py:422] Time since start: 64732.26s, 	Step: 67719, 	{'train/ctc_loss': Array(0.09844289, dtype=float32), 'train/wer': 0.03731490956788204, 'validation/ctc_loss': Array(0.32558015, dtype=float32), 'validation/wer': 0.09411355802929222, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17495584, dtype=float32), 'test/wer': 0.056689618751650316, 'test/num_examples': 2472, 'score': 59120.17745113373, 'total_duration': 64732.26365303993, 'accumulated_submission_time': 59120.17745113373, 'accumulated_eval_time': 5606.822518587112, 'accumulated_logging_time': 2.2776741981506348}
I0328 16:54:31.546923 139660461315840 logging_writer.py:48] [67719] accumulated_eval_time=5606.822519, accumulated_logging_time=2.277674, accumulated_submission_time=59120.177451, global_step=67719, preemption_count=0, score=59120.177451, test/ctc_loss=0.1749558448791504, test/num_examples=2472, test/wer=0.056690, total_duration=64732.263653, train/ctc_loss=0.09844288975000381, train/wer=0.037315, validation/ctc_loss=0.3255801498889923, validation/num_examples=5348, validation/wer=0.094114
I0328 16:58:19.617052 139660461315840 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.9155957102775574, loss=0.9953460693359375
I0328 17:05:18.451193 139660452923136 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.725415825843811, loss=0.9710558652877808
I0328 17:13:04.571087 139660461315840 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.8579286932945251, loss=0.9826660752296448
I0328 17:18:32.040499 139830572304192 spec.py:321] Evaluating on the training split.
I0328 17:19:24.414160 139830572304192 spec.py:333] Evaluating on the validation split.
I0328 17:20:17.821916 139830572304192 spec.py:349] Evaluating on the test split.
I0328 17:20:45.226313 139830572304192 submission_runner.py:422] Time since start: 66305.98s, 	Step: 69398, 	{'train/ctc_loss': Array(0.0982953, dtype=float32), 'train/wer': 0.03653860313900075, 'validation/ctc_loss': Array(0.3255897, dtype=float32), 'validation/wer': 0.09412321268235226, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17495933, dtype=float32), 'test/wer': 0.05662868401275567, 'test/num_examples': 2472, 'score': 60560.585040569305, 'total_duration': 66305.981600523, 'accumulated_submission_time': 60560.585040569305, 'accumulated_eval_time': 5740.001482963562, 'accumulated_logging_time': 2.3341145515441895}
I0328 17:20:45.269255 139660461315840 logging_writer.py:48] [69398] accumulated_eval_time=5740.001483, accumulated_logging_time=2.334115, accumulated_submission_time=60560.585041, global_step=69398, preemption_count=0, score=60560.585041, test/ctc_loss=0.17495933175086975, test/num_examples=2472, test/wer=0.056629, total_duration=66305.981601, train/ctc_loss=0.09829530119895935, train/wer=0.036539, validation/ctc_loss=0.32558968663215637, validation/num_examples=5348, validation/wer=0.094123
I0328 17:22:04.568371 139660452923136 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.8014855980873108, loss=0.943210780620575
I0328 17:29:13.030404 139660461315840 logging_writer.py:48] [69975] global_step=69975, preemption_count=0, score=61068.287711
I0328 17:29:17.107065 139830572304192 submission_runner.py:596] Tuning trial 1/1
I0328 17:29:17.107342 139830572304192 submission_runner.py:597] Hyperparameters: Hyperparameters(learning_rate=0.000590120167916659, beta1=0.737199286155609, beta2=0.05919391544031072, warmup_steps=9999, weight_decay=0.14128519778326312)
I0328 17:29:17.128661 139830572304192 submission_runner.py:598] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(29.724083, dtype=float32), 'train/wer': 1.0418653561834954, 'validation/ctc_loss': Array(29.371702, dtype=float32), 'validation/wer': 1.245691611071956, 'validation/num_examples': 5348, 'test/ctc_loss': Array(29.384619, dtype=float32), 'test/wer': 1.243881136635996, 'test/num_examples': 2472, 'score': 61.5601065158844, 'total_duration': 230.70839548110962, 'accumulated_submission_time': 61.5601065158844, 'accumulated_eval_time': 169.1482331752777, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1696, {'train/ctc_loss': Array(6.410073, dtype=float32), 'train/wer': 0.8939620193198904, 'validation/ctc_loss': Array(6.336784, dtype=float32), 'validation/wer': 0.8628459986290392, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.272086, dtype=float32), 'test/wer': 0.8588548331403734, 'test/num_examples': 2472, 'score': 1501.9804787635803, 'total_duration': 1788.3104214668274, 'accumulated_submission_time': 1501.9804787635803, 'accumulated_eval_time': 286.21294832229614, 'accumulated_logging_time': 0.05088329315185547, 'global_step': 1696, 'preemption_count': 0}), (3425, {'train/ctc_loss': Array(2.6952374, dtype=float32), 'train/wer': 0.6571664491383213, 'validation/ctc_loss': Array(2.9338875, dtype=float32), 'validation/wer': 0.6544406576749664, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.6769202, dtype=float32), 'test/wer': 0.6290089980297767, 'test/num_examples': 2472, 'score': 2942.470513343811, 'total_duration': 3360.8145723342896, 'accumulated_submission_time': 2942.470513343811, 'accumulated_eval_time': 418.0957956314087, 'accumulated_logging_time': 0.10571146011352539, 'global_step': 3425, 'preemption_count': 0}), (5086, {'train/ctc_loss': Array(0.8473239, dtype=float32), 'train/wer': 0.287279071851395, 'validation/ctc_loss': Array(1.2079897, dtype=float32), 'validation/wer': 0.3485329754675266, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.91311014, dtype=float32), 'test/wer': 0.2880791339142445, 'test/num_examples': 2472, 'score': 4383.815589189529, 'total_duration': 4937.585582733154, 'accumulated_submission_time': 4383.815589189529, 'accumulated_eval_time': 553.4030690193176, 'accumulated_logging_time': 0.1536262035369873, 'global_step': 5086, 'preemption_count': 0}), (6753, {'train/ctc_loss': Array(0.578787, dtype=float32), 'train/wer': 0.19471876341777586, 'validation/ctc_loss': Array(0.9009755, dtype=float32), 'validation/wer': 0.26286723886577135, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6343018, dtype=float32), 'test/wer': 0.20654845327321106, 'test/num_examples': 2472, 'score': 5824.133357048035, 'total_duration': 6511.907742738724, 'accumulated_submission_time': 5824.133357048035, 'accumulated_eval_time': 687.2851898670197, 'accumulated_logging_time': 0.20379209518432617, 'global_step': 6753, 'preemption_count': 0}), (8422, {'train/ctc_loss': Array(0.4766765, dtype=float32), 'train/wer': 0.16644197525495227, 'validation/ctc_loss': Array(0.7910166, dtype=float32), 'validation/wer': 0.2347915077671684, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.52758694, dtype=float32), 'test/wer': 0.17581703329067902, 'test/num_examples': 2472, 'score': 7264.523436784744, 'total_duration': 8084.025124788284, 'accumulated_submission_time': 7264.523436784744, 'accumulated_eval_time': 818.8811542987823, 'accumulated_logging_time': 0.2602109909057617, 'global_step': 8422, 'preemption_count': 0}), (10054, {'train/ctc_loss': Array(0.44431105, dtype=float32), 'train/wer': 0.1535921387455922, 'validation/ctc_loss': Array(0.71349037, dtype=float32), 'validation/wer': 0.21470982940228042, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.47078082, dtype=float32), 'test/wer': 0.1561148010480775, 'test/num_examples': 2472, 'score': 8705.031720638275, 'total_duration': 9656.81776881218, 'accumulated_submission_time': 8705.031720638275, 'accumulated_eval_time': 951.0313429832458, 'accumulated_logging_time': 0.32175588607788086, 'global_step': 10054, 'preemption_count': 0}), (11727, {'train/ctc_loss': Array(0.3836643, dtype=float32), 'train/wer': 0.13669623295684172, 'validation/ctc_loss': Array(0.67822397, dtype=float32), 'validation/wer': 0.202506347934387, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4395763, dtype=float32), 'test/wer': 0.14764487234172202, 'test/num_examples': 2472, 'score': 10145.590273141861, 'total_duration': 11229.721556901932, 'accumulated_submission_time': 10145.590273141861, 'accumulated_eval_time': 1083.252146244049, 'accumulated_logging_time': 0.37372469902038574, 'global_step': 11727, 'preemption_count': 0}), (13357, {'train/ctc_loss': Array(0.3055984, dtype=float32), 'train/wer': 0.1128578486180088, 'validation/ctc_loss': Array(0.61342394, dtype=float32), 'validation/wer': 0.18492522471204997, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39000106, dtype=float32), 'test/wer': 0.13125342757906283, 'test/num_examples': 2472, 'score': 11586.739322423935, 'total_duration': 12804.55618262291, 'accumulated_submission_time': 11586.739322423935, 'accumulated_eval_time': 1216.8135154247284, 'accumulated_logging_time': 0.42533135414123535, 'global_step': 13357, 'preemption_count': 0}), (15011, {'train/ctc_loss': Array(0.28416067, dtype=float32), 'train/wer': 0.10289994626773563, 'validation/ctc_loss': Array(0.5857413, dtype=float32), 'validation/wer': 0.17584985083561022, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3622261, dtype=float32), 'test/wer': 0.12172729673186684, 'test/num_examples': 2472, 'score': 13027.020214796066, 'total_duration': 14377.346850395203, 'accumulated_submission_time': 13027.020214796066, 'accumulated_eval_time': 1349.2012033462524, 'accumulated_logging_time': 0.474001407623291, 'global_step': 15011, 'preemption_count': 0}), (16682, {'train/ctc_loss': Array(0.27041918, dtype=float32), 'train/wer': 0.10149785216687135, 'validation/ctc_loss': Array(0.5640194, dtype=float32), 'validation/wer': 0.16954536238740262, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34598127, dtype=float32), 'test/wer': 0.11721812605366319, 'test/num_examples': 2472, 'score': 14467.445353746414, 'total_duration': 15949.384074926376, 'accumulated_submission_time': 14467.445353746414, 'accumulated_eval_time': 1480.6872804164886, 'accumulated_logging_time': 0.5242908000946045, 'global_step': 16682, 'preemption_count': 0}), (18324, {'train/ctc_loss': Array(0.27392852, dtype=float32), 'train/wer': 0.0977127898675269, 'validation/ctc_loss': Array(0.5390841, dtype=float32), 'validation/wer': 0.1617830213271286, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3257008, dtype=float32), 'test/wer': 0.11067779741230475, 'test/num_examples': 2472, 'score': 15908.906670331955, 'total_duration': 17523.260184288025, 'accumulated_submission_time': 15908.906670331955, 'accumulated_eval_time': 1612.9742171764374, 'accumulated_logging_time': 0.5788280963897705, 'global_step': 18324, 'preemption_count': 0}), (20006, {'train/ctc_loss': Array(0.2641379, dtype=float32), 'train/wer': 0.09598754847389664, 'validation/ctc_loss': Array(0.5339303, dtype=float32), 'validation/wer': 0.1605761896946233, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32188797, dtype=float32), 'test/wer': 0.1081997846972559, 'test/num_examples': 2472, 'score': 17348.98264670372, 'total_duration': 19097.63163137436, 'accumulated_submission_time': 17348.98264670372, 'accumulated_eval_time': 1747.1426239013672, 'accumulated_logging_time': 0.6316156387329102, 'global_step': 20006, 'preemption_count': 0}), (21646, {'train/ctc_loss': Array(0.24032316, dtype=float32), 'train/wer': 0.09030764481942587, 'validation/ctc_loss': Array(0.51044405, dtype=float32), 'validation/wer': 0.15583575504214256, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30741346, dtype=float32), 'test/wer': 0.10627018463225885, 'test/num_examples': 2472, 'score': 18789.276383399963, 'total_duration': 20672.09979915619, 'accumulated_submission_time': 18789.276383399963, 'accumulated_eval_time': 1881.1813807487488, 'accumulated_logging_time': 0.6918046474456787, 'global_step': 21646, 'preemption_count': 0}), (23306, {'train/ctc_loss': Array(0.22086065, dtype=float32), 'train/wer': 0.08272454710527427, 'validation/ctc_loss': Array(0.501632, dtype=float32), 'validation/wer': 0.15032294814485841, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.29966813, dtype=float32), 'test/wer': 0.1018828834318445, 'test/num_examples': 2472, 'score': 20229.73876142502, 'total_duration': 22245.6676902771, 'accumulated_submission_time': 20229.73876142502, 'accumulated_eval_time': 2014.161872625351, 'accumulated_logging_time': 0.7436320781707764, 'global_step': 23306, 'preemption_count': 0}), (24979, {'train/ctc_loss': Array(0.20263907, dtype=float32), 'train/wer': 0.07717467852578315, 'validation/ctc_loss': Array(0.48382592, dtype=float32), 'validation/wer': 0.14499357965571508, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28374016, dtype=float32), 'test/wer': 0.09554567058680154, 'test/num_examples': 2472, 'score': 21670.540892362595, 'total_duration': 23820.499814510345, 'accumulated_submission_time': 21670.540892362595, 'accumulated_eval_time': 2148.05703496933, 'accumulated_logging_time': 0.8052024841308594, 'global_step': 24979, 'preemption_count': 0}), (26623, {'train/ctc_loss': Array(0.19714585, dtype=float32), 'train/wer': 0.07220443689846683, 'validation/ctc_loss': Array(0.47429636, dtype=float32), 'validation/wer': 0.14053312994197553, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2735317, dtype=float32), 'test/wer': 0.0924379989031747, 'test/num_examples': 2472, 'score': 23111.013807058334, 'total_duration': 25394.956656455994, 'accumulated_submission_time': 23111.013807058334, 'accumulated_eval_time': 2281.9143664836884, 'accumulated_logging_time': 0.8588449954986572, 'global_step': 26623, 'preemption_count': 0}), (28312, {'train/ctc_loss': Array(0.20399657, dtype=float32), 'train/wer': 0.07608554342173687, 'validation/ctc_loss': Array(0.45795652, dtype=float32), 'validation/wer': 0.1373277851260415, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26537573, dtype=float32), 'test/wer': 0.09101618832896634, 'test/num_examples': 2472, 'score': 24551.39372587204, 'total_duration': 26968.977268457413, 'accumulated_submission_time': 24551.39372587204, 'accumulated_eval_time': 2415.4304938316345, 'accumulated_logging_time': 0.909921407699585, 'global_step': 28312, 'preemption_count': 0}), (29935, {'train/ctc_loss': Array(0.18769386, dtype=float32), 'train/wer': 0.06888805690752527, 'validation/ctc_loss': Array(0.44672412, dtype=float32), 'validation/wer': 0.1347693020651303, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25843588, dtype=float32), 'test/wer': 0.08772571242865558, 'test/num_examples': 2472, 'score': 25991.61189365387, 'total_duration': 28542.296139240265, 'accumulated_submission_time': 25991.61189365387, 'accumulated_eval_time': 2548.385992050171, 'accumulated_logging_time': 0.9827554225921631, 'global_step': 29935, 'preemption_count': 0}), (31582, {'train/ctc_loss': Array(0.19159754, dtype=float32), 'train/wer': 0.07030908876092239, 'validation/ctc_loss': Array(0.43406945, dtype=float32), 'validation/wer': 0.1298164650453286, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2523516, dtype=float32), 'test/wer': 0.08528832287286982, 'test/num_examples': 2472, 'score': 27432.624835014343, 'total_duration': 30118.049394369125, 'accumulated_submission_time': 27432.624835014343, 'accumulated_eval_time': 2682.999590396881, 'accumulated_logging_time': 1.0350024700164795, 'global_step': 31582, 'preemption_count': 0}), (33237, {'train/ctc_loss': Array(0.18278466, dtype=float32), 'train/wer': 0.06397263339620832, 'validation/ctc_loss': Array(0.42616615, dtype=float32), 'validation/wer': 0.1277407146374195, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24534538, dtype=float32), 'test/wer': 0.08037292060203521, 'test/num_examples': 2472, 'score': 28872.809269428253, 'total_duration': 31691.52712202072, 'accumulated_submission_time': 28872.809269428253, 'accumulated_eval_time': 2816.167342185974, 'accumulated_logging_time': 1.0879781246185303, 'global_step': 33237, 'preemption_count': 0}), (34860, {'train/ctc_loss': Array(0.13542211, dtype=float32), 'train/wer': 0.0516230538216332, 'validation/ctc_loss': Array(0.41311878, dtype=float32), 'validation/wer': 0.12366645104608166, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23395407, dtype=float32), 'test/wer': 0.07787459630735483, 'test/num_examples': 2472, 'score': 30313.009445905685, 'total_duration': 33268.0389714241, 'accumulated_submission_time': 30313.009445905685, 'accumulated_eval_time': 2952.363121509552, 'accumulated_logging_time': 1.135054588317871, 'global_step': 34860, 'preemption_count': 0}), (36527, {'train/ctc_loss': Array(0.1507603, dtype=float32), 'train/wer': 0.0563451669447319, 'validation/ctc_loss': Array(0.40173313, dtype=float32), 'validation/wer': 0.12027766782200681, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22683525, dtype=float32), 'test/wer': 0.07344667194767737, 'test/num_examples': 2472, 'score': 31753.026800870895, 'total_duration': 34840.02878165245, 'accumulated_submission_time': 31753.026800870895, 'accumulated_eval_time': 3084.2003774642944, 'accumulated_logging_time': 1.1955199241638184, 'global_step': 36527, 'preemption_count': 0}), (38168, {'train/ctc_loss': Array(0.1802866, dtype=float32), 'train/wer': 0.0659622592385674, 'validation/ctc_loss': Array(0.39695808, dtype=float32), 'validation/wer': 0.11656062639389053, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22254269, dtype=float32), 'test/wer': 0.07407633091625536, 'test/num_examples': 2472, 'score': 33193.06160712242, 'total_duration': 36412.148929834366, 'accumulated_submission_time': 33193.06160712242, 'accumulated_eval_time': 3216.1582770347595, 'accumulated_logging_time': 1.2505128383636475, 'global_step': 38168, 'preemption_count': 0}), (39824, {'train/ctc_loss': Array(0.17747895, dtype=float32), 'train/wer': 0.06445094566912961, 'validation/ctc_loss': Array(0.38370577, dtype=float32), 'validation/wer': 0.11201328480261062, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21735923, dtype=float32), 'test/wer': 0.07090772449373388, 'test/num_examples': 2472, 'score': 34633.50564241409, 'total_duration': 37984.39910316467, 'accumulated_submission_time': 34633.50564241409, 'accumulated_eval_time': 3347.8498339653015, 'accumulated_logging_time': 1.2941467761993408, 'global_step': 39824, 'preemption_count': 0}), (41487, {'train/ctc_loss': Array(0.19878934, dtype=float32), 'train/wer': 0.07492205368789887, 'validation/ctc_loss': Array(0.37435403, dtype=float32), 'validation/wer': 0.10956100292535988, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21286099, dtype=float32), 'test/wer': 0.06930310970284159, 'test/num_examples': 2472, 'score': 36074.44389986992, 'total_duration': 39555.19847750664, 'accumulated_submission_time': 36074.44389986992, 'accumulated_eval_time': 3477.573362827301, 'accumulated_logging_time': 1.3564932346343994, 'global_step': 41487, 'preemption_count': 0}), (43100, {'train/ctc_loss': Array(0.17164013, dtype=float32), 'train/wer': 0.06192559273236816, 'validation/ctc_loss': Array(0.36618376, dtype=float32), 'validation/wer': 0.10800660378269307, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20301427, dtype=float32), 'test/wer': 0.06686572014705584, 'test/num_examples': 2472, 'score': 37515.71615982056, 'total_duration': 41128.17096161842, 'accumulated_submission_time': 37515.71615982056, 'accumulated_eval_time': 3609.1587464809418, 'accumulated_logging_time': 1.403339147567749, 'global_step': 43100, 'preemption_count': 0}), (44769, {'train/ctc_loss': Array(0.15248455, dtype=float32), 'train/wer': 0.05740260318662109, 'validation/ctc_loss': Array(0.3566359, dtype=float32), 'validation/wer': 0.10574741496664318, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19947329, dtype=float32), 'test/wer': 0.06461113480795401, 'test/num_examples': 2472, 'score': 38956.16731572151, 'total_duration': 42701.82305312157, 'accumulated_submission_time': 38956.16731572151, 'accumulated_eval_time': 3742.2331025600433, 'accumulated_logging_time': 1.4563360214233398, 'global_step': 44769, 'preemption_count': 0}), (46372, {'train/ctc_loss': Array(0.12579452, dtype=float32), 'train/wer': 0.047471968156260536, 'validation/ctc_loss': Array(0.3557792, dtype=float32), 'validation/wer': 0.10374890178321441, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.19177322, dtype=float32), 'test/wer': 0.0627018463225885, 'test/num_examples': 2472, 'score': 40396.53971219063, 'total_duration': 44275.375703811646, 'accumulated_submission_time': 40396.53971219063, 'accumulated_eval_time': 3875.2810966968536, 'accumulated_logging_time': 1.516535997390747, 'global_step': 46372, 'preemption_count': 0}), (48023, {'train/ctc_loss': Array(0.13559163, dtype=float32), 'train/wer': 0.05152965073020253, 'validation/ctc_loss': Array(0.34571385, dtype=float32), 'validation/wer': 0.1004083918244398, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18671605, dtype=float32), 'test/wer': 0.06073162309832836, 'test/num_examples': 2472, 'score': 41836.53591775894, 'total_duration': 45847.44951367378, 'accumulated_submission_time': 41836.53591775894, 'accumulated_eval_time': 4007.2338552474976, 'accumulated_logging_time': 1.5684778690338135, 'global_step': 48023, 'preemption_count': 0}), (49661, {'train/ctc_loss': Array(0.11946654, dtype=float32), 'train/wer': 0.04475712935921663, 'validation/ctc_loss': Array(0.34036654, dtype=float32), 'validation/wer': 0.09740579472276664, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18593042, dtype=float32), 'test/wer': 0.059167631466699164, 'test/num_examples': 2472, 'score': 43277.1169128418, 'total_duration': 47422.242717027664, 'accumulated_submission_time': 43277.1169128418, 'accumulated_eval_time': 4141.308844566345, 'accumulated_logging_time': 1.6302440166473389, 'global_step': 49661, 'preemption_count': 0}), (51278, {'train/ctc_loss': Array(0.11783698, dtype=float32), 'train/wer': 0.04436476854815088, 'validation/ctc_loss': Array(0.33751705, dtype=float32), 'validation/wer': 0.09714511909014549, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.18185702, dtype=float32), 'test/wer': 0.05943168200190929, 'test/num_examples': 2472, 'score': 44717.258138895035, 'total_duration': 48997.80965971947, 'accumulated_submission_time': 44717.258138895035, 'accumulated_eval_time': 4276.622076034546, 'accumulated_logging_time': 1.6737875938415527, 'global_step': 51278, 'preemption_count': 0}), (52957, {'train/ctc_loss': Array(0.11180287, dtype=float32), 'train/wer': 0.04214216701173223, 'validation/ctc_loss': Array(0.3338917, dtype=float32), 'validation/wer': 0.09554244668217847, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.1787828, dtype=float32), 'test/wer': 0.05742083561838604, 'test/num_examples': 2472, 'score': 46157.956984996796, 'total_duration': 50571.77580285072, 'accumulated_submission_time': 46157.956984996796, 'accumulated_eval_time': 4409.753199100494, 'accumulated_logging_time': 1.733916997909546, 'global_step': 52957, 'preemption_count': 0}), (54568, {'train/ctc_loss': Array(0.11568967, dtype=float32), 'train/wer': 0.043542460767143985, 'validation/ctc_loss': Array(0.3303629, dtype=float32), 'validation/wer': 0.0955714106413586, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17728579, dtype=float32), 'test/wer': 0.05764426299433307, 'test/num_examples': 2472, 'score': 47598.514798402786, 'total_duration': 52144.421548604965, 'accumulated_submission_time': 47598.514798402786, 'accumulated_eval_time': 4541.6897876262665, 'accumulated_logging_time': 1.8134727478027344, 'global_step': 54568, 'preemption_count': 0}), (56192, {'train/ctc_loss': Array(0.12559758, dtype=float32), 'train/wer': 0.04641740816170945, 'validation/ctc_loss': Array(0.3269986, dtype=float32), 'validation/wer': 0.09448043484557382, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17591009, dtype=float32), 'test/wer': 0.056689618751650316, 'test/num_examples': 2472, 'score': 49038.68143892288, 'total_duration': 53717.037764787674, 'accumulated_submission_time': 49038.68143892288, 'accumulated_eval_time': 4674.014678001404, 'accumulated_logging_time': 1.8659427165985107, 'global_step': 56192, 'preemption_count': 0}), (57818, {'train/ctc_loss': Array(0.10578808, dtype=float32), 'train/wer': 0.04010581298899276, 'validation/ctc_loss': Array(0.3257869, dtype=float32), 'validation/wer': 0.09434526970273323, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17495489, dtype=float32), 'test/wer': 0.056811488229439606, 'test/num_examples': 2472, 'score': 50478.84465932846, 'total_duration': 55291.16542124748, 'accumulated_submission_time': 50478.84465932846, 'accumulated_eval_time': 4807.851681232452, 'accumulated_logging_time': 1.9215850830078125, 'global_step': 57818, 'preemption_count': 0}), (59447, {'train/ctc_loss': Array(0.11182193, dtype=float32), 'train/wer': 0.04132627447400849, 'validation/ctc_loss': Array(0.32557967, dtype=float32), 'validation/wer': 0.09415217664153239, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17495407, dtype=float32), 'test/wer': 0.05662868401275567, 'test/num_examples': 2472, 'score': 51919.05823588371, 'total_duration': 56864.5586206913, 'accumulated_submission_time': 51919.05823588371, 'accumulated_eval_time': 4940.905741930008, 'accumulated_logging_time': 1.9755945205688477, 'global_step': 59447, 'preemption_count': 0}), (61125, {'train/ctc_loss': Array(0.10868546, dtype=float32), 'train/wer': 0.04067535597910994, 'validation/ctc_loss': Array(0.32558396, dtype=float32), 'validation/wer': 0.0941328673354123, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17496096, dtype=float32), 'test/wer': 0.05666930717201877, 'test/num_examples': 2472, 'score': 53359.30217576027, 'total_duration': 58437.636414051056, 'accumulated_submission_time': 53359.30217576027, 'accumulated_eval_time': 5073.60614490509, 'accumulated_logging_time': 2.0332281589508057, 'global_step': 61125, 'preemption_count': 0}), (62761, {'train/ctc_loss': Array(0.1148664, dtype=float32), 'train/wer': 0.042690321515398234, 'validation/ctc_loss': Array(0.32558548, dtype=float32), 'validation/wer': 0.09414252198847234, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17495435, dtype=float32), 'test/wer': 0.05664899559238722, 'test/num_examples': 2472, 'score': 54799.69149470329, 'total_duration': 60011.38698935509, 'accumulated_submission_time': 54799.69149470329, 'accumulated_eval_time': 5206.851982355118, 'accumulated_logging_time': 2.0803353786468506, 'global_step': 62761, 'preemption_count': 0}), (64422, {'train/ctc_loss': Array(0.11367623, dtype=float32), 'train/wer': 0.042284385411775226, 'validation/ctc_loss': Array(0.3255867, dtype=float32), 'validation/wer': 0.09414252198847234, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17495622, dtype=float32), 'test/wer': 0.05664899559238722, 'test/num_examples': 2472, 'score': 56239.69022512436, 'total_duration': 61585.15878009796, 'accumulated_submission_time': 56239.69022512436, 'accumulated_eval_time': 5340.49228644371, 'accumulated_logging_time': 2.139230489730835, 'global_step': 64422, 'preemption_count': 0}), (66067, {'train/ctc_loss': Array(0.11457115, dtype=float32), 'train/wer': 0.04306365739459615, 'validation/ctc_loss': Array(0.32557157, dtype=float32), 'validation/wer': 0.09412321268235226, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17495905, dtype=float32), 'test/wer': 0.05664899559238722, 'test/num_examples': 2472, 'score': 57679.919651031494, 'total_duration': 63158.72807574272, 'accumulated_submission_time': 57679.919651031494, 'accumulated_eval_time': 5473.672495365143, 'accumulated_logging_time': 2.223335027694702, 'global_step': 66067, 'preemption_count': 0}), (67719, {'train/ctc_loss': Array(0.09844289, dtype=float32), 'train/wer': 0.03731490956788204, 'validation/ctc_loss': Array(0.32558015, dtype=float32), 'validation/wer': 0.09411355802929222, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17495584, dtype=float32), 'test/wer': 0.056689618751650316, 'test/num_examples': 2472, 'score': 59120.17745113373, 'total_duration': 64732.26365303993, 'accumulated_submission_time': 59120.17745113373, 'accumulated_eval_time': 5606.822518587112, 'accumulated_logging_time': 2.2776741981506348, 'global_step': 67719, 'preemption_count': 0}), (69398, {'train/ctc_loss': Array(0.0982953, dtype=float32), 'train/wer': 0.03653860313900075, 'validation/ctc_loss': Array(0.3255897, dtype=float32), 'validation/wer': 0.09412321268235226, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.17495933, dtype=float32), 'test/wer': 0.05662868401275567, 'test/num_examples': 2472, 'score': 60560.585040569305, 'total_duration': 66305.981600523, 'accumulated_submission_time': 60560.585040569305, 'accumulated_eval_time': 5740.001482963562, 'accumulated_logging_time': 2.3341145515441895, 'global_step': 69398, 'preemption_count': 0})], 'global_step': 69975}
I0328 17:29:17.128854 139830572304192 submission_runner.py:599] Timing: 61068.28771138191
I0328 17:29:17.128913 139830572304192 submission_runner.py:601] Total number of evals: 43
I0328 17:29:17.128974 139830572304192 submission_runner.py:602] ====================
I0328 17:29:17.134262 139830572304192 submission_runner.py:686] Final librispeech_conformer_gelu score: 0
