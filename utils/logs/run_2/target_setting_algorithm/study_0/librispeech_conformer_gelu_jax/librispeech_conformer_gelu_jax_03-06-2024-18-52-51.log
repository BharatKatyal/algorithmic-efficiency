python3 submission_runner.py --framework=jax --workload=librispeech_conformer_gelu --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --rng_seed=2060265407 --max_global_steps=80000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab --tuning_ruleset=external --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_conformer_gelu/tuning_search_space.json --num_tuning_trials=1 2>&1 | tee -a /logs/librispeech_conformer_gelu_jax_03-06-2024-18-52-51.log
I0306 18:53:12.358784 140085866297152 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/librispeech_conformer_gelu_jax.
I0306 18:53:13.386398 140085866297152 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0306 18:53:13.387122 140085866297152 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0306 18:53:13.387253 140085866297152 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0306 18:53:13.392829 140085866297152 submission_runner.py:547] Using RNG seed 2060265407
I0306 18:53:14.511286 140085866297152 submission_runner.py:556] --- Tuning run 1/1 ---
I0306 18:53:14.511554 140085866297152 submission_runner.py:561] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/librispeech_conformer_gelu_jax/trial_1.
I0306 18:53:14.511748 140085866297152 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_gelu_jax/trial_1/hparams.json.
I0306 18:53:14.694684 140085866297152 submission_runner.py:206] Initializing dataset.
I0306 18:53:14.694892 140085866297152 submission_runner.py:213] Initializing model.
I0306 18:53:19.492327 140085866297152 submission_runner.py:255] Initializing optimizer.
I0306 18:53:20.700650 140085866297152 submission_runner.py:262] Initializing metrics bundle.
I0306 18:53:20.700865 140085866297152 submission_runner.py:280] Initializing checkpoint and logger.
I0306 18:53:20.702064 140085866297152 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/librispeech_conformer_gelu_jax/trial_1 with prefix checkpoint_
I0306 18:53:20.702205 140085866297152 submission_runner.py:300] Saving meta data to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_gelu_jax/trial_1/meta_data_0.json.
I0306 18:53:20.702398 140085866297152 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0306 18:53:20.702461 140085866297152 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0306 18:53:20.968703 140085866297152 logger_utils.py:220] Unable to record git information. Continuing without it.
I0306 18:53:21.218547 140085866297152 submission_runner.py:304] Saving flags to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_gelu_jax/trial_1/flags_0.json.
I0306 18:53:21.232203 140085866297152 submission_runner.py:314] Starting training loop.
I0306 18:53:21.518425 140085866297152 input_pipeline.py:20] Loading split = train-clean-100
I0306 18:53:21.556918 140085866297152 input_pipeline.py:20] Loading split = train-clean-360
I0306 18:53:21.943745 140085866297152 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0306 18:54:22.263328 139907653695232 logging_writer.py:48] [0] global_step=0, grad_norm=30.464458465576172, loss=32.4949951171875
I0306 18:54:22.303848 140085866297152 spec.py:321] Evaluating on the training split.
I0306 18:54:22.481261 140085866297152 input_pipeline.py:20] Loading split = train-clean-100
I0306 18:54:22.521049 140085866297152 input_pipeline.py:20] Loading split = train-clean-360
I0306 18:54:23.002373 140085866297152 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0306 18:55:36.645605 140085866297152 spec.py:333] Evaluating on the validation split.
I0306 18:55:36.767233 140085866297152 input_pipeline.py:20] Loading split = dev-clean
I0306 18:55:36.772501 140085866297152 input_pipeline.py:20] Loading split = dev-other
I0306 18:56:37.538516 140085866297152 spec.py:349] Evaluating on the test split.
I0306 18:56:37.652698 140085866297152 input_pipeline.py:20] Loading split = test-clean
I0306 18:57:13.190413 140085866297152 submission_runner.py:413] Time since start: 231.96s, 	Step: 1, 	{'train/ctc_loss': Array(31.74491, dtype=float32), 'train/wer': 1.220186160797951, 'validation/ctc_loss': Array(30.874546, dtype=float32), 'validation/wer': 1.1701632601832452, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.977095, dtype=float32), 'test/wer': 1.1826214124672476, 'test/num_examples': 2472, 'score': 61.07155728340149, 'total_duration': 231.955472946167, 'accumulated_submission_time': 61.07155728340149, 'accumulated_eval_time': 170.8838496208191, 'accumulated_logging_time': 0}
I0306 18:57:13.216470 139902410811136 logging_writer.py:48] [1] accumulated_eval_time=170.883850, accumulated_logging_time=0, accumulated_submission_time=61.071557, global_step=1, preemption_count=0, score=61.071557, test/ctc_loss=30.977094650268555, test/num_examples=2472, test/wer=1.182621, total_duration=231.955473, train/ctc_loss=31.744909286499023, train/wer=1.220186, validation/ctc_loss=30.87454605102539, validation/num_examples=5348, validation/wer=1.170163
I0306 18:57:37.120518 139914130708224 logging_writer.py:48] [1] global_step=1, grad_norm=36.64719772338867, loss=32.20951461791992
I0306 18:57:37.961438 139914139100928 logging_writer.py:48] [2] global_step=2, grad_norm=35.52029800415039, loss=32.65003967285156
I0306 18:57:38.798525 139914130708224 logging_writer.py:48] [3] global_step=3, grad_norm=35.147579193115234, loss=32.33356857299805
I0306 18:57:39.696212 139914139100928 logging_writer.py:48] [4] global_step=4, grad_norm=35.50776672363281, loss=32.58198928833008
I0306 18:57:40.591512 139914130708224 logging_writer.py:48] [5] global_step=5, grad_norm=35.2456169128418, loss=31.99024200439453
I0306 18:57:41.493374 139914139100928 logging_writer.py:48] [6] global_step=6, grad_norm=34.74673080444336, loss=32.360294342041016
I0306 18:57:42.388119 139914130708224 logging_writer.py:48] [7] global_step=7, grad_norm=35.54281997680664, loss=32.574745178222656
I0306 18:57:43.286574 139914139100928 logging_writer.py:48] [8] global_step=8, grad_norm=37.37941360473633, loss=32.265987396240234
I0306 18:57:44.182053 139914130708224 logging_writer.py:48] [9] global_step=9, grad_norm=39.18672561645508, loss=31.641220092773438
I0306 18:57:45.079685 139914139100928 logging_writer.py:48] [10] global_step=10, grad_norm=41.4542350769043, loss=31.315391540527344
I0306 18:57:45.987202 139914130708224 logging_writer.py:48] [11] global_step=11, grad_norm=45.317222595214844, loss=32.12017059326172
I0306 18:57:46.901473 139914139100928 logging_writer.py:48] [12] global_step=12, grad_norm=47.73670959472656, loss=31.386381149291992
I0306 18:57:47.802837 139914130708224 logging_writer.py:48] [13] global_step=13, grad_norm=49.055938720703125, loss=31.82465171813965
I0306 18:57:48.704072 139914139100928 logging_writer.py:48] [14] global_step=14, grad_norm=54.481666564941406, loss=31.657724380493164
I0306 18:57:49.600876 139914130708224 logging_writer.py:48] [15] global_step=15, grad_norm=65.37554168701172, loss=31.446565628051758
I0306 18:57:50.506413 139914139100928 logging_writer.py:48] [16] global_step=16, grad_norm=66.42589569091797, loss=31.488265991210938
I0306 18:57:51.408283 139914130708224 logging_writer.py:48] [17] global_step=17, grad_norm=69.40567779541016, loss=30.80262565612793
I0306 18:57:52.297331 139914139100928 logging_writer.py:48] [18] global_step=18, grad_norm=75.48394775390625, loss=31.0058536529541
I0306 18:57:53.195960 139914130708224 logging_writer.py:48] [19] global_step=19, grad_norm=77.64881896972656, loss=30.126176834106445
I0306 18:57:54.100826 139914139100928 logging_writer.py:48] [20] global_step=20, grad_norm=84.14006805419922, loss=29.734786987304688
I0306 18:57:54.999325 139914130708224 logging_writer.py:48] [21] global_step=21, grad_norm=83.5785903930664, loss=29.41642189025879
I0306 18:57:55.904351 139914139100928 logging_writer.py:48] [22] global_step=22, grad_norm=71.76046752929688, loss=28.577232360839844
I0306 18:57:56.806806 139914130708224 logging_writer.py:48] [23] global_step=23, grad_norm=77.5221176147461, loss=29.101879119873047
I0306 18:57:57.711848 139914139100928 logging_writer.py:48] [24] global_step=24, grad_norm=88.67713928222656, loss=27.821151733398438
I0306 18:57:58.615448 139914130708224 logging_writer.py:48] [25] global_step=25, grad_norm=82.57574462890625, loss=28.11298370361328
I0306 18:57:59.514828 139914139100928 logging_writer.py:48] [26] global_step=26, grad_norm=79.26614379882812, loss=27.46367835998535
I0306 18:58:00.412237 139914130708224 logging_writer.py:48] [27] global_step=27, grad_norm=88.73355102539062, loss=27.025188446044922
I0306 18:58:01.319437 139914139100928 logging_writer.py:48] [28] global_step=28, grad_norm=90.27354431152344, loss=25.83669090270996
I0306 18:58:02.234709 139914130708224 logging_writer.py:48] [29] global_step=29, grad_norm=87.09091186523438, loss=25.458290100097656
I0306 18:58:03.135546 139914139100928 logging_writer.py:48] [30] global_step=30, grad_norm=69.88463592529297, loss=24.472225189208984
I0306 18:58:04.032594 139914130708224 logging_writer.py:48] [31] global_step=31, grad_norm=65.57689666748047, loss=25.179109573364258
I0306 18:58:04.931764 139914139100928 logging_writer.py:48] [32] global_step=32, grad_norm=76.24666595458984, loss=22.875171661376953
I0306 18:58:05.836146 139914130708224 logging_writer.py:48] [33] global_step=33, grad_norm=61.67951965332031, loss=23.58885383605957
I0306 18:58:06.734201 139914139100928 logging_writer.py:48] [34] global_step=34, grad_norm=55.26443099975586, loss=22.624820709228516
I0306 18:58:07.636204 139914130708224 logging_writer.py:48] [35] global_step=35, grad_norm=47.017662048339844, loss=22.422771453857422
I0306 18:58:08.541118 139914139100928 logging_writer.py:48] [36] global_step=36, grad_norm=37.74200439453125, loss=22.784616470336914
I0306 18:58:09.449585 139914130708224 logging_writer.py:48] [37] global_step=37, grad_norm=28.63629150390625, loss=21.708572387695312
I0306 18:58:10.351982 139914139100928 logging_writer.py:48] [38] global_step=38, grad_norm=26.54256820678711, loss=21.860593795776367
I0306 18:58:11.255306 139914130708224 logging_writer.py:48] [39] global_step=39, grad_norm=24.02916717529297, loss=21.821897506713867
I0306 18:58:12.163043 139914139100928 logging_writer.py:48] [40] global_step=40, grad_norm=23.90888786315918, loss=21.4926700592041
I0306 18:58:13.062579 139914130708224 logging_writer.py:48] [41] global_step=41, grad_norm=33.02815628051758, loss=21.597213745117188
I0306 18:58:13.957331 139914139100928 logging_writer.py:48] [42] global_step=42, grad_norm=39.17957305908203, loss=22.29862403869629
I0306 18:58:14.856315 139914130708224 logging_writer.py:48] [43] global_step=43, grad_norm=33.03580093383789, loss=20.069013595581055
I0306 18:58:15.762707 139914139100928 logging_writer.py:48] [44] global_step=44, grad_norm=56.664493560791016, loss=23.313596725463867
I0306 18:58:16.666752 139914130708224 logging_writer.py:48] [45] global_step=45, grad_norm=53.54928970336914, loss=22.21857452392578
I0306 18:58:17.586203 139914139100928 logging_writer.py:48] [46] global_step=46, grad_norm=60.06269073486328, loss=22.503244400024414
I0306 18:58:18.489351 139914130708224 logging_writer.py:48] [47] global_step=47, grad_norm=57.85415267944336, loss=21.79429054260254
I0306 18:58:19.396156 139914139100928 logging_writer.py:48] [48] global_step=48, grad_norm=66.68928527832031, loss=22.697294235229492
I0306 18:58:20.300118 139914130708224 logging_writer.py:48] [49] global_step=49, grad_norm=64.36134338378906, loss=22.049394607543945
I0306 18:58:21.196079 139914139100928 logging_writer.py:48] [50] global_step=50, grad_norm=63.04560852050781, loss=21.868757247924805
I0306 18:58:22.101356 139914130708224 logging_writer.py:48] [51] global_step=51, grad_norm=61.000301361083984, loss=21.715229034423828
I0306 18:58:23.001391 139914139100928 logging_writer.py:48] [52] global_step=52, grad_norm=65.28620910644531, loss=22.77251434326172
I0306 18:58:23.912295 139914130708224 logging_writer.py:48] [53] global_step=53, grad_norm=64.07827758789062, loss=21.820829391479492
I0306 18:58:24.822465 139914139100928 logging_writer.py:48] [54] global_step=54, grad_norm=52.46800231933594, loss=20.762739181518555
I0306 18:58:25.728332 139914130708224 logging_writer.py:48] [55] global_step=55, grad_norm=48.3474006652832, loss=20.3519287109375
I0306 18:58:26.634946 139914139100928 logging_writer.py:48] [56] global_step=56, grad_norm=41.50926971435547, loss=19.987354278564453
I0306 18:58:27.546571 139914130708224 logging_writer.py:48] [57] global_step=57, grad_norm=39.07915115356445, loss=18.765422821044922
I0306 18:58:28.442274 139914139100928 logging_writer.py:48] [58] global_step=58, grad_norm=41.7198371887207, loss=20.590660095214844
I0306 18:58:29.354532 139914130708224 logging_writer.py:48] [59] global_step=59, grad_norm=34.08403396606445, loss=19.35978126525879
I0306 18:58:30.259658 139914139100928 logging_writer.py:48] [60] global_step=60, grad_norm=34.49566650390625, loss=19.94829750061035
I0306 18:58:31.155283 139914130708224 logging_writer.py:48] [61] global_step=61, grad_norm=22.751169204711914, loss=19.461280822753906
I0306 18:58:32.055853 139914139100928 logging_writer.py:48] [62] global_step=62, grad_norm=25.690650939941406, loss=19.28917121887207
I0306 18:58:32.967401 139914130708224 logging_writer.py:48] [63] global_step=63, grad_norm=23.476762771606445, loss=20.04669189453125
I0306 18:58:33.866292 139914139100928 logging_writer.py:48] [64] global_step=64, grad_norm=20.648595809936523, loss=19.11128044128418
I0306 18:58:34.766696 139914130708224 logging_writer.py:48] [65] global_step=65, grad_norm=26.897592544555664, loss=18.610952377319336
I0306 18:58:35.673039 139914139100928 logging_writer.py:48] [66] global_step=66, grad_norm=25.960140228271484, loss=19.393640518188477
I0306 18:58:36.571375 139914130708224 logging_writer.py:48] [67] global_step=67, grad_norm=33.5615348815918, loss=18.501951217651367
I0306 18:58:37.476809 139914139100928 logging_writer.py:48] [68] global_step=68, grad_norm=34.06671905517578, loss=18.812118530273438
I0306 18:58:38.376955 139914130708224 logging_writer.py:48] [69] global_step=69, grad_norm=38.07262420654297, loss=18.559520721435547
I0306 18:58:39.276212 139914139100928 logging_writer.py:48] [70] global_step=70, grad_norm=37.00848388671875, loss=18.431089401245117
I0306 18:58:40.179324 139914130708224 logging_writer.py:48] [71] global_step=71, grad_norm=40.227294921875, loss=19.001859664916992
I0306 18:58:41.084705 139914139100928 logging_writer.py:48] [72] global_step=72, grad_norm=36.78508377075195, loss=18.73520851135254
I0306 18:58:41.991520 139914130708224 logging_writer.py:48] [73] global_step=73, grad_norm=39.077335357666016, loss=18.822933197021484
I0306 18:58:42.893405 139914139100928 logging_writer.py:48] [74] global_step=74, grad_norm=46.36845397949219, loss=17.80099868774414
I0306 18:58:43.792475 139914130708224 logging_writer.py:48] [75] global_step=75, grad_norm=42.46579360961914, loss=18.238019943237305
I0306 18:58:44.693682 139914139100928 logging_writer.py:48] [76] global_step=76, grad_norm=50.29498291015625, loss=18.01239585876465
I0306 18:58:45.596887 139914130708224 logging_writer.py:48] [77] global_step=77, grad_norm=40.24209213256836, loss=18.587196350097656
I0306 18:58:46.490727 139914139100928 logging_writer.py:48] [78] global_step=78, grad_norm=50.59882736206055, loss=19.07567596435547
I0306 18:58:47.390057 139914130708224 logging_writer.py:48] [79] global_step=79, grad_norm=44.506954193115234, loss=17.729808807373047
I0306 18:58:48.291616 139914139100928 logging_writer.py:48] [80] global_step=80, grad_norm=49.142356872558594, loss=17.36490249633789
I0306 18:58:49.196866 139914130708224 logging_writer.py:48] [81] global_step=81, grad_norm=42.49974060058594, loss=17.29264259338379
I0306 18:58:50.099841 139914139100928 logging_writer.py:48] [82] global_step=82, grad_norm=37.41323471069336, loss=17.05497169494629
I0306 18:58:50.995978 139914130708224 logging_writer.py:48] [83] global_step=83, grad_norm=37.5256233215332, loss=17.1513614654541
I0306 18:58:51.896412 139914139100928 logging_writer.py:48] [84] global_step=84, grad_norm=43.63628005981445, loss=17.337587356567383
I0306 18:58:52.796809 139914130708224 logging_writer.py:48] [85] global_step=85, grad_norm=38.931034088134766, loss=16.3109188079834
I0306 18:58:53.697083 139914139100928 logging_writer.py:48] [86] global_step=86, grad_norm=32.61351776123047, loss=15.898740768432617
I0306 18:58:54.604000 139914130708224 logging_writer.py:48] [87] global_step=87, grad_norm=37.18584442138672, loss=15.753801345825195
I0306 18:58:55.508582 139914139100928 logging_writer.py:48] [88] global_step=88, grad_norm=35.87221908569336, loss=15.602301597595215
I0306 18:58:56.415582 139914130708224 logging_writer.py:48] [89] global_step=89, grad_norm=31.381853103637695, loss=15.635563850402832
I0306 18:58:57.324075 139914139100928 logging_writer.py:48] [90] global_step=90, grad_norm=31.43627166748047, loss=15.567456245422363
I0306 18:58:58.224269 139914130708224 logging_writer.py:48] [91] global_step=91, grad_norm=40.605003356933594, loss=15.140778541564941
I0306 18:58:59.134002 139914139100928 logging_writer.py:48] [92] global_step=92, grad_norm=33.33782196044922, loss=14.619462013244629
I0306 18:59:00.040152 139914130708224 logging_writer.py:48] [93] global_step=93, grad_norm=22.6800537109375, loss=14.23436164855957
I0306 18:59:00.938250 139914139100928 logging_writer.py:48] [94] global_step=94, grad_norm=20.26811408996582, loss=14.143620491027832
I0306 18:59:01.847999 139914130708224 logging_writer.py:48] [95] global_step=95, grad_norm=26.70529556274414, loss=14.106602668762207
I0306 18:59:02.752504 139914139100928 logging_writer.py:48] [96] global_step=96, grad_norm=16.30259895324707, loss=13.78083324432373
I0306 18:59:03.658557 139914130708224 logging_writer.py:48] [97] global_step=97, grad_norm=35.48057556152344, loss=13.785494804382324
I0306 18:59:04.568045 139914139100928 logging_writer.py:48] [98] global_step=98, grad_norm=41.24467849731445, loss=13.380349159240723
I0306 18:59:05.474359 139914130708224 logging_writer.py:48] [99] global_step=99, grad_norm=47.716529846191406, loss=13.22743034362793
I0306 18:59:06.381866 139914139100928 logging_writer.py:48] [100] global_step=100, grad_norm=39.88843536376953, loss=12.62243366241455
I0306 19:04:17.447785 139914130708224 logging_writer.py:48] [500] global_step=500, grad_norm=0.7524611353874207, loss=5.8224711418151855
I0306 19:10:50.173630 139914139100928 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5719282627105713, loss=5.800025939941406
I0306 19:17:24.952074 139915491784448 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.9474573731422424, loss=5.789748668670654
I0306 19:21:13.319631 140085866297152 spec.py:321] Evaluating on the training split.
I0306 19:21:48.891107 140085866297152 spec.py:333] Evaluating on the validation split.
I0306 19:22:32.069937 140085866297152 spec.py:349] Evaluating on the test split.
I0306 19:22:53.424706 140085866297152 submission_runner.py:413] Time since start: 1772.19s, 	Step: 1794, 	{'train/ctc_loss': Array(5.912836, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(5.928612, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.8985214, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1501.1025803089142, 'total_duration': 1772.187667608261, 'accumulated_submission_time': 1501.1025803089142, 'accumulated_eval_time': 270.9841420650482, 'accumulated_logging_time': 0.04010772705078125}
I0306 19:22:53.456183 139915491784448 logging_writer.py:48] [1794] accumulated_eval_time=270.984142, accumulated_logging_time=0.040108, accumulated_submission_time=1501.102580, global_step=1794, preemption_count=0, score=1501.102580, test/ctc_loss=5.898521423339844, test/num_examples=2472, test/wer=0.899580, total_duration=1772.187668, train/ctc_loss=5.912836074829102, train/wer=0.944636, validation/ctc_loss=5.928612232208252, validation/num_examples=5348, validation/wer=0.896618
I0306 19:25:34.390388 139915483391744 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.6057326197624207, loss=5.553079605102539
I0306 19:32:05.325341 139915491784448 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.7939346432685852, loss=5.481483459472656
I0306 19:38:42.261030 139915483391744 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.042109489440918, loss=5.493459224700928
I0306 19:45:17.582392 139915491784448 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.6822099685668945, loss=5.507530212402344
I0306 19:46:54.002634 140085866297152 spec.py:321] Evaluating on the training split.
I0306 19:47:29.906075 140085866297152 spec.py:333] Evaluating on the validation split.
I0306 19:48:13.394628 140085866297152 spec.py:349] Evaluating on the test split.
I0306 19:48:35.342715 140085866297152 submission_runner.py:413] Time since start: 3314.10s, 	Step: 3626, 	{'train/ctc_loss': Array(5.6478033, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': Array(5.815719, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.764541, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2941.5714643001556, 'total_duration': 3314.1048407554626, 'accumulated_submission_time': 2941.5714643001556, 'accumulated_eval_time': 372.3185966014862, 'accumulated_logging_time': 0.08719491958618164}
I0306 19:48:35.374733 139915276744448 logging_writer.py:48] [3626] accumulated_eval_time=372.318597, accumulated_logging_time=0.087195, accumulated_submission_time=2941.571464, global_step=3626, preemption_count=0, score=2941.571464, test/ctc_loss=5.764541149139404, test/num_examples=2472, test/wer=0.899580, total_duration=3314.104841, train/ctc_loss=5.64780330657959, train/wer=0.942722, validation/ctc_loss=5.815719127655029, validation/num_examples=5348, validation/wer=0.896618
I0306 19:53:25.833751 139915268351744 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.8451526761054993, loss=5.498113632202148
I0306 19:59:56.377536 139915276744448 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.970816433429718, loss=5.486921787261963
I0306 20:06:32.679522 139915268351744 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.747565507888794, loss=5.493021011352539
I0306 20:12:35.375306 140085866297152 spec.py:321] Evaluating on the training split.
I0306 20:13:11.674381 140085866297152 spec.py:333] Evaluating on the validation split.
I0306 20:13:55.504442 140085866297152 spec.py:349] Evaluating on the test split.
I0306 20:14:17.484791 140085866297152 submission_runner.py:413] Time since start: 4856.25s, 	Step: 5454, 	{'train/ctc_loss': Array(5.7372236, dtype=float32), 'train/wer': 0.9432768790423328, 'validation/ctc_loss': Array(5.7154565, dtype=float32), 'validation/wer': 0.8965986657269471, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.7174153, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4381.4923849105835, 'total_duration': 4856.247415781021, 'accumulated_submission_time': 4381.4923849105835, 'accumulated_eval_time': 474.42303490638733, 'accumulated_logging_time': 0.13482069969177246}
I0306 20:14:17.517211 139914984904448 logging_writer.py:48] [5454] accumulated_eval_time=474.423035, accumulated_logging_time=0.134821, accumulated_submission_time=4381.492385, global_step=5454, preemption_count=0, score=4381.492385, test/ctc_loss=5.7174153327941895, test/num_examples=2472, test/wer=0.899580, total_duration=4856.247416, train/ctc_loss=5.7372236251831055, train/wer=0.943277, validation/ctc_loss=5.715456485748291, validation/num_examples=5348, validation/wer=0.896599
I0306 20:14:53.812886 139914976511744 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.4603210985660553, loss=5.515466690063477
I0306 20:21:20.490574 139914984904448 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.637030303478241, loss=5.514712810516357
I0306 20:27:57.746247 139914984904448 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.632448136806488, loss=5.5262956619262695
I0306 20:34:29.021750 139914976511744 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5261368751525879, loss=5.492447853088379
I0306 20:38:17.607104 140085866297152 spec.py:321] Evaluating on the training split.
I0306 20:38:54.504202 140085866297152 spec.py:333] Evaluating on the validation split.
I0306 20:39:37.625381 140085866297152 spec.py:349] Evaluating on the test split.
I0306 20:39:59.396793 140085866297152 submission_runner.py:413] Time since start: 6398.16s, 	Step: 7277, 	{'train/ctc_loss': Array(5.480827, dtype=float32), 'train/wer': 0.9408727706856287, 'validation/ctc_loss': Array(5.443987, dtype=float32), 'validation/wer': 0.8959518039719243, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.428039, dtype=float32), 'test/wer': 0.8985233481607865, 'test/num_examples': 2472, 'score': 5821.502820491791, 'total_duration': 6398.1597690582275, 'accumulated_submission_time': 5821.502820491791, 'accumulated_eval_time': 576.2081487178802, 'accumulated_logging_time': 0.1852715015411377}
I0306 20:39:59.428746 139914984904448 logging_writer.py:48] [7277] accumulated_eval_time=576.208149, accumulated_logging_time=0.185272, accumulated_submission_time=5821.502820, global_step=7277, preemption_count=0, score=5821.502820, test/ctc_loss=5.428039073944092, test/num_examples=2472, test/wer=0.898523, total_duration=6398.159769, train/ctc_loss=5.4808268547058105, train/wer=0.940873, validation/ctc_loss=5.443986892700195, validation/num_examples=5348, validation/wer=0.895952
I0306 20:42:51.514984 139914976511744 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.6259804368019104, loss=5.465085029602051
I0306 20:49:17.395779 139914984904448 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5904412865638733, loss=5.50846004486084
I0306 20:56:00.864036 139914984904448 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5279746055603027, loss=5.473182678222656
I0306 21:02:32.933465 139914976511744 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.4839891493320465, loss=5.479814529418945
I0306 21:03:59.842040 140085866297152 spec.py:321] Evaluating on the training split.
I0306 21:04:36.492628 140085866297152 spec.py:333] Evaluating on the validation split.
I0306 21:05:20.422147 140085866297152 spec.py:349] Evaluating on the test split.
I0306 21:05:42.242520 140085866297152 submission_runner.py:413] Time since start: 7941.00s, 	Step: 9104, 	{'train/ctc_loss': Array(5.5783405, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': Array(5.50731, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4930353, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7261.83672618866, 'total_duration': 7941.004846572876, 'accumulated_submission_time': 7261.83672618866, 'accumulated_eval_time': 678.603214263916, 'accumulated_logging_time': 0.23277878761291504}
I0306 21:05:42.277236 139915491784448 logging_writer.py:48] [9104] accumulated_eval_time=678.603214, accumulated_logging_time=0.232779, accumulated_submission_time=7261.836726, global_step=9104, preemption_count=0, score=7261.836726, test/ctc_loss=5.493035316467285, test/num_examples=2472, test/wer=0.899580, total_duration=7941.004847, train/ctc_loss=5.578340530395508, train/wer=0.941551, validation/ctc_loss=5.507309913635254, validation/num_examples=5348, validation/wer=0.896618
I0306 21:10:50.074768 139914114504448 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.42937225103378296, loss=5.516510009765625
I0306 21:17:31.536893 139914106111744 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.6649858355522156, loss=5.489768981933594
I0306 21:24:25.223932 139914114504448 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.8655856251716614, loss=5.531703948974609
I0306 21:29:42.941317 140085866297152 spec.py:321] Evaluating on the training split.
I0306 21:30:19.988770 140085866297152 spec.py:333] Evaluating on the validation split.
I0306 21:31:04.274418 140085866297152 spec.py:349] Evaluating on the test split.
I0306 21:31:26.277273 140085866297152 submission_runner.py:413] Time since start: 9485.04s, 	Step: 10908, 	{'train/ctc_loss': Array(5.553975, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': Array(5.497823, dtype=float32), 'validation/wer': 0.8965214285024667, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4824014, dtype=float32), 'test/wer': 0.8994373692442061, 'test/num_examples': 2472, 'score': 8702.423407554626, 'total_duration': 9485.03901386261, 'accumulated_submission_time': 8702.423407554626, 'accumulated_eval_time': 781.9331517219543, 'accumulated_logging_time': 0.28314208984375}
I0306 21:31:26.311726 139914114504448 logging_writer.py:48] [10908] accumulated_eval_time=781.933152, accumulated_logging_time=0.283142, accumulated_submission_time=8702.423408, global_step=10908, preemption_count=0, score=8702.423408, test/ctc_loss=5.482401371002197, test/num_examples=2472, test/wer=0.899437, total_duration=9485.039014, train/ctc_loss=5.5539751052856445, train/wer=0.942641, validation/ctc_loss=5.497823238372803, validation/num_examples=5348, validation/wer=0.896521
I0306 21:32:37.742788 139914106111744 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.47350576519966125, loss=5.472179412841797
I0306 21:39:07.372113 139914114504448 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.25448843836784363, loss=5.541543006896973
I0306 21:45:39.963582 139914106111744 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.6829423904418945, loss=5.5787553787231445
I0306 21:52:36.693820 139915491784448 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.4469977617263794, loss=5.547677516937256
I0306 21:55:26.722876 140085866297152 spec.py:321] Evaluating on the training split.
I0306 21:56:03.582686 140085866297152 spec.py:333] Evaluating on the validation split.
I0306 21:56:47.504574 140085866297152 spec.py:349] Evaluating on the test split.
I0306 21:57:09.387809 140085866297152 submission_runner.py:413] Time since start: 11028.15s, 	Step: 12723, 	{'train/ctc_loss': Array(6.263767, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': Array(6.1856422, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.2025256, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 10142.756002426147, 'total_duration': 11028.150895118713, 'accumulated_submission_time': 10142.756002426147, 'accumulated_eval_time': 884.5934579372406, 'accumulated_logging_time': 0.33275580406188965}
I0306 21:57:09.421778 139915491784448 logging_writer.py:48] [12723] accumulated_eval_time=884.593458, accumulated_logging_time=0.332756, accumulated_submission_time=10142.756002, global_step=12723, preemption_count=0, score=10142.756002, test/ctc_loss=6.202525615692139, test/num_examples=2472, test/wer=0.899580, total_duration=11028.150895, train/ctc_loss=6.263766765594482, train/wer=0.942824, validation/ctc_loss=6.185642242431641, validation/num_examples=5348, validation/wer=0.896618
I0306 22:00:42.592762 139915483391744 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.5048907995223999, loss=5.530797481536865
I0306 22:07:30.206820 139915491784448 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.4203920364379883, loss=5.516750335693359
I0306 22:13:56.569648 139915483391744 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.45590415596961975, loss=5.5387959480285645
I0306 22:20:54.945784 139915491784448 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.3381023705005646, loss=5.5165114402771
I0306 22:21:10.023347 140085866297152 spec.py:321] Evaluating on the training split.
I0306 22:21:47.842099 140085866297152 spec.py:333] Evaluating on the validation split.
I0306 22:22:32.154871 140085866297152 spec.py:349] Evaluating on the test split.
I0306 22:22:54.439192 140085866297152 submission_runner.py:413] Time since start: 12573.20s, 	Step: 14521, 	{'train/ctc_loss': Array(6.503599, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': Array(6.0460415, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.0825634, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 11583.276812314987, 'total_duration': 12573.201262712479, 'accumulated_submission_time': 11583.276812314987, 'accumulated_eval_time': 989.0036432743073, 'accumulated_logging_time': 0.3832833766937256}
I0306 22:22:54.471398 139915491784448 logging_writer.py:48] [14521] accumulated_eval_time=989.003643, accumulated_logging_time=0.383283, accumulated_submission_time=11583.276812, global_step=14521, preemption_count=0, score=11583.276812, test/ctc_loss=6.082563400268555, test/num_examples=2472, test/wer=0.899580, total_duration=12573.201263, train/ctc_loss=6.503599166870117, train/wer=0.944086, validation/ctc_loss=6.046041488647461, validation/num_examples=5348, validation/wer=0.896618
I0306 22:29:03.058204 139915483391744 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.25671419501304626, loss=5.5247015953063965
I0306 22:35:55.502388 139915491784448 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.4346119463443756, loss=5.497584342956543
I0306 22:42:19.529536 139915483391744 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.5486183762550354, loss=5.512564182281494
I0306 22:46:54.470191 140085866297152 spec.py:321] Evaluating on the training split.
I0306 22:47:31.601596 140085866297152 spec.py:333] Evaluating on the validation split.
I0306 22:48:15.479672 140085866297152 spec.py:349] Evaluating on the test split.
I0306 22:48:37.857192 140085866297152 submission_runner.py:413] Time since start: 14116.62s, 	Step: 16328, 	{'train/ctc_loss': Array(5.595888, dtype=float32), 'train/wer': 0.9427990785714666, 'validation/ctc_loss': Array(5.544391, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.5321684, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 13023.197110891342, 'total_duration': 14116.616592407227, 'accumulated_submission_time': 13023.197110891342, 'accumulated_eval_time': 1092.38232588768, 'accumulated_logging_time': 0.4308288097381592}
I0306 22:48:37.892403 139914908104448 logging_writer.py:48] [16328] accumulated_eval_time=1092.382326, accumulated_logging_time=0.430829, accumulated_submission_time=13023.197111, global_step=16328, preemption_count=0, score=13023.197111, test/ctc_loss=5.532168388366699, test/num_examples=2472, test/wer=0.899580, total_duration=14116.616592, train/ctc_loss=5.595888137817383, train/wer=0.942799, validation/ctc_loss=5.54439115524292, validation/num_examples=5348, validation/wer=0.896618
I0306 22:50:53.813846 139914252744448 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.32177412509918213, loss=5.496461868286133
I0306 22:57:17.610255 139914244351744 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.459114134311676, loss=5.458442211151123
I0306 23:04:16.796322 139914252744448 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.31586360931396484, loss=5.492217063903809
I0306 23:10:44.287295 139914252744448 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.36109527945518494, loss=5.489049911499023
I0306 23:12:38.686685 140085866297152 spec.py:321] Evaluating on the training split.
I0306 23:13:15.714888 140085866297152 spec.py:333] Evaluating on the validation split.
I0306 23:13:59.756321 140085866297152 spec.py:349] Evaluating on the test split.
I0306 23:14:22.221338 140085866297152 submission_runner.py:413] Time since start: 15660.98s, 	Step: 18144, 	{'train/ctc_loss': Array(6.1803946, dtype=float32), 'train/wer': 0.9423383225986367, 'validation/ctc_loss': Array(5.839062, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.870875, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14463.913548469543, 'total_duration': 15660.982357740402, 'accumulated_submission_time': 14463.913548469543, 'accumulated_eval_time': 1195.9102911949158, 'accumulated_logging_time': 0.48088574409484863}
I0306 23:14:22.256686 139914252744448 logging_writer.py:48] [18144] accumulated_eval_time=1195.910291, accumulated_logging_time=0.480886, accumulated_submission_time=14463.913548, global_step=18144, preemption_count=0, score=14463.913548, test/ctc_loss=5.870874881744385, test/num_examples=2472, test/wer=0.899580, total_duration=15660.982358, train/ctc_loss=6.180394649505615, train/wer=0.942338, validation/ctc_loss=5.839062213897705, validation/num_examples=5348, validation/wer=0.896618
I0306 23:18:56.325067 139914244351744 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.7491136789321899, loss=5.516207218170166
I0306 23:25:24.719317 139914252744448 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.5579258799552917, loss=5.517215251922607
I0306 23:32:24.052950 139914244351744 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.34929385781288147, loss=5.489283084869385
I0306 23:38:22.975980 140085866297152 spec.py:321] Evaluating on the training split.
I0306 23:38:59.627733 140085866297152 spec.py:333] Evaluating on the validation split.
I0306 23:39:43.145253 140085866297152 spec.py:349] Evaluating on the test split.
I0306 23:40:05.399697 140085866297152 submission_runner.py:413] Time since start: 17204.16s, 	Step: 19955, 	{'train/ctc_loss': Array(5.586595, dtype=float32), 'train/wer': 0.9431026732594727, 'validation/ctc_loss': Array(5.522736, dtype=float32), 'validation/wer': 0.896579356420827, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.506677, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15904.555320501328, 'total_duration': 17204.162836313248, 'accumulated_submission_time': 15904.555320501328, 'accumulated_eval_time': 1298.3295764923096, 'accumulated_logging_time': 0.5306649208068848}
I0306 23:40:05.431714 139914908104448 logging_writer.py:48] [19955] accumulated_eval_time=1298.329576, accumulated_logging_time=0.530665, accumulated_submission_time=15904.555321, global_step=19955, preemption_count=0, score=15904.555321, test/ctc_loss=5.506677150726318, test/num_examples=2472, test/wer=0.899580, total_duration=17204.162836, train/ctc_loss=5.586595058441162, train/wer=0.943103, validation/ctc_loss=5.522736072540283, validation/num_examples=5348, validation/wer=0.896579
I0306 23:40:40.963234 139914899711744 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.39175570011138916, loss=5.476568222045898
I0306 23:47:20.589561 139914908104448 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.5122693181037903, loss=5.472052097320557
I0306 23:53:58.300942 139914908104448 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.4394697844982147, loss=5.457553386688232
I0307 00:00:52.301481 139914899711744 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.43146881461143494, loss=5.423623561859131
I0307 00:04:06.125029 140085866297152 spec.py:321] Evaluating on the training split.
I0307 00:04:42.924230 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 00:05:26.761795 140085866297152 spec.py:349] Evaluating on the test split.
I0307 00:05:49.412242 140085866297152 submission_runner.py:413] Time since start: 18748.17s, 	Step: 21732, 	{'train/ctc_loss': Array(5.4757457, dtype=float32), 'train/wer': 0.9432716912443612, 'validation/ctc_loss': Array(5.4281535, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3822036, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 17345.169860839844, 'total_duration': 18748.173184871674, 'accumulated_submission_time': 17345.169860839844, 'accumulated_eval_time': 1401.610151052475, 'accumulated_logging_time': 0.5777120590209961}
I0307 00:05:49.448807 139914908104448 logging_writer.py:48] [21732] accumulated_eval_time=1401.610151, accumulated_logging_time=0.577712, accumulated_submission_time=17345.169861, global_step=21732, preemption_count=0, score=17345.169861, test/ctc_loss=5.382203578948975, test/num_examples=2472, test/wer=0.899580, total_duration=18748.173185, train/ctc_loss=5.475745677947998, train/wer=0.943272, validation/ctc_loss=5.4281535148620605, validation/num_examples=5348, validation/wer=0.896618
I0307 00:09:16.597309 139914899711744 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.4003501832485199, loss=5.427583694458008
I0307 00:16:11.681182 139914908104448 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.4276808202266693, loss=5.389502048492432
I0307 00:22:54.974277 139914908104448 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.44882091879844666, loss=5.245877265930176
I0307 00:29:50.014081 140085866297152 spec.py:321] Evaluating on the training split.
I0307 00:30:42.321511 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 00:31:31.402035 140085866297152 spec.py:349] Evaluating on the test split.
I0307 00:31:56.429210 140085866297152 submission_runner.py:413] Time since start: 20315.19s, 	Step: 23499, 	{'train/ctc_loss': Array(4.039452, dtype=float32), 'train/wer': 0.7950457866035601, 'validation/ctc_loss': Array(4.2082353, dtype=float32), 'validation/wer': 0.7809166127615205, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.9759412, dtype=float32), 'test/wer': 0.7644872341722015, 'test/num_examples': 2472, 'score': 18785.657301664352, 'total_duration': 20315.190623521805, 'accumulated_submission_time': 18785.657301664352, 'accumulated_eval_time': 1528.0189802646637, 'accumulated_logging_time': 0.6300628185272217}
I0307 00:31:56.463713 139914062591744 logging_writer.py:48] [23499] accumulated_eval_time=1528.018980, accumulated_logging_time=0.630063, accumulated_submission_time=18785.657302, global_step=23499, preemption_count=0, score=18785.657302, test/ctc_loss=3.9759411811828613, test/num_examples=2472, test/wer=0.764487, total_duration=20315.190624, train/ctc_loss=4.039452075958252, train/wer=0.795046, validation/ctc_loss=4.208235263824463, validation/num_examples=5348, validation/wer=0.780917
I0307 00:31:58.114127 139914054199040 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.5359591245651245, loss=4.195729732513428
I0307 00:38:26.958845 139914062591744 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.9438615441322327, loss=3.774444580078125
I0307 00:45:23.045593 139914054199040 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.8731308579444885, loss=3.6570255756378174
I0307 00:52:11.162320 139913734911744 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.7025246620178223, loss=3.3658130168914795
I0307 00:55:57.385233 140085866297152 spec.py:321] Evaluating on the training split.
I0307 00:56:50.559147 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 00:57:40.148840 140085866297152 spec.py:349] Evaluating on the test split.
I0307 00:58:05.001201 140085866297152 submission_runner.py:413] Time since start: 21883.76s, 	Step: 25293, 	{'train/ctc_loss': Array(2.2726598, dtype=float32), 'train/wer': 0.5717136100671424, 'validation/ctc_loss': Array(2.5912151, dtype=float32), 'validation/wer': 0.5982505768655203, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.2852595, dtype=float32), 'test/wer': 0.5666930717201877, 'test/num_examples': 2472, 'score': 20226.497428417206, 'total_duration': 21883.76142835617, 'accumulated_submission_time': 20226.497428417206, 'accumulated_eval_time': 1655.6274774074554, 'accumulated_logging_time': 0.6829721927642822}
I0307 00:58:05.041252 139915491784448 logging_writer.py:48] [25293] accumulated_eval_time=1655.627477, accumulated_logging_time=0.682972, accumulated_submission_time=20226.497428, global_step=25293, preemption_count=0, score=20226.497428, test/ctc_loss=2.285259485244751, test/num_examples=2472, test/wer=0.566693, total_duration=21883.761428, train/ctc_loss=2.2726597785949707, train/wer=0.571714, validation/ctc_loss=2.591215133666992, validation/num_examples=5348, validation/wer=0.598251
I0307 01:00:45.257764 139915483391744 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.693987250328064, loss=3.247230052947998
I0307 01:07:21.826617 139914114504448 logging_writer.py:48] [26000] global_step=26000, grad_norm=1.0538455247879028, loss=3.060882806777954
I0307 01:14:13.724494 139914106111744 logging_writer.py:48] [26500] global_step=26500, grad_norm=1.2675844430923462, loss=3.118968963623047
I0307 01:21:10.873207 139915491784448 logging_writer.py:48] [27000] global_step=27000, grad_norm=1.180014729499817, loss=2.9713170528411865
I0307 01:22:05.573173 140085866297152 spec.py:321] Evaluating on the training split.
I0307 01:22:59.610222 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 01:23:49.028247 140085866297152 spec.py:349] Evaluating on the test split.
I0307 01:24:14.288084 140085866297152 submission_runner.py:413] Time since start: 23453.05s, 	Step: 27072, 	{'train/ctc_loss': Array(1.665647, dtype=float32), 'train/wer': 0.48148882339132476, 'validation/ctc_loss': Array(2.0445182, dtype=float32), 'validation/wer': 0.5215540129565444, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.7162665, dtype=float32), 'test/wer': 0.47498628968374873, 'test/num_examples': 2472, 'score': 21666.952129125595, 'total_duration': 23453.050668239594, 'accumulated_submission_time': 21666.952129125595, 'accumulated_eval_time': 1784.3372511863708, 'accumulated_logging_time': 0.7387275695800781}
I0307 01:24:14.320667 139915491784448 logging_writer.py:48] [27072] accumulated_eval_time=1784.337251, accumulated_logging_time=0.738728, accumulated_submission_time=21666.952129, global_step=27072, preemption_count=0, score=21666.952129, test/ctc_loss=1.7162665128707886, test/num_examples=2472, test/wer=0.474986, total_duration=23453.050668, train/ctc_loss=1.665647029876709, train/wer=0.481489, validation/ctc_loss=2.044518232345581, validation/num_examples=5348, validation/wer=0.521554
I0307 01:29:47.839041 139915483391744 logging_writer.py:48] [27500] global_step=27500, grad_norm=1.0209559202194214, loss=2.9553558826446533
I0307 01:36:44.390210 139915491784448 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.9766156077384949, loss=2.8156096935272217
I0307 01:43:26.868082 139915483391744 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.9106409549713135, loss=2.6989564895629883
I0307 01:48:14.363778 140085866297152 spec.py:321] Evaluating on the training split.
I0307 01:49:08.401692 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 01:49:58.369134 140085866297152 spec.py:349] Evaluating on the test split.
I0307 01:50:23.310332 140085866297152 submission_runner.py:413] Time since start: 25022.07s, 	Step: 28833, 	{'train/ctc_loss': Array(1.3347486, dtype=float32), 'train/wer': 0.41529185545816244, 'validation/ctc_loss': Array(1.744137, dtype=float32), 'validation/wer': 0.467806559371289, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.4032996, dtype=float32), 'test/wer': 0.4169967298356793, 'test/num_examples': 2472, 'score': 23106.915238142014, 'total_duration': 25022.07224869728, 'accumulated_submission_time': 23106.915238142014, 'accumulated_eval_time': 1913.2779862880707, 'accumulated_logging_time': 0.7904481887817383}
I0307 01:50:23.345442 139914478024448 logging_writer.py:48] [28833] accumulated_eval_time=1913.277986, accumulated_logging_time=0.790448, accumulated_submission_time=23106.915238, global_step=28833, preemption_count=0, score=23106.915238, test/ctc_loss=1.4032995700836182, test/num_examples=2472, test/wer=0.416997, total_duration=25022.072249, train/ctc_loss=1.33474862575531, train/wer=0.415292, validation/ctc_loss=1.7441370487213135, validation/num_examples=5348, validation/wer=0.467807
I0307 01:52:36.477025 139914150344448 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.9691674709320068, loss=2.6738553047180176
I0307 01:59:16.976869 139914141951744 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.9423800706863403, loss=2.597221851348877
I0307 02:06:20.755061 139914478024448 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.7584433555603027, loss=2.479973793029785
I0307 02:12:56.946454 139914469631744 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.0309749841690063, loss=2.4102835655212402
I0307 02:14:23.790080 140085866297152 spec.py:321] Evaluating on the training split.
I0307 02:15:16.999333 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 02:16:06.766556 140085866297152 spec.py:349] Evaluating on the test split.
I0307 02:16:31.531302 140085866297152 submission_runner.py:413] Time since start: 26590.29s, 	Step: 30601, 	{'train/ctc_loss': Array(1.0808085, dtype=float32), 'train/wer': 0.35850017334026696, 'validation/ctc_loss': Array(1.419696, dtype=float32), 'validation/wer': 0.4065864043175608, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.1014445, dtype=float32), 'test/wer': 0.35015132126825504, 'test/num_examples': 2472, 'score': 24547.28302192688, 'total_duration': 26590.293665647507, 'accumulated_submission_time': 24547.28302192688, 'accumulated_eval_time': 2041.0138404369354, 'accumulated_logging_time': 0.8401660919189453}
I0307 02:16:31.567218 139915491784448 logging_writer.py:48] [30601] accumulated_eval_time=2041.013840, accumulated_logging_time=0.840166, accumulated_submission_time=24547.283022, global_step=30601, preemption_count=0, score=24547.283022, test/ctc_loss=1.1014444828033447, test/num_examples=2472, test/wer=0.350151, total_duration=26590.293666, train/ctc_loss=1.0808085203170776, train/wer=0.358500, validation/ctc_loss=1.4196959733963013, validation/num_examples=5348, validation/wer=0.406586
I0307 02:21:44.008218 139915491784448 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.9741865992546082, loss=2.3453547954559326
I0307 02:28:16.537042 139915483391744 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.7897785902023315, loss=2.2545371055603027
I0307 02:35:20.912939 139915491784448 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.8904507160186768, loss=2.1621856689453125
I0307 02:40:32.269071 140085866297152 spec.py:321] Evaluating on the training split.
I0307 02:41:24.794965 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 02:42:13.921196 140085866297152 spec.py:349] Evaluating on the test split.
I0307 02:42:38.877948 140085866297152 submission_runner.py:413] Time since start: 28157.64s, 	Step: 32404, 	{'train/ctc_loss': Array(0.819108, dtype=float32), 'train/wer': 0.27726140673579347, 'validation/ctc_loss': Array(1.1686217, dtype=float32), 'validation/wer': 0.3421029765295384, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.867956, dtype=float32), 'test/wer': 0.28352934007677777, 'test/num_examples': 2472, 'score': 25987.906824588776, 'total_duration': 28157.641040086746, 'accumulated_submission_time': 25987.906824588776, 'accumulated_eval_time': 2167.6180663108826, 'accumulated_logging_time': 0.8903062343597412}
I0307 02:42:38.921314 139915199944448 logging_writer.py:48] [32404] accumulated_eval_time=2167.618066, accumulated_logging_time=0.890306, accumulated_submission_time=25987.906825, global_step=32404, preemption_count=0, score=25987.906825, test/ctc_loss=0.8679559826850891, test/num_examples=2472, test/wer=0.283529, total_duration=28157.641040, train/ctc_loss=0.8191080093383789, train/wer=0.277261, validation/ctc_loss=1.1686216592788696, validation/num_examples=5348, validation/wer=0.342103
I0307 02:43:53.818483 139915191551744 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.8431806564331055, loss=2.137136697769165
I0307 02:50:35.137347 139914872264448 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.9220521450042725, loss=2.0686914920806885
I0307 02:57:02.600545 139914863871744 logging_writer.py:48] [33500] global_step=33500, grad_norm=1.0636979341506958, loss=2.0106101036071777
I0307 03:04:10.402937 139915199944448 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.9101263880729675, loss=1.9751121997833252
I0307 03:06:39.325223 140085866297152 spec.py:321] Evaluating on the training split.
I0307 03:07:32.958680 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 03:08:21.915523 140085866297152 spec.py:349] Evaluating on the test split.
I0307 03:08:46.856241 140085866297152 submission_runner.py:413] Time since start: 29725.62s, 	Step: 34194, 	{'train/ctc_loss': Array(0.690051, dtype=float32), 'train/wer': 0.23719236242031347, 'validation/ctc_loss': Array(1.0043803, dtype=float32), 'validation/wer': 0.29873427498382843, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.71552753, dtype=float32), 'test/wer': 0.23900635752442467, 'test/num_examples': 2472, 'score': 27428.221712589264, 'total_duration': 29725.61915254593, 'accumulated_submission_time': 27428.221712589264, 'accumulated_eval_time': 2295.1442720890045, 'accumulated_logging_time': 0.9597721099853516}
I0307 03:08:46.888389 139915199944448 logging_writer.py:48] [34194] accumulated_eval_time=2295.144272, accumulated_logging_time=0.959772, accumulated_submission_time=27428.221713, global_step=34194, preemption_count=0, score=27428.221713, test/ctc_loss=0.7155275344848633, test/num_examples=2472, test/wer=0.239006, total_duration=29725.619153, train/ctc_loss=0.6900510191917419, train/wer=0.237192, validation/ctc_loss=1.0043803453445435, validation/num_examples=5348, validation/wer=0.298734
I0307 03:12:43.848177 139915191551744 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.9507629871368408, loss=1.9245619773864746
I0307 03:19:43.454767 139915199944448 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.8787979483604431, loss=2.0337979793548584
I0307 03:26:14.994784 139915199944448 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.8025296330451965, loss=1.931670069694519
I0307 03:32:47.727988 140085866297152 spec.py:321] Evaluating on the training split.
I0307 03:33:41.499252 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 03:34:31.062964 140085866297152 spec.py:349] Evaluating on the test split.
I0307 03:34:55.917853 140085866297152 submission_runner.py:413] Time since start: 31294.68s, 	Step: 35958, 	{'train/ctc_loss': Array(0.597442, dtype=float32), 'train/wer': 0.20695461278600125, 'validation/ctc_loss': Array(0.9062112, dtype=float32), 'validation/wer': 0.27678924857835235, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6275954, dtype=float32), 'test/wer': 0.21182946397741353, 'test/num_examples': 2472, 'score': 28868.98548579216, 'total_duration': 31294.67875289917, 'accumulated_submission_time': 28868.98548579216, 'accumulated_eval_time': 2423.3272914886475, 'accumulated_logging_time': 1.0066947937011719}
I0307 03:34:55.952166 139914908104448 logging_writer.py:48] [35958] accumulated_eval_time=2423.327291, accumulated_logging_time=1.006695, accumulated_submission_time=28868.985486, global_step=35958, preemption_count=0, score=28868.985486, test/ctc_loss=0.6275954246520996, test/num_examples=2472, test/wer=0.211829, total_duration=31294.678753, train/ctc_loss=0.5974419713020325, train/wer=0.206955, validation/ctc_loss=0.9062111973762512, validation/num_examples=5348, validation/wer=0.276789
I0307 03:35:29.118057 139914899711744 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.0445806980133057, loss=1.9017144441604614
I0307 03:41:59.183962 139914908104448 logging_writer.py:48] [36500] global_step=36500, grad_norm=1.0199369192123413, loss=1.9531428813934326
I0307 03:49:06.681227 139914899711744 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.8588436245918274, loss=1.7968995571136475
I0307 03:55:44.375378 139914908104448 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.8388424515724182, loss=1.7800583839416504
I0307 03:58:56.322662 140085866297152 spec.py:321] Evaluating on the training split.
I0307 03:59:51.626176 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 04:00:41.376669 140085866297152 spec.py:349] Evaluating on the test split.
I0307 04:01:06.592762 140085866297152 submission_runner.py:413] Time since start: 32865.36s, 	Step: 37736, 	{'train/ctc_loss': Array(0.47785956, dtype=float32), 'train/wer': 0.17165831309000337, 'validation/ctc_loss': Array(0.8496512, dtype=float32), 'validation/wer': 0.2590633055601147, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5750614, dtype=float32), 'test/wer': 0.19263502122560072, 'test/num_examples': 2472, 'score': 30309.27780365944, 'total_duration': 32865.355593681335, 'accumulated_submission_time': 30309.27780365944, 'accumulated_eval_time': 2553.592475414276, 'accumulated_logging_time': 1.056365966796875}
I0307 04:01:06.630231 139914908104448 logging_writer.py:48] [37736] accumulated_eval_time=2553.592475, accumulated_logging_time=1.056366, accumulated_submission_time=30309.277804, global_step=37736, preemption_count=0, score=30309.277804, test/ctc_loss=0.5750613808631897, test/num_examples=2472, test/wer=0.192635, total_duration=32865.355594, train/ctc_loss=0.4778595566749573, train/wer=0.171658, validation/ctc_loss=0.8496512174606323, validation/num_examples=5348, validation/wer=0.259063
I0307 04:04:31.729403 139914899711744 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.9309718608856201, loss=1.755730390548706
I0307 04:11:08.638738 139914252744448 logging_writer.py:48] [38500] global_step=38500, grad_norm=2.3491759300231934, loss=1.7528332471847534
I0307 04:18:13.480449 139914244351744 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.89970463514328, loss=1.7128690481185913
I0307 04:24:57.895400 139914908104448 logging_writer.py:48] [39500] global_step=39500, grad_norm=1.0685272216796875, loss=1.742679238319397
I0307 04:25:06.920760 140085866297152 spec.py:321] Evaluating on the training split.
I0307 04:25:59.881928 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 04:26:49.130771 140085866297152 spec.py:349] Evaluating on the test split.
I0307 04:27:14.221795 140085866297152 submission_runner.py:413] Time since start: 34432.98s, 	Step: 39513, 	{'train/ctc_loss': Array(0.47920355, dtype=float32), 'train/wer': 0.16948427699520602, 'validation/ctc_loss': Array(0.78365797, dtype=float32), 'validation/wer': 0.24075808335827453, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.52596676, dtype=float32), 'test/wer': 0.17811224178904395, 'test/num_examples': 2472, 'score': 31749.487112522125, 'total_duration': 34432.98482298851, 'accumulated_submission_time': 31749.487112522125, 'accumulated_eval_time': 2680.8888108730316, 'accumulated_logging_time': 1.1119632720947266}
I0307 04:27:14.256078 139915491784448 logging_writer.py:48] [39513] accumulated_eval_time=2680.888811, accumulated_logging_time=1.111963, accumulated_submission_time=31749.487113, global_step=39513, preemption_count=0, score=31749.487113, test/ctc_loss=0.5259667634963989, test/num_examples=2472, test/wer=0.178112, total_duration=34432.984823, train/ctc_loss=0.47920355200767517, train/wer=0.169484, validation/ctc_loss=0.783657968044281, validation/num_examples=5348, validation/wer=0.240758
I0307 04:33:41.990999 139915483391744 logging_writer.py:48] [40000] global_step=40000, grad_norm=2.0133321285247803, loss=1.8059431314468384
I0307 04:40:25.720589 139914949064448 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.1490682363510132, loss=1.7319438457489014
I0307 04:47:21.003798 139914940671744 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.0791990756988525, loss=1.771772861480713
I0307 04:51:14.770644 140085866297152 spec.py:321] Evaluating on the training split.
I0307 04:52:07.591079 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 04:52:56.864972 140085866297152 spec.py:349] Evaluating on the test split.
I0307 04:53:21.947276 140085866297152 submission_runner.py:413] Time since start: 36000.71s, 	Step: 41278, 	{'train/ctc_loss': Array(0.55570453, dtype=float32), 'train/wer': 0.19388380531885466, 'validation/ctc_loss': Array(0.7383608, dtype=float32), 'validation/wer': 0.22839047278836036, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48685727, dtype=float32), 'test/wer': 0.1655190624174842, 'test/num_examples': 2472, 'score': 33189.91626691818, 'total_duration': 36000.70971560478, 'accumulated_submission_time': 33189.91626691818, 'accumulated_eval_time': 2808.0603823661804, 'accumulated_logging_time': 1.16729736328125}
I0307 04:53:21.980523 139914150344448 logging_writer.py:48] [41278] accumulated_eval_time=2808.060382, accumulated_logging_time=1.167297, accumulated_submission_time=33189.916267, global_step=41278, preemption_count=0, score=33189.916267, test/ctc_loss=0.48685726523399353, test/num_examples=2472, test/wer=0.165519, total_duration=36000.709716, train/ctc_loss=0.5557045340538025, train/wer=0.193884, validation/ctc_loss=0.7383608222007751, validation/num_examples=5348, validation/wer=0.228390
I0307 04:56:14.839272 139914141951744 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.9779043793678284, loss=1.655831217765808
I0307 05:02:53.895176 139914150344448 logging_writer.py:48] [42000] global_step=42000, grad_norm=1.233265995979309, loss=1.6867095232009888
I0307 05:09:47.486537 139915491784448 logging_writer.py:48] [42500] global_step=42500, grad_norm=1.165114402770996, loss=1.6351367235183716
I0307 05:16:39.457455 139915483391744 logging_writer.py:48] [43000] global_step=43000, grad_norm=1.3607373237609863, loss=1.6263141632080078
I0307 05:17:22.175431 140085866297152 spec.py:321] Evaluating on the training split.
I0307 05:18:14.332396 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 05:19:04.443893 140085866297152 spec.py:349] Evaluating on the test split.
I0307 05:19:29.657050 140085866297152 submission_runner.py:413] Time since start: 37568.42s, 	Step: 43052, 	{'train/ctc_loss': Array(0.5582883, dtype=float32), 'train/wer': 0.19472401561034103, 'validation/ctc_loss': Array(0.7084172, dtype=float32), 'validation/wer': 0.2204543479730056, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4635892, dtype=float32), 'test/wer': 0.15855219060386327, 'test/num_examples': 2472, 'score': 34630.03539419174, 'total_duration': 37568.418976545334, 'accumulated_submission_time': 34630.03539419174, 'accumulated_eval_time': 2935.536192178726, 'accumulated_logging_time': 1.2146832942962646}
I0307 05:19:29.691165 139915199944448 logging_writer.py:48] [43052] accumulated_eval_time=2935.536192, accumulated_logging_time=1.214683, accumulated_submission_time=34630.035394, global_step=43052, preemption_count=0, score=34630.035394, test/ctc_loss=0.4635891914367676, test/num_examples=2472, test/wer=0.158552, total_duration=37568.418977, train/ctc_loss=0.5582882761955261, train/wer=0.194724, validation/ctc_loss=0.7084171772003174, validation/num_examples=5348, validation/wer=0.220454
I0307 05:25:19.934059 139914114504448 logging_writer.py:48] [43500] global_step=43500, grad_norm=1.1113166809082031, loss=1.6656019687652588
I0307 05:32:08.799926 139914106111744 logging_writer.py:48] [44000] global_step=44000, grad_norm=1.0703141689300537, loss=1.6640926599502563
I0307 05:39:01.969125 139915199944448 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.9918208718299866, loss=1.532823920249939
I0307 05:43:29.683725 140085866297152 spec.py:321] Evaluating on the training split.
I0307 05:44:19.821550 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 05:45:09.103612 140085866297152 spec.py:349] Evaluating on the test split.
I0307 05:45:33.950800 140085866297152 submission_runner.py:413] Time since start: 39132.71s, 	Step: 44847, 	{'train/ctc_loss': Array(0.5793242, dtype=float32), 'train/wer': 0.20143198928347888, 'validation/ctc_loss': Array(0.65857506, dtype=float32), 'validation/wer': 0.20489104724021742, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42297193, dtype=float32), 'test/wer': 0.1440700343265696, 'test/num_examples': 2472, 'score': 36069.94803953171, 'total_duration': 39132.711868047714, 'accumulated_submission_time': 36069.94803953171, 'accumulated_eval_time': 3059.7966132164, 'accumulated_logging_time': 1.2639679908752441}
I0307 05:45:33.987294 139914908104448 logging_writer.py:48] [44847] accumulated_eval_time=3059.796613, accumulated_logging_time=1.263968, accumulated_submission_time=36069.948040, global_step=44847, preemption_count=0, score=36069.948040, test/ctc_loss=0.4229719340801239, test/num_examples=2472, test/wer=0.144070, total_duration=39132.711868, train/ctc_loss=0.5793241858482361, train/wer=0.201432, validation/ctc_loss=0.6585750579833984, validation/num_examples=5348, validation/wer=0.204891
I0307 05:47:33.237339 139914899711744 logging_writer.py:48] [45000] global_step=45000, grad_norm=1.0078884363174438, loss=1.5983343124389648
I0307 05:54:08.770871 139914252744448 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.9739001989364624, loss=1.5775198936462402
I0307 06:00:51.409239 139914244351744 logging_writer.py:48] [46000] global_step=46000, grad_norm=1.129465937614441, loss=1.5575515031814575
I0307 06:07:55.344524 139914908104448 logging_writer.py:48] [46500] global_step=46500, grad_norm=1.0638172626495361, loss=1.536390781402588
I0307 06:09:34.247517 140085866297152 spec.py:321] Evaluating on the training split.
I0307 06:10:24.543779 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 06:11:13.555185 140085866297152 spec.py:349] Evaluating on the test split.
I0307 06:11:38.826372 140085866297152 submission_runner.py:413] Time since start: 40697.59s, 	Step: 46629, 	{'train/ctc_loss': Array(0.5043172, dtype=float32), 'train/wer': 0.17479643683507115, 'validation/ctc_loss': Array(0.63248414, dtype=float32), 'validation/wer': 0.19732179924114426, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40458766, dtype=float32), 'test/wer': 0.14147015213373143, 'test/num_examples': 2472, 'score': 37510.127170324326, 'total_duration': 40697.58929014206, 'accumulated_submission_time': 37510.127170324326, 'accumulated_eval_time': 3184.3707044124603, 'accumulated_logging_time': 1.3165147304534912}
I0307 06:11:38.865370 139914908104448 logging_writer.py:48] [46629] accumulated_eval_time=3184.370704, accumulated_logging_time=1.316515, accumulated_submission_time=37510.127170, global_step=46629, preemption_count=0, score=37510.127170, test/ctc_loss=0.40458765625953674, test/num_examples=2472, test/wer=0.141470, total_duration=40697.589290, train/ctc_loss=0.5043172240257263, train/wer=0.174796, validation/ctc_loss=0.6324841380119324, validation/num_examples=5348, validation/wer=0.197322
I0307 06:16:27.097523 139914899711744 logging_writer.py:48] [47000] global_step=47000, grad_norm=1.8933807611465454, loss=1.5698367357254028
I0307 06:23:34.433614 139914252744448 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.2261840105056763, loss=1.516181468963623
I0307 06:30:07.733505 139914244351744 logging_writer.py:48] [48000] global_step=48000, grad_norm=1.9564871788024902, loss=1.547897219657898
I0307 06:35:39.313820 140085866297152 spec.py:321] Evaluating on the training split.
I0307 06:36:31.473538 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 06:37:21.460183 140085866297152 spec.py:349] Evaluating on the test split.
I0307 06:37:46.671791 140085866297152 submission_runner.py:413] Time since start: 42265.43s, 	Step: 48382, 	{'train/ctc_loss': Array(0.4572825, dtype=float32), 'train/wer': 0.16372351441328092, 'validation/ctc_loss': Array(0.6121698, dtype=float32), 'validation/wer': 0.19044768626239417, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38863105, dtype=float32), 'test/wer': 0.13458452663863668, 'test/num_examples': 2472, 'score': 38950.496156454086, 'total_duration': 42265.4326710701, 'accumulated_submission_time': 38950.496156454086, 'accumulated_eval_time': 3311.7218165397644, 'accumulated_logging_time': 1.3715198040008545}
I0307 06:37:46.711137 139914252744448 logging_writer.py:48] [48382] accumulated_eval_time=3311.721817, accumulated_logging_time=1.371520, accumulated_submission_time=38950.496156, global_step=48382, preemption_count=0, score=38950.496156, test/ctc_loss=0.38863104581832886, test/num_examples=2472, test/wer=0.134585, total_duration=42265.432671, train/ctc_loss=0.45728251338005066, train/wer=0.163724, validation/ctc_loss=0.6121698021888733, validation/num_examples=5348, validation/wer=0.190448
I0307 06:39:21.917062 139914252744448 logging_writer.py:48] [48500] global_step=48500, grad_norm=1.0813087224960327, loss=1.5075610876083374
I0307 06:45:57.071244 139914244351744 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.3247766494750977, loss=1.585768222808838
I0307 06:53:07.495230 139914908104448 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.7118804454803467, loss=1.4912527799606323
I0307 06:59:36.508203 139914899711744 logging_writer.py:48] [50000] global_step=50000, grad_norm=1.1215019226074219, loss=1.4188292026519775
I0307 07:01:47.123597 140085866297152 spec.py:321] Evaluating on the training split.
I0307 07:02:40.476755 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 07:03:29.851965 140085866297152 spec.py:349] Evaluating on the test split.
I0307 07:03:54.854759 140085866297152 submission_runner.py:413] Time since start: 43833.62s, 	Step: 50152, 	{'train/ctc_loss': Array(0.38403624, dtype=float32), 'train/wer': 0.13940336490880814, 'validation/ctc_loss': Array(0.5849108, dtype=float32), 'validation/wer': 0.1823281230388986, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36841553, dtype=float32), 'test/wer': 0.1263786484674913, 'test/num_examples': 2472, 'score': 40390.831110715866, 'total_duration': 43833.61747145653, 'accumulated_submission_time': 40390.831110715866, 'accumulated_eval_time': 3439.447977542877, 'accumulated_logging_time': 1.4257633686065674}
I0307 07:03:54.889626 139914478024448 logging_writer.py:48] [50152] accumulated_eval_time=3439.447978, accumulated_logging_time=1.425763, accumulated_submission_time=40390.831111, global_step=50152, preemption_count=0, score=40390.831111, test/ctc_loss=0.3684155344963074, test/num_examples=2472, test/wer=0.126379, total_duration=43833.617471, train/ctc_loss=0.38403624296188354, train/wer=0.139403, validation/ctc_loss=0.5849108099937439, validation/num_examples=5348, validation/wer=0.182328
I0307 07:08:29.709684 139914150344448 logging_writer.py:48] [50500] global_step=50500, grad_norm=1.4274959564208984, loss=1.4880958795547485
I0307 07:15:04.511959 139914141951744 logging_writer.py:48] [51000] global_step=51000, grad_norm=2.0034120082855225, loss=1.4990835189819336
I0307 07:22:28.792432 139914478024448 logging_writer.py:48] [51500] global_step=51500, grad_norm=1.4264180660247803, loss=1.435639500617981
I0307 07:27:55.154363 140085866297152 spec.py:321] Evaluating on the training split.
I0307 07:28:47.573474 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 07:29:36.870703 140085866297152 spec.py:349] Evaluating on the test split.
I0307 07:30:01.942820 140085866297152 submission_runner.py:413] Time since start: 45400.71s, 	Step: 51924, 	{'train/ctc_loss': Array(0.4078082, dtype=float32), 'train/wer': 0.14524214265153942, 'validation/ctc_loss': Array(0.5672598, dtype=float32), 'validation/wer': 0.1777807814476187, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35503602, dtype=float32), 'test/wer': 0.12050860195397396, 'test/num_examples': 2472, 'score': 41831.017669439316, 'total_duration': 45400.7054476738, 'accumulated_submission_time': 41831.017669439316, 'accumulated_eval_time': 3566.2313644886017, 'accumulated_logging_time': 1.4762351512908936}
I0307 07:30:01.978123 139914908104448 logging_writer.py:48] [51924] accumulated_eval_time=3566.231364, accumulated_logging_time=1.476235, accumulated_submission_time=41831.017669, global_step=51924, preemption_count=0, score=41831.017669, test/ctc_loss=0.35503602027893066, test/num_examples=2472, test/wer=0.120509, total_duration=45400.705448, train/ctc_loss=0.40780821442604065, train/wer=0.145242, validation/ctc_loss=0.5672597885131836, validation/num_examples=5348, validation/wer=0.177781
I0307 07:31:01.437696 139914899711744 logging_writer.py:48] [52000] global_step=52000, grad_norm=1.968367338180542, loss=1.4459904432296753
I0307 07:37:55.503316 139914908104448 logging_writer.py:48] [52500] global_step=52500, grad_norm=1.481629490852356, loss=1.4284220933914185
I0307 07:44:27.510555 139914908104448 logging_writer.py:48] [53000] global_step=53000, grad_norm=1.5512199401855469, loss=1.4522016048431396
I0307 07:51:44.391091 139914899711744 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.2261428833007812, loss=1.4847220182418823
I0307 07:54:02.269863 140085866297152 spec.py:321] Evaluating on the training split.
I0307 07:54:55.034763 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 07:55:44.612534 140085866297152 spec.py:349] Evaluating on the test split.
I0307 07:56:09.588066 140085866297152 submission_runner.py:413] Time since start: 46968.35s, 	Step: 53667, 	{'train/ctc_loss': Array(0.36456165, dtype=float32), 'train/wer': 0.13382580312661055, 'validation/ctc_loss': Array(0.5535161, dtype=float32), 'validation/wer': 0.17389961091748168, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34406787, dtype=float32), 'test/wer': 0.11807121239818821, 'test/num_examples': 2472, 'score': 43271.22687149048, 'total_duration': 46968.35110974312, 'accumulated_submission_time': 43271.22687149048, 'accumulated_eval_time': 3693.5450909137726, 'accumulated_logging_time': 1.5323927402496338}
I0307 07:56:09.621928 139914580424448 logging_writer.py:48] [53667] accumulated_eval_time=3693.545091, accumulated_logging_time=1.532393, accumulated_submission_time=43271.226871, global_step=53667, preemption_count=0, score=43271.226871, test/ctc_loss=0.34406787157058716, test/num_examples=2472, test/wer=0.118071, total_duration=46968.351110, train/ctc_loss=0.36456164717674255, train/wer=0.133826, validation/ctc_loss=0.5535160899162292, validation/num_examples=5348, validation/wer=0.173900
I0307 08:00:27.631430 139914572031744 logging_writer.py:48] [54000] global_step=54000, grad_norm=1.5417066812515259, loss=1.4446834325790405
I0307 08:07:35.532230 139914580424448 logging_writer.py:48] [54500] global_step=54500, grad_norm=1.594970703125, loss=1.4132657051086426
I0307 08:14:17.883736 139914580424448 logging_writer.py:48] [55000] global_step=55000, grad_norm=2.0731327533721924, loss=1.3663880825042725
I0307 08:20:10.545385 140085866297152 spec.py:321] Evaluating on the training split.
I0307 08:21:03.383487 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 08:21:53.498660 140085866297152 spec.py:349] Evaluating on the test split.
I0307 08:22:18.767304 140085866297152 submission_runner.py:413] Time since start: 48537.53s, 	Step: 55412, 	{'train/ctc_loss': Array(0.36444324, dtype=float32), 'train/wer': 0.13349027435750008, 'validation/ctc_loss': Array(0.54100156, dtype=float32), 'validation/wer': 0.16998947642816456, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33620584, dtype=float32), 'test/wer': 0.11508541019235066, 'test/num_examples': 2472, 'score': 44712.072283029556, 'total_duration': 48537.52812194824, 'accumulated_submission_time': 44712.072283029556, 'accumulated_eval_time': 3821.760100364685, 'accumulated_logging_time': 1.5818710327148438}
I0307 08:22:18.808341 139914580424448 logging_writer.py:48] [55412] accumulated_eval_time=3821.760100, accumulated_logging_time=1.581871, accumulated_submission_time=44712.072283, global_step=55412, preemption_count=0, score=44712.072283, test/ctc_loss=0.3362058401107788, test/num_examples=2472, test/wer=0.115085, total_duration=48537.528122, train/ctc_loss=0.36444324254989624, train/wer=0.133490, validation/ctc_loss=0.541001558303833, validation/num_examples=5348, validation/wer=0.169989
I0307 08:23:27.573979 139914572031744 logging_writer.py:48] [55500] global_step=55500, grad_norm=1.3548235893249512, loss=1.447517991065979
I0307 08:29:57.271033 139914908104448 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.1400929689407349, loss=1.3825472593307495
I0307 08:37:16.991701 139914899711744 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.760803461074829, loss=1.3682360649108887
I0307 08:44:03.683064 139914580424448 logging_writer.py:48] [57000] global_step=57000, grad_norm=1.1101634502410889, loss=1.4235478639602661
I0307 08:46:18.979959 140085866297152 spec.py:321] Evaluating on the training split.
I0307 08:47:11.170675 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 08:48:00.702178 140085866297152 spec.py:349] Evaluating on the test split.
I0307 08:48:26.158817 140085866297152 submission_runner.py:413] Time since start: 50104.92s, 	Step: 57169, 	{'train/ctc_loss': Array(0.361306, dtype=float32), 'train/wer': 0.12978329882677708, 'validation/ctc_loss': Array(0.5362888, dtype=float32), 'validation/wer': 0.16771097830599457, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33129215, dtype=float32), 'test/wer': 0.1132370564458798, 'test/num_examples': 2472, 'score': 46152.162573337555, 'total_duration': 50104.92146348953, 'accumulated_submission_time': 46152.162573337555, 'accumulated_eval_time': 3948.9338853359222, 'accumulated_logging_time': 1.6429102420806885}
I0307 08:48:26.193102 139915061704448 logging_writer.py:48] [57169] accumulated_eval_time=3948.933885, accumulated_logging_time=1.642910, accumulated_submission_time=46152.162573, global_step=57169, preemption_count=0, score=46152.162573, test/ctc_loss=0.33129215240478516, test/num_examples=2472, test/wer=0.113237, total_duration=50104.921463, train/ctc_loss=0.36130601167678833, train/wer=0.129783, validation/ctc_loss=0.5362887978553772, validation/num_examples=5348, validation/wer=0.167711
I0307 08:52:47.606654 139915053311744 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.3945430517196655, loss=1.412880301475525
I0307 08:59:38.647790 139914734024448 logging_writer.py:48] [58000] global_step=58000, grad_norm=1.2067230939865112, loss=1.3762879371643066
I0307 09:06:52.168693 139914725631744 logging_writer.py:48] [58500] global_step=58500, grad_norm=1.1865322589874268, loss=1.3952122926712036
I0307 09:12:26.219472 140085866297152 spec.py:321] Evaluating on the training split.
I0307 09:13:17.831632 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 09:14:08.447853 140085866297152 spec.py:349] Evaluating on the test split.
I0307 09:14:34.048718 140085866297152 submission_runner.py:413] Time since start: 51672.81s, 	Step: 58895, 	{'train/ctc_loss': Array(0.37408665, dtype=float32), 'train/wer': 0.13559516042260453, 'validation/ctc_loss': Array(0.53304183, dtype=float32), 'validation/wer': 0.16699653397955144, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32993034, dtype=float32), 'test/wer': 0.11207929640688157, 'test/num_examples': 2472, 'score': 47592.11108016968, 'total_duration': 51672.81087613106, 'accumulated_submission_time': 47592.11108016968, 'accumulated_eval_time': 4076.7576253414154, 'accumulated_logging_time': 1.6928706169128418}
I0307 09:14:34.085741 139915491784448 logging_writer.py:48] [58895] accumulated_eval_time=4076.757625, accumulated_logging_time=1.692871, accumulated_submission_time=47592.111080, global_step=58895, preemption_count=0, score=47592.111080, test/ctc_loss=0.3299303352832794, test/num_examples=2472, test/wer=0.112079, total_duration=51672.810876, train/ctc_loss=0.3740866482257843, train/wer=0.135595, validation/ctc_loss=0.5330418348312378, validation/num_examples=5348, validation/wer=0.166997
I0307 09:15:56.033613 139915483391744 logging_writer.py:48] [59000] global_step=59000, grad_norm=1.4858112335205078, loss=1.4334986209869385
I0307 09:22:44.949482 139915491784448 logging_writer.py:48] [59500] global_step=59500, grad_norm=2.307609796524048, loss=1.4219465255737305
I0307 09:29:41.748376 139914836424448 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.8209443092346191, loss=1.410043716430664
I0307 09:36:42.724351 139914828031744 logging_writer.py:48] [60500] global_step=60500, grad_norm=1.4292525053024292, loss=1.3922374248504639
I0307 09:38:34.731458 140085866297152 spec.py:321] Evaluating on the training split.
I0307 09:39:28.306952 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 09:40:18.762363 140085866297152 spec.py:349] Evaluating on the test split.
I0307 09:40:43.972787 140085866297152 submission_runner.py:413] Time since start: 53242.73s, 	Step: 60630, 	{'train/ctc_loss': Array(0.3762794, dtype=float32), 'train/wer': 0.13277752970518084, 'validation/ctc_loss': Array(0.53248405, dtype=float32), 'validation/wer': 0.16678413161223052, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32958347, dtype=float32), 'test/wer': 0.11161213007535596, 'test/num_examples': 2472, 'score': 49032.677661418915, 'total_duration': 53242.73287272453, 'accumulated_submission_time': 49032.677661418915, 'accumulated_eval_time': 4205.99129152298, 'accumulated_logging_time': 1.7464699745178223}
I0307 09:40:44.010029 139914836424448 logging_writer.py:48] [60630] accumulated_eval_time=4205.991292, accumulated_logging_time=1.746470, accumulated_submission_time=49032.677661, global_step=60630, preemption_count=0, score=49032.677661, test/ctc_loss=0.32958346605300903, test/num_examples=2472, test/wer=0.111612, total_duration=53242.732873, train/ctc_loss=0.37627941370010376, train/wer=0.132778, validation/ctc_loss=0.5324840545654297, validation/num_examples=5348, validation/wer=0.166784
I0307 09:45:33.929149 139915491784448 logging_writer.py:48] [61000] global_step=61000, grad_norm=1.3085113763809204, loss=1.4722555875778198
I0307 09:52:26.042435 139915483391744 logging_writer.py:48] [61500] global_step=61500, grad_norm=1.4299390316009521, loss=1.3684277534484863
I0307 09:59:25.297532 139914836424448 logging_writer.py:48] [62000] global_step=62000, grad_norm=1.4421322345733643, loss=1.369373083114624
I0307 10:04:44.096619 140085866297152 spec.py:321] Evaluating on the training split.
I0307 10:05:36.772128 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 10:06:26.537323 140085866297152 spec.py:349] Evaluating on the test split.
I0307 10:06:51.704788 140085866297152 submission_runner.py:413] Time since start: 54810.46s, 	Step: 62392, 	{'train/ctc_loss': Array(0.35936368, dtype=float32), 'train/wer': 0.13032707870961957, 'validation/ctc_loss': Array(0.53247744, dtype=float32), 'validation/wer': 0.16674551299999035, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32956782, dtype=float32), 'test/wer': 0.11153088375682976, 'test/num_examples': 2472, 'score': 50472.684576034546, 'total_duration': 54810.45715737343, 'accumulated_submission_time': 50472.684576034546, 'accumulated_eval_time': 4333.584086894989, 'accumulated_logging_time': 1.800659418106079}
I0307 10:06:51.751395 139915491784448 logging_writer.py:48] [62392] accumulated_eval_time=4333.584087, accumulated_logging_time=1.800659, accumulated_submission_time=50472.684576, global_step=62392, preemption_count=0, score=50472.684576, test/ctc_loss=0.3295678198337555, test/num_examples=2472, test/wer=0.111531, total_duration=54810.457157, train/ctc_loss=0.3593636751174927, train/wer=0.130327, validation/ctc_loss=0.5324774384498596, validation/num_examples=5348, validation/wer=0.166746
I0307 10:08:16.000535 139915483391744 logging_writer.py:48] [62500] global_step=62500, grad_norm=1.3150453567504883, loss=1.4588181972503662
I0307 10:15:01.995804 139915491784448 logging_writer.py:48] [63000] global_step=63000, grad_norm=1.7247933149337769, loss=1.3647092580795288
I0307 10:21:39.793187 139915483391744 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.2003034353256226, loss=1.4464200735092163
I0307 10:28:38.044974 139915491784448 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.3622640371322632, loss=1.3628370761871338
I0307 10:30:52.349493 140085866297152 spec.py:321] Evaluating on the training split.
I0307 10:31:43.468981 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 10:32:33.089536 140085866297152 spec.py:349] Evaluating on the test split.
I0307 10:32:58.267983 140085866297152 submission_runner.py:413] Time since start: 56377.03s, 	Step: 64175, 	{'train/ctc_loss': Array(0.36941895, dtype=float32), 'train/wer': 0.13656382638857195, 'validation/ctc_loss': Array(0.5324699, dtype=float32), 'validation/wer': 0.1667165490408102, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32956153, dtype=float32), 'test/wer': 0.11151057217719822, 'test/num_examples': 2472, 'score': 51913.202800273895, 'total_duration': 56377.03063416481, 'accumulated_submission_time': 51913.202800273895, 'accumulated_eval_time': 4459.497474193573, 'accumulated_logging_time': 1.8636326789855957}
I0307 10:32:58.301981 139914769864448 logging_writer.py:48] [64175] accumulated_eval_time=4459.497474, accumulated_logging_time=1.863633, accumulated_submission_time=51913.202800, global_step=64175, preemption_count=0, score=51913.202800, test/ctc_loss=0.3295615315437317, test/num_examples=2472, test/wer=0.111511, total_duration=56377.030634, train/ctc_loss=0.3694189488887787, train/wer=0.136564, validation/ctc_loss=0.5324699282646179, validation/num_examples=5348, validation/wer=0.166717
I0307 10:37:10.988800 139914761471744 logging_writer.py:48] [64500] global_step=64500, grad_norm=1.6744065284729004, loss=1.3570678234100342
I0307 10:44:14.137511 139914769864448 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.4611181020736694, loss=1.4274530410766602
I0307 10:50:50.511335 139914761471744 logging_writer.py:48] [65500] global_step=65500, grad_norm=1.933491826057434, loss=1.4070943593978882
I0307 10:56:59.368721 140085866297152 spec.py:321] Evaluating on the training split.
I0307 10:57:51.786908 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 10:58:41.610503 140085866297152 spec.py:349] Evaluating on the test split.
I0307 10:59:06.638146 140085866297152 submission_runner.py:413] Time since start: 57945.40s, 	Step: 65921, 	{'train/ctc_loss': Array(0.35998207, dtype=float32), 'train/wer': 0.1303620050886042, 'validation/ctc_loss': Array(0.5324719, dtype=float32), 'validation/wer': 0.1667551676530504, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3295652, dtype=float32), 'test/wer': 0.11157150691609287, 'test/num_examples': 2472, 'score': 53354.19180417061, 'total_duration': 57945.39933538437, 'accumulated_submission_time': 53354.19180417061, 'accumulated_eval_time': 4586.760721206665, 'accumulated_logging_time': 1.913205862045288}
I0307 10:59:06.679170 139915199944448 logging_writer.py:48] [65921] accumulated_eval_time=4586.760721, accumulated_logging_time=1.913206, accumulated_submission_time=53354.191804, global_step=65921, preemption_count=0, score=53354.191804, test/ctc_loss=0.3295651972293854, test/num_examples=2472, test/wer=0.111572, total_duration=57945.399335, train/ctc_loss=0.35998207330703735, train/wer=0.130362, validation/ctc_loss=0.5324718952178955, validation/num_examples=5348, validation/wer=0.166755
I0307 11:00:08.415880 139915191551744 logging_writer.py:48] [66000] global_step=66000, grad_norm=3.29292368888855, loss=1.424471139907837
I0307 11:06:35.180361 139915199944448 logging_writer.py:48] [66500] global_step=66500, grad_norm=1.6588023900985718, loss=1.3964964151382446
I0307 11:13:54.339706 139914442184448 logging_writer.py:48] [67000] global_step=67000, grad_norm=1.7583423852920532, loss=1.3967269659042358
I0307 11:20:29.991887 139914433791744 logging_writer.py:48] [67500] global_step=67500, grad_norm=1.5485082864761353, loss=1.3912572860717773
I0307 11:23:06.740646 140085866297152 spec.py:321] Evaluating on the training split.
I0307 11:23:58.444010 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 11:24:48.066479 140085866297152 spec.py:349] Evaluating on the test split.
I0307 11:25:13.033943 140085866297152 submission_runner.py:413] Time since start: 59511.80s, 	Step: 67678, 	{'train/ctc_loss': Array(0.36828643, dtype=float32), 'train/wer': 0.1313276815249634, 'validation/ctc_loss': Array(0.5324746, dtype=float32), 'validation/wer': 0.1667551676530504, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32956183, dtype=float32), 'test/wer': 0.11153088375682976, 'test/num_examples': 2472, 'score': 54794.17435336113, 'total_duration': 59511.79700446129, 'accumulated_submission_time': 54794.17435336113, 'accumulated_eval_time': 4713.049329996109, 'accumulated_logging_time': 1.9709267616271973}
I0307 11:25:13.072787 139914872264448 logging_writer.py:48] [67678] accumulated_eval_time=4713.049330, accumulated_logging_time=1.970927, accumulated_submission_time=54794.174353, global_step=67678, preemption_count=0, score=54794.174353, test/ctc_loss=0.32956182956695557, test/num_examples=2472, test/wer=0.111531, total_duration=59511.797004, train/ctc_loss=0.3682864308357239, train/wer=0.131328, validation/ctc_loss=0.5324745774269104, validation/num_examples=5348, validation/wer=0.166755
I0307 11:29:25.350577 139914544584448 logging_writer.py:48] [68000] global_step=68000, grad_norm=2.4989736080169678, loss=1.391399621963501
I0307 11:35:57.065414 139914536191744 logging_writer.py:48] [68500] global_step=68500, grad_norm=1.3358668088912964, loss=1.408308982849121
I0307 11:43:22.388280 139914544584448 logging_writer.py:48] [69000] global_step=69000, grad_norm=1.4186224937438965, loss=1.373299241065979
I0307 11:49:13.318967 140085866297152 spec.py:321] Evaluating on the training split.
I0307 11:50:05.149489 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 11:50:55.065252 140085866297152 spec.py:349] Evaluating on the test split.
I0307 11:51:20.932519 140085866297152 submission_runner.py:413] Time since start: 61079.70s, 	Step: 69450, 	{'train/ctc_loss': Array(0.36868584, dtype=float32), 'train/wer': 0.13131973793072604, 'validation/ctc_loss': Array(0.5324691, dtype=float32), 'validation/wer': 0.1667551676530504, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3295609, dtype=float32), 'test/wer': 0.11153088375682976, 'test/num_examples': 2472, 'score': 56234.33962392807, 'total_duration': 61079.69539475441, 'accumulated_submission_time': 56234.33962392807, 'accumulated_eval_time': 4840.658200502396, 'accumulated_logging_time': 2.027392864227295}
I0307 11:51:20.968290 139914544584448 logging_writer.py:48] [69450] accumulated_eval_time=4840.658201, accumulated_logging_time=2.027393, accumulated_submission_time=56234.339624, global_step=69450, preemption_count=0, score=56234.339624, test/ctc_loss=0.32956090569496155, test/num_examples=2472, test/wer=0.111531, total_duration=61079.695395, train/ctc_loss=0.36868584156036377, train/wer=0.131320, validation/ctc_loss=0.5324690937995911, validation/num_examples=5348, validation/wer=0.166755
I0307 11:52:00.476183 139914536191744 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.2462613582611084, loss=1.3329282999038696
I0307 11:58:49.165604 139914544584448 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.1850744485855103, loss=1.389407753944397
I0307 12:05:22.912384 139914544584448 logging_writer.py:48] [70500] global_step=70500, grad_norm=1.304113507270813, loss=1.3460602760314941
I0307 12:12:43.421617 139914536191744 logging_writer.py:48] [71000] global_step=71000, grad_norm=1.223966121673584, loss=1.421403408050537
I0307 12:15:21.117261 140085866297152 spec.py:321] Evaluating on the training split.
I0307 12:16:13.807889 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 12:17:03.300387 140085866297152 spec.py:349] Evaluating on the test split.
I0307 12:17:28.368417 140085866297152 submission_runner.py:413] Time since start: 62647.13s, 	Step: 71192, 	{'train/ctc_loss': Array(0.3731729, dtype=float32), 'train/wer': 0.1361145855240583, 'validation/ctc_loss': Array(0.53247434, dtype=float32), 'validation/wer': 0.16677447695917047, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32956335, dtype=float32), 'test/wer': 0.11153088375682976, 'test/num_examples': 2472, 'score': 57674.40916514397, 'total_duration': 62647.13051748276, 'accumulated_submission_time': 57674.40916514397, 'accumulated_eval_time': 4967.903873920441, 'accumulated_logging_time': 2.079360246658325}
I0307 12:17:28.403997 139914252744448 logging_writer.py:48] [71192] accumulated_eval_time=4967.903874, accumulated_logging_time=2.079360, accumulated_submission_time=57674.409165, global_step=71192, preemption_count=0, score=57674.409165, test/ctc_loss=0.32956334948539734, test/num_examples=2472, test/wer=0.111531, total_duration=62647.130517, train/ctc_loss=0.37317290902137756, train/wer=0.136115, validation/ctc_loss=0.5324743390083313, validation/num_examples=5348, validation/wer=0.166774
I0307 12:21:27.306077 139914244351744 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.2913914918899536, loss=1.4094308614730835
I0307 12:28:27.863130 139914252744448 logging_writer.py:48] [72000] global_step=72000, grad_norm=1.7608715295791626, loss=1.3430553674697876
I0307 12:35:08.877455 139914252744448 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.2752455472946167, loss=1.3441896438598633
I0307 12:41:29.763776 140085866297152 spec.py:321] Evaluating on the training split.
I0307 12:42:21.300304 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 12:43:10.817368 140085866297152 spec.py:349] Evaluating on the test split.
I0307 12:43:36.074225 140085866297152 submission_runner.py:413] Time since start: 64214.84s, 	Step: 72946, 	{'train/ctc_loss': Array(0.34705588, dtype=float32), 'train/wer': 0.12490883289139665, 'validation/ctc_loss': Array(0.53247154, dtype=float32), 'validation/wer': 0.16674551299999035, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32956704, dtype=float32), 'test/wer': 0.11155119533646132, 'test/num_examples': 2472, 'score': 59115.69067811966, 'total_duration': 64214.83722138405, 'accumulated_submission_time': 59115.69067811966, 'accumulated_eval_time': 5094.209578990936, 'accumulated_logging_time': 2.1314005851745605}
I0307 12:43:36.113545 139915199944448 logging_writer.py:48] [72946] accumulated_eval_time=5094.209579, accumulated_logging_time=2.131401, accumulated_submission_time=59115.690678, global_step=72946, preemption_count=0, score=59115.690678, test/ctc_loss=0.3295670449733734, test/num_examples=2472, test/wer=0.111551, total_duration=64214.837221, train/ctc_loss=0.3470558822154999, train/wer=0.124909, validation/ctc_loss=0.5324715375900269, validation/num_examples=5348, validation/wer=0.166746
I0307 12:44:18.669233 139915191551744 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.5149238109588623, loss=1.444419026374817
I0307 12:50:49.195677 139915199944448 logging_writer.py:48] [73500] global_step=73500, grad_norm=1.26658034324646, loss=1.378174066543579
I0307 12:58:04.272276 139915191551744 logging_writer.py:48] [74000] global_step=74000, grad_norm=2.233646869659424, loss=1.417149305343628
I0307 13:04:48.713667 139915199944448 logging_writer.py:48] [74500] global_step=74500, grad_norm=1.7631620168685913, loss=1.370071291923523
I0307 13:07:36.668849 140085866297152 spec.py:321] Evaluating on the training split.
I0307 13:08:28.207854 140085866297152 spec.py:333] Evaluating on the validation split.
I0307 13:09:17.869234 140085866297152 spec.py:349] Evaluating on the test split.
I0307 13:09:43.103092 140085866297152 submission_runner.py:413] Time since start: 65781.86s, 	Step: 74709, 	{'train/ctc_loss': Array(0.33405918, dtype=float32), 'train/wer': 0.11973175193851138, 'validation/ctc_loss': Array(0.5324755, dtype=float32), 'validation/wer': 0.1667358583469303, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3295662, dtype=float32), 'test/wer': 0.11157150691609287, 'test/num_examples': 2472, 'score': 60556.16478013992, 'total_duration': 65781.86285066605, 'accumulated_submission_time': 60556.16478013992, 'accumulated_eval_time': 5220.635853767395, 'accumulated_logging_time': 2.188387870788574}
I0307 13:09:43.141208 139914478024448 logging_writer.py:48] [74709] accumulated_eval_time=5220.635854, accumulated_logging_time=2.188388, accumulated_submission_time=60556.164780, global_step=74709, preemption_count=0, score=60556.164780, test/ctc_loss=0.32956621050834656, test/num_examples=2472, test/wer=0.111572, total_duration=65781.862851, train/ctc_loss=0.3340591788291931, train/wer=0.119732, validation/ctc_loss=0.532475471496582, validation/num_examples=5348, validation/wer=0.166736
I0307 13:13:28.745269 139914469631744 logging_writer.py:48] [75000] global_step=75000, grad_norm=1.3335089683532715, loss=1.3682150840759277
I0307 13:18:15.789686 139914478024448 logging_writer.py:48] [75348] global_step=75348, preemption_count=0, score=61068.753369
I0307 13:18:16.690918 140085866297152 checkpoints.py:490] Saving checkpoint at step: 75348
I0307 13:18:18.265169 140085866297152 checkpoints.py:422] Saved checkpoint at /experiment_runs/variants_target_setting/study_0/librispeech_conformer_gelu_jax/trial_1/checkpoint_75348
I0307 13:18:18.297264 140085866297152 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_gelu_jax/trial_1/checkpoint_75348.
I0307 13:18:21.900537 140085866297152 submission_runner.py:588] Tuning trial 1/1
I0307 13:18:21.900776 140085866297152 submission_runner.py:589] Hyperparameters: Hyperparameters(learning_rate=0.001308209823469072, beta1=0.9731333693827139, beta2=0.9981232922116359, warmup_steps=9999, weight_decay=0.16375311233774334)
I0307 13:18:21.923280 140085866297152 submission_runner.py:590] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.74491, dtype=float32), 'train/wer': 1.220186160797951, 'validation/ctc_loss': Array(30.874546, dtype=float32), 'validation/wer': 1.1701632601832452, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.977095, dtype=float32), 'test/wer': 1.1826214124672476, 'test/num_examples': 2472, 'score': 61.07155728340149, 'total_duration': 231.955472946167, 'accumulated_submission_time': 61.07155728340149, 'accumulated_eval_time': 170.8838496208191, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1794, {'train/ctc_loss': Array(5.912836, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(5.928612, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.8985214, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1501.1025803089142, 'total_duration': 1772.187667608261, 'accumulated_submission_time': 1501.1025803089142, 'accumulated_eval_time': 270.9841420650482, 'accumulated_logging_time': 0.04010772705078125, 'global_step': 1794, 'preemption_count': 0}), (3626, {'train/ctc_loss': Array(5.6478033, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': Array(5.815719, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.764541, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2941.5714643001556, 'total_duration': 3314.1048407554626, 'accumulated_submission_time': 2941.5714643001556, 'accumulated_eval_time': 372.3185966014862, 'accumulated_logging_time': 0.08719491958618164, 'global_step': 3626, 'preemption_count': 0}), (5454, {'train/ctc_loss': Array(5.7372236, dtype=float32), 'train/wer': 0.9432768790423328, 'validation/ctc_loss': Array(5.7154565, dtype=float32), 'validation/wer': 0.8965986657269471, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.7174153, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4381.4923849105835, 'total_duration': 4856.247415781021, 'accumulated_submission_time': 4381.4923849105835, 'accumulated_eval_time': 474.42303490638733, 'accumulated_logging_time': 0.13482069969177246, 'global_step': 5454, 'preemption_count': 0}), (7277, {'train/ctc_loss': Array(5.480827, dtype=float32), 'train/wer': 0.9408727706856287, 'validation/ctc_loss': Array(5.443987, dtype=float32), 'validation/wer': 0.8959518039719243, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.428039, dtype=float32), 'test/wer': 0.8985233481607865, 'test/num_examples': 2472, 'score': 5821.502820491791, 'total_duration': 6398.1597690582275, 'accumulated_submission_time': 5821.502820491791, 'accumulated_eval_time': 576.2081487178802, 'accumulated_logging_time': 0.1852715015411377, 'global_step': 7277, 'preemption_count': 0}), (9104, {'train/ctc_loss': Array(5.5783405, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': Array(5.50731, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4930353, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7261.83672618866, 'total_duration': 7941.004846572876, 'accumulated_submission_time': 7261.83672618866, 'accumulated_eval_time': 678.603214263916, 'accumulated_logging_time': 0.23277878761291504, 'global_step': 9104, 'preemption_count': 0}), (10908, {'train/ctc_loss': Array(5.553975, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': Array(5.497823, dtype=float32), 'validation/wer': 0.8965214285024667, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4824014, dtype=float32), 'test/wer': 0.8994373692442061, 'test/num_examples': 2472, 'score': 8702.423407554626, 'total_duration': 9485.03901386261, 'accumulated_submission_time': 8702.423407554626, 'accumulated_eval_time': 781.9331517219543, 'accumulated_logging_time': 0.28314208984375, 'global_step': 10908, 'preemption_count': 0}), (12723, {'train/ctc_loss': Array(6.263767, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': Array(6.1856422, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.2025256, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 10142.756002426147, 'total_duration': 11028.150895118713, 'accumulated_submission_time': 10142.756002426147, 'accumulated_eval_time': 884.5934579372406, 'accumulated_logging_time': 0.33275580406188965, 'global_step': 12723, 'preemption_count': 0}), (14521, {'train/ctc_loss': Array(6.503599, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': Array(6.0460415, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.0825634, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 11583.276812314987, 'total_duration': 12573.201262712479, 'accumulated_submission_time': 11583.276812314987, 'accumulated_eval_time': 989.0036432743073, 'accumulated_logging_time': 0.3832833766937256, 'global_step': 14521, 'preemption_count': 0}), (16328, {'train/ctc_loss': Array(5.595888, dtype=float32), 'train/wer': 0.9427990785714666, 'validation/ctc_loss': Array(5.544391, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.5321684, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 13023.197110891342, 'total_duration': 14116.616592407227, 'accumulated_submission_time': 13023.197110891342, 'accumulated_eval_time': 1092.38232588768, 'accumulated_logging_time': 0.4308288097381592, 'global_step': 16328, 'preemption_count': 0}), (18144, {'train/ctc_loss': Array(6.1803946, dtype=float32), 'train/wer': 0.9423383225986367, 'validation/ctc_loss': Array(5.839062, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.870875, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14463.913548469543, 'total_duration': 15660.982357740402, 'accumulated_submission_time': 14463.913548469543, 'accumulated_eval_time': 1195.9102911949158, 'accumulated_logging_time': 0.48088574409484863, 'global_step': 18144, 'preemption_count': 0}), (19955, {'train/ctc_loss': Array(5.586595, dtype=float32), 'train/wer': 0.9431026732594727, 'validation/ctc_loss': Array(5.522736, dtype=float32), 'validation/wer': 0.896579356420827, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.506677, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15904.555320501328, 'total_duration': 17204.162836313248, 'accumulated_submission_time': 15904.555320501328, 'accumulated_eval_time': 1298.3295764923096, 'accumulated_logging_time': 0.5306649208068848, 'global_step': 19955, 'preemption_count': 0}), (21732, {'train/ctc_loss': Array(5.4757457, dtype=float32), 'train/wer': 0.9432716912443612, 'validation/ctc_loss': Array(5.4281535, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3822036, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 17345.169860839844, 'total_duration': 18748.173184871674, 'accumulated_submission_time': 17345.169860839844, 'accumulated_eval_time': 1401.610151052475, 'accumulated_logging_time': 0.5777120590209961, 'global_step': 21732, 'preemption_count': 0}), (23499, {'train/ctc_loss': Array(4.039452, dtype=float32), 'train/wer': 0.7950457866035601, 'validation/ctc_loss': Array(4.2082353, dtype=float32), 'validation/wer': 0.7809166127615205, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.9759412, dtype=float32), 'test/wer': 0.7644872341722015, 'test/num_examples': 2472, 'score': 18785.657301664352, 'total_duration': 20315.190623521805, 'accumulated_submission_time': 18785.657301664352, 'accumulated_eval_time': 1528.0189802646637, 'accumulated_logging_time': 0.6300628185272217, 'global_step': 23499, 'preemption_count': 0}), (25293, {'train/ctc_loss': Array(2.2726598, dtype=float32), 'train/wer': 0.5717136100671424, 'validation/ctc_loss': Array(2.5912151, dtype=float32), 'validation/wer': 0.5982505768655203, 'validation/num_examples': 5348, 'test/ctc_loss': Array(2.2852595, dtype=float32), 'test/wer': 0.5666930717201877, 'test/num_examples': 2472, 'score': 20226.497428417206, 'total_duration': 21883.76142835617, 'accumulated_submission_time': 20226.497428417206, 'accumulated_eval_time': 1655.6274774074554, 'accumulated_logging_time': 0.6829721927642822, 'global_step': 25293, 'preemption_count': 0}), (27072, {'train/ctc_loss': Array(1.665647, dtype=float32), 'train/wer': 0.48148882339132476, 'validation/ctc_loss': Array(2.0445182, dtype=float32), 'validation/wer': 0.5215540129565444, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.7162665, dtype=float32), 'test/wer': 0.47498628968374873, 'test/num_examples': 2472, 'score': 21666.952129125595, 'total_duration': 23453.050668239594, 'accumulated_submission_time': 21666.952129125595, 'accumulated_eval_time': 1784.3372511863708, 'accumulated_logging_time': 0.7387275695800781, 'global_step': 27072, 'preemption_count': 0}), (28833, {'train/ctc_loss': Array(1.3347486, dtype=float32), 'train/wer': 0.41529185545816244, 'validation/ctc_loss': Array(1.744137, dtype=float32), 'validation/wer': 0.467806559371289, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.4032996, dtype=float32), 'test/wer': 0.4169967298356793, 'test/num_examples': 2472, 'score': 23106.915238142014, 'total_duration': 25022.07224869728, 'accumulated_submission_time': 23106.915238142014, 'accumulated_eval_time': 1913.2779862880707, 'accumulated_logging_time': 0.7904481887817383, 'global_step': 28833, 'preemption_count': 0}), (30601, {'train/ctc_loss': Array(1.0808085, dtype=float32), 'train/wer': 0.35850017334026696, 'validation/ctc_loss': Array(1.419696, dtype=float32), 'validation/wer': 0.4065864043175608, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.1014445, dtype=float32), 'test/wer': 0.35015132126825504, 'test/num_examples': 2472, 'score': 24547.28302192688, 'total_duration': 26590.293665647507, 'accumulated_submission_time': 24547.28302192688, 'accumulated_eval_time': 2041.0138404369354, 'accumulated_logging_time': 0.8401660919189453, 'global_step': 30601, 'preemption_count': 0}), (32404, {'train/ctc_loss': Array(0.819108, dtype=float32), 'train/wer': 0.27726140673579347, 'validation/ctc_loss': Array(1.1686217, dtype=float32), 'validation/wer': 0.3421029765295384, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.867956, dtype=float32), 'test/wer': 0.28352934007677777, 'test/num_examples': 2472, 'score': 25987.906824588776, 'total_duration': 28157.641040086746, 'accumulated_submission_time': 25987.906824588776, 'accumulated_eval_time': 2167.6180663108826, 'accumulated_logging_time': 0.8903062343597412, 'global_step': 32404, 'preemption_count': 0}), (34194, {'train/ctc_loss': Array(0.690051, dtype=float32), 'train/wer': 0.23719236242031347, 'validation/ctc_loss': Array(1.0043803, dtype=float32), 'validation/wer': 0.29873427498382843, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.71552753, dtype=float32), 'test/wer': 0.23900635752442467, 'test/num_examples': 2472, 'score': 27428.221712589264, 'total_duration': 29725.61915254593, 'accumulated_submission_time': 27428.221712589264, 'accumulated_eval_time': 2295.1442720890045, 'accumulated_logging_time': 0.9597721099853516, 'global_step': 34194, 'preemption_count': 0}), (35958, {'train/ctc_loss': Array(0.597442, dtype=float32), 'train/wer': 0.20695461278600125, 'validation/ctc_loss': Array(0.9062112, dtype=float32), 'validation/wer': 0.27678924857835235, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6275954, dtype=float32), 'test/wer': 0.21182946397741353, 'test/num_examples': 2472, 'score': 28868.98548579216, 'total_duration': 31294.67875289917, 'accumulated_submission_time': 28868.98548579216, 'accumulated_eval_time': 2423.3272914886475, 'accumulated_logging_time': 1.0066947937011719, 'global_step': 35958, 'preemption_count': 0}), (37736, {'train/ctc_loss': Array(0.47785956, dtype=float32), 'train/wer': 0.17165831309000337, 'validation/ctc_loss': Array(0.8496512, dtype=float32), 'validation/wer': 0.2590633055601147, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.5750614, dtype=float32), 'test/wer': 0.19263502122560072, 'test/num_examples': 2472, 'score': 30309.27780365944, 'total_duration': 32865.355593681335, 'accumulated_submission_time': 30309.27780365944, 'accumulated_eval_time': 2553.592475414276, 'accumulated_logging_time': 1.056365966796875, 'global_step': 37736, 'preemption_count': 0}), (39513, {'train/ctc_loss': Array(0.47920355, dtype=float32), 'train/wer': 0.16948427699520602, 'validation/ctc_loss': Array(0.78365797, dtype=float32), 'validation/wer': 0.24075808335827453, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.52596676, dtype=float32), 'test/wer': 0.17811224178904395, 'test/num_examples': 2472, 'score': 31749.487112522125, 'total_duration': 34432.98482298851, 'accumulated_submission_time': 31749.487112522125, 'accumulated_eval_time': 2680.8888108730316, 'accumulated_logging_time': 1.1119632720947266, 'global_step': 39513, 'preemption_count': 0}), (41278, {'train/ctc_loss': Array(0.55570453, dtype=float32), 'train/wer': 0.19388380531885466, 'validation/ctc_loss': Array(0.7383608, dtype=float32), 'validation/wer': 0.22839047278836036, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.48685727, dtype=float32), 'test/wer': 0.1655190624174842, 'test/num_examples': 2472, 'score': 33189.91626691818, 'total_duration': 36000.70971560478, 'accumulated_submission_time': 33189.91626691818, 'accumulated_eval_time': 2808.0603823661804, 'accumulated_logging_time': 1.16729736328125, 'global_step': 41278, 'preemption_count': 0}), (43052, {'train/ctc_loss': Array(0.5582883, dtype=float32), 'train/wer': 0.19472401561034103, 'validation/ctc_loss': Array(0.7084172, dtype=float32), 'validation/wer': 0.2204543479730056, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4635892, dtype=float32), 'test/wer': 0.15855219060386327, 'test/num_examples': 2472, 'score': 34630.03539419174, 'total_duration': 37568.418976545334, 'accumulated_submission_time': 34630.03539419174, 'accumulated_eval_time': 2935.536192178726, 'accumulated_logging_time': 1.2146832942962646, 'global_step': 43052, 'preemption_count': 0}), (44847, {'train/ctc_loss': Array(0.5793242, dtype=float32), 'train/wer': 0.20143198928347888, 'validation/ctc_loss': Array(0.65857506, dtype=float32), 'validation/wer': 0.20489104724021742, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.42297193, dtype=float32), 'test/wer': 0.1440700343265696, 'test/num_examples': 2472, 'score': 36069.94803953171, 'total_duration': 39132.711868047714, 'accumulated_submission_time': 36069.94803953171, 'accumulated_eval_time': 3059.7966132164, 'accumulated_logging_time': 1.2639679908752441, 'global_step': 44847, 'preemption_count': 0}), (46629, {'train/ctc_loss': Array(0.5043172, dtype=float32), 'train/wer': 0.17479643683507115, 'validation/ctc_loss': Array(0.63248414, dtype=float32), 'validation/wer': 0.19732179924114426, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40458766, dtype=float32), 'test/wer': 0.14147015213373143, 'test/num_examples': 2472, 'score': 37510.127170324326, 'total_duration': 40697.58929014206, 'accumulated_submission_time': 37510.127170324326, 'accumulated_eval_time': 3184.3707044124603, 'accumulated_logging_time': 1.3165147304534912, 'global_step': 46629, 'preemption_count': 0}), (48382, {'train/ctc_loss': Array(0.4572825, dtype=float32), 'train/wer': 0.16372351441328092, 'validation/ctc_loss': Array(0.6121698, dtype=float32), 'validation/wer': 0.19044768626239417, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.38863105, dtype=float32), 'test/wer': 0.13458452663863668, 'test/num_examples': 2472, 'score': 38950.496156454086, 'total_duration': 42265.4326710701, 'accumulated_submission_time': 38950.496156454086, 'accumulated_eval_time': 3311.7218165397644, 'accumulated_logging_time': 1.3715198040008545, 'global_step': 48382, 'preemption_count': 0}), (50152, {'train/ctc_loss': Array(0.38403624, dtype=float32), 'train/wer': 0.13940336490880814, 'validation/ctc_loss': Array(0.5849108, dtype=float32), 'validation/wer': 0.1823281230388986, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.36841553, dtype=float32), 'test/wer': 0.1263786484674913, 'test/num_examples': 2472, 'score': 40390.831110715866, 'total_duration': 43833.61747145653, 'accumulated_submission_time': 40390.831110715866, 'accumulated_eval_time': 3439.447977542877, 'accumulated_logging_time': 1.4257633686065674, 'global_step': 50152, 'preemption_count': 0}), (51924, {'train/ctc_loss': Array(0.4078082, dtype=float32), 'train/wer': 0.14524214265153942, 'validation/ctc_loss': Array(0.5672598, dtype=float32), 'validation/wer': 0.1777807814476187, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35503602, dtype=float32), 'test/wer': 0.12050860195397396, 'test/num_examples': 2472, 'score': 41831.017669439316, 'total_duration': 45400.7054476738, 'accumulated_submission_time': 41831.017669439316, 'accumulated_eval_time': 3566.2313644886017, 'accumulated_logging_time': 1.4762351512908936, 'global_step': 51924, 'preemption_count': 0}), (53667, {'train/ctc_loss': Array(0.36456165, dtype=float32), 'train/wer': 0.13382580312661055, 'validation/ctc_loss': Array(0.5535161, dtype=float32), 'validation/wer': 0.17389961091748168, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34406787, dtype=float32), 'test/wer': 0.11807121239818821, 'test/num_examples': 2472, 'score': 43271.22687149048, 'total_duration': 46968.35110974312, 'accumulated_submission_time': 43271.22687149048, 'accumulated_eval_time': 3693.5450909137726, 'accumulated_logging_time': 1.5323927402496338, 'global_step': 53667, 'preemption_count': 0}), (55412, {'train/ctc_loss': Array(0.36444324, dtype=float32), 'train/wer': 0.13349027435750008, 'validation/ctc_loss': Array(0.54100156, dtype=float32), 'validation/wer': 0.16998947642816456, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33620584, dtype=float32), 'test/wer': 0.11508541019235066, 'test/num_examples': 2472, 'score': 44712.072283029556, 'total_duration': 48537.52812194824, 'accumulated_submission_time': 44712.072283029556, 'accumulated_eval_time': 3821.760100364685, 'accumulated_logging_time': 1.5818710327148438, 'global_step': 55412, 'preemption_count': 0}), (57169, {'train/ctc_loss': Array(0.361306, dtype=float32), 'train/wer': 0.12978329882677708, 'validation/ctc_loss': Array(0.5362888, dtype=float32), 'validation/wer': 0.16771097830599457, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33129215, dtype=float32), 'test/wer': 0.1132370564458798, 'test/num_examples': 2472, 'score': 46152.162573337555, 'total_duration': 50104.92146348953, 'accumulated_submission_time': 46152.162573337555, 'accumulated_eval_time': 3948.9338853359222, 'accumulated_logging_time': 1.6429102420806885, 'global_step': 57169, 'preemption_count': 0}), (58895, {'train/ctc_loss': Array(0.37408665, dtype=float32), 'train/wer': 0.13559516042260453, 'validation/ctc_loss': Array(0.53304183, dtype=float32), 'validation/wer': 0.16699653397955144, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32993034, dtype=float32), 'test/wer': 0.11207929640688157, 'test/num_examples': 2472, 'score': 47592.11108016968, 'total_duration': 51672.81087613106, 'accumulated_submission_time': 47592.11108016968, 'accumulated_eval_time': 4076.7576253414154, 'accumulated_logging_time': 1.6928706169128418, 'global_step': 58895, 'preemption_count': 0}), (60630, {'train/ctc_loss': Array(0.3762794, dtype=float32), 'train/wer': 0.13277752970518084, 'validation/ctc_loss': Array(0.53248405, dtype=float32), 'validation/wer': 0.16678413161223052, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32958347, dtype=float32), 'test/wer': 0.11161213007535596, 'test/num_examples': 2472, 'score': 49032.677661418915, 'total_duration': 53242.73287272453, 'accumulated_submission_time': 49032.677661418915, 'accumulated_eval_time': 4205.99129152298, 'accumulated_logging_time': 1.7464699745178223, 'global_step': 60630, 'preemption_count': 0}), (62392, {'train/ctc_loss': Array(0.35936368, dtype=float32), 'train/wer': 0.13032707870961957, 'validation/ctc_loss': Array(0.53247744, dtype=float32), 'validation/wer': 0.16674551299999035, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32956782, dtype=float32), 'test/wer': 0.11153088375682976, 'test/num_examples': 2472, 'score': 50472.684576034546, 'total_duration': 54810.45715737343, 'accumulated_submission_time': 50472.684576034546, 'accumulated_eval_time': 4333.584086894989, 'accumulated_logging_time': 1.800659418106079, 'global_step': 62392, 'preemption_count': 0}), (64175, {'train/ctc_loss': Array(0.36941895, dtype=float32), 'train/wer': 0.13656382638857195, 'validation/ctc_loss': Array(0.5324699, dtype=float32), 'validation/wer': 0.1667165490408102, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32956153, dtype=float32), 'test/wer': 0.11151057217719822, 'test/num_examples': 2472, 'score': 51913.202800273895, 'total_duration': 56377.03063416481, 'accumulated_submission_time': 51913.202800273895, 'accumulated_eval_time': 4459.497474193573, 'accumulated_logging_time': 1.8636326789855957, 'global_step': 64175, 'preemption_count': 0}), (65921, {'train/ctc_loss': Array(0.35998207, dtype=float32), 'train/wer': 0.1303620050886042, 'validation/ctc_loss': Array(0.5324719, dtype=float32), 'validation/wer': 0.1667551676530504, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3295652, dtype=float32), 'test/wer': 0.11157150691609287, 'test/num_examples': 2472, 'score': 53354.19180417061, 'total_duration': 57945.39933538437, 'accumulated_submission_time': 53354.19180417061, 'accumulated_eval_time': 4586.760721206665, 'accumulated_logging_time': 1.913205862045288, 'global_step': 65921, 'preemption_count': 0}), (67678, {'train/ctc_loss': Array(0.36828643, dtype=float32), 'train/wer': 0.1313276815249634, 'validation/ctc_loss': Array(0.5324746, dtype=float32), 'validation/wer': 0.1667551676530504, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32956183, dtype=float32), 'test/wer': 0.11153088375682976, 'test/num_examples': 2472, 'score': 54794.17435336113, 'total_duration': 59511.79700446129, 'accumulated_submission_time': 54794.17435336113, 'accumulated_eval_time': 4713.049329996109, 'accumulated_logging_time': 1.9709267616271973, 'global_step': 67678, 'preemption_count': 0}), (69450, {'train/ctc_loss': Array(0.36868584, dtype=float32), 'train/wer': 0.13131973793072604, 'validation/ctc_loss': Array(0.5324691, dtype=float32), 'validation/wer': 0.1667551676530504, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3295609, dtype=float32), 'test/wer': 0.11153088375682976, 'test/num_examples': 2472, 'score': 56234.33962392807, 'total_duration': 61079.69539475441, 'accumulated_submission_time': 56234.33962392807, 'accumulated_eval_time': 4840.658200502396, 'accumulated_logging_time': 2.027392864227295, 'global_step': 69450, 'preemption_count': 0}), (71192, {'train/ctc_loss': Array(0.3731729, dtype=float32), 'train/wer': 0.1361145855240583, 'validation/ctc_loss': Array(0.53247434, dtype=float32), 'validation/wer': 0.16677447695917047, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32956335, dtype=float32), 'test/wer': 0.11153088375682976, 'test/num_examples': 2472, 'score': 57674.40916514397, 'total_duration': 62647.13051748276, 'accumulated_submission_time': 57674.40916514397, 'accumulated_eval_time': 4967.903873920441, 'accumulated_logging_time': 2.079360246658325, 'global_step': 71192, 'preemption_count': 0}), (72946, {'train/ctc_loss': Array(0.34705588, dtype=float32), 'train/wer': 0.12490883289139665, 'validation/ctc_loss': Array(0.53247154, dtype=float32), 'validation/wer': 0.16674551299999035, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.32956704, dtype=float32), 'test/wer': 0.11155119533646132, 'test/num_examples': 2472, 'score': 59115.69067811966, 'total_duration': 64214.83722138405, 'accumulated_submission_time': 59115.69067811966, 'accumulated_eval_time': 5094.209578990936, 'accumulated_logging_time': 2.1314005851745605, 'global_step': 72946, 'preemption_count': 0}), (74709, {'train/ctc_loss': Array(0.33405918, dtype=float32), 'train/wer': 0.11973175193851138, 'validation/ctc_loss': Array(0.5324755, dtype=float32), 'validation/wer': 0.1667358583469303, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3295662, dtype=float32), 'test/wer': 0.11157150691609287, 'test/num_examples': 2472, 'score': 60556.16478013992, 'total_duration': 65781.86285066605, 'accumulated_submission_time': 60556.16478013992, 'accumulated_eval_time': 5220.635853767395, 'accumulated_logging_time': 2.188387870788574, 'global_step': 74709, 'preemption_count': 0})], 'global_step': 75348}
I0307 13:18:21.923535 140085866297152 submission_runner.py:591] Timing: 61068.753368616104
I0307 13:18:21.923615 140085866297152 submission_runner.py:593] Total number of evals: 43
I0307 13:18:21.923664 140085866297152 submission_runner.py:594] ====================
I0307 13:18:21.928508 140085866297152 submission_runner.py:678] Final librispeech_conformer_gelu score: 61068.753368616104
