python3 submission_runner.py --framework=jax --workload=wmt_glu_tanh --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/wmt_glu_tanh/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --num_tuning_trials=1 --rng_seed=1692305324 --max_global_steps=133333 2>&1 | tee -a /logs/wmt_glu_tanh_jax_02-16-2024-13-57-47.log
I0216 13:58:08.493848 140416697075520 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/wmt_glu_tanh_jax.
I0216 13:58:09.462540 140416697075520 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Host Interpreter
I0216 13:58:09.464001 140416697075520 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0216 13:58:09.464162 140416697075520 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0216 13:58:09.470901 140416697075520 submission_runner.py:542] Using RNG seed 1692305324
I0216 13:58:10.512197 140416697075520 submission_runner.py:551] --- Tuning run 1/1 ---
I0216 13:58:10.512400 140416697075520 submission_runner.py:556] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/wmt_glu_tanh_jax/trial_1.
I0216 13:58:10.512589 140416697075520 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/wmt_glu_tanh_jax/trial_1/hparams.json.
I0216 13:58:10.693299 140416697075520 submission_runner.py:206] Initializing dataset.
I0216 13:58:10.705572 140416697075520 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0216 13:58:10.709805 140416697075520 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0216 13:58:10.854097 140416697075520 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0216 13:58:12.782364 140416697075520 submission_runner.py:213] Initializing model.
I0216 13:58:22.927048 140416697075520 submission_runner.py:255] Initializing optimizer.
I0216 13:58:24.030584 140416697075520 submission_runner.py:262] Initializing metrics bundle.
I0216 13:58:24.030785 140416697075520 submission_runner.py:280] Initializing checkpoint and logger.
I0216 13:58:24.031876 140416697075520 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/wmt_glu_tanh_jax/trial_1 with prefix checkpoint_
I0216 13:58:24.032016 140416697075520 submission_runner.py:300] Saving meta data to /experiment_runs/variants_target_setting/study_0/wmt_glu_tanh_jax/trial_1/meta_data_0.json.
I0216 13:58:24.032216 140416697075520 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0216 13:58:24.032277 140416697075520 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0216 13:58:24.351921 140416697075520 logger_utils.py:220] Unable to record git information. Continuing without it.
I0216 13:58:24.640966 140416697075520 submission_runner.py:304] Saving flags to /experiment_runs/variants_target_setting/study_0/wmt_glu_tanh_jax/trial_1/flags_0.json.
I0216 13:58:24.650991 140416697075520 submission_runner.py:314] Starting training loop.
I0216 13:59:06.547487 140255525807872 logging_writer.py:48] [0] global_step=0, grad_norm=5.079477787017822, loss=11.344841003417969
I0216 13:59:06.567601 140416697075520 spec.py:321] Evaluating on the training split.
I0216 13:59:06.571547 140416697075520 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0216 13:59:06.574509 140416697075520 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0216 13:59:06.616053 140416697075520 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0216 13:59:14.714460 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 14:04:06.645969 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 14:04:06.649442 140416697075520 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0216 14:04:06.653277 140416697075520 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0216 14:04:06.691192 140416697075520 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0216 14:04:13.912842 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 14:08:55.603109 140416697075520 spec.py:349] Evaluating on the test split.
I0216 14:08:55.605883 140416697075520 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0216 14:08:55.609564 140416697075520 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0216 14:08:55.644191 140416697075520 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0216 14:08:58.648008 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 14:13:40.235927 140416697075520 submission_runner.py:408] Time since start: 915.58s, 	Step: 1, 	{'train/accuracy': 0.0005723770591430366, 'train/loss': 11.380516052246094, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.400991439819336, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.396333694458008, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 41.916555404663086, 'total_duration': 915.5848548412323, 'accumulated_submission_time': 41.916555404663086, 'accumulated_eval_time': 873.6682517528534, 'accumulated_logging_time': 0}
I0216 14:13:40.254286 140247055709952 logging_writer.py:48] [1] accumulated_eval_time=873.668252, accumulated_logging_time=0, accumulated_submission_time=41.916555, global_step=1, preemption_count=0, score=41.916555, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.396334, test/num_examples=3003, total_duration=915.584855, train/accuracy=0.000572, train/bleu=0.000000, train/loss=11.380516, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.400991, validation/num_examples=3000
I0216 14:13:40.658010 140247047317248 logging_writer.py:48] [1] global_step=1, grad_norm=5.044062614440918, loss=11.344125747680664
I0216 14:13:41.050802 140247055709952 logging_writer.py:48] [2] global_step=2, grad_norm=5.112849712371826, loss=11.36449909210205
I0216 14:13:41.446400 140247047317248 logging_writer.py:48] [3] global_step=3, grad_norm=5.012523174285889, loss=11.359868049621582
I0216 14:13:41.844170 140247055709952 logging_writer.py:48] [4] global_step=4, grad_norm=5.002748012542725, loss=11.35462474822998
I0216 14:13:42.238099 140247047317248 logging_writer.py:48] [5] global_step=5, grad_norm=5.030730724334717, loss=11.351221084594727
I0216 14:13:42.634943 140247055709952 logging_writer.py:48] [6] global_step=6, grad_norm=5.12074613571167, loss=11.34710693359375
I0216 14:13:43.029120 140247047317248 logging_writer.py:48] [7] global_step=7, grad_norm=5.126148700714111, loss=11.360057830810547
I0216 14:13:43.422489 140247055709952 logging_writer.py:48] [8] global_step=8, grad_norm=5.121731758117676, loss=11.349968910217285
I0216 14:13:43.816580 140247047317248 logging_writer.py:48] [9] global_step=9, grad_norm=5.105287551879883, loss=11.333063125610352
I0216 14:13:44.212258 140247055709952 logging_writer.py:48] [10] global_step=10, grad_norm=5.078235149383545, loss=11.337382316589355
I0216 14:13:44.605593 140247047317248 logging_writer.py:48] [11] global_step=11, grad_norm=5.149835109710693, loss=11.330323219299316
I0216 14:13:45.001666 140247055709952 logging_writer.py:48] [12] global_step=12, grad_norm=5.06043004989624, loss=11.317307472229004
I0216 14:13:45.395944 140247047317248 logging_writer.py:48] [13] global_step=13, grad_norm=5.03558349609375, loss=11.30052661895752
I0216 14:13:45.791001 140247055709952 logging_writer.py:48] [14] global_step=14, grad_norm=5.049305438995361, loss=11.310195922851562
I0216 14:13:46.187776 140247047317248 logging_writer.py:48] [15] global_step=15, grad_norm=5.097631931304932, loss=11.310895919799805
I0216 14:13:46.585810 140247055709952 logging_writer.py:48] [16] global_step=16, grad_norm=5.082996368408203, loss=11.297172546386719
I0216 14:13:46.982290 140247047317248 logging_writer.py:48] [17] global_step=17, grad_norm=5.064715385437012, loss=11.284012794494629
I0216 14:13:47.379613 140247055709952 logging_writer.py:48] [18] global_step=18, grad_norm=5.067811489105225, loss=11.2640962600708
I0216 14:13:47.773658 140247047317248 logging_writer.py:48] [19] global_step=19, grad_norm=5.040135860443115, loss=11.270402908325195
I0216 14:13:48.167593 140247055709952 logging_writer.py:48] [20] global_step=20, grad_norm=5.007742881774902, loss=11.255958557128906
I0216 14:13:48.561669 140247047317248 logging_writer.py:48] [21] global_step=21, grad_norm=5.023861885070801, loss=11.242900848388672
I0216 14:13:48.957456 140247055709952 logging_writer.py:48] [22] global_step=22, grad_norm=5.05231237411499, loss=11.209325790405273
I0216 14:13:49.354028 140247047317248 logging_writer.py:48] [23] global_step=23, grad_norm=5.058366298675537, loss=11.228645324707031
I0216 14:13:49.750341 140247055709952 logging_writer.py:48] [24] global_step=24, grad_norm=4.949268817901611, loss=11.20216178894043
I0216 14:13:50.144562 140247047317248 logging_writer.py:48] [25] global_step=25, grad_norm=5.032678127288818, loss=11.203908920288086
I0216 14:13:50.540286 140247055709952 logging_writer.py:48] [26] global_step=26, grad_norm=4.945023536682129, loss=11.18358039855957
I0216 14:13:50.933594 140247047317248 logging_writer.py:48] [27] global_step=27, grad_norm=4.943665981292725, loss=11.181729316711426
I0216 14:13:51.326987 140247055709952 logging_writer.py:48] [28] global_step=28, grad_norm=4.9466962814331055, loss=11.163162231445312
I0216 14:13:51.721171 140247047317248 logging_writer.py:48] [29] global_step=29, grad_norm=4.998404026031494, loss=11.139018058776855
I0216 14:13:52.117433 140247055709952 logging_writer.py:48] [30] global_step=30, grad_norm=4.904088497161865, loss=11.117988586425781
I0216 14:13:52.511459 140247047317248 logging_writer.py:48] [31] global_step=31, grad_norm=4.940848350524902, loss=11.116813659667969
I0216 14:13:52.905493 140247055709952 logging_writer.py:48] [32] global_step=32, grad_norm=4.904927730560303, loss=11.091838836669922
I0216 14:13:53.301442 140247047317248 logging_writer.py:48] [33] global_step=33, grad_norm=4.86792516708374, loss=11.083398818969727
I0216 14:13:53.695901 140247055709952 logging_writer.py:48] [34] global_step=34, grad_norm=4.823779582977295, loss=11.065254211425781
I0216 14:13:54.090333 140247047317248 logging_writer.py:48] [35] global_step=35, grad_norm=4.803548336029053, loss=11.051538467407227
I0216 14:13:54.484728 140247055709952 logging_writer.py:48] [36] global_step=36, grad_norm=4.710340976715088, loss=11.042579650878906
I0216 14:13:54.878289 140247047317248 logging_writer.py:48] [37] global_step=37, grad_norm=4.771107196807861, loss=11.02960205078125
I0216 14:13:55.274950 140247055709952 logging_writer.py:48] [38] global_step=38, grad_norm=4.726343154907227, loss=11.0049467086792
I0216 14:13:55.668812 140247047317248 logging_writer.py:48] [39] global_step=39, grad_norm=4.696045875549316, loss=10.980180740356445
I0216 14:13:56.064214 140247055709952 logging_writer.py:48] [40] global_step=40, grad_norm=4.66654109954834, loss=10.965993881225586
I0216 14:13:56.460784 140247047317248 logging_writer.py:48] [41] global_step=41, grad_norm=4.596678733825684, loss=10.938074111938477
I0216 14:13:56.856575 140247055709952 logging_writer.py:48] [42] global_step=42, grad_norm=4.6002936363220215, loss=10.907150268554688
I0216 14:13:57.250815 140247047317248 logging_writer.py:48] [43] global_step=43, grad_norm=4.602419853210449, loss=10.900116920471191
I0216 14:13:57.647653 140247055709952 logging_writer.py:48] [44] global_step=44, grad_norm=4.528102874755859, loss=10.877187728881836
I0216 14:13:58.040755 140247047317248 logging_writer.py:48] [45] global_step=45, grad_norm=4.477261066436768, loss=10.861032485961914
I0216 14:13:58.436435 140247055709952 logging_writer.py:48] [46] global_step=46, grad_norm=4.438033103942871, loss=10.823477745056152
I0216 14:13:58.830393 140247047317248 logging_writer.py:48] [47] global_step=47, grad_norm=4.4040846824646, loss=10.816411972045898
I0216 14:13:59.225820 140247055709952 logging_writer.py:48] [48] global_step=48, grad_norm=4.3316755294799805, loss=10.798928260803223
I0216 14:13:59.618390 140247047317248 logging_writer.py:48] [49] global_step=49, grad_norm=4.294878005981445, loss=10.778074264526367
I0216 14:14:00.013837 140247055709952 logging_writer.py:48] [50] global_step=50, grad_norm=4.277029037475586, loss=10.744963645935059
I0216 14:14:00.409169 140247047317248 logging_writer.py:48] [51] global_step=51, grad_norm=4.2262959480285645, loss=10.722662925720215
I0216 14:14:00.807002 140247055709952 logging_writer.py:48] [52] global_step=52, grad_norm=4.203362464904785, loss=10.709898948669434
I0216 14:14:01.202362 140247047317248 logging_writer.py:48] [53] global_step=53, grad_norm=4.100973606109619, loss=10.684649467468262
I0216 14:14:01.598022 140247055709952 logging_writer.py:48] [54] global_step=54, grad_norm=4.05159854888916, loss=10.659876823425293
I0216 14:14:01.994226 140247047317248 logging_writer.py:48] [55] global_step=55, grad_norm=3.979814052581787, loss=10.649679183959961
I0216 14:14:02.390655 140247055709952 logging_writer.py:48] [56] global_step=56, grad_norm=3.9786572456359863, loss=10.626500129699707
I0216 14:14:02.786915 140247047317248 logging_writer.py:48] [57] global_step=57, grad_norm=3.883849620819092, loss=10.611127853393555
I0216 14:14:03.182654 140247055709952 logging_writer.py:48] [58] global_step=58, grad_norm=3.8813469409942627, loss=10.565808296203613
I0216 14:14:03.577196 140247047317248 logging_writer.py:48] [59] global_step=59, grad_norm=3.8197648525238037, loss=10.556217193603516
I0216 14:14:03.974481 140247055709952 logging_writer.py:48] [60] global_step=60, grad_norm=3.7181296348571777, loss=10.543571472167969
I0216 14:14:04.370954 140247047317248 logging_writer.py:48] [61] global_step=61, grad_norm=3.6879355907440186, loss=10.51113224029541
I0216 14:14:04.766298 140247055709952 logging_writer.py:48] [62] global_step=62, grad_norm=3.6199491024017334, loss=10.498419761657715
I0216 14:14:05.162052 140247047317248 logging_writer.py:48] [63] global_step=63, grad_norm=3.555788278579712, loss=10.464877128601074
I0216 14:14:05.558061 140247055709952 logging_writer.py:48] [64] global_step=64, grad_norm=3.5221378803253174, loss=10.458867073059082
I0216 14:14:05.955448 140247047317248 logging_writer.py:48] [65] global_step=65, grad_norm=3.521076202392578, loss=10.406620979309082
I0216 14:14:06.351554 140247055709952 logging_writer.py:48] [66] global_step=66, grad_norm=3.4440126419067383, loss=10.394696235656738
I0216 14:14:06.745728 140247047317248 logging_writer.py:48] [67] global_step=67, grad_norm=3.4266490936279297, loss=10.357017517089844
I0216 14:14:07.140813 140247055709952 logging_writer.py:48] [68] global_step=68, grad_norm=3.3645126819610596, loss=10.338980674743652
I0216 14:14:07.537913 140247047317248 logging_writer.py:48] [69] global_step=69, grad_norm=3.293846845626831, loss=10.340044975280762
I0216 14:14:07.934604 140247055709952 logging_writer.py:48] [70] global_step=70, grad_norm=3.2161331176757812, loss=10.299372673034668
I0216 14:14:08.328984 140247047317248 logging_writer.py:48] [71] global_step=71, grad_norm=3.1711838245391846, loss=10.281785011291504
I0216 14:14:08.724233 140247055709952 logging_writer.py:48] [72] global_step=72, grad_norm=3.114701509475708, loss=10.265619277954102
I0216 14:14:09.120424 140247047317248 logging_writer.py:48] [73] global_step=73, grad_norm=3.1002769470214844, loss=10.219342231750488
I0216 14:14:09.519100 140247055709952 logging_writer.py:48] [74] global_step=74, grad_norm=3.0218942165374756, loss=10.195775985717773
I0216 14:14:09.915926 140247047317248 logging_writer.py:48] [75] global_step=75, grad_norm=2.9813146591186523, loss=10.184002876281738
I0216 14:14:10.309889 140247055709952 logging_writer.py:48] [76] global_step=76, grad_norm=2.928178310394287, loss=10.169370651245117
I0216 14:14:10.705698 140247047317248 logging_writer.py:48] [77] global_step=77, grad_norm=2.879025459289551, loss=10.13772964477539
I0216 14:14:11.104421 140247055709952 logging_writer.py:48] [78] global_step=78, grad_norm=2.8466711044311523, loss=10.111909866333008
I0216 14:14:11.499163 140247047317248 logging_writer.py:48] [79] global_step=79, grad_norm=2.7544937133789062, loss=10.100006103515625
I0216 14:14:11.895516 140247055709952 logging_writer.py:48] [80] global_step=80, grad_norm=2.694226026535034, loss=10.079257011413574
I0216 14:14:12.290961 140247047317248 logging_writer.py:48] [81] global_step=81, grad_norm=2.6914572715759277, loss=10.038847923278809
I0216 14:14:12.686252 140247055709952 logging_writer.py:48] [82] global_step=82, grad_norm=2.693460702896118, loss=10.030306816101074
I0216 14:14:13.082435 140247047317248 logging_writer.py:48] [83] global_step=83, grad_norm=2.6004600524902344, loss=10.019265174865723
I0216 14:14:13.478513 140247055709952 logging_writer.py:48] [84] global_step=84, grad_norm=2.523930788040161, loss=9.990185737609863
I0216 14:14:13.874675 140247047317248 logging_writer.py:48] [85] global_step=85, grad_norm=2.5145602226257324, loss=9.940520286560059
I0216 14:14:14.271046 140247055709952 logging_writer.py:48] [86] global_step=86, grad_norm=2.461123466491699, loss=9.92713451385498
I0216 14:14:14.666916 140247047317248 logging_writer.py:48] [87] global_step=87, grad_norm=2.397411584854126, loss=9.910297393798828
I0216 14:14:15.064197 140247055709952 logging_writer.py:48] [88] global_step=88, grad_norm=2.4025440216064453, loss=9.869553565979004
I0216 14:14:15.461389 140247047317248 logging_writer.py:48] [89] global_step=89, grad_norm=2.31624436378479, loss=9.88699722290039
I0216 14:14:15.858932 140247055709952 logging_writer.py:48] [90] global_step=90, grad_norm=2.272340774536133, loss=9.85551643371582
I0216 14:14:16.254399 140247047317248 logging_writer.py:48] [91] global_step=91, grad_norm=2.2384891510009766, loss=9.839138984680176
I0216 14:14:16.652895 140247055709952 logging_writer.py:48] [92] global_step=92, grad_norm=2.1967058181762695, loss=9.810988426208496
I0216 14:14:17.050070 140247047317248 logging_writer.py:48] [93] global_step=93, grad_norm=2.161064624786377, loss=9.779712677001953
I0216 14:14:17.447271 140247055709952 logging_writer.py:48] [94] global_step=94, grad_norm=2.0944080352783203, loss=9.77684211730957
I0216 14:14:17.840536 140247047317248 logging_writer.py:48] [95] global_step=95, grad_norm=2.0645833015441895, loss=9.748749732971191
I0216 14:14:18.235892 140247055709952 logging_writer.py:48] [96] global_step=96, grad_norm=2.0301127433776855, loss=9.721888542175293
I0216 14:14:18.631964 140247047317248 logging_writer.py:48] [97] global_step=97, grad_norm=1.9805511236190796, loss=9.704705238342285
I0216 14:14:19.029472 140247055709952 logging_writer.py:48] [98] global_step=98, grad_norm=1.933902621269226, loss=9.690582275390625
I0216 14:14:19.425398 140247047317248 logging_writer.py:48] [99] global_step=99, grad_norm=1.8895394802093506, loss=9.685685157775879
I0216 14:14:19.822091 140247055709952 logging_writer.py:48] [100] global_step=100, grad_norm=1.8757978677749634, loss=9.645528793334961
I0216 14:16:52.900265 140247047317248 logging_writer.py:48] [500] global_step=500, grad_norm=0.28677719831466675, loss=7.864734649658203
I0216 14:20:04.298087 140247055709952 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.3496831953525543, loss=7.097085952758789
I0216 14:23:15.721308 140247047317248 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.5260927677154541, loss=6.620783805847168
I0216 14:26:27.198367 140247055709952 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.5840777158737183, loss=6.165810585021973
I0216 14:27:40.435327 140416697075520 spec.py:321] Evaluating on the training split.
I0216 14:27:43.622739 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 14:32:27.703968 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 14:32:30.597729 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 14:37:14.068512 140416697075520 spec.py:349] Evaluating on the test split.
I0216 14:37:16.963086 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 14:42:00.322390 140416697075520 submission_runner.py:408] Time since start: 2615.67s, 	Step: 2193, 	{'train/accuracy': 0.27149611711502075, 'train/loss': 5.766429424285889, 'train/bleu': 0.014532577310484868, 'validation/accuracy': 0.25340044498443604, 'validation/loss': 5.971210956573486, 'validation/bleu': 0.009315679799413432, 'validation/num_examples': 3000, 'test/accuracy': 0.23529139161109924, 'test/loss': 6.23386812210083, 'test/bleu': 0.005846151290142995, 'test/num_examples': 3003, 'score': 882.0145854949951, 'total_duration': 2615.6713082790375, 'accumulated_submission_time': 882.0145854949951, 'accumulated_eval_time': 1733.5552592277527, 'accumulated_logging_time': 0.028280973434448242}
I0216 14:42:00.337988 140247047317248 logging_writer.py:48] [2193] accumulated_eval_time=1733.555259, accumulated_logging_time=0.028281, accumulated_submission_time=882.014585, global_step=2193, preemption_count=0, score=882.014585, test/accuracy=0.235291, test/bleu=0.005846, test/loss=6.233868, test/num_examples=3003, total_duration=2615.671308, train/accuracy=0.271496, train/bleu=0.014533, train/loss=5.766429, validation/accuracy=0.253400, validation/bleu=0.009316, validation/loss=5.971211, validation/num_examples=3000
I0216 14:43:58.254503 140247055709952 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.5091130137443542, loss=5.688037872314453
I0216 14:47:09.784511 140247047317248 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.4141494631767273, loss=5.233813285827637
I0216 14:50:21.289903 140247055709952 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.4727701246738434, loss=4.888878345489502
I0216 14:53:32.789943 140247047317248 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.5044587254524231, loss=4.720768928527832
I0216 14:56:00.344054 140416697075520 spec.py:321] Evaluating on the training split.
I0216 14:56:03.525872 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 15:00:45.991926 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 15:00:48.872453 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 15:05:31.802932 140416697075520 spec.py:349] Evaluating on the test split.
I0216 15:05:34.679478 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 15:10:16.609834 140416697075520 submission_runner.py:408] Time since start: 4311.96s, 	Step: 4387, 	{'train/accuracy': 0.38233137130737305, 'train/loss': 4.276638984680176, 'train/bleu': 0.009903980841895732, 'validation/accuracy': 0.358396053314209, 'validation/loss': 4.472900390625, 'validation/bleu': 0.008907415885516034, 'validation/num_examples': 3000, 'test/accuracy': 0.33998024463653564, 'test/loss': 4.736346244812012, 'test/bleu': 0.008253558126543463, 'test/num_examples': 3003, 'score': 1721.9357450008392, 'total_duration': 4311.958748579025, 'accumulated_submission_time': 1721.9357450008392, 'accumulated_eval_time': 2589.820998430252, 'accumulated_logging_time': 0.05459952354431152}
I0216 15:10:16.624968 140247055709952 logging_writer.py:48] [4387] accumulated_eval_time=2589.820998, accumulated_logging_time=0.054600, accumulated_submission_time=1721.935745, global_step=4387, preemption_count=0, score=1721.935745, test/accuracy=0.339980, test/bleu=0.008254, test/loss=4.736346, test/num_examples=3003, total_duration=4311.958749, train/accuracy=0.382331, train/bleu=0.009904, train/loss=4.276639, validation/accuracy=0.358396, validation/bleu=0.008907, validation/loss=4.472900, validation/num_examples=3000
I0216 15:11:00.247657 140247047317248 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.5915138721466064, loss=4.304568290710449
I0216 15:14:11.786192 140247055709952 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6122428774833679, loss=4.0145063400268555
I0216 15:17:23.352951 140247047317248 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.5300625562667847, loss=3.9084811210632324
I0216 15:20:34.916283 140247055709952 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.5445243120193481, loss=3.6802518367767334
I0216 15:23:46.498980 140247047317248 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.534134030342102, loss=3.381643772125244
I0216 15:24:16.854346 140416697075520 spec.py:321] Evaluating on the training split.
I0216 15:24:20.040932 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 15:29:04.782496 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 15:29:07.663773 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 15:33:52.784078 140416697075520 spec.py:349] Evaluating on the test split.
I0216 15:33:55.677136 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 15:38:40.147385 140416697075520 submission_runner.py:408] Time since start: 6015.50s, 	Step: 6581, 	{'train/accuracy': 0.49364981055259705, 'train/loss': 3.177924156188965, 'train/bleu': 0.005156220687101846, 'validation/accuracy': 0.4825730621814728, 'validation/loss': 3.292743444442749, 'validation/bleu': 0.0031226845099319267, 'validation/num_examples': 3000, 'test/accuracy': 0.4754401445388794, 'test/loss': 3.4052774906158447, 'test/bleu': 0.0015457053663014697, 'test/num_examples': 3003, 'score': 2562.0812520980835, 'total_duration': 6015.496306180954, 'accumulated_submission_time': 2562.0812520980835, 'accumulated_eval_time': 3453.1139719486237, 'accumulated_logging_time': 0.07984018325805664}
I0216 15:38:40.162840 140247055709952 logging_writer.py:48] [6581] accumulated_eval_time=3453.113972, accumulated_logging_time=0.079840, accumulated_submission_time=2562.081252, global_step=6581, preemption_count=0, score=2562.081252, test/accuracy=0.475440, test/bleu=0.001546, test/loss=3.405277, test/num_examples=3003, total_duration=6015.496306, train/accuracy=0.493650, train/bleu=0.005156, train/loss=3.177924, validation/accuracy=0.482573, validation/bleu=0.003123, validation/loss=3.292743, validation/num_examples=3000
I0216 15:41:21.014521 140247047317248 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5665594935417175, loss=3.309856414794922
I0216 15:44:32.593950 140247055709952 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.4929957687854767, loss=3.018547773361206
I0216 15:47:44.161139 140247047317248 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5054410696029663, loss=3.091970205307007
I0216 15:50:55.728746 140247055709952 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.47754979133605957, loss=2.9900238513946533
I0216 15:52:40.397312 140416697075520 spec.py:321] Evaluating on the training split.
I0216 15:52:43.587074 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 15:57:28.566640 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 15:57:31.461851 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 16:02:16.461020 140416697075520 spec.py:349] Evaluating on the test split.
I0216 16:02:19.348104 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 16:07:04.488849 140416697075520 submission_runner.py:408] Time since start: 7719.84s, 	Step: 8775, 	{'train/accuracy': 0.5443810224533081, 'train/loss': 2.7035303115844727, 'train/bleu': 0.006256229117476515, 'validation/accuracy': 0.5446305871009827, 'validation/loss': 2.684821844100952, 'validation/bleu': 0.002952351193251204, 'validation/num_examples': 3000, 'test/accuracy': 0.544698178768158, 'test/loss': 2.712599992752075, 'test/bleu': 0.0008381065657549013, 'test/num_examples': 3003, 'score': 3402.230988740921, 'total_duration': 7719.837748765945, 'accumulated_submission_time': 3402.230988740921, 'accumulated_eval_time': 4317.205441474915, 'accumulated_logging_time': 0.10492157936096191}
I0216 16:07:04.504594 140247047317248 logging_writer.py:48] [8775] accumulated_eval_time=4317.205441, accumulated_logging_time=0.104922, accumulated_submission_time=3402.230989, global_step=8775, preemption_count=0, score=3402.230989, test/accuracy=0.544698, test/bleu=0.000838, test/loss=2.712600, test/num_examples=3003, total_duration=7719.837749, train/accuracy=0.544381, train/bleu=0.006256, train/loss=2.703530, validation/accuracy=0.544631, validation/bleu=0.002952, validation/loss=2.684822, validation/num_examples=3000
I0216 16:08:31.068458 140247055709952 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.46187785267829895, loss=2.810666084289551
I0216 16:11:42.690442 140247047317248 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.4686893820762634, loss=2.6244382858276367
I0216 16:14:54.260441 140247055709952 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.4847704768180847, loss=2.6660163402557373
I0216 16:18:05.799778 140247047317248 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.44570717215538025, loss=2.5393121242523193
I0216 16:21:04.810003 140416697075520 spec.py:321] Evaluating on the training split.
I0216 16:21:07.998101 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 16:25:51.607683 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 16:25:54.503161 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 16:30:37.739690 140416697075520 spec.py:349] Evaluating on the test split.
I0216 16:30:40.622802 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 16:35:24.195183 140416697075520 submission_runner.py:408] Time since start: 9419.54s, 	Step: 10969, 	{'train/accuracy': 0.5733675956726074, 'train/loss': 2.416719675064087, 'train/bleu': 0.0031168829488226546, 'validation/accuracy': 0.5787404775619507, 'validation/loss': 2.352992057800293, 'validation/bleu': 0.0007022588163779613, 'validation/num_examples': 3000, 'test/accuracy': 0.5823950171470642, 'test/loss': 2.3416526317596436, 'test/bleu': 0.0007694039752120449, 'test/num_examples': 3003, 'score': 4242.451108455658, 'total_duration': 9419.544093370438, 'accumulated_submission_time': 4242.451108455658, 'accumulated_eval_time': 5176.590566635132, 'accumulated_logging_time': 0.13073062896728516}
I0216 16:35:24.211645 140247055709952 logging_writer.py:48] [10969] accumulated_eval_time=5176.590567, accumulated_logging_time=0.130731, accumulated_submission_time=4242.451108, global_step=10969, preemption_count=0, score=4242.451108, test/accuracy=0.582395, test/bleu=0.000769, test/loss=2.341653, test/num_examples=3003, total_duration=9419.544093, train/accuracy=0.573368, train/bleu=0.003117, train/loss=2.416720, validation/accuracy=0.578740, validation/bleu=0.000702, validation/loss=2.352992, validation/num_examples=3000
I0216 16:35:36.474863 140247047317248 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.42163801193237305, loss=2.571899175643921
I0216 16:38:47.960106 140247055709952 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.41867244243621826, loss=2.5363473892211914
I0216 16:41:59.560636 140247047317248 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.4078879952430725, loss=2.5552403926849365
I0216 16:45:11.185941 140247055709952 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.40088143944740295, loss=2.34892201423645
I0216 16:48:22.825573 140247047317248 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.3971879482269287, loss=2.381906032562256
I0216 16:49:24.217890 140416697075520 spec.py:321] Evaluating on the training split.
I0216 16:49:27.402894 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 16:54:11.773684 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 16:54:14.664495 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 16:58:58.323095 140416697075520 spec.py:349] Evaluating on the test split.
I0216 16:59:01.212778 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 17:03:45.136747 140416697075520 submission_runner.py:408] Time since start: 11120.49s, 	Step: 13162, 	{'train/accuracy': 0.5986971259117126, 'train/loss': 2.189364433288574, 'train/bleu': 0.00310057377157679, 'validation/accuracy': 0.6016912460327148, 'validation/loss': 2.156778335571289, 'validation/bleu': 0.0017017048911783994, 'validation/num_examples': 3000, 'test/accuracy': 0.605856716632843, 'test/loss': 2.1299784183502197, 'test/bleu': 0.0009892606605730098, 'test/num_examples': 3003, 'score': 5082.376412630081, 'total_duration': 11120.485644102097, 'accumulated_submission_time': 5082.376412630081, 'accumulated_eval_time': 6037.509339094162, 'accumulated_logging_time': 0.15807676315307617}
I0216 17:03:45.152430 140247055709952 logging_writer.py:48] [13162] accumulated_eval_time=6037.509339, accumulated_logging_time=0.158077, accumulated_submission_time=5082.376413, global_step=13162, preemption_count=0, score=5082.376413, test/accuracy=0.605857, test/bleu=0.000989, test/loss=2.129978, test/num_examples=3003, total_duration=11120.485644, train/accuracy=0.598697, train/bleu=0.003101, train/loss=2.189364, validation/accuracy=0.601691, validation/bleu=0.001702, validation/loss=2.156778, validation/num_examples=3000
I0216 17:05:54.982063 140247047317248 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.38200289011001587, loss=2.286606788635254
I0216 17:09:06.580356 140247055709952 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.3829789459705353, loss=2.296746253967285
I0216 17:12:18.134074 140247047317248 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.38272106647491455, loss=2.3626275062561035
I0216 17:15:29.716518 140247055709952 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.37945419549942017, loss=2.2820839881896973
I0216 17:17:45.438824 140416697075520 spec.py:321] Evaluating on the training split.
I0216 17:17:48.620501 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 17:22:32.364785 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 17:22:35.246514 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 17:27:17.478279 140416697075520 spec.py:349] Evaluating on the test split.
I0216 17:27:20.366888 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 17:32:03.444144 140416697075520 submission_runner.py:408] Time since start: 12818.79s, 	Step: 15356, 	{'train/accuracy': 0.6066553592681885, 'train/loss': 2.1151750087738037, 'train/bleu': 0.008914114737132578, 'validation/accuracy': 0.6143879294395447, 'validation/loss': 2.042186975479126, 'validation/bleu': 0.0016059347693763726, 'validation/num_examples': 3000, 'test/accuracy': 0.6196037530899048, 'test/loss': 1.9998146295547485, 'test/bleu': 0.00426576729714753, 'test/num_examples': 3003, 'score': 5922.58389544487, 'total_duration': 12818.793059825897, 'accumulated_submission_time': 5922.58389544487, 'accumulated_eval_time': 6895.514606714249, 'accumulated_logging_time': 0.18300485610961914}
I0216 17:32:03.459652 140247047317248 logging_writer.py:48] [15356] accumulated_eval_time=6895.514607, accumulated_logging_time=0.183005, accumulated_submission_time=5922.583895, global_step=15356, preemption_count=0, score=5922.583895, test/accuracy=0.619604, test/bleu=0.004266, test/loss=1.999815, test/num_examples=3003, total_duration=12818.793060, train/accuracy=0.606655, train/bleu=0.008914, train/loss=2.115175, validation/accuracy=0.614388, validation/bleu=0.001606, validation/loss=2.042187, validation/num_examples=3000
I0216 17:32:58.993669 140247055709952 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.36402153968811035, loss=2.227755069732666
I0216 17:36:10.593707 140247047317248 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.378065824508667, loss=2.1873388290405273
I0216 17:39:22.194985 140247055709952 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.37372931838035583, loss=2.3009259700775146
I0216 17:42:33.822509 140247047317248 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.3671785295009613, loss=2.2321932315826416
I0216 17:45:45.391022 140247055709952 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.3605819344520569, loss=2.180551767349243
I0216 17:46:03.486333 140416697075520 spec.py:321] Evaluating on the training split.
I0216 17:46:06.666681 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 17:50:49.964136 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 17:50:52.853926 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 17:55:35.711526 140416697075520 spec.py:349] Evaluating on the test split.
I0216 17:55:38.591647 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 18:00:20.371693 140416697075520 submission_runner.py:408] Time since start: 14515.72s, 	Step: 17549, 	{'train/accuracy': 0.610589861869812, 'train/loss': 2.0721302032470703, 'train/bleu': 0.02105470065041811, 'validation/accuracy': 0.6241087913513184, 'validation/loss': 1.9620773792266846, 'validation/bleu': 0.019104750406913857, 'validation/num_examples': 3000, 'test/accuracy': 0.6300737857818604, 'test/loss': 1.9128855466842651, 'test/bleu': 0.005294413107827698, 'test/num_examples': 3003, 'score': 6762.530175209045, 'total_duration': 14515.720617055893, 'accumulated_submission_time': 6762.530175209045, 'accumulated_eval_time': 7752.399913311005, 'accumulated_logging_time': 0.2091994285583496}
I0216 18:00:20.387166 140247047317248 logging_writer.py:48] [17549] accumulated_eval_time=7752.399913, accumulated_logging_time=0.209199, accumulated_submission_time=6762.530175, global_step=17549, preemption_count=0, score=6762.530175, test/accuracy=0.630074, test/bleu=0.005294, test/loss=1.912886, test/num_examples=3003, total_duration=14515.720617, train/accuracy=0.610590, train/bleu=0.021055, train/loss=2.072130, validation/accuracy=0.624109, validation/bleu=0.019105, validation/loss=1.962077, validation/num_examples=3000
I0216 18:03:13.585146 140247055709952 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.35547077655792236, loss=2.15966534614563
I0216 18:06:25.168134 140247047317248 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.36749476194381714, loss=2.1888067722320557
I0216 18:09:36.760314 140247055709952 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.36776018142700195, loss=2.0407395362854004
I0216 18:12:48.390788 140247047317248 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.348580539226532, loss=2.1123411655426025
I0216 18:14:20.469985 140416697075520 spec.py:321] Evaluating on the training split.
I0216 18:14:23.661341 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 18:19:06.545416 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 18:19:09.440201 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 18:23:52.414414 140416697075520 spec.py:349] Evaluating on the test split.
I0216 18:23:55.312684 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 18:28:38.086763 140416697075520 submission_runner.py:408] Time since start: 16213.44s, 	Step: 19742, 	{'train/accuracy': 0.6215307712554932, 'train/loss': 1.979943871498108, 'train/bleu': 0.010806342326823197, 'validation/accuracy': 0.6300727725028992, 'validation/loss': 1.905492901802063, 'validation/bleu': 0.007966807985084534, 'validation/num_examples': 3000, 'test/accuracy': 0.6384637951850891, 'test/loss': 1.8455276489257812, 'test/bleu': 0.002181394648611847, 'test/num_examples': 3003, 'score': 7602.453125476837, 'total_duration': 16213.435671329498, 'accumulated_submission_time': 7602.453125476837, 'accumulated_eval_time': 8610.016618728638, 'accumulated_logging_time': 0.3149287700653076}
I0216 18:28:38.103005 140247055709952 logging_writer.py:48] [19742] accumulated_eval_time=8610.016619, accumulated_logging_time=0.314929, accumulated_submission_time=7602.453125, global_step=19742, preemption_count=0, score=7602.453125, test/accuracy=0.638464, test/bleu=0.002181, test/loss=1.845528, test/num_examples=3003, total_duration=16213.435671, train/accuracy=0.621531, train/bleu=0.010806, train/loss=1.979944, validation/accuracy=0.630073, validation/bleu=0.007967, validation/loss=1.905493, validation/num_examples=3000
I0216 18:30:17.364729 140247047317248 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.351133793592453, loss=2.150602340698242
I0216 18:33:28.907656 140247055709952 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.34154975414276123, loss=2.006378412246704
I0216 18:36:40.536414 140247047317248 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.3550163209438324, loss=2.029448986053467
I0216 18:39:52.215785 140247055709952 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.3440214991569519, loss=2.0628414154052734
I0216 18:42:38.219177 140416697075520 spec.py:321] Evaluating on the training split.
I0216 18:42:41.401824 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 18:47:24.080432 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 18:47:26.966308 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 18:52:09.528776 140416697075520 spec.py:349] Evaluating on the test split.
I0216 18:52:12.437184 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 18:56:54.384209 140416697075520 submission_runner.py:408] Time since start: 17909.73s, 	Step: 21935, 	{'train/accuracy': 0.6242388486862183, 'train/loss': 1.9554890394210815, 'train/bleu': 0.0061010295652887585, 'validation/accuracy': 0.6365947127342224, 'validation/loss': 1.8519504070281982, 'validation/bleu': 0.004946113062229061, 'validation/num_examples': 3000, 'test/accuracy': 0.6442972421646118, 'test/loss': 1.7925695180892944, 'test/bleu': 0.003300320959813985, 'test/num_examples': 3003, 'score': 8442.490117073059, 'total_duration': 17909.733082532883, 'accumulated_submission_time': 8442.490117073059, 'accumulated_eval_time': 9466.181564569473, 'accumulated_logging_time': 0.34085750579833984}
I0216 18:56:54.401365 140247047317248 logging_writer.py:48] [21935] accumulated_eval_time=9466.181565, accumulated_logging_time=0.340858, accumulated_submission_time=8442.490117, global_step=21935, preemption_count=0, score=8442.490117, test/accuracy=0.644297, test/bleu=0.003300, test/loss=1.792570, test/num_examples=3003, total_duration=17909.733083, train/accuracy=0.624239, train/bleu=0.006101, train/loss=1.955489, validation/accuracy=0.636595, validation/bleu=0.004946, validation/loss=1.851950, validation/num_examples=3000
I0216 18:57:19.661804 140247055709952 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.36233341693878174, loss=2.1551315784454346
I0216 19:00:31.242830 140247047317248 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.33213183283805847, loss=1.9763388633728027
I0216 19:03:42.856699 140247055709952 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.3421015441417694, loss=2.1303117275238037
I0216 19:06:54.401601 140247047317248 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.3430602252483368, loss=1.9866400957107544
I0216 19:10:06.045028 140247055709952 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.33268192410469055, loss=1.9883744716644287
I0216 19:10:54.431930 140416697075520 spec.py:321] Evaluating on the training split.
I0216 19:10:57.624226 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 19:15:40.201353 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 19:15:43.093202 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 19:20:25.231758 140416697075520 spec.py:349] Evaluating on the test split.
I0216 19:20:28.125405 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 19:25:10.440577 140416697075520 submission_runner.py:408] Time since start: 19605.79s, 	Step: 24128, 	{'train/accuracy': 0.6233453154563904, 'train/loss': 1.9470256567001343, 'train/bleu': 0.00044969548344227846, 'validation/accuracy': 0.6403268575668335, 'validation/loss': 1.8160721063613892, 'validation/bleu': 0.0014958437466859455, 'validation/num_examples': 3000, 'test/accuracy': 0.648399293422699, 'test/loss': 1.7543072700500488, 'test/bleu': 0.0005283949438422056, 'test/num_examples': 3003, 'score': 9282.439644575119, 'total_duration': 19605.789501428604, 'accumulated_submission_time': 9282.439644575119, 'accumulated_eval_time': 10322.19015789032, 'accumulated_logging_time': 0.36908841133117676}
I0216 19:25:10.457080 140247047317248 logging_writer.py:48] [24128] accumulated_eval_time=10322.190158, accumulated_logging_time=0.369088, accumulated_submission_time=9282.439645, global_step=24128, preemption_count=0, score=9282.439645, test/accuracy=0.648399, test/bleu=0.000528, test/loss=1.754307, test/num_examples=3003, total_duration=19605.789501, train/accuracy=0.623345, train/bleu=0.000450, train/loss=1.947026, validation/accuracy=0.640327, validation/bleu=0.001496, validation/loss=1.816072, validation/num_examples=3000
I0216 19:27:33.335743 140247055709952 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.3362564444541931, loss=2.0471792221069336
I0216 19:30:44.945682 140247047317248 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.3389732241630554, loss=1.9549152851104736
I0216 19:33:56.579910 140247055709952 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.3365009129047394, loss=2.063382863998413
I0216 19:37:08.151859 140247047317248 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.3449895679950714, loss=2.0438787937164307
I0216 19:39:10.548744 140416697075520 spec.py:321] Evaluating on the training split.
I0216 19:39:13.739689 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 19:43:57.543679 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 19:44:00.441830 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 19:48:43.684469 140416697075520 spec.py:349] Evaluating on the test split.
I0216 19:48:46.568400 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 19:53:29.539950 140416697075520 submission_runner.py:408] Time since start: 21304.89s, 	Step: 26321, 	{'train/accuracy': 0.632300078868866, 'train/loss': 1.8779854774475098, 'train/bleu': 0.0037231667337293516, 'validation/accuracy': 0.64434415102005, 'validation/loss': 1.7871403694152832, 'validation/bleu': 0.0008904067767186454, 'validation/num_examples': 3000, 'test/accuracy': 0.6516762971878052, 'test/loss': 1.7249469757080078, 'test/bleu': 0.0010723612396463004, 'test/num_examples': 3003, 'score': 10122.452534914017, 'total_duration': 21304.88884949684, 'accumulated_submission_time': 10122.452534914017, 'accumulated_eval_time': 11181.18128657341, 'accumulated_logging_time': 0.3946828842163086}
I0216 19:53:29.556370 140247055709952 logging_writer.py:48] [26321] accumulated_eval_time=11181.181287, accumulated_logging_time=0.394683, accumulated_submission_time=10122.452535, global_step=26321, preemption_count=0, score=10122.452535, test/accuracy=0.651676, test/bleu=0.001072, test/loss=1.724947, test/num_examples=3003, total_duration=21304.888849, train/accuracy=0.632300, train/bleu=0.003723, train/loss=1.877985, validation/accuracy=0.644344, validation/bleu=0.000890, validation/loss=1.787140, validation/num_examples=3000
I0216 19:54:38.511575 140247047317248 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.33561238646507263, loss=1.9862778186798096
I0216 19:57:50.122608 140247055709952 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.33403903245925903, loss=2.0650582313537598
I0216 20:01:01.748464 140247047317248 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.33541539311408997, loss=1.9905979633331299
I0216 20:04:13.462139 140247055709952 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.3835531175136566, loss=1.9928979873657227
I0216 20:07:25.131151 140247047317248 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.32728901505470276, loss=1.959662675857544
I0216 20:07:29.831150 140416697075520 spec.py:321] Evaluating on the training split.
I0216 20:07:33.024617 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 20:12:16.334434 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 20:12:19.230578 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 20:17:03.058147 140416697075520 spec.py:349] Evaluating on the test split.
I0216 20:17:05.965252 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 20:21:48.895463 140416697075520 submission_runner.py:408] Time since start: 23004.24s, 	Step: 28514, 	{'train/accuracy': 0.6297993659973145, 'train/loss': 1.8970668315887451, 'train/bleu': 0.0033591441688744826, 'validation/accuracy': 0.6466379761695862, 'validation/loss': 1.762953758239746, 'validation/bleu': 0.0021784559056038646, 'validation/num_examples': 3000, 'test/accuracy': 0.6564987897872925, 'test/loss': 1.6947009563446045, 'test/bleu': 0.0023120369508580012, 'test/num_examples': 3003, 'score': 10962.647480487823, 'total_duration': 23004.244362831116, 'accumulated_submission_time': 10962.647480487823, 'accumulated_eval_time': 12040.245519399643, 'accumulated_logging_time': 0.42037463188171387}
I0216 20:21:48.913196 140247055709952 logging_writer.py:48] [28514] accumulated_eval_time=12040.245519, accumulated_logging_time=0.420375, accumulated_submission_time=10962.647480, global_step=28514, preemption_count=0, score=10962.647480, test/accuracy=0.656499, test/bleu=0.002312, test/loss=1.694701, test/num_examples=3003, total_duration=23004.244363, train/accuracy=0.629799, train/bleu=0.003359, train/loss=1.897067, validation/accuracy=0.646638, validation/bleu=0.002178, validation/loss=1.762954, validation/num_examples=3000
I0216 20:24:55.366489 140247047317248 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.33647409081459045, loss=1.9851500988006592
I0216 20:28:07.008375 140247055709952 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.3427031636238098, loss=2.0768752098083496
I0216 20:31:18.545388 140247047317248 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.33496350049972534, loss=2.001729726791382
I0216 20:34:30.090485 140247055709952 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.3286937177181244, loss=1.9215539693832397
I0216 20:35:49.139924 140416697075520 spec.py:321] Evaluating on the training split.
I0216 20:35:52.327064 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 20:40:36.821456 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 20:40:39.722606 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 20:45:23.664103 140416697075520 spec.py:349] Evaluating on the test split.
I0216 20:45:26.554939 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 20:50:11.110222 140416697075520 submission_runner.py:408] Time since start: 24706.46s, 	Step: 30708, 	{'train/accuracy': 0.634867250919342, 'train/loss': 1.8564034700393677, 'train/bleu': 0.01048386916559227, 'validation/accuracy': 0.6502833366394043, 'validation/loss': 1.733353614807129, 'validation/bleu': 0.00461938209771408, 'validation/num_examples': 3000, 'test/accuracy': 0.6586369276046753, 'test/loss': 1.6724879741668701, 'test/bleu': 0.0025960866073588963, 'test/num_examples': 3003, 'score': 11802.79470205307, 'total_duration': 24706.45909667015, 'accumulated_submission_time': 11802.79470205307, 'accumulated_eval_time': 12902.215717554092, 'accumulated_logging_time': 0.4477818012237549}
I0216 20:50:11.126406 140247047317248 logging_writer.py:48] [30708] accumulated_eval_time=12902.215718, accumulated_logging_time=0.447782, accumulated_submission_time=11802.794702, global_step=30708, preemption_count=0, score=11802.794702, test/accuracy=0.658637, test/bleu=0.002596, test/loss=1.672488, test/num_examples=3003, total_duration=24706.459097, train/accuracy=0.634867, train/bleu=0.010484, train/loss=1.856403, validation/accuracy=0.650283, validation/bleu=0.004619, validation/loss=1.733354, validation/num_examples=3000
I0216 20:52:03.296861 140247055709952 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.34577441215515137, loss=2.0847723484039307
I0216 20:55:14.897764 140247047317248 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.32582205533981323, loss=1.882798194885254
I0216 20:58:26.517417 140247055709952 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.33955642580986023, loss=1.9667551517486572
I0216 21:01:38.075975 140247047317248 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.3265865743160248, loss=1.916669249534607
I0216 21:04:11.119873 140416697075520 spec.py:321] Evaluating on the training split.
I0216 21:04:14.305981 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 21:08:57.633553 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 21:09:00.518596 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 21:13:43.070133 140416697075520 spec.py:349] Evaluating on the test split.
I0216 21:13:45.957422 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 21:18:28.787192 140416697075520 submission_runner.py:408] Time since start: 26404.14s, 	Step: 32901, 	{'train/accuracy': 0.6375442743301392, 'train/loss': 1.8300745487213135, 'train/bleu': 0.01311878610116507, 'validation/accuracy': 0.6522051692008972, 'validation/loss': 1.7190943956375122, 'validation/bleu': 0.004717050921352025, 'validation/num_examples': 3000, 'test/accuracy': 0.6612515449523926, 'test/loss': 1.6526808738708496, 'test/bleu': 0.0028829720099501567, 'test/num_examples': 3003, 'score': 12642.70935177803, 'total_duration': 26404.136111021042, 'accumulated_submission_time': 12642.70935177803, 'accumulated_eval_time': 13759.882985591888, 'accumulated_logging_time': 0.47329092025756836}
I0216 21:18:28.804098 140247055709952 logging_writer.py:48] [32901] accumulated_eval_time=13759.882986, accumulated_logging_time=0.473291, accumulated_submission_time=12642.709352, global_step=32901, preemption_count=0, score=12642.709352, test/accuracy=0.661252, test/bleu=0.002883, test/loss=1.652681, test/num_examples=3003, total_duration=26404.136111, train/accuracy=0.637544, train/bleu=0.013119, train/loss=1.830075, validation/accuracy=0.652205, validation/bleu=0.004717, validation/loss=1.719094, validation/num_examples=3000
I0216 21:19:07.076291 140247047317248 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.31814685463905334, loss=1.8346976041793823
I0216 21:22:18.690225 140247055709952 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.3363289535045624, loss=2.0256221294403076
I0216 21:25:30.341980 140247047317248 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.3289545476436615, loss=1.817838191986084
I0216 21:28:42.106694 140247055709952 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.3374667167663574, loss=1.895235538482666
I0216 21:31:53.771164 140247047317248 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.32886210083961487, loss=1.9408221244812012
I0216 21:32:29.109268 140416697075520 spec.py:321] Evaluating on the training split.
I0216 21:32:32.293080 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 21:37:16.289695 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 21:37:19.177114 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 21:42:03.018952 140416697075520 spec.py:349] Evaluating on the test split.
I0216 21:42:05.914603 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 21:46:49.717823 140416697075520 submission_runner.py:408] Time since start: 28105.07s, 	Step: 35094, 	{'train/accuracy': 0.6390344500541687, 'train/loss': 1.83194899559021, 'train/bleu': 0.006193797489718212, 'validation/accuracy': 0.6540898084640503, 'validation/loss': 1.7004808187484741, 'validation/bleu': 0.00409028039754382, 'validation/num_examples': 3000, 'test/accuracy': 0.6639126539230347, 'test/loss': 1.6305878162384033, 'test/bleu': 0.0027078365974043134, 'test/num_examples': 3003, 'score': 13482.934636831284, 'total_duration': 28105.066716194153, 'accumulated_submission_time': 13482.934636831284, 'accumulated_eval_time': 14620.491453409195, 'accumulated_logging_time': 0.4997875690460205}
I0216 21:46:49.736409 140247055709952 logging_writer.py:48] [35094] accumulated_eval_time=14620.491453, accumulated_logging_time=0.499788, accumulated_submission_time=13482.934637, global_step=35094, preemption_count=0, score=13482.934637, test/accuracy=0.663913, test/bleu=0.002708, test/loss=1.630588, test/num_examples=3003, total_duration=28105.066716, train/accuracy=0.639034, train/bleu=0.006194, train/loss=1.831949, validation/accuracy=0.654090, validation/bleu=0.004090, validation/loss=1.700481, validation/num_examples=3000
I0216 21:49:25.632144 140247047317248 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.32712921500205994, loss=1.9028348922729492
I0216 21:52:37.218425 140247055709952 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.3397487998008728, loss=1.8315976858139038
I0216 21:55:48.779416 140247047317248 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.32465630769729614, loss=1.9115058183670044
I0216 21:59:00.467706 140247055709952 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.33443018794059753, loss=1.9345831871032715
I0216 22:00:49.760433 140416697075520 spec.py:321] Evaluating on the training split.
I0216 22:00:52.954900 140416697075520 workload.py:181] Translating evaluation dataset.
W0216 22:05:37.018434 140416697075520 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 22:05:37.018672 140416697075520 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 22:05:37.018728 140416697075520 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 22:05:37.384390 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 22:05:40.279994 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 22:10:23.952735 140416697075520 spec.py:349] Evaluating on the test split.
I0216 22:10:26.846335 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 22:15:10.420097 140416697075520 submission_runner.py:408] Time since start: 29805.77s, 	Step: 37287, 	{'train/accuracy': 0.6384992599487305, 'train/loss': 1.818699836730957, 'train/bleu': 0.008049935093720246, 'validation/accuracy': 0.656098484992981, 'validation/loss': 1.686850905418396, 'validation/bleu': 0.0063396983352546794, 'validation/num_examples': 3000, 'test/accuracy': 0.6654348969459534, 'test/loss': 1.613417148590088, 'test/bleu': 0.0023529110368085593, 'test/num_examples': 3003, 'score': 14322.877689361572, 'total_duration': 29805.768976449966, 'accumulated_submission_time': 14322.877689361572, 'accumulated_eval_time': 15481.151032209396, 'accumulated_logging_time': 0.5286056995391846}
I0216 22:15:10.436598 140247047317248 logging_writer.py:48] [37287] accumulated_eval_time=15481.151032, accumulated_logging_time=0.528606, accumulated_submission_time=14322.877689, global_step=37287, preemption_count=0, score=14322.877689, test/accuracy=0.665435, test/bleu=0.002353, test/loss=1.613417, test/num_examples=3003, total_duration=29805.768976, train/accuracy=0.638499, train/bleu=0.008050, train/loss=1.818700, validation/accuracy=0.656098, validation/bleu=0.006340, validation/loss=1.686851, validation/num_examples=3000
I0216 22:16:32.322313 140247055709952 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.3266306221485138, loss=1.7992578744888306
I0216 22:19:43.940999 140247047317248 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.3208748996257782, loss=1.7968676090240479
I0216 22:22:55.577192 140247055709952 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.32273006439208984, loss=1.8636479377746582
I0216 22:26:07.216013 140247047317248 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.327251672744751, loss=1.9151159524917603
I0216 22:29:10.540122 140416697075520 spec.py:321] Evaluating on the training split.
I0216 22:29:13.725283 140416697075520 workload.py:181] Translating evaluation dataset.
W0216 22:33:57.099816 140416697075520 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 22:33:57.100035 140416697075520 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 22:33:57.100084 140416697075520 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 22:33:58.447293 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 22:34:01.336965 140416697075520 workload.py:181] Translating evaluation dataset.
W0216 22:38:43.820711 140416697075520 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 22:38:43.820939 140416697075520 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 22:38:43.820992 140416697075520 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 22:38:45.102787 140416697075520 spec.py:349] Evaluating on the test split.
I0216 22:38:47.986428 140416697075520 workload.py:181] Translating evaluation dataset.
W0216 22:43:30.368171 140416697075520 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 22:43:30.368422 140416697075520 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 22:43:30.368484 140416697075520 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 22:43:31.946444 140416697075520 submission_runner.py:408] Time since start: 31507.30s, 	Step: 39480, 	{'train/accuracy': 0.6432995200157166, 'train/loss': 1.7872945070266724, 'train/bleu': 0.008900397775850605, 'validation/accuracy': 0.6590867042541504, 'validation/loss': 1.6713515520095825, 'validation/bleu': 0.006769154660290232, 'validation/num_examples': 3000, 'test/accuracy': 0.6688048243522644, 'test/loss': 1.6009329557418823, 'test/bleu': 0.003654078079764917, 'test/num_examples': 3003, 'score': 15162.900104045868, 'total_duration': 31507.29537653923, 'accumulated_submission_time': 15162.900104045868, 'accumulated_eval_time': 16342.55731344223, 'accumulated_logging_time': 0.5559475421905518}
I0216 22:43:31.963334 140247055709952 logging_writer.py:48] [39480] accumulated_eval_time=16342.557313, accumulated_logging_time=0.555948, accumulated_submission_time=15162.900104, global_step=39480, preemption_count=0, score=15162.900104, test/accuracy=0.668805, test/bleu=0.003654, test/loss=1.600933, test/num_examples=3003, total_duration=31507.295377, train/accuracy=0.643300, train/bleu=0.008900, train/loss=1.787295, validation/accuracy=0.659087, validation/bleu=0.006769, validation/loss=1.671352, validation/num_examples=3000
I0216 22:43:40.019068 140247047317248 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.32538902759552, loss=1.8142976760864258
I0216 22:46:51.479184 140247055709952 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.3268325626850128, loss=1.7885205745697021
I0216 22:50:03.085981 140247047317248 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.3187316656112671, loss=1.8891315460205078
I0216 22:53:14.653325 140247055709952 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.32377931475639343, loss=1.8765212297439575
I0216 22:56:26.303177 140247047317248 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.3379310369491577, loss=1.864605188369751
I0216 22:57:32.319453 140416697075520 spec.py:321] Evaluating on the training split.
I0216 22:57:35.506551 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 23:02:18.555522 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 23:02:21.438231 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 23:07:05.424973 140416697075520 spec.py:349] Evaluating on the test split.
I0216 23:07:08.326176 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 23:11:51.402652 140416697075520 submission_runner.py:408] Time since start: 33206.75s, 	Step: 41674, 	{'train/accuracy': 0.6402793526649475, 'train/loss': 1.8023650646209717, 'train/bleu': 0.018014787017851364, 'validation/accuracy': 0.6588882803916931, 'validation/loss': 1.6617416143417358, 'validation/bleu': 0.013407421763180685, 'validation/num_examples': 3000, 'test/accuracy': 0.6703155040740967, 'test/loss': 1.5892505645751953, 'test/bleu': 0.0038609307749633793, 'test/num_examples': 3003, 'score': 16003.174306154251, 'total_duration': 33206.75154232979, 'accumulated_submission_time': 16003.174306154251, 'accumulated_eval_time': 17201.640429973602, 'accumulated_logging_time': 0.5845160484313965}
I0216 23:11:51.421088 140247055709952 logging_writer.py:48] [41674] accumulated_eval_time=17201.640430, accumulated_logging_time=0.584516, accumulated_submission_time=16003.174306, global_step=41674, preemption_count=0, score=16003.174306, test/accuracy=0.670316, test/bleu=0.003861, test/loss=1.589251, test/num_examples=3003, total_duration=33206.751542, train/accuracy=0.640279, train/bleu=0.018015, train/loss=1.802365, validation/accuracy=0.658888, validation/bleu=0.013407, validation/loss=1.661742, validation/num_examples=3000
I0216 23:13:56.682870 140247047317248 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.32742640376091003, loss=1.9392415285110474
I0216 23:17:08.345391 140247055709952 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.3295470178127289, loss=1.8441752195358276
I0216 23:20:19.928319 140247047317248 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.3327880799770355, loss=1.8738337755203247
I0216 23:23:31.517798 140247055709952 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.3400871157646179, loss=1.7715609073638916
I0216 23:25:51.436187 140416697075520 spec.py:321] Evaluating on the training split.
I0216 23:25:54.621243 140416697075520 workload.py:181] Translating evaluation dataset.
W0216 23:30:36.638867 140416697075520 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 23:30:36.639106 140416697075520 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 23:30:36.639159 140416697075520 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 23:30:37.661984 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 23:30:40.555023 140416697075520 workload.py:181] Translating evaluation dataset.
W0216 23:35:22.051469 140416697075520 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 23:35:22.051691 140416697075520 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 23:35:22.051741 140416697075520 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 23:35:23.044631 140416697075520 spec.py:349] Evaluating on the test split.
I0216 23:35:25.936422 140416697075520 workload.py:181] Translating evaluation dataset.
W0216 23:40:07.663247 140416697075520 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 23:40:07.663489 140416697075520 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 23:40:07.663551 140416697075520 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 23:40:08.600394 140416697075520 submission_runner.py:408] Time since start: 34903.95s, 	Step: 43867, 	{'train/accuracy': 0.6594968438148499, 'train/loss': 1.6783331632614136, 'train/bleu': 0.012818265713863708, 'validation/accuracy': 0.6599546074867249, 'validation/loss': 1.6518120765686035, 'validation/bleu': 0.002466626391783668, 'validation/num_examples': 3000, 'test/accuracy': 0.6709197759628296, 'test/loss': 1.5779694318771362, 'test/bleu': 0.00790887381916585, 'test/num_examples': 3003, 'score': 16843.107570886612, 'total_duration': 34903.94935750961, 'accumulated_submission_time': 16843.107570886612, 'accumulated_eval_time': 18058.804636716843, 'accumulated_logging_time': 0.6142547130584717}
I0216 23:40:08.617467 140247047317248 logging_writer.py:48] [43867] accumulated_eval_time=18058.804637, accumulated_logging_time=0.614255, accumulated_submission_time=16843.107571, global_step=43867, preemption_count=0, score=16843.107571, test/accuracy=0.670920, test/bleu=0.007909, test/loss=1.577969, test/num_examples=3003, total_duration=34903.949358, train/accuracy=0.659497, train/bleu=0.012818, train/loss=1.678333, validation/accuracy=0.659955, validation/bleu=0.002467, validation/loss=1.651812, validation/num_examples=3000
I0216 23:40:59.884829 140247055709952 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.3268049657344818, loss=1.779832363128662
I0216 23:44:11.472104 140247047317248 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.3339229226112366, loss=1.8599085807800293
I0216 23:47:23.073896 140247055709952 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.3207143545150757, loss=1.737860083580017
I0216 23:50:34.678779 140247047317248 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.33014702796936035, loss=1.9067513942718506
I0216 23:53:46.285950 140247055709952 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.33536872267723083, loss=1.7667632102966309
I0216 23:54:08.613852 140416697075520 spec.py:321] Evaluating on the training split.
I0216 23:54:11.805064 140416697075520 workload.py:181] Translating evaluation dataset.
I0216 23:58:54.461794 140416697075520 spec.py:333] Evaluating on the validation split.
I0216 23:58:57.347634 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 00:03:39.590190 140416697075520 spec.py:349] Evaluating on the test split.
I0217 00:03:42.480973 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 00:08:24.615430 140416697075520 submission_runner.py:408] Time since start: 36599.96s, 	Step: 46060, 	{'train/accuracy': 0.6494045853614807, 'train/loss': 1.734161376953125, 'train/bleu': 0.0029831771182819147, 'validation/accuracy': 0.6632031798362732, 'validation/loss': 1.6402801275253296, 'validation/bleu': 0.001929465412214975, 'validation/num_examples': 3000, 'test/accuracy': 0.673267126083374, 'test/loss': 1.5677440166473389, 'test/bleu': 0.0011721331427797547, 'test/num_examples': 3003, 'score': 17683.02337527275, 'total_duration': 36599.964326143265, 'accumulated_submission_time': 17683.02337527275, 'accumulated_eval_time': 18914.806124925613, 'accumulated_logging_time': 0.6422410011291504}
I0217 00:08:24.633367 140247047317248 logging_writer.py:48] [46060] accumulated_eval_time=18914.806125, accumulated_logging_time=0.642241, accumulated_submission_time=17683.023375, global_step=46060, preemption_count=0, score=17683.023375, test/accuracy=0.673267, test/bleu=0.001172, test/loss=1.567744, test/num_examples=3003, total_duration=36599.964326, train/accuracy=0.649405, train/bleu=0.002983, train/loss=1.734161, validation/accuracy=0.663203, validation/bleu=0.001929, validation/loss=1.640280, validation/num_examples=3000
I0217 00:11:13.578237 140247055709952 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.33663812279701233, loss=1.9313584566116333
I0217 00:14:25.193308 140247047317248 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.33555343747138977, loss=1.8482218980789185
I0217 00:17:36.830308 140247055709952 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.3253486454486847, loss=1.8836134672164917
I0217 00:20:48.430461 140247047317248 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.3316112160682678, loss=1.896776556968689
I0217 00:22:24.699362 140416697075520 spec.py:321] Evaluating on the training split.
I0217 00:22:27.890619 140416697075520 workload.py:181] Translating evaluation dataset.
W0217 00:27:10.338299 140416697075520 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0217 00:27:10.338560 140416697075520 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0217 00:27:10.338614 140416697075520 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0217 00:27:10.446824 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 00:27:13.330436 140416697075520 workload.py:181] Translating evaluation dataset.
W0217 00:31:55.809951 140416697075520 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0217 00:31:55.810188 140416697075520 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0217 00:31:55.810258 140416697075520 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0217 00:31:56.031521 140416697075520 spec.py:349] Evaluating on the test split.
I0217 00:31:58.920624 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 00:36:41.380335 140416697075520 submission_runner.py:408] Time since start: 38296.73s, 	Step: 48253, 	{'train/accuracy': 0.6464726328849792, 'train/loss': 1.7595447301864624, 'train/bleu': 0.0030684778900967095, 'validation/accuracy': 0.6628063917160034, 'validation/loss': 1.6305805444717407, 'validation/bleu': 0.002109776036021225, 'validation/num_examples': 3000, 'test/accuracy': 0.6747080683708191, 'test/loss': 1.5579394102096558, 'test/bleu': 0.0034029668119348014, 'test/num_examples': 3003, 'score': 18523.008571386337, 'total_duration': 38296.72925019264, 'accumulated_submission_time': 18523.008571386337, 'accumulated_eval_time': 19771.487041950226, 'accumulated_logging_time': 0.6708183288574219}
I0217 00:36:41.398000 140247055709952 logging_writer.py:48] [48253] accumulated_eval_time=19771.487042, accumulated_logging_time=0.670818, accumulated_submission_time=18523.008571, global_step=48253, preemption_count=0, score=18523.008571, test/accuracy=0.674708, test/bleu=0.003403, test/loss=1.557939, test/num_examples=3003, total_duration=38296.729250, train/accuracy=0.646473, train/bleu=0.003068, train/loss=1.759545, validation/accuracy=0.662806, validation/bleu=0.002110, validation/loss=1.630581, validation/num_examples=3000
I0217 00:38:16.403472 140247047317248 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.33388006687164307, loss=1.7963989973068237
I0217 00:41:28.017338 140247055709952 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.3288036286830902, loss=1.8482705354690552
I0217 00:44:39.643912 140247047317248 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.33800172805786133, loss=1.7235816717147827
I0217 00:47:51.207986 140247055709952 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.3373493254184723, loss=1.9138264656066895
I0217 00:50:41.497517 140416697075520 spec.py:321] Evaluating on the training split.
I0217 00:50:44.681354 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 00:55:27.279114 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 00:55:30.163018 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 01:00:12.149344 140416697075520 spec.py:349] Evaluating on the test split.
I0217 01:00:15.047700 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 01:04:57.143800 140416697075520 submission_runner.py:408] Time since start: 39992.49s, 	Step: 50446, 	{'train/accuracy': 0.6579204797744751, 'train/loss': 1.681386947631836, 'train/bleu': 4.726848962627669e-11, 'validation/accuracy': 0.6643686890602112, 'validation/loss': 1.6211494207382202, 'validation/bleu': 4.353011344483678e-07, 'validation/num_examples': 3000, 'test/accuracy': 0.6763697862625122, 'test/loss': 1.5470092296600342, 'test/bleu': 3.832120235842256e-09, 'test/num_examples': 3003, 'score': 19363.02797317505, 'total_duration': 39992.49269723892, 'accumulated_submission_time': 19363.02797317505, 'accumulated_eval_time': 20627.133256673813, 'accumulated_logging_time': 0.697906494140625}
I0217 01:04:57.162344 140247047317248 logging_writer.py:48] [50446] accumulated_eval_time=20627.133257, accumulated_logging_time=0.697906, accumulated_submission_time=19363.027973, global_step=50446, preemption_count=0, score=19363.027973, test/accuracy=0.676370, test/bleu=0.000000, test/loss=1.547009, test/num_examples=3003, total_duration=39992.492697, train/accuracy=0.657920, train/bleu=0.000000, train/loss=1.681387, validation/accuracy=0.664369, validation/bleu=0.000000, validation/loss=1.621149, validation/num_examples=3000
I0217 01:05:18.226397 140247055709952 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.3389066457748413, loss=1.8509916067123413
I0217 01:08:29.781928 140247047317248 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.33757126331329346, loss=1.7855154275894165
I0217 01:11:41.371635 140247055709952 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.32818758487701416, loss=1.7172927856445312
I0217 01:14:52.950857 140247047317248 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.33212095499038696, loss=1.8105016946792603
I0217 01:18:04.607088 140247055709952 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.35038435459136963, loss=1.9011179208755493
I0217 01:18:57.173714 140416697075520 spec.py:321] Evaluating on the training split.
I0217 01:19:00.360986 140416697075520 workload.py:181] Translating evaluation dataset.
W0217 01:23:42.471960 140416697075520 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0217 01:23:42.472183 140416697075520 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0217 01:23:42.472234 140416697075520 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0217 01:23:43.387484 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 01:23:46.269052 140416697075520 workload.py:181] Translating evaluation dataset.
W0217 01:28:28.000957 140416697075520 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0217 01:28:28.001265 140416697075520 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0217 01:28:28.001322 140416697075520 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0217 01:28:28.798166 140416697075520 spec.py:349] Evaluating on the test split.
I0217 01:28:31.691514 140416697075520 workload.py:181] Translating evaluation dataset.
W0217 01:33:13.621905 140416697075520 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0217 01:33:13.622130 140416697075520 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0217 01:33:13.622181 140416697075520 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0217 01:33:14.217045 140416697075520 submission_runner.py:408] Time since start: 41689.57s, 	Step: 52639, 	{'train/accuracy': 0.6519097685813904, 'train/loss': 1.7235392332077026, 'train/bleu': 0.011317894781530157, 'validation/accuracy': 0.6658565998077393, 'validation/loss': 1.6115913391113281, 'validation/bleu': 0.00246963718736365, 'validation/num_examples': 3000, 'test/accuracy': 0.6758236289024353, 'test/loss': 1.5376360416412354, 'test/bleu': 0.0018983356855068302, 'test/num_examples': 3003, 'score': 20202.959358930588, 'total_duration': 41689.566000938416, 'accumulated_submission_time': 20202.959358930588, 'accumulated_eval_time': 21484.176567316055, 'accumulated_logging_time': 0.7261979579925537}
I0217 01:33:14.235413 140247047317248 logging_writer.py:48] [52639] accumulated_eval_time=21484.176567, accumulated_logging_time=0.726198, accumulated_submission_time=20202.959359, global_step=52639, preemption_count=0, score=20202.959359, test/accuracy=0.675824, test/bleu=0.001898, test/loss=1.537636, test/num_examples=3003, total_duration=41689.566001, train/accuracy=0.651910, train/bleu=0.011318, train/loss=1.723539, validation/accuracy=0.665857, validation/bleu=0.002470, validation/loss=1.611591, validation/num_examples=3000
I0217 01:35:32.807224 140247055709952 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.3381946384906769, loss=1.8506497144699097
I0217 01:38:44.359162 140247047317248 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.3357922434806824, loss=1.8502840995788574
I0217 01:41:55.917410 140247055709952 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.34374889731407166, loss=1.795591115951538
I0217 01:45:07.459993 140247047317248 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.3423755466938019, loss=1.8697761297225952
I0217 01:47:14.394460 140416697075520 spec.py:321] Evaluating on the training split.
I0217 01:47:17.587349 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 01:52:00.348637 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 01:52:03.243685 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 01:56:45.996186 140416697075520 spec.py:349] Evaluating on the test split.
I0217 01:56:48.898737 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 02:01:31.130173 140416697075520 submission_runner.py:408] Time since start: 43386.48s, 	Step: 54833, 	{'train/accuracy': 0.6488131880760193, 'train/loss': 1.7384616136550903, 'train/bleu': 0.00015914353197347627, 'validation/accuracy': 0.6657201647758484, 'validation/loss': 1.6080960035324097, 'validation/bleu': 0.00015626255059858376, 'validation/num_examples': 3000, 'test/accuracy': 0.6779385209083557, 'test/loss': 1.5288050174713135, 'test/bleu': 7.63189865179375e-05, 'test/num_examples': 3003, 'score': 21043.03815627098, 'total_duration': 43386.479078769684, 'accumulated_submission_time': 21043.03815627098, 'accumulated_eval_time': 22340.912219285965, 'accumulated_logging_time': 0.7540411949157715}
I0217 02:01:31.148744 140247055709952 logging_writer.py:48] [54833] accumulated_eval_time=22340.912219, accumulated_logging_time=0.754041, accumulated_submission_time=21043.038156, global_step=54833, preemption_count=0, score=21043.038156, test/accuracy=0.677939, test/bleu=0.000076, test/loss=1.528805, test/num_examples=3003, total_duration=43386.479079, train/accuracy=0.648813, train/bleu=0.000159, train/loss=1.738462, validation/accuracy=0.665720, validation/bleu=0.000156, validation/loss=1.608096, validation/num_examples=3000
I0217 02:02:35.445183 140247047317248 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.3391234278678894, loss=1.8457605838775635
I0217 02:05:47.017900 140247055709952 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.33750811219215393, loss=1.7635564804077148
I0217 02:08:58.621566 140247047317248 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.33936357498168945, loss=1.7874794006347656
I0217 02:12:10.210921 140247055709952 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.34646502137184143, loss=1.782148003578186
I0217 02:15:21.745607 140247047317248 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.33989953994750977, loss=1.8279935121536255
I0217 02:15:31.423799 140416697075520 spec.py:321] Evaluating on the training split.
I0217 02:15:34.613233 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 02:20:17.282666 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 02:20:20.167962 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 02:25:02.465908 140416697075520 spec.py:349] Evaluating on the test split.
I0217 02:25:05.359709 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 02:29:48.372840 140416697075520 submission_runner.py:408] Time since start: 45083.72s, 	Step: 57027, 	{'train/accuracy': 0.6556254029273987, 'train/loss': 1.6882045269012451, 'train/bleu': 2.3922584155053653e-07, 'validation/accuracy': 0.6680760383605957, 'validation/loss': 1.6013891696929932, 'validation/bleu': 3.257229736809888e-05, 'validation/num_examples': 3000, 'test/accuracy': 0.6789262890815735, 'test/loss': 1.520471453666687, 'test/bleu': 3.4626696722555804e-07, 'test/num_examples': 3003, 'score': 21883.23281097412, 'total_duration': 45083.72175168991, 'accumulated_submission_time': 21883.23281097412, 'accumulated_eval_time': 23197.861188173294, 'accumulated_logging_time': 0.7827584743499756}
I0217 02:29:48.391580 140247055709952 logging_writer.py:48] [57027] accumulated_eval_time=23197.861188, accumulated_logging_time=0.782758, accumulated_submission_time=21883.232811, global_step=57027, preemption_count=0, score=21883.232811, test/accuracy=0.678926, test/bleu=0.000000, test/loss=1.520471, test/num_examples=3003, total_duration=45083.721752, train/accuracy=0.655625, train/bleu=0.000000, train/loss=1.688205, validation/accuracy=0.668076, validation/bleu=0.000033, validation/loss=1.601389, validation/num_examples=3000
I0217 02:32:50.007656 140247047317248 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.33943745493888855, loss=1.7546970844268799
I0217 02:36:01.579694 140247055709952 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.34085536003112793, loss=1.7898670434951782
I0217 02:39:13.165572 140247047317248 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.3373136520385742, loss=1.7421596050262451
I0217 02:42:24.809315 140247055709952 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.3373519480228424, loss=1.7222137451171875
I0217 02:43:48.464424 140416697075520 spec.py:321] Evaluating on the training split.
I0217 02:43:51.651024 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 02:48:34.707271 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 02:48:37.595293 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 02:53:20.578629 140416697075520 spec.py:349] Evaluating on the test split.
I0217 02:53:23.480625 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 02:58:05.979543 140416697075520 submission_runner.py:408] Time since start: 46781.33s, 	Step: 59220, 	{'train/accuracy': 0.6575570702552795, 'train/loss': 1.6820569038391113, 'train/bleu': 0.00037022543641225867, 'validation/accuracy': 0.6692291498184204, 'validation/loss': 1.5894312858581543, 'validation/bleu': 0.0005184220410265468, 'validation/num_examples': 3000, 'test/accuracy': 0.6805298924446106, 'test/loss': 1.5156587362289429, 'test/bleu': 0.0007760878452398538, 'test/num_examples': 3003, 'score': 22723.22510910034, 'total_duration': 46781.328458070755, 'accumulated_submission_time': 22723.22510910034, 'accumulated_eval_time': 24055.376248836517, 'accumulated_logging_time': 0.8115298748016357}
I0217 02:58:05.997400 140247047317248 logging_writer.py:48] [59220] accumulated_eval_time=24055.376249, accumulated_logging_time=0.811530, accumulated_submission_time=22723.225109, global_step=59220, preemption_count=0, score=22723.225109, test/accuracy=0.680530, test/bleu=0.000776, test/loss=1.515659, test/num_examples=3003, total_duration=46781.328458, train/accuracy=0.657557, train/bleu=0.000370, train/loss=1.682057, validation/accuracy=0.669229, validation/bleu=0.000518, validation/loss=1.589431, validation/num_examples=3000
I0217 02:59:53.568039 140247055709952 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.3502193093299866, loss=1.8096144199371338
I0217 03:03:05.216591 140247047317248 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.3599572479724884, loss=1.8701245784759521
I0217 03:06:16.827009 140247055709952 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.336443692445755, loss=1.7586137056350708
I0217 03:09:28.422794 140247047317248 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.3435554504394531, loss=1.7713932991027832
I0217 03:12:05.997426 140416697075520 spec.py:321] Evaluating on the training split.
I0217 03:12:09.177881 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 03:16:51.657773 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 03:16:54.579620 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 03:21:37.648786 140416697075520 spec.py:349] Evaluating on the test split.
I0217 03:21:40.554285 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 03:26:22.589185 140416697075520 submission_runner.py:408] Time since start: 48477.94s, 	Step: 61413, 	{'train/accuracy': 0.6530603170394897, 'train/loss': 1.7072131633758545, 'train/bleu': 1.9221522374183195e-73, 'validation/accuracy': 0.6698119044303894, 'validation/loss': 1.586490273475647, 'validation/bleu': 5.0905459771434816e-18, 'validation/num_examples': 3000, 'test/accuracy': 0.6805298924446106, 'test/loss': 1.5095101594924927, 'test/bleu': 2.6753783328205294e-34, 'test/num_examples': 3003, 'score': 23563.144961118698, 'total_duration': 48477.938062906265, 'accumulated_submission_time': 23563.144961118698, 'accumulated_eval_time': 24911.96792125702, 'accumulated_logging_time': 0.8393943309783936}
I0217 03:26:22.608581 140247055709952 logging_writer.py:48] [61413] accumulated_eval_time=24911.967921, accumulated_logging_time=0.839394, accumulated_submission_time=23563.144961, global_step=61413, preemption_count=0, score=23563.144961, test/accuracy=0.680530, test/bleu=0.000000, test/loss=1.509510, test/num_examples=3003, total_duration=48477.938063, train/accuracy=0.653060, train/bleu=0.000000, train/loss=1.707213, validation/accuracy=0.669812, validation/bleu=0.000000, validation/loss=1.586490, validation/num_examples=3000
I0217 03:26:56.313410 140247047317248 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.34818035364151, loss=1.6931777000427246
I0217 03:30:07.813683 140247055709952 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.3453027606010437, loss=1.763382911682129
I0217 03:33:19.405044 140247047317248 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.3511101007461548, loss=1.7914142608642578
I0217 03:36:30.963762 140247055709952 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.34978899359703064, loss=1.7671760320663452
I0217 03:39:42.577594 140247047317248 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.348276287317276, loss=1.8052570819854736
I0217 03:40:22.902517 140416697075520 spec.py:321] Evaluating on the training split.
I0217 03:40:26.089244 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 03:45:08.545954 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 03:45:11.437106 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 03:49:54.488997 140416697075520 spec.py:349] Evaluating on the test split.
I0217 03:49:57.387785 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 03:54:39.212526 140416697075520 submission_runner.py:408] Time since start: 50174.56s, 	Step: 63607, 	{'train/accuracy': 0.6595078110694885, 'train/loss': 1.6708544492721558, 'train/bleu': 3.993306699776565e-09, 'validation/accuracy': 0.670332670211792, 'validation/loss': 1.5785328149795532, 'validation/bleu': 1.0997585442434252e-05, 'validation/num_examples': 3000, 'test/accuracy': 0.682993471622467, 'test/loss': 1.4995191097259521, 'test/bleu': 6.173476556787166e-16, 'test/num_examples': 3003, 'score': 24403.35770726204, 'total_duration': 50174.56140899658, 'accumulated_submission_time': 24403.35770726204, 'accumulated_eval_time': 25768.277834892273, 'accumulated_logging_time': 0.8691816329956055}
I0217 03:54:39.230680 140247055709952 logging_writer.py:48] [63607] accumulated_eval_time=25768.277835, accumulated_logging_time=0.869182, accumulated_submission_time=24403.357707, global_step=63607, preemption_count=0, score=24403.357707, test/accuracy=0.682993, test/bleu=0.000000, test/loss=1.499519, test/num_examples=3003, total_duration=50174.561409, train/accuracy=0.659508, train/bleu=0.000000, train/loss=1.670854, validation/accuracy=0.670333, validation/bleu=0.000011, validation/loss=1.578533, validation/num_examples=3000
I0217 03:57:10.152108 140247047317248 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.3477762043476105, loss=1.750396966934204
I0217 04:00:21.753407 140247055709952 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.36029350757598877, loss=1.8931326866149902
I0217 04:03:33.286894 140247047317248 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.3492571711540222, loss=1.7345823049545288
I0217 04:06:44.878531 140247055709952 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.3468407094478607, loss=1.7726198434829712
I0217 04:08:39.567054 140416697075520 spec.py:321] Evaluating on the training split.
I0217 04:08:42.754456 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 04:13:25.347753 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 04:13:28.244294 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 04:18:10.215503 140416697075520 spec.py:349] Evaluating on the test split.
I0217 04:18:13.101263 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 04:22:55.195233 140416697075520 submission_runner.py:408] Time since start: 51870.54s, 	Step: 65801, 	{'train/accuracy': 0.6581171154975891, 'train/loss': 1.6827095746994019, 'train/bleu': 1.3145112412286955e-13, 'validation/accuracy': 0.6711013913154602, 'validation/loss': 1.570637583732605, 'validation/bleu': 1.1356152574398593e-08, 'validation/num_examples': 3000, 'test/accuracy': 0.6834350228309631, 'test/loss': 1.493088722229004, 'test/bleu': 1.0301622792678375e-13, 'test/num_examples': 3003, 'score': 25243.613652467728, 'total_duration': 51870.5441570282, 'accumulated_submission_time': 25243.613652467728, 'accumulated_eval_time': 26623.905986070633, 'accumulated_logging_time': 0.8976736068725586}
I0217 04:22:55.213981 140247047317248 logging_writer.py:48] [65801] accumulated_eval_time=26623.905986, accumulated_logging_time=0.897674, accumulated_submission_time=25243.613652, global_step=65801, preemption_count=0, score=25243.613652, test/accuracy=0.683435, test/bleu=0.000000, test/loss=1.493089, test/num_examples=3003, total_duration=51870.544157, train/accuracy=0.658117, train/bleu=0.000000, train/loss=1.682710, validation/accuracy=0.671101, validation/bleu=0.000000, validation/loss=1.570638, validation/num_examples=3000
I0217 04:24:11.800696 140247055709952 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.34949731826782227, loss=1.7220933437347412
I0217 04:27:23.388770 140247047317248 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.3658100366592407, loss=1.7820535898208618
I0217 04:30:35.022538 140247055709952 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.3684491515159607, loss=1.7716418504714966
I0217 04:33:46.674282 140247047317248 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.3527669906616211, loss=1.802310824394226
I0217 04:36:55.360781 140416697075520 spec.py:321] Evaluating on the training split.
I0217 04:36:58.547713 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 04:41:42.203148 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 04:41:45.097996 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 04:46:28.366999 140416697075520 spec.py:349] Evaluating on the test split.
I0217 04:46:31.269493 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 04:51:14.105675 140416697075520 submission_runner.py:408] Time since start: 53569.45s, 	Step: 67994, 	{'train/accuracy': 0.65663081407547, 'train/loss': 1.6920562982559204, 'train/bleu': 5.7880635545361155e-75, 'validation/accuracy': 0.6715725660324097, 'validation/loss': 1.5655851364135742, 'validation/bleu': 3.4923757610281014e-110, 'validation/num_examples': 3000, 'test/accuracy': 0.6832374930381775, 'test/loss': 1.4881362915039062, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26083.677993774414, 'total_duration': 53569.45456409454, 'accumulated_submission_time': 26083.677993774414, 'accumulated_eval_time': 27482.650806188583, 'accumulated_logging_time': 0.9275956153869629}
I0217 04:51:14.125273 140247055709952 logging_writer.py:48] [67994] accumulated_eval_time=27482.650806, accumulated_logging_time=0.927596, accumulated_submission_time=26083.677994, global_step=67994, preemption_count=0, score=26083.677994, test/accuracy=0.683237, test/bleu=0.000000, test/loss=1.488136, test/num_examples=3003, total_duration=53569.454564, train/accuracy=0.656631, train/bleu=0.000000, train/loss=1.692056, validation/accuracy=0.671573, validation/bleu=0.000000, validation/loss=1.565585, validation/num_examples=3000
I0217 04:51:16.825420 140247047317248 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.36506742238998413, loss=1.7934082746505737
I0217 04:54:28.358260 140247055709952 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.3554757535457611, loss=1.7494454383850098
I0217 04:57:39.924169 140247047317248 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.3592386841773987, loss=1.7561463117599487
I0217 05:00:51.527618 140247055709952 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.35086116194725037, loss=1.7037181854248047
I0217 05:04:03.109557 140247047317248 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.3662349283695221, loss=1.798277735710144
I0217 05:05:14.151629 140416697075520 spec.py:321] Evaluating on the training split.
I0217 05:05:17.345387 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 05:09:59.915031 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 05:10:02.810156 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 05:14:45.654232 140416697075520 spec.py:349] Evaluating on the test split.
I0217 05:14:48.547200 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 05:19:31.035001 140416697075520 submission_runner.py:408] Time since start: 55266.38s, 	Step: 70187, 	{'train/accuracy': 0.6597678661346436, 'train/loss': 1.6584603786468506, 'train/bleu': 3.172754784343179e-33, 'validation/accuracy': 0.6727628707885742, 'validation/loss': 1.562662959098816, 'validation/bleu': 2.5994512896945813e-15, 'validation/num_examples': 3000, 'test/accuracy': 0.6852827072143555, 'test/loss': 1.4839190244674683, 'test/bleu': 4.750764566167227e-68, 'test/num_examples': 3003, 'score': 26923.62469100952, 'total_duration': 55266.38389849663, 'accumulated_submission_time': 26923.62469100952, 'accumulated_eval_time': 28339.534108638763, 'accumulated_logging_time': 0.956641435623169}
I0217 05:19:31.053701 140247055709952 logging_writer.py:48] [70187] accumulated_eval_time=28339.534109, accumulated_logging_time=0.956641, accumulated_submission_time=26923.624691, global_step=70187, preemption_count=0, score=26923.624691, test/accuracy=0.685283, test/bleu=0.000000, test/loss=1.483919, test/num_examples=3003, total_duration=55266.383898, train/accuracy=0.659768, train/bleu=0.000000, train/loss=1.658460, validation/accuracy=0.672763, validation/bleu=0.000000, validation/loss=1.562663, validation/num_examples=3000
I0217 05:21:31.328456 140247047317248 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.363345742225647, loss=1.8137611150741577
I0217 05:24:43.000102 140247055709952 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.3555572032928467, loss=1.7341434955596924
I0217 05:27:54.670661 140247047317248 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.36907443404197693, loss=1.78621506690979
I0217 05:31:06.225400 140247055709952 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.37042662501335144, loss=1.7469325065612793
I0217 05:33:31.181191 140416697075520 spec.py:321] Evaluating on the training split.
I0217 05:33:34.369721 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 05:38:16.904018 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 05:38:19.790657 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 05:43:01.691921 140416697075520 spec.py:349] Evaluating on the test split.
I0217 05:43:04.589634 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 05:47:46.806583 140416697075520 submission_runner.py:408] Time since start: 56962.16s, 	Step: 72380, 	{'train/accuracy': 0.6577961444854736, 'train/loss': 1.6833745241165161, 'train/bleu': 2.9953901460861085e-59, 'validation/accuracy': 0.6741639971733093, 'validation/loss': 1.5556203126907349, 'validation/bleu': 1.374083968779809e-28, 'validation/num_examples': 3000, 'test/accuracy': 0.6867468357086182, 'test/loss': 1.4756821393966675, 'test/bleu': 1.2972737967095044e-27, 'test/num_examples': 3003, 'score': 27763.672098875046, 'total_duration': 56962.155440330505, 'accumulated_submission_time': 27763.672098875046, 'accumulated_eval_time': 29195.159410238266, 'accumulated_logging_time': 0.9846923351287842}
I0217 05:47:46.830269 140247047317248 logging_writer.py:48] [72380] accumulated_eval_time=29195.159410, accumulated_logging_time=0.984692, accumulated_submission_time=27763.672099, global_step=72380, preemption_count=0, score=27763.672099, test/accuracy=0.686747, test/bleu=0.000000, test/loss=1.475682, test/num_examples=3003, total_duration=56962.155440, train/accuracy=0.657796, train/bleu=0.000000, train/loss=1.683375, validation/accuracy=0.674164, validation/bleu=0.000000, validation/loss=1.555620, validation/num_examples=3000
I0217 05:48:33.135277 140247055709952 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.3529297113418579, loss=1.66357421875
I0217 05:51:44.736947 140247047317248 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.3717738389968872, loss=1.7210370302200317
I0217 05:54:56.289753 140247055709952 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.3701700270175934, loss=1.7719902992248535
I0217 05:58:07.951360 140247047317248 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.36653199791908264, loss=1.7739497423171997
I0217 06:01:19.539085 140247055709952 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.36982715129852295, loss=1.7343430519104004
I0217 06:01:46.820724 140416697075520 spec.py:321] Evaluating on the training split.
I0217 06:01:50.005245 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 06:06:33.233473 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 06:06:36.119418 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 06:11:17.992249 140416697075520 spec.py:349] Evaluating on the test split.
I0217 06:11:20.881250 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 06:16:03.679236 140416697075520 submission_runner.py:408] Time since start: 58659.03s, 	Step: 74573, 	{'train/accuracy': 0.6599732637405396, 'train/loss': 1.6659163236618042, 'train/bleu': 3.433113828622719e-35, 'validation/accuracy': 0.6741020083427429, 'validation/loss': 1.5530149936676025, 'validation/bleu': 8.257218124896524e-21, 'validation/num_examples': 3000, 'test/accuracy': 0.6866306662559509, 'test/loss': 1.471996545791626, 'test/bleu': 2.1755551091241117e-28, 'test/num_examples': 3003, 'score': 28603.582304239273, 'total_duration': 58659.028136730194, 'accumulated_submission_time': 28603.582304239273, 'accumulated_eval_time': 30052.017840385437, 'accumulated_logging_time': 1.0185394287109375}
I0217 06:16:03.698342 140247047317248 logging_writer.py:48] [74573] accumulated_eval_time=30052.017840, accumulated_logging_time=1.018539, accumulated_submission_time=28603.582304, global_step=74573, preemption_count=0, score=28603.582304, test/accuracy=0.686631, test/bleu=0.000000, test/loss=1.471997, test/num_examples=3003, total_duration=58659.028137, train/accuracy=0.659973, train/bleu=0.000000, train/loss=1.665916, validation/accuracy=0.674102, validation/bleu=0.000000, validation/loss=1.553015, validation/num_examples=3000
I0217 06:18:47.590079 140247055709952 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.3634255528450012, loss=1.7028098106384277
I0217 06:21:59.147249 140247047317248 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.3797229528427124, loss=1.7696939706802368
I0217 06:25:10.837140 140247055709952 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.3761656582355499, loss=1.7779654264450073
I0217 06:28:22.413136 140247047317248 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.3650856614112854, loss=1.7361891269683838
I0217 06:30:03.732086 140416697075520 spec.py:321] Evaluating on the training split.
I0217 06:30:06.919762 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 06:34:49.349541 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 06:34:52.232659 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 06:39:34.269904 140416697075520 spec.py:349] Evaluating on the test split.
I0217 06:39:37.160022 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 06:44:20.236834 140416697075520 submission_runner.py:408] Time since start: 60355.59s, 	Step: 76766, 	{'train/accuracy': 0.6621755957603455, 'train/loss': 1.6525546312332153, 'train/bleu': 7.149810695207492e-14, 'validation/accuracy': 0.6751435399055481, 'validation/loss': 1.5478153228759766, 'validation/bleu': 5.920515743949042e-10, 'validation/num_examples': 3000, 'test/accuracy': 0.686851441860199, 'test/loss': 1.4702234268188477, 'test/bleu': 7.566292641313918e-14, 'test/num_examples': 3003, 'score': 29443.534519672394, 'total_duration': 60355.585752010345, 'accumulated_submission_time': 29443.534519672394, 'accumulated_eval_time': 30908.522542238235, 'accumulated_logging_time': 1.0486669540405273}
I0217 06:44:20.255593 140247055709952 logging_writer.py:48] [76766] accumulated_eval_time=30908.522542, accumulated_logging_time=1.048667, accumulated_submission_time=29443.534520, global_step=76766, preemption_count=0, score=29443.534520, test/accuracy=0.686851, test/bleu=0.000000, test/loss=1.470223, test/num_examples=3003, total_duration=60355.585752, train/accuracy=0.662176, train/bleu=0.000000, train/loss=1.652555, validation/accuracy=0.675144, validation/bleu=0.000000, validation/loss=1.547815, validation/num_examples=3000
I0217 06:45:50.272364 140247047317248 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.37391942739486694, loss=1.7282854318618774
I0217 06:49:01.866220 140247055709952 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.3724052309989929, loss=1.6601165533065796
I0217 06:52:13.420355 140247047317248 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.37697404623031616, loss=1.7945233583450317
I0217 06:55:25.067921 140247055709952 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.36704742908477783, loss=1.7752008438110352
I0217 06:58:20.243901 140416697075520 spec.py:321] Evaluating on the training split.
I0217 06:58:23.429765 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 07:03:06.025656 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 07:03:08.908497 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 07:07:51.388741 140416697075520 spec.py:349] Evaluating on the test split.
I0217 07:07:54.281750 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 07:12:37.412338 140416697075520 submission_runner.py:408] Time since start: 62052.76s, 	Step: 78959, 	{'train/accuracy': 0.6613059639930725, 'train/loss': 1.6557270288467407, 'train/bleu': 7.88032955656721e-18, 'validation/accuracy': 0.6753666996955872, 'validation/loss': 1.543614149093628, 'validation/bleu': 5.310967810711563e-16, 'validation/num_examples': 3000, 'test/accuracy': 0.6877810955047607, 'test/loss': 1.4642019271850586, 'test/bleu': 6.51375530429964e-28, 'test/num_examples': 3003, 'score': 30283.443378448486, 'total_duration': 62052.761239528656, 'accumulated_submission_time': 30283.443378448486, 'accumulated_eval_time': 31765.690917015076, 'accumulated_logging_time': 1.0765984058380127}
I0217 07:12:37.432387 140247047317248 logging_writer.py:48] [78959] accumulated_eval_time=31765.690917, accumulated_logging_time=1.076598, accumulated_submission_time=30283.443378, global_step=78959, preemption_count=0, score=30283.443378, test/accuracy=0.687781, test/bleu=0.000000, test/loss=1.464202, test/num_examples=3003, total_duration=62052.761240, train/accuracy=0.661306, train/bleu=0.000000, train/loss=1.655727, validation/accuracy=0.675367, validation/bleu=0.000000, validation/loss=1.543614, validation/num_examples=3000
I0217 07:12:53.525837 140247055709952 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.37769410014152527, loss=1.8078360557556152
I0217 07:16:05.128464 140247047317248 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.3679294288158417, loss=1.7021008729934692
I0217 07:19:16.726239 140247055709952 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.375563383102417, loss=1.7172249555587769
I0217 07:22:28.361101 140247047317248 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.38275575637817383, loss=1.7578548192977905
I0217 07:25:39.951265 140247055709952 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.36367806792259216, loss=1.683807611465454
I0217 07:26:37.545016 140416697075520 spec.py:321] Evaluating on the training split.
I0217 07:26:40.742107 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 07:31:23.317120 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 07:31:26.199869 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 07:36:08.037779 140416697075520 spec.py:349] Evaluating on the test split.
I0217 07:36:10.936972 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 07:40:52.903505 140416697075520 submission_runner.py:408] Time since start: 63748.25s, 	Step: 81152, 	{'train/accuracy': 0.6602025628089905, 'train/loss': 1.6547356843948364, 'train/bleu': 3.082688338920834e-10, 'validation/accuracy': 0.6759246587753296, 'validation/loss': 1.5399972200393677, 'validation/bleu': 3.585892732838403e-12, 'validation/num_examples': 3000, 'test/accuracy': 0.6881877779960632, 'test/loss': 1.460843801498413, 'test/bleu': 2.2006409592584095e-23, 'test/num_examples': 3003, 'score': 31123.47504043579, 'total_duration': 63748.25242686272, 'accumulated_submission_time': 31123.47504043579, 'accumulated_eval_time': 32621.049352645874, 'accumulated_logging_time': 1.107264757156372}
I0217 07:40:52.922587 140247047317248 logging_writer.py:48] [81152] accumulated_eval_time=32621.049353, accumulated_logging_time=1.107265, accumulated_submission_time=31123.475040, global_step=81152, preemption_count=0, score=31123.475040, test/accuracy=0.688188, test/bleu=0.000000, test/loss=1.460844, test/num_examples=3003, total_duration=63748.252427, train/accuracy=0.660203, train/bleu=0.000000, train/loss=1.654736, validation/accuracy=0.675925, validation/bleu=0.000000, validation/loss=1.539997, validation/num_examples=3000
I0217 07:43:06.633137 140247055709952 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.37120991945266724, loss=1.6912710666656494
I0217 07:46:18.228425 140247047317248 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.37584567070007324, loss=1.6827075481414795
I0217 07:49:29.884761 140247055709952 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.37518006563186646, loss=1.7131692171096802
I0217 07:52:41.460916 140247047317248 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.37544724345207214, loss=1.6489293575286865
I0217 07:54:52.977216 140416697075520 spec.py:321] Evaluating on the training split.
I0217 07:54:56.172793 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 07:59:38.593473 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 07:59:41.482172 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 08:04:23.459747 140416697075520 spec.py:349] Evaluating on the test split.
I0217 08:04:26.349913 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 08:09:08.302901 140416697075520 submission_runner.py:408] Time since start: 65443.65s, 	Step: 83345, 	{'train/accuracy': 0.665103018283844, 'train/loss': 1.6287139654159546, 'train/bleu': 7.842692905719594e-25, 'validation/accuracy': 0.6762842535972595, 'validation/loss': 1.5398870706558228, 'validation/bleu': 2.2081536343624706e-14, 'validation/num_examples': 3000, 'test/accuracy': 0.6889082789421082, 'test/loss': 1.4589356184005737, 'test/bleu': 1.0038464729692575e-116, 'test/num_examples': 3003, 'score': 31963.449004411697, 'total_duration': 65443.651799440384, 'accumulated_submission_time': 31963.449004411697, 'accumulated_eval_time': 33476.37496852875, 'accumulated_logging_time': 1.136979579925537}
I0217 08:09:08.322016 140247055709952 logging_writer.py:48] [83345] accumulated_eval_time=33476.374969, accumulated_logging_time=1.136980, accumulated_submission_time=31963.449004, global_step=83345, preemption_count=0, score=31963.449004, test/accuracy=0.688908, test/bleu=0.000000, test/loss=1.458936, test/num_examples=3003, total_duration=65443.651799, train/accuracy=0.665103, train/bleu=0.000000, train/loss=1.628714, validation/accuracy=0.676284, validation/bleu=0.000000, validation/loss=1.539887, validation/num_examples=3000
I0217 08:10:08.080645 140247047317248 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.38025471568107605, loss=1.7329508066177368
I0217 08:13:19.704255 140247055709952 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.3750944137573242, loss=1.7265836000442505
I0217 08:16:31.311562 140247047317248 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.38179275393486023, loss=1.7441242933273315
I0217 08:19:42.916372 140247055709952 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.38763678073883057, loss=1.7618911266326904
I0217 08:22:54.546092 140247047317248 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.3795630633831024, loss=1.7310690879821777
I0217 08:23:08.444893 140416697075520 spec.py:321] Evaluating on the training split.
I0217 08:23:11.629145 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 08:27:54.381171 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 08:27:57.266805 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 08:32:39.305227 140416697075520 spec.py:349] Evaluating on the test split.
I0217 08:32:42.198535 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 08:37:24.287038 140416697075520 submission_runner.py:408] Time since start: 67139.64s, 	Step: 85538, 	{'train/accuracy': 0.6634867787361145, 'train/loss': 1.6380850076675415, 'train/bleu': 7.299821481975685e-22, 'validation/accuracy': 0.675986647605896, 'validation/loss': 1.537553310394287, 'validation/bleu': 1.996271621908041e-16, 'validation/num_examples': 3000, 'test/accuracy': 0.6893614530563354, 'test/loss': 1.455039381980896, 'test/bleu': 5.389467207958059e-49, 'test/num_examples': 3003, 'score': 32803.49252653122, 'total_duration': 67139.63595438004, 'accumulated_submission_time': 32803.49252653122, 'accumulated_eval_time': 34332.217041015625, 'accumulated_logging_time': 1.1654882431030273}
I0217 08:37:24.305976 140247055709952 logging_writer.py:48] [85538] accumulated_eval_time=34332.217041, accumulated_logging_time=1.165488, accumulated_submission_time=32803.492527, global_step=85538, preemption_count=0, score=32803.492527, test/accuracy=0.689361, test/bleu=0.000000, test/loss=1.455039, test/num_examples=3003, total_duration=67139.635954, train/accuracy=0.663487, train/bleu=0.000000, train/loss=1.638085, validation/accuracy=0.675987, validation/bleu=0.000000, validation/loss=1.537553, validation/num_examples=3000
I0217 08:40:21.633914 140247047317248 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.37790125608444214, loss=1.7378653287887573
I0217 08:43:33.256595 140247055709952 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.3791889548301697, loss=1.7371773719787598
I0217 08:46:44.894463 140247047317248 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.38157951831817627, loss=1.6723319292068481
I0217 08:49:56.478148 140247055709952 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.37749388813972473, loss=1.662202000617981
I0217 08:51:24.330333 140416697075520 spec.py:321] Evaluating on the training split.
I0217 08:51:27.519484 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 08:56:10.016131 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 08:56:12.902199 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 09:00:55.565681 140416697075520 spec.py:349] Evaluating on the test split.
I0217 09:00:58.456416 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 09:05:40.666020 140416697075520 submission_runner.py:408] Time since start: 68836.01s, 	Step: 87731, 	{'train/accuracy': 0.666720449924469, 'train/loss': 1.615303874015808, 'train/bleu': 1.0550796882758134e-35, 'validation/accuracy': 0.6763338446617126, 'validation/loss': 1.5350775718688965, 'validation/bleu': 1.3886112009180192e-15, 'validation/num_examples': 3000, 'test/accuracy': 0.6897681951522827, 'test/loss': 1.452293872833252, 'test/bleu': 5.0952082229375916e-65, 'test/num_examples': 3003, 'score': 33643.43664717674, 'total_duration': 68836.01491498947, 'accumulated_submission_time': 33643.43664717674, 'accumulated_eval_time': 35188.552652835846, 'accumulated_logging_time': 1.1946303844451904}
I0217 09:05:40.685687 140247047317248 logging_writer.py:48] [87731] accumulated_eval_time=35188.552653, accumulated_logging_time=1.194630, accumulated_submission_time=33643.436647, global_step=87731, preemption_count=0, score=33643.436647, test/accuracy=0.689768, test/bleu=0.000000, test/loss=1.452294, test/num_examples=3003, total_duration=68836.014915, train/accuracy=0.666720, train/bleu=0.000000, train/loss=1.615304, validation/accuracy=0.676334, validation/bleu=0.000000, validation/loss=1.535078, validation/num_examples=3000
I0217 09:07:24.112262 140247055709952 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.38487666845321655, loss=1.7301639318466187
I0217 09:10:35.683605 140247047317248 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.3792479932308197, loss=1.6392946243286133
I0217 09:13:47.274307 140247055709952 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.3739369809627533, loss=1.6400039196014404
I0217 09:16:58.830478 140247047317248 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.38148754835128784, loss=1.6386048793792725
I0217 09:19:41.045802 140416697075520 spec.py:321] Evaluating on the training split.
I0217 09:19:44.228567 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 09:24:26.586130 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 09:24:29.468054 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 09:29:11.272451 140416697075520 spec.py:349] Evaluating on the test split.
I0217 09:29:14.167193 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 09:33:55.972682 140416697075520 submission_runner.py:408] Time since start: 70531.32s, 	Step: 89925, 	{'train/accuracy': 0.6668577790260315, 'train/loss': 1.6161235570907593, 'train/bleu': 5.753700356710845e-16, 'validation/accuracy': 0.677065372467041, 'validation/loss': 1.5341765880584717, 'validation/bleu': 2.1646827340380213e-12, 'validation/num_examples': 3000, 'test/accuracy': 0.6897100806236267, 'test/loss': 1.452409029006958, 'test/bleu': 1.1159244511417395e-64, 'test/num_examples': 3003, 'score': 34483.717282533646, 'total_duration': 70531.32159113884, 'accumulated_submission_time': 34483.717282533646, 'accumulated_eval_time': 36043.47945904732, 'accumulated_logging_time': 1.2234370708465576}
I0217 09:33:55.991955 140247055709952 logging_writer.py:48] [89925] accumulated_eval_time=36043.479459, accumulated_logging_time=1.223437, accumulated_submission_time=34483.717283, global_step=89925, preemption_count=0, score=34483.717283, test/accuracy=0.689710, test/bleu=0.000000, test/loss=1.452409, test/num_examples=3003, total_duration=70531.321591, train/accuracy=0.666858, train/bleu=0.000000, train/loss=1.616124, validation/accuracy=0.677065, validation/bleu=0.000000, validation/loss=1.534177, validation/num_examples=3000
I0217 09:34:25.099565 140247047317248 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.37686070799827576, loss=1.7543739080429077
I0217 09:37:36.658837 140247055709952 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.3931708037853241, loss=1.7914026975631714
I0217 09:40:48.274013 140247047317248 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.37659305334091187, loss=1.6138876676559448
I0217 09:43:59.806347 140247055709952 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.37824663519859314, loss=1.6666736602783203
I0217 09:47:11.435936 140247047317248 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.38205817341804504, loss=1.7111546993255615
I0217 09:47:55.981540 140416697075520 spec.py:321] Evaluating on the training split.
I0217 09:47:59.171769 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 09:52:42.249221 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 09:52:45.141726 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 09:57:27.173183 140416697075520 spec.py:349] Evaluating on the test split.
I0217 09:57:30.062342 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 10:02:11.859525 140416697075520 submission_runner.py:408] Time since start: 72227.21s, 	Step: 92118, 	{'train/accuracy': 0.6644206643104553, 'train/loss': 1.6344032287597656, 'train/bleu': 1.8677004315358872e-15, 'validation/accuracy': 0.6770901679992676, 'validation/loss': 1.5322757959365845, 'validation/bleu': 1.041932489766618e-08, 'validation/num_examples': 3000, 'test/accuracy': 0.6894544363021851, 'test/loss': 1.4509806632995605, 'test/bleu': 6.525035483503889e-24, 'test/num_examples': 3003, 'score': 35323.6275074482, 'total_duration': 72227.20843577385, 'accumulated_submission_time': 35323.6275074482, 'accumulated_eval_time': 36899.35737490654, 'accumulated_logging_time': 1.2518982887268066}
I0217 10:02:11.878942 140247055709952 logging_writer.py:48] [92118] accumulated_eval_time=36899.357375, accumulated_logging_time=1.251898, accumulated_submission_time=35323.627507, global_step=92118, preemption_count=0, score=35323.627507, test/accuracy=0.689454, test/bleu=0.000000, test/loss=1.450981, test/num_examples=3003, total_duration=72227.208436, train/accuracy=0.664421, train/bleu=0.000000, train/loss=1.634403, validation/accuracy=0.677090, validation/bleu=0.000000, validation/loss=1.532276, validation/num_examples=3000
I0217 10:04:38.681384 140247047317248 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.38094329833984375, loss=1.7033216953277588
I0217 10:07:50.345942 140247055709952 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.38178592920303345, loss=1.6842771768569946
I0217 10:11:01.915706 140247047317248 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.3807218670845032, loss=1.791860818862915
I0217 10:14:13.471668 140247055709952 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.3740616738796234, loss=1.6891587972640991
I0217 10:16:11.996695 140416697075520 spec.py:321] Evaluating on the training split.
I0217 10:16:15.188998 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 10:20:57.760221 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 10:21:00.645576 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 10:25:42.618192 140416697075520 spec.py:349] Evaluating on the test split.
I0217 10:25:45.505249 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 10:30:27.786207 140416697075520 submission_runner.py:408] Time since start: 73923.14s, 	Step: 94311, 	{'train/accuracy': 0.6639440059661865, 'train/loss': 1.6363450288772583, 'train/bleu': 4.729026642061335e-16, 'validation/accuracy': 0.6773505806922913, 'validation/loss': 1.5321143865585327, 'validation/bleu': 4.4303980933181974e-14, 'validation/num_examples': 3000, 'test/accuracy': 0.6895125508308411, 'test/loss': 1.4501503705978394, 'test/bleu': 4.159748339163509e-52, 'test/num_examples': 3003, 'score': 36163.66510796547, 'total_duration': 73923.13512277603, 'accumulated_submission_time': 36163.66510796547, 'accumulated_eval_time': 37755.14686727524, 'accumulated_logging_time': 1.2815837860107422}
I0217 10:30:27.806302 140247047317248 logging_writer.py:48] [94311] accumulated_eval_time=37755.146867, accumulated_logging_time=1.281584, accumulated_submission_time=36163.665108, global_step=94311, preemption_count=0, score=36163.665108, test/accuracy=0.689513, test/bleu=0.000000, test/loss=1.450150, test/num_examples=3003, total_duration=73923.135123, train/accuracy=0.663944, train/bleu=0.000000, train/loss=1.636345, validation/accuracy=0.677351, validation/bleu=0.000000, validation/loss=1.532114, validation/num_examples=3000
I0217 10:31:40.535403 140247055709952 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.38393303751945496, loss=1.7633391618728638
I0217 10:34:52.163534 140247047317248 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.38356637954711914, loss=1.690072774887085
I0217 10:38:03.760040 140247055709952 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.3784653842449188, loss=1.6373008489608765
I0217 10:41:15.365406 140247047317248 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.37692347168922424, loss=1.702516794204712
I0217 10:44:26.942091 140247055709952 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.38260698318481445, loss=1.675878643989563
I0217 10:44:27.806916 140416697075520 spec.py:321] Evaluating on the training split.
I0217 10:44:30.993589 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 10:49:14.312356 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 10:49:17.199521 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 10:53:58.798275 140416697075520 spec.py:349] Evaluating on the test split.
I0217 10:54:01.693067 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 10:58:43.941327 140416697075520 submission_runner.py:408] Time since start: 75619.29s, 	Step: 96504, 	{'train/accuracy': 0.6668611764907837, 'train/loss': 1.6193124055862427, 'train/bleu': 6.958976294101557e-21, 'validation/accuracy': 0.6774745583534241, 'validation/loss': 1.5317516326904297, 'validation/bleu': 7.129035705814547e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6895706653594971, 'test/loss': 1.4500772953033447, 'test/bleu': 2.3831647633285975e-43, 'test/num_examples': 3003, 'score': 37003.584347724915, 'total_duration': 75619.29023313522, 'accumulated_submission_time': 37003.584347724915, 'accumulated_eval_time': 38611.28120183945, 'accumulated_logging_time': 1.312791347503662}
I0217 10:58:43.961635 140247047317248 logging_writer.py:48] [96504] accumulated_eval_time=38611.281202, accumulated_logging_time=1.312791, accumulated_submission_time=37003.584348, global_step=96504, preemption_count=0, score=37003.584348, test/accuracy=0.689571, test/bleu=0.000000, test/loss=1.450077, test/num_examples=3003, total_duration=75619.290233, train/accuracy=0.666861, train/bleu=0.000000, train/loss=1.619312, validation/accuracy=0.677475, validation/bleu=0.000000, validation/loss=1.531752, validation/num_examples=3000
I0217 11:01:54.347938 140247055709952 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.3928929567337036, loss=1.7969586849212646
I0217 11:05:05.897562 140247047317248 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.38389015197753906, loss=1.687238097190857
I0217 11:08:17.520103 140247055709952 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.3907434642314911, loss=1.7582567930221558
I0217 11:11:29.076133 140247047317248 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.3736594319343567, loss=1.6281739473342896
I0217 11:12:44.304331 140416697075520 spec.py:321] Evaluating on the training split.
I0217 11:12:47.488977 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 11:17:29.826880 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 11:17:32.710375 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 11:22:16.062466 140416697075520 spec.py:349] Evaluating on the test split.
I0217 11:22:18.952235 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 11:27:01.517014 140416697075520 submission_runner.py:408] Time since start: 77316.87s, 	Step: 98698, 	{'train/accuracy': 0.6659024953842163, 'train/loss': 1.6249374151229858, 'train/bleu': 8.48075740095282e-18, 'validation/accuracy': 0.6773257255554199, 'validation/loss': 1.5316590070724487, 'validation/bleu': 1.9773533311381334e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6894544363021851, 'test/loss': 1.4499237537384033, 'test/bleu': 6.800218506898445e-53, 'test/num_examples': 3003, 'score': 37843.84718418121, 'total_duration': 77316.86589956284, 'accumulated_submission_time': 37843.84718418121, 'accumulated_eval_time': 39468.493797540665, 'accumulated_logging_time': 1.3427517414093018}
I0217 11:27:01.540143 140247055709952 logging_writer.py:48] [98698] accumulated_eval_time=39468.493798, accumulated_logging_time=1.342752, accumulated_submission_time=37843.847184, global_step=98698, preemption_count=0, score=37843.847184, test/accuracy=0.689454, test/bleu=0.000000, test/loss=1.449924, test/num_examples=3003, total_duration=77316.865900, train/accuracy=0.665902, train/bleu=0.000000, train/loss=1.624937, validation/accuracy=0.677326, validation/bleu=0.000000, validation/loss=1.531659, validation/num_examples=3000
I0217 11:28:57.612000 140247047317248 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.3795144259929657, loss=1.7029473781585693
I0217 11:32:09.207631 140247055709952 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.38156065344810486, loss=1.7070826292037964
I0217 11:35:20.804635 140247047317248 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.38663196563720703, loss=1.7407788038253784
I0217 11:38:32.530730 140247055709952 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.3725252151489258, loss=1.588367223739624
I0217 11:41:01.682468 140416697075520 spec.py:321] Evaluating on the training split.
I0217 11:41:04.876813 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 11:45:47.050103 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 11:45:49.934637 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 11:50:32.402582 140416697075520 spec.py:349] Evaluating on the test split.
I0217 11:50:35.300468 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 11:55:17.577483 140416697075520 submission_runner.py:408] Time since start: 79012.93s, 	Step: 100891, 	{'train/accuracy': 0.6657024025917053, 'train/loss': 1.6258277893066406, 'train/bleu': 1.6210096065143052e-16, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 38683.90804576874, 'total_duration': 79012.92638611794, 'accumulated_submission_time': 38683.90804576874, 'accumulated_eval_time': 40324.388748407364, 'accumulated_logging_time': 1.3765370845794678}
I0217 11:55:17.598016 140247047317248 logging_writer.py:48] [100891] accumulated_eval_time=40324.388748, accumulated_logging_time=1.376537, accumulated_submission_time=38683.908046, global_step=100891, preemption_count=0, score=38683.908046, test/accuracy=0.689373, test/bleu=0.000000, test/loss=1.449911, test/num_examples=3003, total_duration=79012.926386, train/accuracy=0.665702, train/bleu=0.000000, train/loss=1.625828, validation/accuracy=0.677289, validation/bleu=0.000000, validation/loss=1.531644, validation/num_examples=3000
I0217 11:55:59.738536 140247055709952 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.3841296434402466, loss=1.6828409433364868
I0217 11:59:11.286953 140247047317248 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.3722432255744934, loss=1.7445645332336426
I0217 12:02:22.910208 140247055709952 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.3888561725616455, loss=1.7267595529556274
I0217 12:05:34.535001 140247047317248 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.373744934797287, loss=1.6925926208496094
I0217 12:08:46.183618 140247055709952 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.37889859080314636, loss=1.6872344017028809
I0217 12:09:17.682853 140416697075520 spec.py:321] Evaluating on the training split.
I0217 12:09:20.868266 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 12:14:04.489962 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 12:14:07.373870 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 12:18:50.229978 140416697075520 spec.py:349] Evaluating on the test split.
I0217 12:18:53.129718 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 12:23:35.007952 140416697075520 submission_runner.py:408] Time since start: 80710.36s, 	Step: 103084, 	{'train/accuracy': 0.6672636866569519, 'train/loss': 1.6149146556854248, 'train/bleu': 2.411098563653968e-16, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 39523.91212558746, 'total_duration': 80710.3568725586, 'accumulated_submission_time': 39523.91212558746, 'accumulated_eval_time': 41181.713782548904, 'accumulated_logging_time': 1.4078152179718018}
I0217 12:23:35.028992 140247047317248 logging_writer.py:48] [103084] accumulated_eval_time=41181.713783, accumulated_logging_time=1.407815, accumulated_submission_time=39523.912126, global_step=103084, preemption_count=0, score=39523.912126, test/accuracy=0.689373, test/bleu=0.000000, test/loss=1.449911, test/num_examples=3003, total_duration=80710.356873, train/accuracy=0.667264, train/bleu=0.000000, train/loss=1.614915, validation/accuracy=0.677289, validation/bleu=0.000000, validation/loss=1.531644, validation/num_examples=3000
I0217 12:26:14.835814 140247055709952 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.38727474212646484, loss=1.7194342613220215
I0217 12:29:26.439956 140247047317248 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.3871333599090576, loss=1.6596039533615112
I0217 12:32:38.133048 140247055709952 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.37424811720848083, loss=1.563024878501892
I0217 12:35:49.726601 140247047317248 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.3749545216560364, loss=1.651020884513855
I0217 12:37:35.204322 140416697075520 spec.py:321] Evaluating on the training split.
I0217 12:37:38.394834 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 12:42:20.650533 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 12:42:23.546705 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 12:47:05.586873 140416697075520 spec.py:349] Evaluating on the test split.
I0217 12:47:08.478807 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 12:51:51.039279 140416697075520 submission_runner.py:408] Time since start: 82406.39s, 	Step: 105277, 	{'train/accuracy': 0.6629880666732788, 'train/loss': 1.6416219472885132, 'train/bleu': 7.106031843432135e-15, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 40364.00815534592, 'total_duration': 82406.38815188408, 'accumulated_submission_time': 40364.00815534592, 'accumulated_eval_time': 42037.548650979996, 'accumulated_logging_time': 1.4379754066467285}
I0217 12:51:51.060522 140247055709952 logging_writer.py:48] [105277] accumulated_eval_time=42037.548651, accumulated_logging_time=1.437975, accumulated_submission_time=40364.008155, global_step=105277, preemption_count=0, score=40364.008155, test/accuracy=0.689373, test/bleu=0.000000, test/loss=1.449911, test/num_examples=3003, total_duration=82406.388152, train/accuracy=0.662988, train/bleu=0.000000, train/loss=1.641622, validation/accuracy=0.677289, validation/bleu=0.000000, validation/loss=1.531644, validation/num_examples=3000
I0217 12:53:16.848358 140247047317248 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.3864435851573944, loss=1.7174410820007324
I0217 12:56:28.477098 140247055709952 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.3816516101360321, loss=1.696717381477356
I0217 12:59:40.166679 140247047317248 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.38158276677131653, loss=1.6614220142364502
I0217 13:02:51.786347 140247055709952 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.38087981939315796, loss=1.763318657875061
I0217 13:05:51.301300 140416697075520 spec.py:321] Evaluating on the training split.
I0217 13:05:54.496776 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 13:10:38.145233 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 13:10:41.036233 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 13:15:22.804290 140416697075520 spec.py:349] Evaluating on the test split.
I0217 13:15:25.695798 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 13:20:07.996247 140416697075520 submission_runner.py:408] Time since start: 84103.35s, 	Step: 107470, 	{'train/accuracy': 0.6645278334617615, 'train/loss': 1.6353766918182373, 'train/bleu': 5.730438106700271e-20, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 41204.166110515594, 'total_duration': 84103.34514284134, 'accumulated_submission_time': 41204.166110515594, 'accumulated_eval_time': 42894.243529081345, 'accumulated_logging_time': 1.4703781604766846}
I0217 13:20:08.016830 140247047317248 logging_writer.py:48] [107470] accumulated_eval_time=42894.243529, accumulated_logging_time=1.470378, accumulated_submission_time=41204.166111, global_step=107470, preemption_count=0, score=41204.166111, test/accuracy=0.689373, test/bleu=0.000000, test/loss=1.449911, test/num_examples=3003, total_duration=84103.345143, train/accuracy=0.664528, train/bleu=0.000000, train/loss=1.635377, validation/accuracy=0.677289, validation/bleu=0.000000, validation/loss=1.531644, validation/num_examples=3000
I0217 13:20:19.903676 140247055709952 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.37722963094711304, loss=1.6834183931350708
I0217 13:23:31.423180 140247047317248 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.3841973841190338, loss=1.7050522565841675
I0217 13:26:43.038537 140247055709952 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.3768475353717804, loss=1.6811081171035767
I0217 13:29:54.698651 140247047317248 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.38163232803344727, loss=1.6928696632385254
I0217 13:33:06.373114 140247055709952 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.3940178453922272, loss=1.7320178747177124
I0217 13:34:08.160796 140416697075520 spec.py:321] Evaluating on the training split.
I0217 13:34:11.348299 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 13:38:54.400569 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 13:38:57.287811 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 13:43:39.473682 140416697075520 spec.py:349] Evaluating on the test split.
I0217 13:43:42.375051 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 13:48:24.107560 140416697075520 submission_runner.py:408] Time since start: 85799.46s, 	Step: 109663, 	{'train/accuracy': 0.6674066185951233, 'train/loss': 1.6146180629730225, 'train/bleu': 6.748841156792323e-20, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 42044.22923755646, 'total_duration': 85799.4564781189, 'accumulated_submission_time': 42044.22923755646, 'accumulated_eval_time': 43750.19023013115, 'accumulated_logging_time': 1.5016522407531738}
I0217 13:48:24.128229 140247047317248 logging_writer.py:48] [109663] accumulated_eval_time=43750.190230, accumulated_logging_time=1.501652, accumulated_submission_time=42044.229238, global_step=109663, preemption_count=0, score=42044.229238, test/accuracy=0.689373, test/bleu=0.000000, test/loss=1.449911, test/num_examples=3003, total_duration=85799.456478, train/accuracy=0.667407, train/bleu=0.000000, train/loss=1.614618, validation/accuracy=0.677289, validation/bleu=0.000000, validation/loss=1.531644, validation/num_examples=3000
I0217 13:50:33.621692 140247055709952 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.37410658597946167, loss=1.636130452156067
I0217 13:53:45.211615 140247047317248 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.3826133906841278, loss=1.6945699453353882
I0217 13:56:56.883857 140247055709952 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.37514692544937134, loss=1.6051571369171143
I0217 14:00:08.603213 140247047317248 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.37966519594192505, loss=1.7015594244003296
I0217 14:02:24.341999 140416697075520 spec.py:321] Evaluating on the training split.
I0217 14:02:27.530813 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 14:07:10.510393 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 14:07:13.394095 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 14:11:55.503174 140416697075520 spec.py:349] Evaluating on the test split.
I0217 14:11:58.403015 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 14:16:40.568873 140416697075520 submission_runner.py:408] Time since start: 87495.92s, 	Step: 111856, 	{'train/accuracy': 0.6615363359451294, 'train/loss': 1.6487631797790527, 'train/bleu': 2.1646841932618231e-16, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 42884.362718105316, 'total_duration': 87495.91778969765, 'accumulated_submission_time': 42884.362718105316, 'accumulated_eval_time': 44606.41705441475, 'accumulated_logging_time': 1.5323512554168701}
I0217 14:16:40.589838 140247055709952 logging_writer.py:48] [111856] accumulated_eval_time=44606.417054, accumulated_logging_time=1.532351, accumulated_submission_time=42884.362718, global_step=111856, preemption_count=0, score=42884.362718, test/accuracy=0.689373, test/bleu=0.000000, test/loss=1.449911, test/num_examples=3003, total_duration=87495.917790, train/accuracy=0.661536, train/bleu=0.000000, train/loss=1.648763, validation/accuracy=0.677289, validation/bleu=0.000000, validation/loss=1.531644, validation/num_examples=3000
I0217 14:17:36.134509 140247047317248 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.3806949555873871, loss=1.7395771741867065
I0217 14:20:47.769258 140247055709952 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.38453635573387146, loss=1.732509732246399
I0217 14:23:59.438322 140247047317248 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.38213595747947693, loss=1.6987833976745605
I0217 14:27:11.122787 140247055709952 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.3783719837665558, loss=1.6529531478881836
I0217 14:30:22.801255 140247047317248 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.3907407522201538, loss=1.712217092514038
I0217 14:30:40.923727 140416697075520 spec.py:321] Evaluating on the training split.
I0217 14:30:44.109908 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 14:35:27.430922 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 14:35:30.319993 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 14:40:13.534110 140416697075520 spec.py:349] Evaluating on the test split.
I0217 14:40:16.423315 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 14:44:59.188166 140416697075520 submission_runner.py:408] Time since start: 89194.54s, 	Step: 114049, 	{'train/accuracy': 0.6631463170051575, 'train/loss': 1.6365150213241577, 'train/bleu': 6.612449708341315e-18, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 43724.61556696892, 'total_duration': 89194.5370836258, 'accumulated_submission_time': 43724.61556696892, 'accumulated_eval_time': 45464.681428432465, 'accumulated_logging_time': 1.5638012886047363}
I0217 14:44:59.210832 140247055709952 logging_writer.py:48] [114049] accumulated_eval_time=45464.681428, accumulated_logging_time=1.563801, accumulated_submission_time=43724.615567, global_step=114049, preemption_count=0, score=43724.615567, test/accuracy=0.689373, test/bleu=0.000000, test/loss=1.449911, test/num_examples=3003, total_duration=89194.537084, train/accuracy=0.663146, train/bleu=0.000000, train/loss=1.636515, validation/accuracy=0.677289, validation/bleu=0.000000, validation/loss=1.531644, validation/num_examples=3000
I0217 14:47:52.429394 140247047317248 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.37140920758247375, loss=1.7090353965759277
I0217 14:51:04.163797 140247055709952 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.3814655840396881, loss=1.6605113744735718
I0217 14:54:15.859545 140247047317248 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.38620463013648987, loss=1.7317314147949219
I0217 14:57:27.551973 140247055709952 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.39075103402137756, loss=1.8367305994033813
I0217 14:58:59.256606 140416697075520 spec.py:321] Evaluating on the training split.
I0217 14:59:02.439167 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 15:03:44.883094 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 15:03:47.772375 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 15:08:29.564824 140416697075520 spec.py:349] Evaluating on the test split.
I0217 15:08:32.453855 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 15:13:14.685543 140416697075520 submission_runner.py:408] Time since start: 90890.03s, 	Step: 116241, 	{'train/accuracy': 0.6620974540710449, 'train/loss': 1.6439599990844727, 'train/bleu': 9.304555741295878e-18, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 44564.58071374893, 'total_duration': 90890.03444361687, 'accumulated_submission_time': 44564.58071374893, 'accumulated_eval_time': 46320.11028981209, 'accumulated_logging_time': 1.5969150066375732}
I0217 15:13:14.707123 140247047317248 logging_writer.py:48] [116241] accumulated_eval_time=46320.110290, accumulated_logging_time=1.596915, accumulated_submission_time=44564.580714, global_step=116241, preemption_count=0, score=44564.580714, test/accuracy=0.689373, test/bleu=0.000000, test/loss=1.449911, test/num_examples=3003, total_duration=90890.034444, train/accuracy=0.662097, train/bleu=0.000000, train/loss=1.643960, validation/accuracy=0.677289, validation/bleu=0.000000, validation/loss=1.531644, validation/num_examples=3000
I0217 15:14:54.326281 140247055709952 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.38860243558883667, loss=1.6728352308273315
I0217 15:18:06.014177 140247047317248 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.3754342496395111, loss=1.670844316482544
I0217 15:21:17.684775 140247055709952 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.3827545940876007, loss=1.6806472539901733
I0217 15:24:29.380244 140247047317248 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.3813498318195343, loss=1.6754602193832397
I0217 15:27:14.768030 140416697075520 spec.py:321] Evaluating on the training split.
I0217 15:27:17.957223 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 15:32:00.204448 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 15:32:03.093194 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 15:36:44.883775 140416697075520 spec.py:349] Evaluating on the test split.
I0217 15:36:47.784617 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 15:41:30.164743 140416697075520 submission_runner.py:408] Time since start: 92585.51s, 	Step: 118433, 	{'train/accuracy': 0.6632860898971558, 'train/loss': 1.6343814134597778, 'train/bleu': 3.4164877850608e-17, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 45404.56107854843, 'total_duration': 92585.51359510422, 'accumulated_submission_time': 45404.56107854843, 'accumulated_eval_time': 47175.50688838959, 'accumulated_logging_time': 1.6293854713439941}
I0217 15:41:30.186690 140247055709952 logging_writer.py:48] [118433] accumulated_eval_time=47175.506888, accumulated_logging_time=1.629385, accumulated_submission_time=45404.561079, global_step=118433, preemption_count=0, score=45404.561079, test/accuracy=0.689373, test/bleu=0.000000, test/loss=1.449911, test/num_examples=3003, total_duration=92585.513595, train/accuracy=0.663286, train/bleu=0.000000, train/loss=1.634381, validation/accuracy=0.677289, validation/bleu=0.000000, validation/loss=1.531644, validation/num_examples=3000
I0217 15:41:56.221559 140247047317248 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.3783046305179596, loss=1.688715934753418
I0217 15:45:07.830418 140247055709952 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.3883827328681946, loss=1.7759300470352173
I0217 15:48:19.492941 140247047317248 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.38550037145614624, loss=1.7228662967681885
I0217 15:51:31.094450 140247055709952 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.3842799961566925, loss=1.6665208339691162
I0217 15:54:42.826534 140247047317248 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.38132810592651367, loss=1.6402778625488281
I0217 15:55:30.444934 140416697075520 spec.py:321] Evaluating on the training split.
I0217 15:55:33.637021 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 16:00:16.179252 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 16:00:19.076744 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 16:05:00.978933 140416697075520 spec.py:349] Evaluating on the test split.
I0217 16:05:03.878734 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 16:09:45.961351 140416697075520 submission_runner.py:408] Time since start: 94281.31s, 	Step: 120626, 	{'train/accuracy': 0.6642124652862549, 'train/loss': 1.6329617500305176, 'train/bleu': 3.900978586004138e-15, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 46244.73825931549, 'total_duration': 94281.31026983261, 'accumulated_submission_time': 46244.73825931549, 'accumulated_eval_time': 48031.023241996765, 'accumulated_logging_time': 1.661921739578247}
I0217 16:09:45.982718 140247055709952 logging_writer.py:48] [120626] accumulated_eval_time=48031.023242, accumulated_logging_time=1.661922, accumulated_submission_time=46244.738259, global_step=120626, preemption_count=0, score=46244.738259, test/accuracy=0.689373, test/bleu=0.000000, test/loss=1.449911, test/num_examples=3003, total_duration=94281.310270, train/accuracy=0.664212, train/bleu=0.000000, train/loss=1.632962, validation/accuracy=0.677289, validation/bleu=0.000000, validation/loss=1.531644, validation/num_examples=3000
I0217 16:12:09.693380 140247047317248 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.3768084645271301, loss=1.6985487937927246
I0217 16:15:21.395889 140247055709952 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.3691486418247223, loss=1.6500526666641235
I0217 16:18:33.070606 140247047317248 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.37498152256011963, loss=1.7042802572250366
I0217 16:21:44.788970 140247055709952 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.38514888286590576, loss=1.7049551010131836
I0217 16:23:46.092909 140416697075520 spec.py:321] Evaluating on the training split.
I0217 16:23:49.287335 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 16:28:31.657018 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 16:28:34.548012 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 16:33:16.922360 140416697075520 spec.py:349] Evaluating on the test split.
I0217 16:33:19.829207 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 16:38:01.688536 140416697075520 submission_runner.py:408] Time since start: 95977.04s, 	Step: 122818, 	{'train/accuracy': 0.6649504899978638, 'train/loss': 1.6239640712738037, 'train/bleu': 3.694180749962438e-15, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 47084.7688806057, 'total_duration': 95977.03742218018, 'accumulated_submission_time': 47084.7688806057, 'accumulated_eval_time': 48886.61877822876, 'accumulated_logging_time': 1.692708969116211}
I0217 16:38:01.712619 140247047317248 logging_writer.py:48] [122818] accumulated_eval_time=48886.618778, accumulated_logging_time=1.692709, accumulated_submission_time=47084.768881, global_step=122818, preemption_count=0, score=47084.768881, test/accuracy=0.689373, test/bleu=0.000000, test/loss=1.449911, test/num_examples=3003, total_duration=95977.037422, train/accuracy=0.664950, train/bleu=0.000000, train/loss=1.623964, validation/accuracy=0.677289, validation/bleu=0.000000, validation/loss=1.531644, validation/num_examples=3000
I0217 16:39:11.814854 140247055709952 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.3829526901245117, loss=1.6525702476501465
I0217 16:42:23.476007 140247047317248 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.36793407797813416, loss=1.6939780712127686
I0217 16:45:35.086828 140247055709952 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.38309913873672485, loss=1.6983119249343872
I0217 16:48:46.732480 140247047317248 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.3882019817829132, loss=1.7109800577163696
I0217 16:51:58.400974 140247055709952 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.3817327320575714, loss=1.7200088500976562
I0217 16:52:01.950569 140416697075520 spec.py:321] Evaluating on the training split.
I0217 16:52:05.134579 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 16:56:47.330635 140416697075520 spec.py:333] Evaluating on the validation split.
I0217 16:56:50.226137 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 17:01:32.067754 140416697075520 spec.py:349] Evaluating on the test split.
I0217 17:01:34.966937 140416697075520 workload.py:181] Translating evaluation dataset.
I0217 17:06:16.726873 140416697075520 submission_runner.py:408] Time since start: 97672.08s, 	Step: 125011, 	{'train/accuracy': 0.6631479859352112, 'train/loss': 1.6372288465499878, 'train/bleu': 1.5441187034698963e-16, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 47924.92582654953, 'total_duration': 97672.07578969002, 'accumulated_submission_time': 47924.92582654953, 'accumulated_eval_time': 49741.39501070976, 'accumulated_logging_time': 1.7272353172302246}
I0217 17:06:16.747936 140247047317248 logging_writer.py:48] [125011] accumulated_eval_time=49741.395011, accumulated_logging_time=1.727235, accumulated_submission_time=47924.925827, global_step=125011, preemption_count=0, score=47924.925827, test/accuracy=0.689373, test/bleu=0.000000, test/loss=1.449911, test/num_examples=3003, total_duration=97672.075790, train/accuracy=0.663148, train/bleu=0.000000, train/loss=1.637229, validation/accuracy=0.677289, validation/bleu=0.000000, validation/loss=1.531644, validation/num_examples=3000
I0217 17:09:24.534081 140247055709952 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.3842957019805908, loss=1.668539047241211
I0217 17:10:02.977708 140247047317248 logging_writer.py:48] [125602] global_step=125602, preemption_count=0, score=48151.112932
I0217 17:10:04.496254 140416697075520 checkpoints.py:490] Saving checkpoint at step: 125602
I0217 17:10:08.677581 140416697075520 checkpoints.py:422] Saved checkpoint at /experiment_runs/variants_target_setting/study_0/wmt_glu_tanh_jax/trial_1/checkpoint_125602
I0217 17:10:08.683252 140416697075520 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/variants_target_setting/study_0/wmt_glu_tanh_jax/trial_1/checkpoint_125602.
I0217 17:10:08.744770 140416697075520 submission_runner.py:583] Tuning trial 1/1
I0217 17:10:08.744957 140416697075520 submission_runner.py:584] Hyperparameters: Hyperparameters(learning_rate=0.0002111193022461917, beta1=0.8748186204170956, beta2=0.8576876516215266, warmup_steps=9999, weight_decay=0.18033280763289028, label_smoothing=0.0)
I0217 17:10:08.747410 140416697075520 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0005723770591430366, 'train/loss': 11.380516052246094, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.400991439819336, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.396333694458008, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 41.916555404663086, 'total_duration': 915.5848548412323, 'accumulated_submission_time': 41.916555404663086, 'accumulated_eval_time': 873.6682517528534, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2193, {'train/accuracy': 0.27149611711502075, 'train/loss': 5.766429424285889, 'train/bleu': 0.014532577310484868, 'validation/accuracy': 0.25340044498443604, 'validation/loss': 5.971210956573486, 'validation/bleu': 0.009315679799413432, 'validation/num_examples': 3000, 'test/accuracy': 0.23529139161109924, 'test/loss': 6.23386812210083, 'test/bleu': 0.005846151290142995, 'test/num_examples': 3003, 'score': 882.0145854949951, 'total_duration': 2615.6713082790375, 'accumulated_submission_time': 882.0145854949951, 'accumulated_eval_time': 1733.5552592277527, 'accumulated_logging_time': 0.028280973434448242, 'global_step': 2193, 'preemption_count': 0}), (4387, {'train/accuracy': 0.38233137130737305, 'train/loss': 4.276638984680176, 'train/bleu': 0.009903980841895732, 'validation/accuracy': 0.358396053314209, 'validation/loss': 4.472900390625, 'validation/bleu': 0.008907415885516034, 'validation/num_examples': 3000, 'test/accuracy': 0.33998024463653564, 'test/loss': 4.736346244812012, 'test/bleu': 0.008253558126543463, 'test/num_examples': 3003, 'score': 1721.9357450008392, 'total_duration': 4311.958748579025, 'accumulated_submission_time': 1721.9357450008392, 'accumulated_eval_time': 2589.820998430252, 'accumulated_logging_time': 0.05459952354431152, 'global_step': 4387, 'preemption_count': 0}), (6581, {'train/accuracy': 0.49364981055259705, 'train/loss': 3.177924156188965, 'train/bleu': 0.005156220687101846, 'validation/accuracy': 0.4825730621814728, 'validation/loss': 3.292743444442749, 'validation/bleu': 0.0031226845099319267, 'validation/num_examples': 3000, 'test/accuracy': 0.4754401445388794, 'test/loss': 3.4052774906158447, 'test/bleu': 0.0015457053663014697, 'test/num_examples': 3003, 'score': 2562.0812520980835, 'total_duration': 6015.496306180954, 'accumulated_submission_time': 2562.0812520980835, 'accumulated_eval_time': 3453.1139719486237, 'accumulated_logging_time': 0.07984018325805664, 'global_step': 6581, 'preemption_count': 0}), (8775, {'train/accuracy': 0.5443810224533081, 'train/loss': 2.7035303115844727, 'train/bleu': 0.006256229117476515, 'validation/accuracy': 0.5446305871009827, 'validation/loss': 2.684821844100952, 'validation/bleu': 0.002952351193251204, 'validation/num_examples': 3000, 'test/accuracy': 0.544698178768158, 'test/loss': 2.712599992752075, 'test/bleu': 0.0008381065657549013, 'test/num_examples': 3003, 'score': 3402.230988740921, 'total_duration': 7719.837748765945, 'accumulated_submission_time': 3402.230988740921, 'accumulated_eval_time': 4317.205441474915, 'accumulated_logging_time': 0.10492157936096191, 'global_step': 8775, 'preemption_count': 0}), (10969, {'train/accuracy': 0.5733675956726074, 'train/loss': 2.416719675064087, 'train/bleu': 0.0031168829488226546, 'validation/accuracy': 0.5787404775619507, 'validation/loss': 2.352992057800293, 'validation/bleu': 0.0007022588163779613, 'validation/num_examples': 3000, 'test/accuracy': 0.5823950171470642, 'test/loss': 2.3416526317596436, 'test/bleu': 0.0007694039752120449, 'test/num_examples': 3003, 'score': 4242.451108455658, 'total_duration': 9419.544093370438, 'accumulated_submission_time': 4242.451108455658, 'accumulated_eval_time': 5176.590566635132, 'accumulated_logging_time': 0.13073062896728516, 'global_step': 10969, 'preemption_count': 0}), (13162, {'train/accuracy': 0.5986971259117126, 'train/loss': 2.189364433288574, 'train/bleu': 0.00310057377157679, 'validation/accuracy': 0.6016912460327148, 'validation/loss': 2.156778335571289, 'validation/bleu': 0.0017017048911783994, 'validation/num_examples': 3000, 'test/accuracy': 0.605856716632843, 'test/loss': 2.1299784183502197, 'test/bleu': 0.0009892606605730098, 'test/num_examples': 3003, 'score': 5082.376412630081, 'total_duration': 11120.485644102097, 'accumulated_submission_time': 5082.376412630081, 'accumulated_eval_time': 6037.509339094162, 'accumulated_logging_time': 0.15807676315307617, 'global_step': 13162, 'preemption_count': 0}), (15356, {'train/accuracy': 0.6066553592681885, 'train/loss': 2.1151750087738037, 'train/bleu': 0.008914114737132578, 'validation/accuracy': 0.6143879294395447, 'validation/loss': 2.042186975479126, 'validation/bleu': 0.0016059347693763726, 'validation/num_examples': 3000, 'test/accuracy': 0.6196037530899048, 'test/loss': 1.9998146295547485, 'test/bleu': 0.00426576729714753, 'test/num_examples': 3003, 'score': 5922.58389544487, 'total_duration': 12818.793059825897, 'accumulated_submission_time': 5922.58389544487, 'accumulated_eval_time': 6895.514606714249, 'accumulated_logging_time': 0.18300485610961914, 'global_step': 15356, 'preemption_count': 0}), (17549, {'train/accuracy': 0.610589861869812, 'train/loss': 2.0721302032470703, 'train/bleu': 0.02105470065041811, 'validation/accuracy': 0.6241087913513184, 'validation/loss': 1.9620773792266846, 'validation/bleu': 0.019104750406913857, 'validation/num_examples': 3000, 'test/accuracy': 0.6300737857818604, 'test/loss': 1.9128855466842651, 'test/bleu': 0.005294413107827698, 'test/num_examples': 3003, 'score': 6762.530175209045, 'total_duration': 14515.720617055893, 'accumulated_submission_time': 6762.530175209045, 'accumulated_eval_time': 7752.399913311005, 'accumulated_logging_time': 0.2091994285583496, 'global_step': 17549, 'preemption_count': 0}), (19742, {'train/accuracy': 0.6215307712554932, 'train/loss': 1.979943871498108, 'train/bleu': 0.010806342326823197, 'validation/accuracy': 0.6300727725028992, 'validation/loss': 1.905492901802063, 'validation/bleu': 0.007966807985084534, 'validation/num_examples': 3000, 'test/accuracy': 0.6384637951850891, 'test/loss': 1.8455276489257812, 'test/bleu': 0.002181394648611847, 'test/num_examples': 3003, 'score': 7602.453125476837, 'total_duration': 16213.435671329498, 'accumulated_submission_time': 7602.453125476837, 'accumulated_eval_time': 8610.016618728638, 'accumulated_logging_time': 0.3149287700653076, 'global_step': 19742, 'preemption_count': 0}), (21935, {'train/accuracy': 0.6242388486862183, 'train/loss': 1.9554890394210815, 'train/bleu': 0.0061010295652887585, 'validation/accuracy': 0.6365947127342224, 'validation/loss': 1.8519504070281982, 'validation/bleu': 0.004946113062229061, 'validation/num_examples': 3000, 'test/accuracy': 0.6442972421646118, 'test/loss': 1.7925695180892944, 'test/bleu': 0.003300320959813985, 'test/num_examples': 3003, 'score': 8442.490117073059, 'total_duration': 17909.733082532883, 'accumulated_submission_time': 8442.490117073059, 'accumulated_eval_time': 9466.181564569473, 'accumulated_logging_time': 0.34085750579833984, 'global_step': 21935, 'preemption_count': 0}), (24128, {'train/accuracy': 0.6233453154563904, 'train/loss': 1.9470256567001343, 'train/bleu': 0.00044969548344227846, 'validation/accuracy': 0.6403268575668335, 'validation/loss': 1.8160721063613892, 'validation/bleu': 0.0014958437466859455, 'validation/num_examples': 3000, 'test/accuracy': 0.648399293422699, 'test/loss': 1.7543072700500488, 'test/bleu': 0.0005283949438422056, 'test/num_examples': 3003, 'score': 9282.439644575119, 'total_duration': 19605.789501428604, 'accumulated_submission_time': 9282.439644575119, 'accumulated_eval_time': 10322.19015789032, 'accumulated_logging_time': 0.36908841133117676, 'global_step': 24128, 'preemption_count': 0}), (26321, {'train/accuracy': 0.632300078868866, 'train/loss': 1.8779854774475098, 'train/bleu': 0.0037231667337293516, 'validation/accuracy': 0.64434415102005, 'validation/loss': 1.7871403694152832, 'validation/bleu': 0.0008904067767186454, 'validation/num_examples': 3000, 'test/accuracy': 0.6516762971878052, 'test/loss': 1.7249469757080078, 'test/bleu': 0.0010723612396463004, 'test/num_examples': 3003, 'score': 10122.452534914017, 'total_duration': 21304.88884949684, 'accumulated_submission_time': 10122.452534914017, 'accumulated_eval_time': 11181.18128657341, 'accumulated_logging_time': 0.3946828842163086, 'global_step': 26321, 'preemption_count': 0}), (28514, {'train/accuracy': 0.6297993659973145, 'train/loss': 1.8970668315887451, 'train/bleu': 0.0033591441688744826, 'validation/accuracy': 0.6466379761695862, 'validation/loss': 1.762953758239746, 'validation/bleu': 0.0021784559056038646, 'validation/num_examples': 3000, 'test/accuracy': 0.6564987897872925, 'test/loss': 1.6947009563446045, 'test/bleu': 0.0023120369508580012, 'test/num_examples': 3003, 'score': 10962.647480487823, 'total_duration': 23004.244362831116, 'accumulated_submission_time': 10962.647480487823, 'accumulated_eval_time': 12040.245519399643, 'accumulated_logging_time': 0.42037463188171387, 'global_step': 28514, 'preemption_count': 0}), (30708, {'train/accuracy': 0.634867250919342, 'train/loss': 1.8564034700393677, 'train/bleu': 0.01048386916559227, 'validation/accuracy': 0.6502833366394043, 'validation/loss': 1.733353614807129, 'validation/bleu': 0.00461938209771408, 'validation/num_examples': 3000, 'test/accuracy': 0.6586369276046753, 'test/loss': 1.6724879741668701, 'test/bleu': 0.0025960866073588963, 'test/num_examples': 3003, 'score': 11802.79470205307, 'total_duration': 24706.45909667015, 'accumulated_submission_time': 11802.79470205307, 'accumulated_eval_time': 12902.215717554092, 'accumulated_logging_time': 0.4477818012237549, 'global_step': 30708, 'preemption_count': 0}), (32901, {'train/accuracy': 0.6375442743301392, 'train/loss': 1.8300745487213135, 'train/bleu': 0.01311878610116507, 'validation/accuracy': 0.6522051692008972, 'validation/loss': 1.7190943956375122, 'validation/bleu': 0.004717050921352025, 'validation/num_examples': 3000, 'test/accuracy': 0.6612515449523926, 'test/loss': 1.6526808738708496, 'test/bleu': 0.0028829720099501567, 'test/num_examples': 3003, 'score': 12642.70935177803, 'total_duration': 26404.136111021042, 'accumulated_submission_time': 12642.70935177803, 'accumulated_eval_time': 13759.882985591888, 'accumulated_logging_time': 0.47329092025756836, 'global_step': 32901, 'preemption_count': 0}), (35094, {'train/accuracy': 0.6390344500541687, 'train/loss': 1.83194899559021, 'train/bleu': 0.006193797489718212, 'validation/accuracy': 0.6540898084640503, 'validation/loss': 1.7004808187484741, 'validation/bleu': 0.00409028039754382, 'validation/num_examples': 3000, 'test/accuracy': 0.6639126539230347, 'test/loss': 1.6305878162384033, 'test/bleu': 0.0027078365974043134, 'test/num_examples': 3003, 'score': 13482.934636831284, 'total_duration': 28105.066716194153, 'accumulated_submission_time': 13482.934636831284, 'accumulated_eval_time': 14620.491453409195, 'accumulated_logging_time': 0.4997875690460205, 'global_step': 35094, 'preemption_count': 0}), (37287, {'train/accuracy': 0.6384992599487305, 'train/loss': 1.818699836730957, 'train/bleu': 0.008049935093720246, 'validation/accuracy': 0.656098484992981, 'validation/loss': 1.686850905418396, 'validation/bleu': 0.0063396983352546794, 'validation/num_examples': 3000, 'test/accuracy': 0.6654348969459534, 'test/loss': 1.613417148590088, 'test/bleu': 0.0023529110368085593, 'test/num_examples': 3003, 'score': 14322.877689361572, 'total_duration': 29805.768976449966, 'accumulated_submission_time': 14322.877689361572, 'accumulated_eval_time': 15481.151032209396, 'accumulated_logging_time': 0.5286056995391846, 'global_step': 37287, 'preemption_count': 0}), (39480, {'train/accuracy': 0.6432995200157166, 'train/loss': 1.7872945070266724, 'train/bleu': 0.008900397775850605, 'validation/accuracy': 0.6590867042541504, 'validation/loss': 1.6713515520095825, 'validation/bleu': 0.006769154660290232, 'validation/num_examples': 3000, 'test/accuracy': 0.6688048243522644, 'test/loss': 1.6009329557418823, 'test/bleu': 0.003654078079764917, 'test/num_examples': 3003, 'score': 15162.900104045868, 'total_duration': 31507.29537653923, 'accumulated_submission_time': 15162.900104045868, 'accumulated_eval_time': 16342.55731344223, 'accumulated_logging_time': 0.5559475421905518, 'global_step': 39480, 'preemption_count': 0}), (41674, {'train/accuracy': 0.6402793526649475, 'train/loss': 1.8023650646209717, 'train/bleu': 0.018014787017851364, 'validation/accuracy': 0.6588882803916931, 'validation/loss': 1.6617416143417358, 'validation/bleu': 0.013407421763180685, 'validation/num_examples': 3000, 'test/accuracy': 0.6703155040740967, 'test/loss': 1.5892505645751953, 'test/bleu': 0.0038609307749633793, 'test/num_examples': 3003, 'score': 16003.174306154251, 'total_duration': 33206.75154232979, 'accumulated_submission_time': 16003.174306154251, 'accumulated_eval_time': 17201.640429973602, 'accumulated_logging_time': 0.5845160484313965, 'global_step': 41674, 'preemption_count': 0}), (43867, {'train/accuracy': 0.6594968438148499, 'train/loss': 1.6783331632614136, 'train/bleu': 0.012818265713863708, 'validation/accuracy': 0.6599546074867249, 'validation/loss': 1.6518120765686035, 'validation/bleu': 0.002466626391783668, 'validation/num_examples': 3000, 'test/accuracy': 0.6709197759628296, 'test/loss': 1.5779694318771362, 'test/bleu': 0.00790887381916585, 'test/num_examples': 3003, 'score': 16843.107570886612, 'total_duration': 34903.94935750961, 'accumulated_submission_time': 16843.107570886612, 'accumulated_eval_time': 18058.804636716843, 'accumulated_logging_time': 0.6142547130584717, 'global_step': 43867, 'preemption_count': 0}), (46060, {'train/accuracy': 0.6494045853614807, 'train/loss': 1.734161376953125, 'train/bleu': 0.0029831771182819147, 'validation/accuracy': 0.6632031798362732, 'validation/loss': 1.6402801275253296, 'validation/bleu': 0.001929465412214975, 'validation/num_examples': 3000, 'test/accuracy': 0.673267126083374, 'test/loss': 1.5677440166473389, 'test/bleu': 0.0011721331427797547, 'test/num_examples': 3003, 'score': 17683.02337527275, 'total_duration': 36599.964326143265, 'accumulated_submission_time': 17683.02337527275, 'accumulated_eval_time': 18914.806124925613, 'accumulated_logging_time': 0.6422410011291504, 'global_step': 46060, 'preemption_count': 0}), (48253, {'train/accuracy': 0.6464726328849792, 'train/loss': 1.7595447301864624, 'train/bleu': 0.0030684778900967095, 'validation/accuracy': 0.6628063917160034, 'validation/loss': 1.6305805444717407, 'validation/bleu': 0.002109776036021225, 'validation/num_examples': 3000, 'test/accuracy': 0.6747080683708191, 'test/loss': 1.5579394102096558, 'test/bleu': 0.0034029668119348014, 'test/num_examples': 3003, 'score': 18523.008571386337, 'total_duration': 38296.72925019264, 'accumulated_submission_time': 18523.008571386337, 'accumulated_eval_time': 19771.487041950226, 'accumulated_logging_time': 0.6708183288574219, 'global_step': 48253, 'preemption_count': 0}), (50446, {'train/accuracy': 0.6579204797744751, 'train/loss': 1.681386947631836, 'train/bleu': 4.726848962627669e-11, 'validation/accuracy': 0.6643686890602112, 'validation/loss': 1.6211494207382202, 'validation/bleu': 4.353011344483678e-07, 'validation/num_examples': 3000, 'test/accuracy': 0.6763697862625122, 'test/loss': 1.5470092296600342, 'test/bleu': 3.832120235842256e-09, 'test/num_examples': 3003, 'score': 19363.02797317505, 'total_duration': 39992.49269723892, 'accumulated_submission_time': 19363.02797317505, 'accumulated_eval_time': 20627.133256673813, 'accumulated_logging_time': 0.697906494140625, 'global_step': 50446, 'preemption_count': 0}), (52639, {'train/accuracy': 0.6519097685813904, 'train/loss': 1.7235392332077026, 'train/bleu': 0.011317894781530157, 'validation/accuracy': 0.6658565998077393, 'validation/loss': 1.6115913391113281, 'validation/bleu': 0.00246963718736365, 'validation/num_examples': 3000, 'test/accuracy': 0.6758236289024353, 'test/loss': 1.5376360416412354, 'test/bleu': 0.0018983356855068302, 'test/num_examples': 3003, 'score': 20202.959358930588, 'total_duration': 41689.566000938416, 'accumulated_submission_time': 20202.959358930588, 'accumulated_eval_time': 21484.176567316055, 'accumulated_logging_time': 0.7261979579925537, 'global_step': 52639, 'preemption_count': 0}), (54833, {'train/accuracy': 0.6488131880760193, 'train/loss': 1.7384616136550903, 'train/bleu': 0.00015914353197347627, 'validation/accuracy': 0.6657201647758484, 'validation/loss': 1.6080960035324097, 'validation/bleu': 0.00015626255059858376, 'validation/num_examples': 3000, 'test/accuracy': 0.6779385209083557, 'test/loss': 1.5288050174713135, 'test/bleu': 7.63189865179375e-05, 'test/num_examples': 3003, 'score': 21043.03815627098, 'total_duration': 43386.479078769684, 'accumulated_submission_time': 21043.03815627098, 'accumulated_eval_time': 22340.912219285965, 'accumulated_logging_time': 0.7540411949157715, 'global_step': 54833, 'preemption_count': 0}), (57027, {'train/accuracy': 0.6556254029273987, 'train/loss': 1.6882045269012451, 'train/bleu': 2.3922584155053653e-07, 'validation/accuracy': 0.6680760383605957, 'validation/loss': 1.6013891696929932, 'validation/bleu': 3.257229736809888e-05, 'validation/num_examples': 3000, 'test/accuracy': 0.6789262890815735, 'test/loss': 1.520471453666687, 'test/bleu': 3.4626696722555804e-07, 'test/num_examples': 3003, 'score': 21883.23281097412, 'total_duration': 45083.72175168991, 'accumulated_submission_time': 21883.23281097412, 'accumulated_eval_time': 23197.861188173294, 'accumulated_logging_time': 0.7827584743499756, 'global_step': 57027, 'preemption_count': 0}), (59220, {'train/accuracy': 0.6575570702552795, 'train/loss': 1.6820569038391113, 'train/bleu': 0.00037022543641225867, 'validation/accuracy': 0.6692291498184204, 'validation/loss': 1.5894312858581543, 'validation/bleu': 0.0005184220410265468, 'validation/num_examples': 3000, 'test/accuracy': 0.6805298924446106, 'test/loss': 1.5156587362289429, 'test/bleu': 0.0007760878452398538, 'test/num_examples': 3003, 'score': 22723.22510910034, 'total_duration': 46781.328458070755, 'accumulated_submission_time': 22723.22510910034, 'accumulated_eval_time': 24055.376248836517, 'accumulated_logging_time': 0.8115298748016357, 'global_step': 59220, 'preemption_count': 0}), (61413, {'train/accuracy': 0.6530603170394897, 'train/loss': 1.7072131633758545, 'train/bleu': 1.9221522374183195e-73, 'validation/accuracy': 0.6698119044303894, 'validation/loss': 1.586490273475647, 'validation/bleu': 5.0905459771434816e-18, 'validation/num_examples': 3000, 'test/accuracy': 0.6805298924446106, 'test/loss': 1.5095101594924927, 'test/bleu': 2.6753783328205294e-34, 'test/num_examples': 3003, 'score': 23563.144961118698, 'total_duration': 48477.938062906265, 'accumulated_submission_time': 23563.144961118698, 'accumulated_eval_time': 24911.96792125702, 'accumulated_logging_time': 0.8393943309783936, 'global_step': 61413, 'preemption_count': 0}), (63607, {'train/accuracy': 0.6595078110694885, 'train/loss': 1.6708544492721558, 'train/bleu': 3.993306699776565e-09, 'validation/accuracy': 0.670332670211792, 'validation/loss': 1.5785328149795532, 'validation/bleu': 1.0997585442434252e-05, 'validation/num_examples': 3000, 'test/accuracy': 0.682993471622467, 'test/loss': 1.4995191097259521, 'test/bleu': 6.173476556787166e-16, 'test/num_examples': 3003, 'score': 24403.35770726204, 'total_duration': 50174.56140899658, 'accumulated_submission_time': 24403.35770726204, 'accumulated_eval_time': 25768.277834892273, 'accumulated_logging_time': 0.8691816329956055, 'global_step': 63607, 'preemption_count': 0}), (65801, {'train/accuracy': 0.6581171154975891, 'train/loss': 1.6827095746994019, 'train/bleu': 1.3145112412286955e-13, 'validation/accuracy': 0.6711013913154602, 'validation/loss': 1.570637583732605, 'validation/bleu': 1.1356152574398593e-08, 'validation/num_examples': 3000, 'test/accuracy': 0.6834350228309631, 'test/loss': 1.493088722229004, 'test/bleu': 1.0301622792678375e-13, 'test/num_examples': 3003, 'score': 25243.613652467728, 'total_duration': 51870.5441570282, 'accumulated_submission_time': 25243.613652467728, 'accumulated_eval_time': 26623.905986070633, 'accumulated_logging_time': 0.8976736068725586, 'global_step': 65801, 'preemption_count': 0}), (67994, {'train/accuracy': 0.65663081407547, 'train/loss': 1.6920562982559204, 'train/bleu': 5.7880635545361155e-75, 'validation/accuracy': 0.6715725660324097, 'validation/loss': 1.5655851364135742, 'validation/bleu': 3.4923757610281014e-110, 'validation/num_examples': 3000, 'test/accuracy': 0.6832374930381775, 'test/loss': 1.4881362915039062, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 26083.677993774414, 'total_duration': 53569.45456409454, 'accumulated_submission_time': 26083.677993774414, 'accumulated_eval_time': 27482.650806188583, 'accumulated_logging_time': 0.9275956153869629, 'global_step': 67994, 'preemption_count': 0}), (70187, {'train/accuracy': 0.6597678661346436, 'train/loss': 1.6584603786468506, 'train/bleu': 3.172754784343179e-33, 'validation/accuracy': 0.6727628707885742, 'validation/loss': 1.562662959098816, 'validation/bleu': 2.5994512896945813e-15, 'validation/num_examples': 3000, 'test/accuracy': 0.6852827072143555, 'test/loss': 1.4839190244674683, 'test/bleu': 4.750764566167227e-68, 'test/num_examples': 3003, 'score': 26923.62469100952, 'total_duration': 55266.38389849663, 'accumulated_submission_time': 26923.62469100952, 'accumulated_eval_time': 28339.534108638763, 'accumulated_logging_time': 0.956641435623169, 'global_step': 70187, 'preemption_count': 0}), (72380, {'train/accuracy': 0.6577961444854736, 'train/loss': 1.6833745241165161, 'train/bleu': 2.9953901460861085e-59, 'validation/accuracy': 0.6741639971733093, 'validation/loss': 1.5556203126907349, 'validation/bleu': 1.374083968779809e-28, 'validation/num_examples': 3000, 'test/accuracy': 0.6867468357086182, 'test/loss': 1.4756821393966675, 'test/bleu': 1.2972737967095044e-27, 'test/num_examples': 3003, 'score': 27763.672098875046, 'total_duration': 56962.155440330505, 'accumulated_submission_time': 27763.672098875046, 'accumulated_eval_time': 29195.159410238266, 'accumulated_logging_time': 0.9846923351287842, 'global_step': 72380, 'preemption_count': 0}), (74573, {'train/accuracy': 0.6599732637405396, 'train/loss': 1.6659163236618042, 'train/bleu': 3.433113828622719e-35, 'validation/accuracy': 0.6741020083427429, 'validation/loss': 1.5530149936676025, 'validation/bleu': 8.257218124896524e-21, 'validation/num_examples': 3000, 'test/accuracy': 0.6866306662559509, 'test/loss': 1.471996545791626, 'test/bleu': 2.1755551091241117e-28, 'test/num_examples': 3003, 'score': 28603.582304239273, 'total_duration': 58659.028136730194, 'accumulated_submission_time': 28603.582304239273, 'accumulated_eval_time': 30052.017840385437, 'accumulated_logging_time': 1.0185394287109375, 'global_step': 74573, 'preemption_count': 0}), (76766, {'train/accuracy': 0.6621755957603455, 'train/loss': 1.6525546312332153, 'train/bleu': 7.149810695207492e-14, 'validation/accuracy': 0.6751435399055481, 'validation/loss': 1.5478153228759766, 'validation/bleu': 5.920515743949042e-10, 'validation/num_examples': 3000, 'test/accuracy': 0.686851441860199, 'test/loss': 1.4702234268188477, 'test/bleu': 7.566292641313918e-14, 'test/num_examples': 3003, 'score': 29443.534519672394, 'total_duration': 60355.585752010345, 'accumulated_submission_time': 29443.534519672394, 'accumulated_eval_time': 30908.522542238235, 'accumulated_logging_time': 1.0486669540405273, 'global_step': 76766, 'preemption_count': 0}), (78959, {'train/accuracy': 0.6613059639930725, 'train/loss': 1.6557270288467407, 'train/bleu': 7.88032955656721e-18, 'validation/accuracy': 0.6753666996955872, 'validation/loss': 1.543614149093628, 'validation/bleu': 5.310967810711563e-16, 'validation/num_examples': 3000, 'test/accuracy': 0.6877810955047607, 'test/loss': 1.4642019271850586, 'test/bleu': 6.51375530429964e-28, 'test/num_examples': 3003, 'score': 30283.443378448486, 'total_duration': 62052.761239528656, 'accumulated_submission_time': 30283.443378448486, 'accumulated_eval_time': 31765.690917015076, 'accumulated_logging_time': 1.0765984058380127, 'global_step': 78959, 'preemption_count': 0}), (81152, {'train/accuracy': 0.6602025628089905, 'train/loss': 1.6547356843948364, 'train/bleu': 3.082688338920834e-10, 'validation/accuracy': 0.6759246587753296, 'validation/loss': 1.5399972200393677, 'validation/bleu': 3.585892732838403e-12, 'validation/num_examples': 3000, 'test/accuracy': 0.6881877779960632, 'test/loss': 1.460843801498413, 'test/bleu': 2.2006409592584095e-23, 'test/num_examples': 3003, 'score': 31123.47504043579, 'total_duration': 63748.25242686272, 'accumulated_submission_time': 31123.47504043579, 'accumulated_eval_time': 32621.049352645874, 'accumulated_logging_time': 1.107264757156372, 'global_step': 81152, 'preemption_count': 0}), (83345, {'train/accuracy': 0.665103018283844, 'train/loss': 1.6287139654159546, 'train/bleu': 7.842692905719594e-25, 'validation/accuracy': 0.6762842535972595, 'validation/loss': 1.5398870706558228, 'validation/bleu': 2.2081536343624706e-14, 'validation/num_examples': 3000, 'test/accuracy': 0.6889082789421082, 'test/loss': 1.4589356184005737, 'test/bleu': 1.0038464729692575e-116, 'test/num_examples': 3003, 'score': 31963.449004411697, 'total_duration': 65443.651799440384, 'accumulated_submission_time': 31963.449004411697, 'accumulated_eval_time': 33476.37496852875, 'accumulated_logging_time': 1.136979579925537, 'global_step': 83345, 'preemption_count': 0}), (85538, {'train/accuracy': 0.6634867787361145, 'train/loss': 1.6380850076675415, 'train/bleu': 7.299821481975685e-22, 'validation/accuracy': 0.675986647605896, 'validation/loss': 1.537553310394287, 'validation/bleu': 1.996271621908041e-16, 'validation/num_examples': 3000, 'test/accuracy': 0.6893614530563354, 'test/loss': 1.455039381980896, 'test/bleu': 5.389467207958059e-49, 'test/num_examples': 3003, 'score': 32803.49252653122, 'total_duration': 67139.63595438004, 'accumulated_submission_time': 32803.49252653122, 'accumulated_eval_time': 34332.217041015625, 'accumulated_logging_time': 1.1654882431030273, 'global_step': 85538, 'preemption_count': 0}), (87731, {'train/accuracy': 0.666720449924469, 'train/loss': 1.615303874015808, 'train/bleu': 1.0550796882758134e-35, 'validation/accuracy': 0.6763338446617126, 'validation/loss': 1.5350775718688965, 'validation/bleu': 1.3886112009180192e-15, 'validation/num_examples': 3000, 'test/accuracy': 0.6897681951522827, 'test/loss': 1.452293872833252, 'test/bleu': 5.0952082229375916e-65, 'test/num_examples': 3003, 'score': 33643.43664717674, 'total_duration': 68836.01491498947, 'accumulated_submission_time': 33643.43664717674, 'accumulated_eval_time': 35188.552652835846, 'accumulated_logging_time': 1.1946303844451904, 'global_step': 87731, 'preemption_count': 0}), (89925, {'train/accuracy': 0.6668577790260315, 'train/loss': 1.6161235570907593, 'train/bleu': 5.753700356710845e-16, 'validation/accuracy': 0.677065372467041, 'validation/loss': 1.5341765880584717, 'validation/bleu': 2.1646827340380213e-12, 'validation/num_examples': 3000, 'test/accuracy': 0.6897100806236267, 'test/loss': 1.452409029006958, 'test/bleu': 1.1159244511417395e-64, 'test/num_examples': 3003, 'score': 34483.717282533646, 'total_duration': 70531.32159113884, 'accumulated_submission_time': 34483.717282533646, 'accumulated_eval_time': 36043.47945904732, 'accumulated_logging_time': 1.2234370708465576, 'global_step': 89925, 'preemption_count': 0}), (92118, {'train/accuracy': 0.6644206643104553, 'train/loss': 1.6344032287597656, 'train/bleu': 1.8677004315358872e-15, 'validation/accuracy': 0.6770901679992676, 'validation/loss': 1.5322757959365845, 'validation/bleu': 1.041932489766618e-08, 'validation/num_examples': 3000, 'test/accuracy': 0.6894544363021851, 'test/loss': 1.4509806632995605, 'test/bleu': 6.525035483503889e-24, 'test/num_examples': 3003, 'score': 35323.6275074482, 'total_duration': 72227.20843577385, 'accumulated_submission_time': 35323.6275074482, 'accumulated_eval_time': 36899.35737490654, 'accumulated_logging_time': 1.2518982887268066, 'global_step': 92118, 'preemption_count': 0}), (94311, {'train/accuracy': 0.6639440059661865, 'train/loss': 1.6363450288772583, 'train/bleu': 4.729026642061335e-16, 'validation/accuracy': 0.6773505806922913, 'validation/loss': 1.5321143865585327, 'validation/bleu': 4.4303980933181974e-14, 'validation/num_examples': 3000, 'test/accuracy': 0.6895125508308411, 'test/loss': 1.4501503705978394, 'test/bleu': 4.159748339163509e-52, 'test/num_examples': 3003, 'score': 36163.66510796547, 'total_duration': 73923.13512277603, 'accumulated_submission_time': 36163.66510796547, 'accumulated_eval_time': 37755.14686727524, 'accumulated_logging_time': 1.2815837860107422, 'global_step': 94311, 'preemption_count': 0}), (96504, {'train/accuracy': 0.6668611764907837, 'train/loss': 1.6193124055862427, 'train/bleu': 6.958976294101557e-21, 'validation/accuracy': 0.6774745583534241, 'validation/loss': 1.5317516326904297, 'validation/bleu': 7.129035705814547e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6895706653594971, 'test/loss': 1.4500772953033447, 'test/bleu': 2.3831647633285975e-43, 'test/num_examples': 3003, 'score': 37003.584347724915, 'total_duration': 75619.29023313522, 'accumulated_submission_time': 37003.584347724915, 'accumulated_eval_time': 38611.28120183945, 'accumulated_logging_time': 1.312791347503662, 'global_step': 96504, 'preemption_count': 0}), (98698, {'train/accuracy': 0.6659024953842163, 'train/loss': 1.6249374151229858, 'train/bleu': 8.48075740095282e-18, 'validation/accuracy': 0.6773257255554199, 'validation/loss': 1.5316590070724487, 'validation/bleu': 1.9773533311381334e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6894544363021851, 'test/loss': 1.4499237537384033, 'test/bleu': 6.800218506898445e-53, 'test/num_examples': 3003, 'score': 37843.84718418121, 'total_duration': 77316.86589956284, 'accumulated_submission_time': 37843.84718418121, 'accumulated_eval_time': 39468.493797540665, 'accumulated_logging_time': 1.3427517414093018, 'global_step': 98698, 'preemption_count': 0}), (100891, {'train/accuracy': 0.6657024025917053, 'train/loss': 1.6258277893066406, 'train/bleu': 1.6210096065143052e-16, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 38683.90804576874, 'total_duration': 79012.92638611794, 'accumulated_submission_time': 38683.90804576874, 'accumulated_eval_time': 40324.388748407364, 'accumulated_logging_time': 1.3765370845794678, 'global_step': 100891, 'preemption_count': 0}), (103084, {'train/accuracy': 0.6672636866569519, 'train/loss': 1.6149146556854248, 'train/bleu': 2.411098563653968e-16, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 39523.91212558746, 'total_duration': 80710.3568725586, 'accumulated_submission_time': 39523.91212558746, 'accumulated_eval_time': 41181.713782548904, 'accumulated_logging_time': 1.4078152179718018, 'global_step': 103084, 'preemption_count': 0}), (105277, {'train/accuracy': 0.6629880666732788, 'train/loss': 1.6416219472885132, 'train/bleu': 7.106031843432135e-15, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 40364.00815534592, 'total_duration': 82406.38815188408, 'accumulated_submission_time': 40364.00815534592, 'accumulated_eval_time': 42037.548650979996, 'accumulated_logging_time': 1.4379754066467285, 'global_step': 105277, 'preemption_count': 0}), (107470, {'train/accuracy': 0.6645278334617615, 'train/loss': 1.6353766918182373, 'train/bleu': 5.730438106700271e-20, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 41204.166110515594, 'total_duration': 84103.34514284134, 'accumulated_submission_time': 41204.166110515594, 'accumulated_eval_time': 42894.243529081345, 'accumulated_logging_time': 1.4703781604766846, 'global_step': 107470, 'preemption_count': 0}), (109663, {'train/accuracy': 0.6674066185951233, 'train/loss': 1.6146180629730225, 'train/bleu': 6.748841156792323e-20, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 42044.22923755646, 'total_duration': 85799.4564781189, 'accumulated_submission_time': 42044.22923755646, 'accumulated_eval_time': 43750.19023013115, 'accumulated_logging_time': 1.5016522407531738, 'global_step': 109663, 'preemption_count': 0}), (111856, {'train/accuracy': 0.6615363359451294, 'train/loss': 1.6487631797790527, 'train/bleu': 2.1646841932618231e-16, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 42884.362718105316, 'total_duration': 87495.91778969765, 'accumulated_submission_time': 42884.362718105316, 'accumulated_eval_time': 44606.41705441475, 'accumulated_logging_time': 1.5323512554168701, 'global_step': 111856, 'preemption_count': 0}), (114049, {'train/accuracy': 0.6631463170051575, 'train/loss': 1.6365150213241577, 'train/bleu': 6.612449708341315e-18, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 43724.61556696892, 'total_duration': 89194.5370836258, 'accumulated_submission_time': 43724.61556696892, 'accumulated_eval_time': 45464.681428432465, 'accumulated_logging_time': 1.5638012886047363, 'global_step': 114049, 'preemption_count': 0}), (116241, {'train/accuracy': 0.6620974540710449, 'train/loss': 1.6439599990844727, 'train/bleu': 9.304555741295878e-18, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 44564.58071374893, 'total_duration': 90890.03444361687, 'accumulated_submission_time': 44564.58071374893, 'accumulated_eval_time': 46320.11028981209, 'accumulated_logging_time': 1.5969150066375732, 'global_step': 116241, 'preemption_count': 0}), (118433, {'train/accuracy': 0.6632860898971558, 'train/loss': 1.6343814134597778, 'train/bleu': 3.4164877850608e-17, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 45404.56107854843, 'total_duration': 92585.51359510422, 'accumulated_submission_time': 45404.56107854843, 'accumulated_eval_time': 47175.50688838959, 'accumulated_logging_time': 1.6293854713439941, 'global_step': 118433, 'preemption_count': 0}), (120626, {'train/accuracy': 0.6642124652862549, 'train/loss': 1.6329617500305176, 'train/bleu': 3.900978586004138e-15, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 46244.73825931549, 'total_duration': 94281.31026983261, 'accumulated_submission_time': 46244.73825931549, 'accumulated_eval_time': 48031.023241996765, 'accumulated_logging_time': 1.661921739578247, 'global_step': 120626, 'preemption_count': 0}), (122818, {'train/accuracy': 0.6649504899978638, 'train/loss': 1.6239640712738037, 'train/bleu': 3.694180749962438e-15, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 47084.7688806057, 'total_duration': 95977.03742218018, 'accumulated_submission_time': 47084.7688806057, 'accumulated_eval_time': 48886.61877822876, 'accumulated_logging_time': 1.692708969116211, 'global_step': 122818, 'preemption_count': 0}), (125011, {'train/accuracy': 0.6631479859352112, 'train/loss': 1.6372288465499878, 'train/bleu': 1.5441187034698963e-16, 'validation/accuracy': 0.6772885322570801, 'validation/loss': 1.5316435098648071, 'validation/bleu': 2.0046544821312638e-11, 'validation/num_examples': 3000, 'test/accuracy': 0.6893730759620667, 'test/loss': 1.4499106407165527, 'test/bleu': 1.2937818430774934e-52, 'test/num_examples': 3003, 'score': 47924.92582654953, 'total_duration': 97672.07578969002, 'accumulated_submission_time': 47924.92582654953, 'accumulated_eval_time': 49741.39501070976, 'accumulated_logging_time': 1.7272353172302246, 'global_step': 125011, 'preemption_count': 0})], 'global_step': 125602}
I0217 17:10:08.747594 140416697075520 submission_runner.py:586] Timing: 48151.1129322052
I0217 17:10:08.747658 140416697075520 submission_runner.py:588] Total number of evals: 58
I0217 17:10:08.747697 140416697075520 submission_runner.py:589] ====================
I0217 17:10:08.747889 140416697075520 submission_runner.py:673] Final wmt_glu_tanh score: 48151.1129322052
