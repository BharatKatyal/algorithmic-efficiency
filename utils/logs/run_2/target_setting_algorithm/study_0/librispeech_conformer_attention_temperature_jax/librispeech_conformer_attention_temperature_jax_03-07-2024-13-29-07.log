python3 submission_runner.py --framework=jax --workload=librispeech_conformer_attention_temperature --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --rng_seed=2060265407 --max_global_steps=80000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab --tuning_ruleset=external --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_conformer_attention_temperature/tuning_search_space.json --num_tuning_trials=1 2>&1 | tee -a /logs/librispeech_conformer_attention_temperature_jax_03-07-2024-13-29-07.log
I0307 13:29:27.646625 140017254528832 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/librispeech_conformer_attention_temperature_jax.
I0307 13:29:28.647969 140017254528832 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0307 13:29:28.648593 140017254528832 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0307 13:29:28.648721 140017254528832 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0307 13:29:28.653835 140017254528832 submission_runner.py:547] Using RNG seed 2060265407
I0307 13:29:29.807612 140017254528832 submission_runner.py:556] --- Tuning run 1/1 ---
I0307 13:29:29.807807 140017254528832 submission_runner.py:561] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/librispeech_conformer_attention_temperature_jax/trial_1.
I0307 13:29:29.807991 140017254528832 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_attention_temperature_jax/trial_1/hparams.json.
I0307 13:29:29.993545 140017254528832 submission_runner.py:206] Initializing dataset.
I0307 13:29:29.993769 140017254528832 submission_runner.py:213] Initializing model.
I0307 13:29:34.801619 140017254528832 submission_runner.py:255] Initializing optimizer.
I0307 13:29:36.015531 140017254528832 submission_runner.py:262] Initializing metrics bundle.
I0307 13:29:36.015725 140017254528832 submission_runner.py:280] Initializing checkpoint and logger.
I0307 13:29:36.016851 140017254528832 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/librispeech_conformer_attention_temperature_jax/trial_1 with prefix checkpoint_
I0307 13:29:36.016984 140017254528832 submission_runner.py:300] Saving meta data to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_attention_temperature_jax/trial_1/meta_data_0.json.
I0307 13:29:36.017165 140017254528832 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0307 13:29:36.017223 140017254528832 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0307 13:29:36.302002 140017254528832 logger_utils.py:220] Unable to record git information. Continuing without it.
I0307 13:29:36.557766 140017254528832 submission_runner.py:304] Saving flags to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_attention_temperature_jax/trial_1/flags_0.json.
I0307 13:29:36.572191 140017254528832 submission_runner.py:314] Starting training loop.
I0307 13:29:36.871540 140017254528832 input_pipeline.py:20] Loading split = train-clean-100
I0307 13:29:36.909734 140017254528832 input_pipeline.py:20] Loading split = train-clean-360
I0307 13:29:37.306989 140017254528832 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0307 13:30:38.060091 139844521023232 logging_writer.py:48] [0] global_step=0, grad_norm=48.05097579956055, loss=32.137508392333984
I0307 13:30:38.100151 140017254528832 spec.py:321] Evaluating on the training split.
I0307 13:30:38.278594 140017254528832 input_pipeline.py:20] Loading split = train-clean-100
I0307 13:30:38.316589 140017254528832 input_pipeline.py:20] Loading split = train-clean-360
I0307 13:30:38.771484 140017254528832 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0307 13:31:31.943246 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 13:31:32.063023 140017254528832 input_pipeline.py:20] Loading split = dev-clean
I0307 13:31:32.067892 140017254528832 input_pipeline.py:20] Loading split = dev-other
I0307 13:32:26.539308 140017254528832 spec.py:349] Evaluating on the test split.
I0307 13:32:26.655727 140017254528832 input_pipeline.py:20] Loading split = test-clean
I0307 13:32:59.812202 140017254528832 submission_runner.py:413] Time since start: 203.24s, 	Step: 1, 	{'train/ctc_loss': Array(31.646616, dtype=float32), 'train/wer': 0.9441569664535743, 'validation/ctc_loss': Array(30.636509, dtype=float32), 'validation/wer': 0.8989737103797175, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.752209, dtype=float32), 'test/wer': 0.9018138240610972, 'test/num_examples': 2472, 'score': 61.52786350250244, 'total_duration': 203.23771166801453, 'accumulated_submission_time': 61.52786350250244, 'accumulated_eval_time': 141.70978260040283, 'accumulated_logging_time': 0}
I0307 13:32:59.840360 139835444557568 logging_writer.py:48] [1] accumulated_eval_time=141.709783, accumulated_logging_time=0, accumulated_submission_time=61.527864, global_step=1, preemption_count=0, score=61.527864, test/ctc_loss=30.752208709716797, test/num_examples=2472, test/wer=0.901814, total_duration=203.237712, train/ctc_loss=31.646615982055664, train/wer=0.944157, validation/ctc_loss=30.63650894165039, validation/num_examples=5348, validation/wer=0.898974
I0307 13:33:23.846963 139846847776512 logging_writer.py:48] [1] global_step=1, grad_norm=61.16460037231445, loss=32.52214813232422
I0307 13:33:24.741751 139846856169216 logging_writer.py:48] [2] global_step=2, grad_norm=57.32834243774414, loss=32.131465911865234
I0307 13:33:25.635790 139846847776512 logging_writer.py:48] [3] global_step=3, grad_norm=63.64292907714844, loss=32.5694694519043
I0307 13:33:26.538999 139846856169216 logging_writer.py:48] [4] global_step=4, grad_norm=62.25773620605469, loss=31.76923179626465
I0307 13:33:27.440327 139846847776512 logging_writer.py:48] [5] global_step=5, grad_norm=72.36178588867188, loss=32.08964157104492
I0307 13:33:28.340613 139846856169216 logging_writer.py:48] [6] global_step=6, grad_norm=77.48860931396484, loss=32.63473129272461
I0307 13:33:29.234827 139846847776512 logging_writer.py:48] [7] global_step=7, grad_norm=85.9795150756836, loss=31.687694549560547
I0307 13:33:30.120015 139846856169216 logging_writer.py:48] [8] global_step=8, grad_norm=94.2949447631836, loss=31.37372398376465
I0307 13:33:31.013480 139846847776512 logging_writer.py:48] [9] global_step=9, grad_norm=96.06610107421875, loss=32.35019302368164
I0307 13:33:31.904823 139846856169216 logging_writer.py:48] [10] global_step=10, grad_norm=109.91362762451172, loss=32.15981674194336
I0307 13:33:32.800092 139846847776512 logging_writer.py:48] [11] global_step=11, grad_norm=94.77661895751953, loss=30.67860984802246
I0307 13:33:33.699343 139846856169216 logging_writer.py:48] [12] global_step=12, grad_norm=105.25074005126953, loss=30.268186569213867
I0307 13:33:34.606942 139846847776512 logging_writer.py:48] [13] global_step=13, grad_norm=114.66567993164062, loss=29.772254943847656
I0307 13:33:35.514385 139846856169216 logging_writer.py:48] [14] global_step=14, grad_norm=113.80933380126953, loss=30.17627716064453
I0307 13:33:36.406826 139846847776512 logging_writer.py:48] [15] global_step=15, grad_norm=112.82484436035156, loss=28.946866989135742
I0307 13:33:37.293279 139846856169216 logging_writer.py:48] [16] global_step=16, grad_norm=112.60874938964844, loss=29.01922035217285
I0307 13:33:38.187989 139846847776512 logging_writer.py:48] [17] global_step=17, grad_norm=121.89698028564453, loss=28.94314956665039
I0307 13:33:39.093131 139846856169216 logging_writer.py:48] [18] global_step=18, grad_norm=114.20651245117188, loss=27.740398406982422
I0307 13:33:39.984581 139846847776512 logging_writer.py:48] [19] global_step=19, grad_norm=100.93749237060547, loss=27.44719123840332
I0307 13:33:40.884425 139846856169216 logging_writer.py:48] [20] global_step=20, grad_norm=99.76428985595703, loss=27.19824981689453
I0307 13:33:41.781198 139846847776512 logging_writer.py:48] [21] global_step=21, grad_norm=108.6192855834961, loss=27.186826705932617
I0307 13:33:42.690216 139846856169216 logging_writer.py:48] [22] global_step=22, grad_norm=105.76724243164062, loss=25.937088012695312
I0307 13:33:43.583244 139846847776512 logging_writer.py:48] [23] global_step=23, grad_norm=87.5290756225586, loss=25.930984497070312
I0307 13:33:44.477618 139846856169216 logging_writer.py:48] [24] global_step=24, grad_norm=104.75358581542969, loss=24.042953491210938
I0307 13:33:45.370750 139846847776512 logging_writer.py:48] [25] global_step=25, grad_norm=79.91506958007812, loss=24.38340187072754
I0307 13:33:46.259013 139846856169216 logging_writer.py:48] [26] global_step=26, grad_norm=69.4893569946289, loss=24.18906593322754
I0307 13:33:47.146415 139846847776512 logging_writer.py:48] [27] global_step=27, grad_norm=57.302921295166016, loss=25.30801010131836
I0307 13:33:48.046460 139846856169216 logging_writer.py:48] [28] global_step=28, grad_norm=62.71635055541992, loss=22.49370002746582
I0307 13:33:48.953157 139846847776512 logging_writer.py:48] [29] global_step=29, grad_norm=43.0429573059082, loss=24.763547897338867
I0307 13:33:49.856197 139846856169216 logging_writer.py:48] [30] global_step=30, grad_norm=34.013954162597656, loss=24.316373825073242
I0307 13:33:50.748239 139846847776512 logging_writer.py:48] [31] global_step=31, grad_norm=32.854270935058594, loss=23.229446411132812
I0307 13:33:51.631854 139846856169216 logging_writer.py:48] [32] global_step=32, grad_norm=34.51655197143555, loss=23.07158851623535
I0307 13:33:52.541317 139846847776512 logging_writer.py:48] [33] global_step=33, grad_norm=25.011720657348633, loss=21.82012939453125
I0307 13:33:53.444703 139846856169216 logging_writer.py:48] [34] global_step=34, grad_norm=34.176490783691406, loss=22.59376335144043
I0307 13:33:54.343846 139846847776512 logging_writer.py:48] [35] global_step=35, grad_norm=51.56919479370117, loss=23.28571319580078
I0307 13:33:55.240101 139846856169216 logging_writer.py:48] [36] global_step=36, grad_norm=46.32991409301758, loss=21.594558715820312
I0307 13:33:56.144581 139846847776512 logging_writer.py:48] [37] global_step=37, grad_norm=54.221229553222656, loss=23.30790901184082
I0307 13:33:57.045376 139846856169216 logging_writer.py:48] [38] global_step=38, grad_norm=60.719635009765625, loss=23.601625442504883
I0307 13:33:57.938450 139846847776512 logging_writer.py:48] [39] global_step=39, grad_norm=64.75535583496094, loss=23.31270408630371
I0307 13:33:58.827430 139846856169216 logging_writer.py:48] [40] global_step=40, grad_norm=58.20951461791992, loss=22.415678024291992
I0307 13:33:59.721100 139846847776512 logging_writer.py:48] [41] global_step=41, grad_norm=66.431884765625, loss=23.07826042175293
I0307 13:34:00.623340 139846856169216 logging_writer.py:48] [42] global_step=42, grad_norm=73.77591705322266, loss=23.798707962036133
I0307 13:34:01.522455 139846847776512 logging_writer.py:48] [43] global_step=43, grad_norm=61.377464294433594, loss=22.145429611206055
I0307 13:34:02.431162 139846856169216 logging_writer.py:48] [44] global_step=44, grad_norm=68.23892211914062, loss=22.56920623779297
I0307 13:34:03.343637 139846847776512 logging_writer.py:48] [45] global_step=45, grad_norm=51.13955307006836, loss=21.662399291992188
I0307 13:34:04.250928 139846856169216 logging_writer.py:48] [46] global_step=46, grad_norm=70.78575897216797, loss=23.566171646118164
I0307 13:34:05.141366 139846847776512 logging_writer.py:48] [47] global_step=47, grad_norm=64.41971588134766, loss=22.622554779052734
I0307 13:34:06.034229 139846856169216 logging_writer.py:48] [48] global_step=48, grad_norm=61.216548919677734, loss=22.82856559753418
I0307 13:34:06.926506 139846847776512 logging_writer.py:48] [49] global_step=49, grad_norm=58.964080810546875, loss=22.04323387145996
I0307 13:34:07.816614 139846856169216 logging_writer.py:48] [50] global_step=50, grad_norm=36.68626403808594, loss=20.83504295349121
I0307 13:34:08.716916 139846847776512 logging_writer.py:48] [51] global_step=51, grad_norm=45.97040557861328, loss=21.436037063598633
I0307 13:34:09.618704 139846856169216 logging_writer.py:48] [52] global_step=52, grad_norm=54.23230743408203, loss=22.74518585205078
I0307 13:34:10.522808 139846847776512 logging_writer.py:48] [53] global_step=53, grad_norm=18.824052810668945, loss=19.210002899169922
I0307 13:34:11.430984 139846856169216 logging_writer.py:48] [54] global_step=54, grad_norm=22.769533157348633, loss=20.434572219848633
I0307 13:34:12.336329 139846847776512 logging_writer.py:48] [55] global_step=55, grad_norm=26.354265213012695, loss=21.528621673583984
I0307 13:34:13.229191 139846856169216 logging_writer.py:48] [56] global_step=56, grad_norm=14.61715316772461, loss=19.792579650878906
I0307 13:34:14.122326 139846847776512 logging_writer.py:48] [57] global_step=57, grad_norm=20.864795684814453, loss=21.472070693969727
I0307 13:34:15.012835 139846856169216 logging_writer.py:48] [58] global_step=58, grad_norm=16.39817237854004, loss=21.298280715942383
I0307 13:34:15.913151 139846847776512 logging_writer.py:48] [59] global_step=59, grad_norm=22.618616104125977, loss=19.683494567871094
I0307 13:34:16.823709 139846856169216 logging_writer.py:48] [60] global_step=60, grad_norm=26.909591674804688, loss=20.262235641479492
I0307 13:34:17.728598 139846847776512 logging_writer.py:48] [61] global_step=61, grad_norm=35.66477584838867, loss=20.84801483154297
I0307 13:34:18.633550 139846856169216 logging_writer.py:48] [62] global_step=62, grad_norm=38.35943603515625, loss=20.17183494567871
I0307 13:34:19.539634 139846847776512 logging_writer.py:48] [63] global_step=63, grad_norm=41.03558349609375, loss=21.006914138793945
I0307 13:34:20.431078 139846856169216 logging_writer.py:48] [64] global_step=64, grad_norm=42.50555419921875, loss=20.760786056518555
I0307 13:34:21.325943 139846847776512 logging_writer.py:48] [65] global_step=65, grad_norm=36.99201202392578, loss=21.38990592956543
I0307 13:34:22.208454 139846856169216 logging_writer.py:48] [66] global_step=66, grad_norm=39.482666015625, loss=20.637046813964844
I0307 13:34:23.100743 139846847776512 logging_writer.py:48] [67] global_step=67, grad_norm=40.046634674072266, loss=20.123367309570312
I0307 13:34:24.007874 139846856169216 logging_writer.py:48] [68] global_step=68, grad_norm=36.186912536621094, loss=21.27483367919922
I0307 13:34:24.915433 139846847776512 logging_writer.py:48] [69] global_step=69, grad_norm=32.79106140136719, loss=21.350135803222656
I0307 13:34:25.819057 139846856169216 logging_writer.py:48] [70] global_step=70, grad_norm=29.404733657836914, loss=22.05328941345215
I0307 13:34:26.720318 139846847776512 logging_writer.py:48] [71] global_step=71, grad_norm=30.126285552978516, loss=21.30245590209961
I0307 13:34:27.613712 139846856169216 logging_writer.py:48] [72] global_step=72, grad_norm=39.66709518432617, loss=19.901473999023438
I0307 13:34:28.506527 139846847776512 logging_writer.py:48] [73] global_step=73, grad_norm=39.4452018737793, loss=21.00924301147461
I0307 13:34:29.410317 139846856169216 logging_writer.py:48] [74] global_step=74, grad_norm=33.31745910644531, loss=20.826271057128906
I0307 13:34:30.306066 139846847776512 logging_writer.py:48] [75] global_step=75, grad_norm=40.81230163574219, loss=21.19617462158203
I0307 13:34:31.200444 139846856169216 logging_writer.py:48] [76] global_step=76, grad_norm=41.37428283691406, loss=19.70464324951172
I0307 13:34:32.108921 139846847776512 logging_writer.py:48] [77] global_step=77, grad_norm=42.06355285644531, loss=19.15786361694336
I0307 13:34:33.008430 139846856169216 logging_writer.py:48] [78] global_step=78, grad_norm=42.96949005126953, loss=19.415668487548828
I0307 13:34:33.907003 139846847776512 logging_writer.py:48] [79] global_step=79, grad_norm=32.03895568847656, loss=20.868560791015625
I0307 13:34:34.791196 139846856169216 logging_writer.py:48] [80] global_step=80, grad_norm=32.92867660522461, loss=19.066877365112305
I0307 13:34:35.683123 139846847776512 logging_writer.py:48] [81] global_step=81, grad_norm=33.566829681396484, loss=18.612224578857422
I0307 13:34:36.577563 139846856169216 logging_writer.py:48] [82] global_step=82, grad_norm=23.114025115966797, loss=18.244298934936523
I0307 13:34:37.467830 139846847776512 logging_writer.py:48] [83] global_step=83, grad_norm=21.92595863342285, loss=20.232154846191406
I0307 13:34:38.364219 139846856169216 logging_writer.py:48] [84] global_step=84, grad_norm=23.00580406188965, loss=19.527996063232422
I0307 13:34:39.263539 139846847776512 logging_writer.py:48] [85] global_step=85, grad_norm=20.9632511138916, loss=18.61344337463379
I0307 13:34:40.166923 139846856169216 logging_writer.py:48] [86] global_step=86, grad_norm=21.13602066040039, loss=18.625131607055664
I0307 13:34:41.071746 139846847776512 logging_writer.py:48] [87] global_step=87, grad_norm=18.387910842895508, loss=19.09469223022461
I0307 13:34:41.963513 139846856169216 logging_writer.py:48] [88] global_step=88, grad_norm=23.613887786865234, loss=19.281137466430664
I0307 13:34:42.846959 139846847776512 logging_writer.py:48] [89] global_step=89, grad_norm=22.53850555419922, loss=18.16582489013672
I0307 13:34:43.766324 139846856169216 logging_writer.py:48] [90] global_step=90, grad_norm=22.088489532470703, loss=18.564044952392578
I0307 13:34:44.668655 139846847776512 logging_writer.py:48] [91] global_step=91, grad_norm=20.159507751464844, loss=18.078819274902344
I0307 13:34:45.557953 139846856169216 logging_writer.py:48] [92] global_step=92, grad_norm=21.96746826171875, loss=17.950767517089844
I0307 13:34:46.464946 139846847776512 logging_writer.py:48] [93] global_step=93, grad_norm=23.66179847717285, loss=18.459590911865234
I0307 13:34:47.371617 139846856169216 logging_writer.py:48] [94] global_step=94, grad_norm=21.49105453491211, loss=17.3275146484375
I0307 13:34:48.270889 139846847776512 logging_writer.py:48] [95] global_step=95, grad_norm=26.471643447875977, loss=18.80909538269043
I0307 13:34:49.178030 139846856169216 logging_writer.py:48] [96] global_step=96, grad_norm=28.524213790893555, loss=17.985328674316406
I0307 13:34:50.069964 139846847776512 logging_writer.py:48] [97] global_step=97, grad_norm=22.23380470275879, loss=17.60284996032715
I0307 13:34:50.957532 139846856169216 logging_writer.py:48] [98] global_step=98, grad_norm=23.57373809814453, loss=17.807315826416016
I0307 13:34:51.853909 139846847776512 logging_writer.py:48] [99] global_step=99, grad_norm=22.786739349365234, loss=17.571077346801758
I0307 13:34:52.746456 139846856169216 logging_writer.py:48] [100] global_step=100, grad_norm=20.221071243286133, loss=17.433069229125977
I0307 13:40:00.506891 139846847776512 logging_writer.py:48] [500] global_step=500, grad_norm=0.540791392326355, loss=5.892580509185791
I0307 13:47:01.850183 139846856169216 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.5996791124343872, loss=5.791827201843262
I0307 13:53:31.352063 139846906525440 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.7664309144020081, loss=5.791614055633545
I0307 13:57:00.363354 140017254528832 spec.py:321] Evaluating on the training split.
I0307 13:57:36.384265 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 13:58:20.525165 140017254528832 spec.py:349] Evaluating on the test split.
I0307 13:58:42.434353 140017254528832 submission_runner.py:413] Time since start: 1745.86s, 	Step: 1761, 	{'train/ctc_loss': Array(5.9389296, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(5.964965, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.9309573, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1501.9751040935516, 'total_duration': 1745.8570942878723, 'accumulated_submission_time': 1501.9751040935516, 'accumulated_eval_time': 243.77579021453857, 'accumulated_logging_time': 0.04176211357116699}
I0307 13:58:42.467673 139846906525440 logging_writer.py:48] [1761] accumulated_eval_time=243.775790, accumulated_logging_time=0.041762, accumulated_submission_time=1501.975104, global_step=1761, preemption_count=0, score=1501.975104, test/ctc_loss=5.930957317352295, test/num_examples=2472, test/wer=0.899580, total_duration=1745.857094, train/ctc_loss=5.938929557800293, train/wer=0.944636, validation/ctc_loss=5.964964866638184, validation/num_examples=5348, validation/wer=0.896618
I0307 14:01:46.624939 139846898132736 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.7942060232162476, loss=5.772058486938477
I0307 14:08:14.693712 139846906525440 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.7037714719772339, loss=5.648930549621582
I0307 14:15:17.173582 139846898132736 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.6703287363052368, loss=5.554168701171875
I0307 14:21:50.172946 139846906525440 logging_writer.py:48] [3500] global_step=3500, grad_norm=2.2902722358703613, loss=5.539026260375977
I0307 14:22:42.554074 140017254528832 spec.py:321] Evaluating on the training split.
I0307 14:23:18.875703 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 14:24:02.828418 140017254528832 spec.py:349] Evaluating on the test split.
I0307 14:24:25.122916 140017254528832 submission_runner.py:413] Time since start: 3288.55s, 	Step: 3570, 	{'train/ctc_loss': Array(5.678523, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': Array(5.6549478, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.644139, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2941.9806768894196, 'total_duration': 3288.545496702194, 'accumulated_submission_time': 2941.9806768894196, 'accumulated_eval_time': 346.3394613265991, 'accumulated_logging_time': 0.08922410011291504}
I0307 14:24:25.155630 139846906525440 logging_writer.py:48] [3570] accumulated_eval_time=346.339461, accumulated_logging_time=0.089224, accumulated_submission_time=2941.980677, global_step=3570, preemption_count=0, score=2941.980677, test/ctc_loss=5.644138813018799, test/num_examples=2472, test/wer=0.899580, total_duration=3288.545497, train/ctc_loss=5.678523063659668, train/wer=0.942722, validation/ctc_loss=5.654947757720947, validation/num_examples=5348, validation/wer=0.896618
I0307 14:30:07.905220 139846898132736 logging_writer.py:48] [4000] global_step=4000, grad_norm=1.321779489517212, loss=5.550782680511475
I0307 14:36:45.144573 139846906525440 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.7193095088005066, loss=5.505916595458984
I0307 14:43:49.232366 139846898132736 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.8297095894813538, loss=5.541741847991943
I0307 14:48:25.145385 140017254528832 spec.py:321] Evaluating on the training split.
I0307 14:49:01.235082 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 14:49:45.524752 140017254528832 spec.py:349] Evaluating on the test split.
I0307 14:50:08.204304 140017254528832 submission_runner.py:413] Time since start: 4831.63s, 	Step: 5339, 	{'train/ctc_loss': Array(7.206813, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': Array(9.036783, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(8.352105, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4381.889694213867, 'total_duration': 4831.627207994461, 'accumulated_submission_time': 4381.889694213867, 'accumulated_eval_time': 449.3937108516693, 'accumulated_logging_time': 0.13728642463684082}
I0307 14:50:08.239416 139846906525440 logging_writer.py:48] [5339] accumulated_eval_time=449.393711, accumulated_logging_time=0.137286, accumulated_submission_time=4381.889694, global_step=5339, preemption_count=0, score=4381.889694, test/ctc_loss=8.352105140686035, test/num_examples=2472, test/wer=0.899580, total_duration=4831.627208, train/ctc_loss=7.206812858581543, train/wer=0.943324, validation/ctc_loss=9.036783218383789, validation/num_examples=5348, validation/wer=0.896618
I0307 14:52:12.044407 139846898132736 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.8413538336753845, loss=5.529685020446777
I0307 14:59:00.271105 139846906525440 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.9012669324874878, loss=5.5499162673950195
I0307 15:05:44.935577 139846906525440 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.5639044046401978, loss=5.5000996589660645
I0307 15:12:43.364375 139846898132736 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.4400286376476288, loss=5.505295276641846
I0307 15:14:08.571685 140017254528832 spec.py:321] Evaluating on the training split.
I0307 15:14:45.964167 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 15:15:30.742132 140017254528832 spec.py:349] Evaluating on the test split.
I0307 15:15:53.178268 140017254528832 submission_runner.py:413] Time since start: 6376.60s, 	Step: 7099, 	{'train/ctc_loss': Array(7.6976004, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': Array(9.880598, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(9.57103, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 5822.1394028663635, 'total_duration': 6376.600991487503, 'accumulated_submission_time': 5822.1394028663635, 'accumulated_eval_time': 553.9952628612518, 'accumulated_logging_time': 0.1885666847229004}
I0307 15:15:53.213029 139846906525440 logging_writer.py:48] [7099] accumulated_eval_time=553.995263, accumulated_logging_time=0.188567, accumulated_submission_time=5822.139403, global_step=7099, preemption_count=0, score=5822.139403, test/ctc_loss=9.571029663085938, test/num_examples=2472, test/wer=0.899580, total_duration=6376.600991, train/ctc_loss=7.697600364685059, train/wer=0.943700, validation/ctc_loss=9.880598068237305, validation/num_examples=5348, validation/wer=0.896618
I0307 15:21:03.276943 139846906525440 logging_writer.py:48] [7500] global_step=7500, grad_norm=1.5778841972351074, loss=5.523179054260254
I0307 15:28:05.215470 139846898132736 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.506023645401001, loss=5.487238883972168
I0307 15:34:57.454920 139846906525440 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.657090961933136, loss=5.481348991394043
I0307 15:39:53.834213 140017254528832 spec.py:321] Evaluating on the training split.
I0307 15:40:31.169414 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 15:41:15.406043 140017254528832 spec.py:349] Evaluating on the test split.
I0307 15:41:37.701226 140017254528832 submission_runner.py:413] Time since start: 7921.12s, 	Step: 8863, 	{'train/ctc_loss': Array(5.786989, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': Array(5.6355996, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.645039, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7262.676467657089, 'total_duration': 7921.123960733414, 'accumulated_submission_time': 7262.676467657089, 'accumulated_eval_time': 657.8572702407837, 'accumulated_logging_time': 0.2408149242401123}
I0307 15:41:37.738856 139846906525440 logging_writer.py:48] [8863] accumulated_eval_time=657.857270, accumulated_logging_time=0.240815, accumulated_submission_time=7262.676468, global_step=8863, preemption_count=0, score=7262.676468, test/ctc_loss=5.645039081573486, test/num_examples=2472, test/wer=0.899580, total_duration=7921.123961, train/ctc_loss=5.786989212036133, train/wer=0.941551, validation/ctc_loss=5.635599613189697, validation/num_examples=5348, validation/wer=0.896618
I0307 15:43:23.054693 139846898132736 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.295857697725296, loss=5.489320278167725
I0307 15:49:55.880956 139846906525440 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.3505402207374573, loss=5.485228061676025
I0307 15:56:49.714313 139846898132736 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.42070022225379944, loss=5.563780307769775
I0307 16:03:42.701276 139846906525440 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.2608318030834198, loss=5.500757694244385
I0307 16:05:37.923333 140017254528832 spec.py:321] Evaluating on the training split.
I0307 16:06:16.325604 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 16:07:00.305404 140017254528832 spec.py:349] Evaluating on the test split.
I0307 16:07:23.077596 140017254528832 submission_runner.py:413] Time since start: 9466.50s, 	Step: 10653, 	{'train/ctc_loss': Array(5.8362217, dtype=float32), 'train/wer': 0.9285253212839623, 'validation/ctc_loss': Array(5.6233115, dtype=float32), 'validation/wer': 0.8941270745435763, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.6422524, dtype=float32), 'test/wer': 0.8957812849105274, 'test/num_examples': 2472, 'score': 8702.776113510132, 'total_duration': 9466.500397920609, 'accumulated_submission_time': 8702.776113510132, 'accumulated_eval_time': 763.006599187851, 'accumulated_logging_time': 0.29567575454711914}
I0307 16:07:23.109392 139846906525440 logging_writer.py:48] [10653] accumulated_eval_time=763.006599, accumulated_logging_time=0.295676, accumulated_submission_time=8702.776114, global_step=10653, preemption_count=0, score=8702.776114, test/ctc_loss=5.642252445220947, test/num_examples=2472, test/wer=0.895781, total_duration=9466.500398, train/ctc_loss=5.836221694946289, train/wer=0.928525, validation/ctc_loss=5.623311519622803, validation/num_examples=5348, validation/wer=0.894127
I0307 16:11:50.755236 139846898132736 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.35962432622909546, loss=5.501194953918457
I0307 16:18:47.423911 139846906525440 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.2395525574684143, loss=5.473858833312988
I0307 16:25:26.900497 139846898132736 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.8794156908988953, loss=5.51468563079834
I0307 16:31:23.567775 140017254528832 spec.py:321] Evaluating on the training split.
I0307 16:32:03.223551 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 16:32:47.728323 140017254528832 spec.py:349] Evaluating on the test split.
I0307 16:33:10.317151 140017254528832 submission_runner.py:413] Time since start: 11013.74s, 	Step: 12417, 	{'train/ctc_loss': Array(5.510566, dtype=float32), 'train/wer': 0.9294503647927169, 'validation/ctc_loss': Array(5.462328, dtype=float32), 'validation/wer': 0.894300858298657, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4465885, dtype=float32), 'test/wer': 0.8958219080697906, 'test/num_examples': 2472, 'score': 10143.150477647781, 'total_duration': 11013.738177537918, 'accumulated_submission_time': 10143.150477647781, 'accumulated_eval_time': 869.7494282722473, 'accumulated_logging_time': 0.3447258472442627}
I0307 16:33:10.353208 139846906525440 logging_writer.py:48] [12417] accumulated_eval_time=869.749428, accumulated_logging_time=0.344726, accumulated_submission_time=10143.150478, global_step=12417, preemption_count=0, score=10143.150478, test/ctc_loss=5.446588516235352, test/num_examples=2472, test/wer=0.895822, total_duration=11013.738178, train/ctc_loss=5.510566234588623, train/wer=0.929450, validation/ctc_loss=5.46232795715332, validation/num_examples=5348, validation/wer=0.894301
I0307 16:34:14.068133 139846898132736 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.29672786593437195, loss=5.480110168457031
I0307 16:40:40.440694 139846906525440 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.30011531710624695, loss=5.479266166687012
I0307 16:47:42.654769 139846906525440 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.340970516204834, loss=5.4446635246276855
I0307 16:54:17.055521 139846898132736 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.4196029603481293, loss=5.477620601654053
I0307 16:57:11.047735 140017254528832 spec.py:321] Evaluating on the training split.
I0307 16:57:48.235289 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 16:58:32.145591 140017254528832 spec.py:349] Evaluating on the test split.
I0307 16:58:54.552817 140017254528832 submission_runner.py:413] Time since start: 12557.98s, 	Step: 14203, 	{'train/ctc_loss': Array(5.4679193, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': Array(5.435099, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.412527, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 11583.75929760933, 'total_duration': 12557.975610017776, 'accumulated_submission_time': 11583.75929760933, 'accumulated_eval_time': 973.2495701313019, 'accumulated_logging_time': 0.40009021759033203}
I0307 16:58:54.584864 139846906525440 logging_writer.py:48] [14203] accumulated_eval_time=973.249570, accumulated_logging_time=0.400090, accumulated_submission_time=11583.759298, global_step=14203, preemption_count=0, score=11583.759298, test/ctc_loss=5.412527084350586, test/num_examples=2472, test/wer=0.899580, total_duration=12557.975610, train/ctc_loss=5.46791934967041, train/wer=0.944086, validation/ctc_loss=5.435099124908447, validation/num_examples=5348, validation/wer=0.896618
I0307 17:02:43.481513 139846906525440 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.5489893555641174, loss=5.465061664581299
I0307 17:09:21.590112 139846898132736 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.5473220348358154, loss=5.500636577606201
I0307 17:16:39.423037 139846906525440 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6509114503860474, loss=5.443511486053467
I0307 17:22:54.587654 140017254528832 spec.py:321] Evaluating on the training split.
I0307 17:23:31.914716 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 17:24:16.369518 140017254528832 spec.py:349] Evaluating on the test split.
I0307 17:24:38.712560 140017254528832 submission_runner.py:413] Time since start: 14102.14s, 	Step: 15983, 	{'train/ctc_loss': Array(8.226454, dtype=float32), 'train/wer': 0.9427990785714666, 'validation/ctc_loss': Array(7.8200235, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(7.9349136, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 13023.681303739548, 'total_duration': 14102.13569688797, 'accumulated_submission_time': 13023.681303739548, 'accumulated_eval_time': 1077.3698818683624, 'accumulated_logging_time': 0.4471440315246582}
I0307 17:24:38.748890 139846906525440 logging_writer.py:48] [15983] accumulated_eval_time=1077.369882, accumulated_logging_time=0.447144, accumulated_submission_time=13023.681304, global_step=15983, preemption_count=0, score=13023.681304, test/ctc_loss=7.934913635253906, test/num_examples=2472, test/wer=0.899580, total_duration=14102.135697, train/ctc_loss=8.22645378112793, train/wer=0.942799, validation/ctc_loss=7.820023536682129, validation/num_examples=5348, validation/wer=0.896618
I0307 17:24:52.443749 139846898132736 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.5296592712402344, loss=5.46295166015625
I0307 17:31:35.558561 139846906525440 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.40581175684928894, loss=5.458922386169434
I0307 17:38:00.320042 139846898132736 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.4433206021785736, loss=5.433351039886475
I0307 17:45:10.853732 139846906525440 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.4203179180622101, loss=5.491558074951172
I0307 17:48:39.203191 140017254528832 spec.py:321] Evaluating on the training split.
I0307 17:49:16.278134 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 17:50:00.441598 140017254528832 spec.py:349] Evaluating on the test split.
I0307 17:50:23.059663 140017254528832 submission_runner.py:413] Time since start: 15646.48s, 	Step: 17771, 	{'train/ctc_loss': Array(9.104383, dtype=float32), 'train/wer': 0.9423383225986367, 'validation/ctc_loss': Array(8.864255, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(8.938903, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14464.052798748016, 'total_duration': 15646.482597112656, 'accumulated_submission_time': 14464.052798748016, 'accumulated_eval_time': 1181.2217242717743, 'accumulated_logging_time': 0.4993119239807129}
I0307 17:50:23.094722 139846906525440 logging_writer.py:48] [17771] accumulated_eval_time=1181.221724, accumulated_logging_time=0.499312, accumulated_submission_time=14464.052799, global_step=17771, preemption_count=0, score=14464.052799, test/ctc_loss=8.938902854919434, test/num_examples=2472, test/wer=0.899580, total_duration=15646.482597, train/ctc_loss=9.10438346862793, train/wer=0.942338, validation/ctc_loss=8.86425495147705, validation/num_examples=5348, validation/wer=0.896618
I0307 17:53:17.303039 139846898132736 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.27690228819847107, loss=5.4959611892700195
I0307 18:00:16.385746 139846906525440 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.3182556927204132, loss=5.470162391662598
I0307 18:06:46.673047 139846906525440 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.28298017382621765, loss=5.479712009429932
I0307 18:14:05.665310 139846898132736 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.36965638399124146, loss=5.4597487449646
I0307 18:14:23.386805 140017254528832 spec.py:321] Evaluating on the training split.
I0307 18:15:00.701452 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 18:15:44.878176 140017254528832 spec.py:349] Evaluating on the test split.
I0307 18:16:07.184919 140017254528832 submission_runner.py:413] Time since start: 17190.61s, 	Step: 19522, 	{'train/ctc_loss': Array(5.4958663, dtype=float32), 'train/wer': 0.9431396916893625, 'validation/ctc_loss': Array(5.467418, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4494553, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15904.257827997208, 'total_duration': 17190.606981039047, 'accumulated_submission_time': 15904.257827997208, 'accumulated_eval_time': 1285.0141394138336, 'accumulated_logging_time': 0.5547926425933838}
I0307 18:16:07.220412 139846906525440 logging_writer.py:48] [19522] accumulated_eval_time=1285.014139, accumulated_logging_time=0.554793, accumulated_submission_time=15904.257828, global_step=19522, preemption_count=0, score=15904.257828, test/ctc_loss=5.449455261230469, test/num_examples=2472, test/wer=0.899580, total_duration=17190.606981, train/ctc_loss=5.495866298675537, train/wer=0.943140, validation/ctc_loss=5.467418193817139, validation/num_examples=5348, validation/wer=0.896618
I0307 18:22:17.559496 139846906525440 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.6590219736099243, loss=5.469727993011475
I0307 18:29:33.397524 139846898132736 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.3300698697566986, loss=5.4561686515808105
I0307 18:36:05.965371 139846906525440 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.6509290337562561, loss=5.44356107711792
I0307 18:40:07.852743 140017254528832 spec.py:321] Evaluating on the training split.
I0307 18:40:45.285358 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 18:41:29.705784 140017254528832 spec.py:349] Evaluating on the test split.
I0307 18:41:51.952607 140017254528832 submission_runner.py:413] Time since start: 18735.38s, 	Step: 21288, 	{'train/ctc_loss': Array(6.357177, dtype=float32), 'train/wer': 0.9432716912443612, 'validation/ctc_loss': Array(6.344222, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.3780193, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 17344.80734229088, 'total_duration': 18735.375351190567, 'accumulated_submission_time': 17344.80734229088, 'accumulated_eval_time': 1389.1090364456177, 'accumulated_logging_time': 0.6050007343292236}
I0307 18:41:51.986116 139846906525440 logging_writer.py:48] [21288] accumulated_eval_time=1389.109036, accumulated_logging_time=0.605001, accumulated_submission_time=17344.807342, global_step=21288, preemption_count=0, score=17344.807342, test/ctc_loss=6.378019332885742, test/num_examples=2472, test/wer=0.899580, total_duration=18735.375351, train/ctc_loss=6.357176780700684, train/wer=0.943272, validation/ctc_loss=6.344222068786621, validation/num_examples=5348, validation/wer=0.896618
I0307 18:44:33.532915 139846898132736 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.35817068815231323, loss=5.441559314727783
I0307 18:50:58.392194 139846906525440 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.284166157245636, loss=5.435043811798096
I0307 18:58:02.817013 139846898132736 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.3798617422580719, loss=5.440423011779785
I0307 19:04:41.194914 139846906525440 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.35023245215415955, loss=5.426255702972412
I0307 19:05:52.656683 140017254528832 spec.py:321] Evaluating on the training split.
I0307 19:06:30.343576 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 19:07:14.494189 140017254528832 spec.py:349] Evaluating on the test split.
I0307 19:07:36.896092 140017254528832 submission_runner.py:413] Time since start: 20280.32s, 	Step: 23095, 	{'train/ctc_loss': Array(5.62643, dtype=float32), 'train/wer': 0.944685667249717, 'validation/ctc_loss': Array(5.537226, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.536327, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 18785.39287829399, 'total_duration': 20280.319013118744, 'accumulated_submission_time': 18785.39287829399, 'accumulated_eval_time': 1493.3436286449432, 'accumulated_logging_time': 0.6555466651916504}
I0307 19:07:36.928669 139846906525440 logging_writer.py:48] [23095] accumulated_eval_time=1493.343629, accumulated_logging_time=0.655547, accumulated_submission_time=18785.392878, global_step=23095, preemption_count=0, score=18785.392878, test/ctc_loss=5.536326885223389, test/num_examples=2472, test/wer=0.899580, total_duration=20280.319013, train/ctc_loss=5.626430034637451, train/wer=0.944686, validation/ctc_loss=5.53722620010376, validation/num_examples=5348, validation/wer=0.896618
I0307 19:12:54.661887 139846898132736 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.394268274307251, loss=5.444490909576416
I0307 19:19:37.313836 139846906525440 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.5788581967353821, loss=5.473756790161133
I0307 19:26:33.574154 139846898132736 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.362383097410202, loss=5.439096927642822
I0307 19:31:36.935594 140017254528832 spec.py:321] Evaluating on the training split.
I0307 19:32:14.014741 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 19:32:58.342058 140017254528832 spec.py:349] Evaluating on the test split.
I0307 19:33:21.208286 140017254528832 submission_runner.py:413] Time since start: 21824.63s, 	Step: 24866, 	{'train/ctc_loss': Array(5.5369515, dtype=float32), 'train/wer': 0.9432456399645285, 'validation/ctc_loss': Array(5.5097246, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4994817, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 20225.317187309265, 'total_duration': 21824.630343437195, 'accumulated_submission_time': 20225.317187309265, 'accumulated_eval_time': 1597.6108083724976, 'accumulated_logging_time': 0.7038488388061523}
I0307 19:33:21.244596 139846906525440 logging_writer.py:48] [24866] accumulated_eval_time=1597.610808, accumulated_logging_time=0.703849, accumulated_submission_time=20225.317187, global_step=24866, preemption_count=0, score=20225.317187, test/ctc_loss=5.499481678009033, test/num_examples=2472, test/wer=0.899580, total_duration=21824.630343, train/ctc_loss=5.536951541900635, train/wer=0.943246, validation/ctc_loss=5.5097246170043945, validation/num_examples=5348, validation/wer=0.896618
I0307 19:35:03.507271 139846898132736 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.2980706989765167, loss=5.464625835418701
I0307 19:41:43.149341 139846906525440 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.43368256092071533, loss=5.441343307495117
I0307 19:48:30.984247 139846906525440 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.48854464292526245, loss=5.441425800323486
I0307 19:55:17.720160 139846898132736 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.4735528230667114, loss=5.431991100311279
I0307 19:57:21.267957 140017254528832 spec.py:321] Evaluating on the training split.
I0307 19:57:58.781207 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 19:58:42.704077 140017254528832 spec.py:349] Evaluating on the test split.
I0307 19:59:04.797888 140017254528832 submission_runner.py:413] Time since start: 23368.22s, 	Step: 26645, 	{'train/ctc_loss': Array(5.503868, dtype=float32), 'train/wer': 0.9439109001278072, 'validation/ctc_loss': Array(5.458417, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.44211, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 21665.257386922836, 'total_duration': 23368.220731019974, 'accumulated_submission_time': 21665.257386922836, 'accumulated_eval_time': 1701.1358575820923, 'accumulated_logging_time': 0.7563469409942627}
I0307 19:59:04.830248 139846906525440 logging_writer.py:48] [26645] accumulated_eval_time=1701.135858, accumulated_logging_time=0.756347, accumulated_submission_time=21665.257387, global_step=26645, preemption_count=0, score=21665.257387, test/ctc_loss=5.442110061645508, test/num_examples=2472, test/wer=0.899580, total_duration=23368.220731, train/ctc_loss=5.503868103027344, train/wer=0.943911, validation/ctc_loss=5.458416938781738, validation/num_examples=5348, validation/wer=0.896618
I0307 20:03:38.490072 139846906525440 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.31653961539268494, loss=5.4353437423706055
I0307 20:10:26.802621 139846898132736 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.32459449768066406, loss=5.404058933258057
I0307 20:17:18.817628 139846906525440 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.4693662226200104, loss=5.460217475891113
I0307 20:23:05.409525 140017254528832 spec.py:321] Evaluating on the training split.
I0307 20:23:42.752507 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 20:24:26.739900 140017254528832 spec.py:349] Evaluating on the test split.
I0307 20:24:48.994481 140017254528832 submission_runner.py:413] Time since start: 24912.42s, 	Step: 28440, 	{'train/ctc_loss': Array(5.4748054, dtype=float32), 'train/wer': 0.9450143703143059, 'validation/ctc_loss': Array(5.4371943, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.417845, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 23105.754850387573, 'total_duration': 24912.41552066803, 'accumulated_submission_time': 23105.754850387573, 'accumulated_eval_time': 1804.7141313552856, 'accumulated_logging_time': 0.8027734756469727}
I0307 20:24:49.039539 139846906525440 logging_writer.py:48] [28440] accumulated_eval_time=1804.714131, accumulated_logging_time=0.802773, accumulated_submission_time=23105.754850, global_step=28440, preemption_count=0, score=23105.754850, test/ctc_loss=5.417844772338867, test/num_examples=2472, test/wer=0.899580, total_duration=24912.415521, train/ctc_loss=5.4748053550720215, train/wer=0.945014, validation/ctc_loss=5.437194347381592, validation/num_examples=5348, validation/wer=0.896618
I0307 20:25:35.364617 139846898132736 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.3455173373222351, loss=5.4433369636535645
I0307 20:32:07.318091 139846906525440 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.4117562770843506, loss=5.459162712097168
I0307 20:38:48.580870 139846898132736 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.4310980439186096, loss=5.462425708770752
I0307 20:45:49.417241 139846906525440 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.6031847596168518, loss=5.438047409057617
I0307 20:48:49.701493 140017254528832 spec.py:321] Evaluating on the training split.
I0307 20:49:26.817907 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 20:50:11.195597 140017254528832 spec.py:349] Evaluating on the test split.
I0307 20:50:33.505129 140017254528832 submission_runner.py:413] Time since start: 26456.93s, 	Step: 30239, 	{'train/ctc_loss': Array(5.4860835, dtype=float32), 'train/wer': 0.9417576703068122, 'validation/ctc_loss': Array(5.4399314, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.421852, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 24546.3284304142, 'total_duration': 26456.92817234993, 'accumulated_submission_time': 24546.3284304142, 'accumulated_eval_time': 1908.5130717754364, 'accumulated_logging_time': 0.8684427738189697}
I0307 20:50:33.539758 139846906525440 logging_writer.py:48] [30239] accumulated_eval_time=1908.513072, accumulated_logging_time=0.868443, accumulated_submission_time=24546.328430, global_step=30239, preemption_count=0, score=24546.328430, test/ctc_loss=5.421852111816406, test/num_examples=2472, test/wer=0.899580, total_duration=26456.928172, train/ctc_loss=5.486083507537842, train/wer=0.941758, validation/ctc_loss=5.439931392669678, validation/num_examples=5348, validation/wer=0.896618
I0307 20:53:52.345999 139846898132736 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.5354561805725098, loss=5.433141708374023
I0307 21:00:49.159853 139846906525440 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.4698134660720825, loss=5.42913293838501
I0307 21:07:22.268359 139846898132736 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.32250234484672546, loss=5.425903797149658
I0307 21:14:32.204000 139846906525440 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.33080410957336426, loss=5.474167823791504
I0307 21:14:34.200553 140017254528832 spec.py:321] Evaluating on the training split.
I0307 21:15:11.897470 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 21:15:56.087788 140017254528832 spec.py:349] Evaluating on the test split.
I0307 21:16:18.427428 140017254528832 submission_runner.py:413] Time since start: 28001.85s, 	Step: 32004, 	{'train/ctc_loss': Array(5.515235, dtype=float32), 'train/wer': 0.9416600198590335, 'validation/ctc_loss': Array(5.506643, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4905233, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 25986.90826368332, 'total_duration': 28001.848637342453, 'accumulated_submission_time': 25986.90826368332, 'accumulated_eval_time': 2012.7334079742432, 'accumulated_logging_time': 0.9174127578735352}
I0307 21:16:18.462738 139846906525440 logging_writer.py:48] [32004] accumulated_eval_time=2012.733408, accumulated_logging_time=0.917413, accumulated_submission_time=25986.908264, global_step=32004, preemption_count=0, score=25986.908264, test/ctc_loss=5.490523338317871, test/num_examples=2472, test/wer=0.899580, total_duration=28001.848637, train/ctc_loss=5.51523494720459, train/wer=0.941660, validation/ctc_loss=5.506642818450928, validation/num_examples=5348, validation/wer=0.896618
I0307 21:22:35.280009 139846898132736 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.31633952260017395, loss=5.448723316192627
I0307 21:29:40.698522 139846906525440 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.40316763520240784, loss=5.440650463104248
I0307 21:36:07.838425 139846898132736 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.5123612284660339, loss=5.426141738891602
I0307 21:40:18.711869 140017254528832 spec.py:321] Evaluating on the training split.
I0307 21:40:56.158810 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 21:41:40.065869 140017254528832 spec.py:349] Evaluating on the test split.
I0307 21:42:02.716686 140017254528832 submission_runner.py:413] Time since start: 29546.14s, 	Step: 33791, 	{'train/ctc_loss': Array(5.5589013, dtype=float32), 'train/wer': 0.9447677853176417, 'validation/ctc_loss': Array(5.448199, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4482307, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 27427.07439470291, 'total_duration': 29546.137358427048, 'accumulated_submission_time': 27427.07439470291, 'accumulated_eval_time': 2116.731162548065, 'accumulated_logging_time': 0.9689841270446777}
I0307 21:42:02.751283 139846906525440 logging_writer.py:48] [33791] accumulated_eval_time=2116.731163, accumulated_logging_time=0.968984, accumulated_submission_time=27427.074395, global_step=33791, preemption_count=0, score=27427.074395, test/ctc_loss=5.448230743408203, test/num_examples=2472, test/wer=0.899580, total_duration=29546.137358, train/ctc_loss=5.558901309967041, train/wer=0.944768, validation/ctc_loss=5.4481987953186035, validation/num_examples=5348, validation/wer=0.896618
I0307 21:44:45.001036 139846906525440 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.6737526655197144, loss=5.458489894866943
I0307 21:51:11.872249 139846898132736 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.4037374258041382, loss=5.44381046295166
I0307 21:58:20.957857 139846906525440 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.5831573009490967, loss=5.434389591217041
I0307 22:04:49.001410 139846906525440 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.29086634516716003, loss=5.419672012329102
I0307 22:06:02.862968 140017254528832 spec.py:321] Evaluating on the training split.
I0307 22:06:40.079329 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 22:07:24.288595 140017254528832 spec.py:349] Evaluating on the test split.
I0307 22:07:46.556814 140017254528832 submission_runner.py:413] Time since start: 31089.98s, 	Step: 35586, 	{'train/ctc_loss': Array(5.4749928, dtype=float32), 'train/wer': 0.9427091658940503, 'validation/ctc_loss': Array(5.4578133, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4331236, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 28867.101755142212, 'total_duration': 31089.976769685745, 'accumulated_submission_time': 28867.101755142212, 'accumulated_eval_time': 2220.417268514633, 'accumulated_logging_time': 1.0206682682037354}
I0307 22:07:46.593824 139846906525440 logging_writer.py:48] [35586] accumulated_eval_time=2220.417269, accumulated_logging_time=1.020668, accumulated_submission_time=28867.101755, global_step=35586, preemption_count=0, score=28867.101755, test/ctc_loss=5.433123588562012, test/num_examples=2472, test/wer=0.899580, total_duration=31089.976770, train/ctc_loss=5.474992752075195, train/wer=0.942709, validation/ctc_loss=5.457813262939453, validation/num_examples=5348, validation/wer=0.896618
I0307 22:13:10.540295 139846898132736 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.27715468406677246, loss=5.429178237915039
I0307 22:19:38.773786 139846906525440 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.31439870595932007, loss=5.429627895355225
I0307 22:26:43.329449 139846898132736 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.6445927619934082, loss=5.440412998199463
I0307 22:31:46.813882 140017254528832 spec.py:321] Evaluating on the training split.
I0307 22:32:24.072049 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 22:33:08.182687 140017254528832 spec.py:349] Evaluating on the test split.
I0307 22:33:30.950593 140017254528832 submission_runner.py:413] Time since start: 32634.37s, 	Step: 37388, 	{'train/ctc_loss': Array(5.6269917, dtype=float32), 'train/wer': 0.9448971433842748, 'validation/ctc_loss': Array(5.5650015, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.562459, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 30307.23932147026, 'total_duration': 32634.37336921692, 'accumulated_submission_time': 30307.23932147026, 'accumulated_eval_time': 2324.5490913391113, 'accumulated_logging_time': 1.0720014572143555}
I0307 22:33:30.986309 139846906525440 logging_writer.py:48] [37388] accumulated_eval_time=2324.549091, accumulated_logging_time=1.072001, accumulated_submission_time=30307.239321, global_step=37388, preemption_count=0, score=30307.239321, test/ctc_loss=5.5624589920043945, test/num_examples=2472, test/wer=0.899580, total_duration=32634.373369, train/ctc_loss=5.6269917488098145, train/wer=0.944897, validation/ctc_loss=5.565001487731934, validation/num_examples=5348, validation/wer=0.896618
I0307 22:34:56.753823 139846898132736 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.5254136919975281, loss=5.419024467468262
I0307 22:41:40.149661 139846906525440 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.3607941269874573, loss=5.416964054107666
I0307 22:48:14.043450 139846906525440 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.2989073097705841, loss=5.433477401733398
I0307 22:55:12.596827 139846898132736 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.47371140122413635, loss=5.434185981750488
I0307 22:57:31.229095 140017254528832 spec.py:321] Evaluating on the training split.
I0307 22:58:08.616364 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 22:58:52.548265 140017254528832 spec.py:349] Evaluating on the test split.
I0307 22:59:14.905336 140017254528832 submission_runner.py:413] Time since start: 34178.33s, 	Step: 39162, 	{'train/ctc_loss': Array(5.609433, dtype=float32), 'train/wer': 0.9432324554919642, 'validation/ctc_loss': Array(5.6505947, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.625185, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 31747.398077964783, 'total_duration': 34178.32664036751, 'accumulated_submission_time': 31747.398077964783, 'accumulated_eval_time': 2428.2190799713135, 'accumulated_logging_time': 1.1256511211395264}
I0307 22:59:14.941011 139846906525440 logging_writer.py:48] [39162] accumulated_eval_time=2428.219080, accumulated_logging_time=1.125651, accumulated_submission_time=31747.398078, global_step=39162, preemption_count=0, score=31747.398078, test/ctc_loss=5.625185012817383, test/num_examples=2472, test/wer=0.899580, total_duration=34178.326640, train/ctc_loss=5.609433174133301, train/wer=0.943232, validation/ctc_loss=5.650594711303711, validation/num_examples=5348, validation/wer=0.896618
I0307 23:03:32.302162 139846898132736 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.4603964686393738, loss=5.399759292602539
I0307 23:10:14.128651 139846906525440 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.4944697916507721, loss=5.45558500289917
I0307 23:16:52.226687 139846906525440 logging_writer.py:48] [40500] global_step=40500, grad_norm=1.0659971237182617, loss=5.447968006134033
I0307 23:23:15.269206 140017254528832 spec.py:321] Evaluating on the training split.
I0307 23:23:52.961438 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 23:24:37.106220 140017254528832 spec.py:349] Evaluating on the test split.
I0307 23:24:59.659168 140017254528832 submission_runner.py:413] Time since start: 35723.08s, 	Step: 40963, 	{'train/ctc_loss': Array(5.8990645, dtype=float32), 'train/wer': 0.941680272071945, 'validation/ctc_loss': Array(6.010586, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.000703, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 33187.64439082146, 'total_duration': 35723.08069252968, 'accumulated_submission_time': 33187.64439082146, 'accumulated_eval_time': 2532.6028385162354, 'accumulated_logging_time': 1.1763088703155518}
I0307 23:24:59.697164 139846906525440 logging_writer.py:48] [40963] accumulated_eval_time=2532.602839, accumulated_logging_time=1.176309, accumulated_submission_time=33187.644391, global_step=40963, preemption_count=0, score=33187.644391, test/ctc_loss=6.000702857971191, test/num_examples=2472, test/wer=0.899580, total_duration=35723.080693, train/ctc_loss=5.899064540863037, train/wer=0.941680, validation/ctc_loss=6.010585784912109, validation/num_examples=5348, validation/wer=0.896618
I0307 23:25:28.536741 139846898132736 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.5051197409629822, loss=5.429859638214111
I0307 23:31:50.591806 139846906525440 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.4337086081504822, loss=5.395797252655029
I0307 23:38:40.642584 139846898132736 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.41001784801483154, loss=5.428154945373535
I0307 23:45:24.518525 139846906525440 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.35001569986343384, loss=5.40834379196167
I0307 23:49:00.454370 140017254528832 spec.py:321] Evaluating on the training split.
I0307 23:49:37.612701 140017254528832 spec.py:333] Evaluating on the validation split.
I0307 23:50:21.610724 140017254528832 spec.py:349] Evaluating on the test split.
I0307 23:50:43.780244 140017254528832 submission_runner.py:413] Time since start: 37267.20s, 	Step: 42779, 	{'train/ctc_loss': Array(5.428235, dtype=float32), 'train/wer': 0.940312095793757, 'validation/ctc_loss': Array(5.4031405, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3880925, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 34628.318608284, 'total_duration': 37267.20153093338, 'accumulated_submission_time': 34628.318608284, 'accumulated_eval_time': 2635.9222514629364, 'accumulated_logging_time': 1.2293226718902588}
I0307 23:50:43.816821 139846906525440 logging_writer.py:48] [42779] accumulated_eval_time=2635.922251, accumulated_logging_time=1.229323, accumulated_submission_time=34628.318608, global_step=42779, preemption_count=0, score=34628.318608, test/ctc_loss=5.388092517852783, test/num_examples=2472, test/wer=0.899580, total_duration=37267.201531, train/ctc_loss=5.428235054016113, train/wer=0.940312, validation/ctc_loss=5.403140544891357, validation/num_examples=5348, validation/wer=0.896618
I0307 23:53:32.222278 139846898132736 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.41134464740753174, loss=5.400577545166016
I0308 00:00:09.396860 139846906525440 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.5648326873779297, loss=5.426978588104248
I0308 00:07:02.494668 139846898132736 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.3258955478668213, loss=5.427757263183594
I0308 00:14:01.615664 139846906525440 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.31465473771095276, loss=5.431797981262207
I0308 00:14:43.853679 140017254528832 spec.py:321] Evaluating on the training split.
I0308 00:15:22.725162 140017254528832 spec.py:333] Evaluating on the validation split.
I0308 00:16:07.176278 140017254528832 spec.py:349] Evaluating on the test split.
I0308 00:16:29.566617 140017254528832 submission_runner.py:413] Time since start: 38812.99s, 	Step: 44557, 	{'train/ctc_loss': Array(5.4203744, dtype=float32), 'train/wer': 0.9371047844119075, 'validation/ctc_loss': Array(5.401307, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.382077, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 36068.27321147919, 'total_duration': 38812.989600896835, 'accumulated_submission_time': 36068.27321147919, 'accumulated_eval_time': 2741.6304364204407, 'accumulated_logging_time': 1.280510425567627}
I0308 00:16:29.600718 139846906525440 logging_writer.py:48] [44557] accumulated_eval_time=2741.630436, accumulated_logging_time=1.280510, accumulated_submission_time=36068.273211, global_step=44557, preemption_count=0, score=36068.273211, test/ctc_loss=5.382077217102051, test/num_examples=2472, test/wer=0.899580, total_duration=38812.989601, train/ctc_loss=5.420374393463135, train/wer=0.937105, validation/ctc_loss=5.401307106018066, validation/num_examples=5348, validation/wer=0.896618
I0308 00:22:20.925736 139846898132736 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.5103699564933777, loss=5.395140647888184
I0308 00:29:19.948045 139846906525440 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.4857083559036255, loss=5.4077467918396
I0308 00:36:05.901102 139846898132736 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.3095872700214386, loss=5.4476752281188965
I0308 00:40:30.162315 140017254528832 spec.py:321] Evaluating on the training split.
I0308 00:41:07.230381 140017254528832 spec.py:333] Evaluating on the validation split.
I0308 00:41:51.465605 140017254528832 spec.py:349] Evaluating on the test split.
I0308 00:42:13.723544 140017254528832 submission_runner.py:413] Time since start: 40357.14s, 	Step: 46307, 	{'train/ctc_loss': Array(5.480328, dtype=float32), 'train/wer': 0.9366967129626904, 'validation/ctc_loss': Array(5.5026855, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4791045, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 37508.74952530861, 'total_duration': 40357.14412713051, 'accumulated_submission_time': 37508.74952530861, 'accumulated_eval_time': 2845.1845121383667, 'accumulated_logging_time': 1.3334717750549316}
I0308 00:42:13.761034 139846906525440 logging_writer.py:48] [46307] accumulated_eval_time=2845.184512, accumulated_logging_time=1.333472, accumulated_submission_time=37508.749525, global_step=46307, preemption_count=0, score=37508.749525, test/ctc_loss=5.479104518890381, test/num_examples=2472, test/wer=0.899580, total_duration=40357.144127, train/ctc_loss=5.48032808303833, train/wer=0.936697, validation/ctc_loss=5.502685546875, validation/num_examples=5348, validation/wer=0.896618
I0308 00:44:44.099330 139846906525440 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.3201579749584198, loss=5.426304340362549
I0308 00:51:26.425949 139846898132736 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.3924736976623535, loss=5.4300150871276855
I0308 00:58:28.105525 139846906525440 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.885195791721344, loss=5.413118362426758
I0308 01:05:03.184337 139846898132736 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.5881942510604858, loss=5.40523624420166
I0308 01:06:14.141906 140017254528832 spec.py:321] Evaluating on the training split.
I0308 01:06:51.259617 140017254528832 spec.py:333] Evaluating on the validation split.
I0308 01:07:35.439292 140017254528832 spec.py:349] Evaluating on the test split.
I0308 01:07:57.984694 140017254528832 submission_runner.py:413] Time since start: 41901.41s, 	Step: 48081, 	{'train/ctc_loss': Array(5.549283, dtype=float32), 'train/wer': 0.9401422956587576, 'validation/ctc_loss': Array(5.516625, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4945273, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 38949.04762458801, 'total_duration': 41901.40736937523, 'accumulated_submission_time': 38949.04762458801, 'accumulated_eval_time': 2949.0222511291504, 'accumulated_logging_time': 1.3872625827789307}
I0308 01:07:58.019123 139846906525440 logging_writer.py:48] [48081] accumulated_eval_time=2949.022251, accumulated_logging_time=1.387263, accumulated_submission_time=38949.047625, global_step=48081, preemption_count=0, score=38949.047625, test/ctc_loss=5.494527339935303, test/num_examples=2472, test/wer=0.899580, total_duration=41901.407369, train/ctc_loss=5.549283027648926, train/wer=0.940142, validation/ctc_loss=5.516624927520752, validation/num_examples=5348, validation/wer=0.896618
I0308 01:13:22.220659 139846906525440 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.45409828424453735, loss=5.448259353637695
I0308 01:19:58.465031 139846898132736 logging_writer.py:48] [49000] global_step=49000, grad_norm=1.0807631015777588, loss=5.426605701446533
I0308 01:27:08.699602 139846906525440 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.3072447180747986, loss=5.437501907348633
I0308 01:31:58.683854 140017254528832 spec.py:321] Evaluating on the training split.
I0308 01:32:36.581464 140017254528832 spec.py:333] Evaluating on the validation split.
I0308 01:33:21.052273 140017254528832 spec.py:349] Evaluating on the test split.
I0308 01:33:43.890409 140017254528832 submission_runner.py:413] Time since start: 43447.31s, 	Step: 49883, 	{'train/ctc_loss': Array(5.442956, dtype=float32), 'train/wer': 0.9415328062295403, 'validation/ctc_loss': Array(5.413966, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3947864, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 40389.62878370285, 'total_duration': 43447.31326317787, 'accumulated_submission_time': 40389.62878370285, 'accumulated_eval_time': 3054.2239389419556, 'accumulated_logging_time': 1.436805009841919}
I0308 01:33:43.928457 139846906525440 logging_writer.py:48] [49883] accumulated_eval_time=3054.223939, accumulated_logging_time=1.436805, accumulated_submission_time=40389.628784, global_step=49883, preemption_count=0, score=40389.628784, test/ctc_loss=5.394786357879639, test/num_examples=2472, test/wer=0.899580, total_duration=43447.313263, train/ctc_loss=5.44295597076416, train/wer=0.941533, validation/ctc_loss=5.413966178894043, validation/num_examples=5348, validation/wer=0.896618
I0308 01:35:13.498374 139846898132736 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.5878328680992126, loss=5.404835224151611
I0308 01:42:10.009626 139846906525440 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.29202550649642944, loss=5.426526069641113
I0308 01:48:39.016166 139846898132736 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.585635781288147, loss=5.429150581359863
I0308 01:55:55.952376 139846906525440 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.6587796807289124, loss=5.424870491027832
I0308 01:57:44.126703 140017254528832 spec.py:321] Evaluating on the training split.
I0308 01:58:21.174053 140017254528832 spec.py:333] Evaluating on the validation split.
I0308 01:59:05.240027 140017254528832 spec.py:349] Evaluating on the test split.
I0308 01:59:27.741908 140017254528832 submission_runner.py:413] Time since start: 44991.16s, 	Step: 51644, 	{'train/ctc_loss': Array(5.4087667, dtype=float32), 'train/wer': 0.941605522275386, 'validation/ctc_loss': Array(5.388261, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3714447, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 41829.744005680084, 'total_duration': 44991.162705898285, 'accumulated_submission_time': 41829.744005680084, 'accumulated_eval_time': 3157.832223176956, 'accumulated_logging_time': 1.4919836521148682}
I0308 01:59:27.778232 139846906525440 logging_writer.py:48] [51644] accumulated_eval_time=3157.832223, accumulated_logging_time=1.491984, accumulated_submission_time=41829.744006, global_step=51644, preemption_count=0, score=41829.744006, test/ctc_loss=5.3714447021484375, test/num_examples=2472, test/wer=0.899580, total_duration=44991.162706, train/ctc_loss=5.408766746520996, train/wer=0.941606, validation/ctc_loss=5.388260841369629, validation/num_examples=5348, validation/wer=0.896618
I0308 02:03:58.645358 139846898132736 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.34689781069755554, loss=5.418524742126465
I0308 02:11:05.018987 139846906525440 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.6221368908882141, loss=5.40104341506958
I0308 02:17:32.268494 139846906525440 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.5263166427612305, loss=5.389361381530762
I0308 02:23:28.541109 140017254528832 spec.py:321] Evaluating on the training split.
I0308 02:24:05.585290 140017254528832 spec.py:333] Evaluating on the validation split.
I0308 02:24:50.215577 140017254528832 spec.py:349] Evaluating on the test split.
I0308 02:25:12.632373 140017254528832 submission_runner.py:413] Time since start: 46536.06s, 	Step: 53410, 	{'train/ctc_loss': Array(5.411853, dtype=float32), 'train/wer': 0.9422779591135544, 'validation/ctc_loss': Array(5.382742, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.365683, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 43270.42512345314, 'total_duration': 46536.05523133278, 'accumulated_submission_time': 43270.42512345314, 'accumulated_eval_time': 3261.918598175049, 'accumulated_logging_time': 1.544386386871338}
I0308 02:25:12.671106 139846906525440 logging_writer.py:48] [53410] accumulated_eval_time=3261.918598, accumulated_logging_time=1.544386, accumulated_submission_time=43270.425123, global_step=53410, preemption_count=0, score=43270.425123, test/ctc_loss=5.365683078765869, test/num_examples=2472, test/wer=0.899580, total_duration=46536.055231, train/ctc_loss=5.411852836608887, train/wer=0.942278, validation/ctc_loss=5.382741928100586, validation/num_examples=5348, validation/wer=0.896618
I0308 02:26:21.606732 139846898132736 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.5844478607177734, loss=5.4157185554504395
I0308 02:32:49.006888 139846906525440 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.732211709022522, loss=5.431214809417725
I0308 02:40:02.073997 139846898132736 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.48309412598609924, loss=5.414592266082764
I0308 02:46:33.939759 139846906525440 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.436545729637146, loss=5.41110372543335
I0308 02:49:12.925453 140017254528832 spec.py:321] Evaluating on the training split.
I0308 02:49:50.338261 140017254528832 spec.py:333] Evaluating on the validation split.
I0308 02:50:34.595508 140017254528832 spec.py:349] Evaluating on the test split.
I0308 02:50:57.283798 140017254528832 submission_runner.py:413] Time since start: 48080.70s, 	Step: 55191, 	{'train/ctc_loss': Array(5.421169, dtype=float32), 'train/wer': 0.9416786903741633, 'validation/ctc_loss': Array(5.384265, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.36615, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 44710.594696998596, 'total_duration': 48080.70483779907, 'accumulated_submission_time': 44710.594696998596, 'accumulated_eval_time': 3366.2702553272247, 'accumulated_logging_time': 1.6008696556091309}
I0308 02:50:57.321749 139846906525440 logging_writer.py:48] [55191] accumulated_eval_time=3366.270255, accumulated_logging_time=1.600870, accumulated_submission_time=44710.594697, global_step=55191, preemption_count=0, score=44710.594697, test/ctc_loss=5.36614990234375, test/num_examples=2472, test/wer=0.899580, total_duration=48080.704838, train/ctc_loss=5.421168804168701, train/wer=0.941679, validation/ctc_loss=5.384264945983887, validation/num_examples=5348, validation/wer=0.896618
I0308 02:54:52.864818 139846898132736 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.60505610704422, loss=5.410856246948242
I0308 03:01:24.272365 139846906525440 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.3816901445388794, loss=5.393692493438721
I0308 03:08:30.709166 139846898132736 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.4901757836341858, loss=5.403756141662598
I0308 03:14:57.575793 140017254528832 spec.py:321] Evaluating on the training split.
I0308 03:15:34.761640 140017254528832 spec.py:333] Evaluating on the validation split.
I0308 03:16:18.907105 140017254528832 spec.py:349] Evaluating on the test split.
I0308 03:16:41.510933 140017254528832 submission_runner.py:413] Time since start: 49624.93s, 	Step: 56984, 	{'train/ctc_loss': Array(5.4246964, dtype=float32), 'train/wer': 0.9406376811594203, 'validation/ctc_loss': Array(5.384198, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.366017, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 46150.76512384415, 'total_duration': 49624.93389749527, 'accumulated_submission_time': 46150.76512384415, 'accumulated_eval_time': 3470.200798034668, 'accumulated_logging_time': 1.6559462547302246}
I0308 03:16:41.546109 139846906525440 logging_writer.py:48] [56984] accumulated_eval_time=3470.200798, accumulated_logging_time=1.655946, accumulated_submission_time=46150.765124, global_step=56984, preemption_count=0, score=46150.765124, test/ctc_loss=5.366016864776611, test/num_examples=2472, test/wer=0.899580, total_duration=49624.933897, train/ctc_loss=5.424696445465088, train/wer=0.940638, validation/ctc_loss=5.384198188781738, validation/num_examples=5348, validation/wer=0.896618
I0308 03:16:54.468741 139846898132736 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.3412434160709381, loss=5.400559425354004
I0308 03:23:36.751246 139846906525440 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.5506466031074524, loss=5.391900539398193
I0308 03:30:18.299210 139846906525440 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.6501120924949646, loss=5.43438196182251
I0308 03:37:23.345826 139846898132736 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.4615706205368042, loss=5.428701400756836
I0308 03:40:42.203612 140017254528832 spec.py:321] Evaluating on the training split.
I0308 03:41:19.586396 140017254528832 spec.py:333] Evaluating on the validation split.
I0308 03:42:03.724770 140017254528832 spec.py:349] Evaluating on the test split.
I0308 03:42:26.307465 140017254528832 submission_runner.py:413] Time since start: 51169.73s, 	Step: 58729, 	{'train/ctc_loss': Array(5.421024, dtype=float32), 'train/wer': 0.9384636516603139, 'validation/ctc_loss': Array(5.382616, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3641405, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 47591.339581012726, 'total_duration': 51169.72738194466, 'accumulated_submission_time': 47591.339581012726, 'accumulated_eval_time': 3574.296988248825, 'accumulated_logging_time': 1.7087180614471436}
I0308 03:42:26.346205 139846906525440 logging_writer.py:48] [58729] accumulated_eval_time=3574.296988, accumulated_logging_time=1.708718, accumulated_submission_time=47591.339581, global_step=58729, preemption_count=0, score=47591.339581, test/ctc_loss=5.364140510559082, test/num_examples=2472, test/wer=0.899580, total_duration=51169.727382, train/ctc_loss=5.421023845672607, train/wer=0.938464, validation/ctc_loss=5.38261604309082, validation/num_examples=5348, validation/wer=0.896618
I0308 03:45:52.941777 139846898132736 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.6319035887718201, loss=5.390525817871094
I0308 03:52:36.662217 139846906525440 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.4854958653450012, loss=5.379836559295654
I0308 03:59:25.166285 139846906525440 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.5121004581451416, loss=5.415006637573242
I0308 04:06:20.039859 139846898132736 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.32298457622528076, loss=5.408547401428223
I0308 04:06:26.638378 140017254528832 spec.py:321] Evaluating on the training split.
I0308 04:07:04.268481 140017254528832 spec.py:333] Evaluating on the validation split.
I0308 04:07:49.211913 140017254528832 spec.py:349] Evaluating on the test split.
I0308 04:08:11.887477 140017254528832 submission_runner.py:413] Time since start: 52715.31s, 	Step: 60509, 	{'train/ctc_loss': Array(5.4091215, dtype=float32), 'train/wer': 0.9414500266829305, 'validation/ctc_loss': Array(5.3822837, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.364134, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 49031.55167889595, 'total_duration': 52715.30888700485, 'accumulated_submission_time': 49031.55167889595, 'accumulated_eval_time': 3679.5397565364838, 'accumulated_logging_time': 1.7614917755126953}
I0308 04:08:11.922867 139846906525440 logging_writer.py:48] [60509] accumulated_eval_time=3679.539757, accumulated_logging_time=1.761492, accumulated_submission_time=49031.551679, global_step=60509, preemption_count=0, score=49031.551679, test/ctc_loss=5.364133834838867, test/num_examples=2472, test/wer=0.899580, total_duration=52715.308887, train/ctc_loss=5.409121513366699, train/wer=0.941450, validation/ctc_loss=5.382283687591553, validation/num_examples=5348, validation/wer=0.896618
I0308 04:14:28.612919 139846906525440 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.5146873593330383, loss=5.396374225616455
I0308 04:21:22.953835 139846898132736 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.3881152868270874, loss=5.417255401611328
I0308 04:28:18.855006 139846906525440 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.31842824816703796, loss=5.415255069732666
I0308 04:32:12.425002 140017254528832 spec.py:321] Evaluating on the training split.
I0308 04:32:49.899963 140017254528832 spec.py:333] Evaluating on the validation split.
I0308 04:33:34.858466 140017254528832 spec.py:349] Evaluating on the test split.
I0308 04:33:57.370311 140017254528832 submission_runner.py:413] Time since start: 54260.79s, 	Step: 62303, 	{'train/ctc_loss': Array(5.416031, dtype=float32), 'train/wer': 0.9397403994888694, 'validation/ctc_loss': Array(5.3822937, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3641806, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 50471.970804691315, 'total_duration': 54260.792806625366, 'accumulated_submission_time': 50471.970804691315, 'accumulated_eval_time': 3784.479830980301, 'accumulated_logging_time': 1.8119473457336426}
I0308 04:33:57.415859 139846906525440 logging_writer.py:48] [62303] accumulated_eval_time=3784.479831, accumulated_logging_time=1.811947, accumulated_submission_time=50471.970805, global_step=62303, preemption_count=0, score=50471.970805, test/ctc_loss=5.364180564880371, test/num_examples=2472, test/wer=0.899580, total_duration=54260.792807, train/ctc_loss=5.4160308837890625, train/wer=0.939740, validation/ctc_loss=5.382293701171875, validation/num_examples=5348, validation/wer=0.896618
I0308 04:36:27.645208 139846898132736 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.4191361665725708, loss=5.405299186706543
I0308 04:43:17.510943 139846906525440 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.4070851504802704, loss=5.402647495269775
I0308 04:50:00.559317 139846898132736 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.3929999768733978, loss=5.395596504211426
I0308 04:57:05.503293 139846906525440 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.4591711163520813, loss=5.3967413902282715
I0308 04:57:57.428494 140017254528832 spec.py:321] Evaluating on the training split.
I0308 04:58:34.421695 140017254528832 spec.py:333] Evaluating on the validation split.
I0308 04:59:19.164980 140017254528832 spec.py:349] Evaluating on the test split.
I0308 04:59:41.530730 140017254528832 submission_runner.py:413] Time since start: 55804.95s, 	Step: 64070, 	{'train/ctc_loss': Array(5.4193106, dtype=float32), 'train/wer': 0.9386609465565241, 'validation/ctc_loss': Array(5.3822846, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.364198, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 51911.893756866455, 'total_duration': 55804.953489780426, 'accumulated_submission_time': 51911.893756866455, 'accumulated_eval_time': 3888.5770647525787, 'accumulated_logging_time': 1.881136178970337}
I0308 04:59:41.564267 139846906525440 logging_writer.py:48] [64070] accumulated_eval_time=3888.577065, accumulated_logging_time=1.881136, accumulated_submission_time=51911.893757, global_step=64070, preemption_count=0, score=51911.893757, test/ctc_loss=5.364198207855225, test/num_examples=2472, test/wer=0.899580, total_duration=55804.953490, train/ctc_loss=5.419310569763184, train/wer=0.938661, validation/ctc_loss=5.382284641265869, validation/num_examples=5348, validation/wer=0.896618
I0308 05:05:17.105767 139846898132736 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.49956461787223816, loss=5.415963649749756
I0308 05:12:20.512621 139846906525440 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.40306541323661804, loss=5.414709568023682
I0308 05:18:55.334355 139846898132736 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.5862102508544922, loss=5.419684410095215
I0308 05:23:41.663449 140017254528832 spec.py:321] Evaluating on the training split.
I0308 05:24:18.838305 140017254528832 spec.py:333] Evaluating on the validation split.
I0308 05:25:03.310577 140017254528832 spec.py:349] Evaluating on the test split.
I0308 05:25:25.963508 140017254528832 submission_runner.py:413] Time since start: 57349.39s, 	Step: 65836, 	{'train/ctc_loss': Array(5.4082317, dtype=float32), 'train/wer': 0.9400080346382181, 'validation/ctc_loss': Array(5.3822803, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.364198, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 53351.90572834015, 'total_duration': 57349.386419057846, 'accumulated_submission_time': 53351.90572834015, 'accumulated_eval_time': 3992.872305393219, 'accumulated_logging_time': 1.9347491264343262}
I0308 05:25:25.997869 139846906525440 logging_writer.py:48] [65836] accumulated_eval_time=3992.872305, accumulated_logging_time=1.934749, accumulated_submission_time=53351.905728, global_step=65836, preemption_count=0, score=53351.905728, test/ctc_loss=5.364198207855225, test/num_examples=2472, test/wer=0.899580, total_duration=57349.386419, train/ctc_loss=5.408231735229492, train/wer=0.940008, validation/ctc_loss=5.382280349731445, validation/num_examples=5348, validation/wer=0.896618
I0308 05:27:34.605262 139846906525440 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.4503934681415558, loss=5.409567356109619
I0308 05:34:09.627659 139846898132736 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.5567088723182678, loss=5.398810863494873
I0308 05:41:17.448173 139846906525440 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.5052833557128906, loss=5.395885944366455
I0308 05:47:44.662309 139846898132736 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.36724039912223816, loss=5.4168901443481445
I0308 05:49:26.238771 140017254528832 spec.py:321] Evaluating on the training split.
I0308 05:50:03.659861 140017254528832 spec.py:333] Evaluating on the validation split.
I0308 05:50:47.963344 140017254528832 spec.py:349] Evaluating on the test split.
I0308 05:51:10.578193 140017254528832 submission_runner.py:413] Time since start: 58894.00s, 	Step: 67618, 	{'train/ctc_loss': Array(5.4043484, dtype=float32), 'train/wer': 0.938782830496991, 'validation/ctc_loss': Array(5.382287, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3642063, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 54792.06509613991, 'total_duration': 58893.9998357296, 'accumulated_submission_time': 54792.06509613991, 'accumulated_eval_time': 4097.205637931824, 'accumulated_logging_time': 1.983492136001587}
I0308 05:51:10.613439 139846906525440 logging_writer.py:48] [67618] accumulated_eval_time=4097.205638, accumulated_logging_time=1.983492, accumulated_submission_time=54792.065096, global_step=67618, preemption_count=0, score=54792.065096, test/ctc_loss=5.364206314086914, test/num_examples=2472, test/wer=0.899580, total_duration=58893.999836, train/ctc_loss=5.404348373413086, train/wer=0.938783, validation/ctc_loss=5.38228702545166, validation/num_examples=5348, validation/wer=0.896618
I0308 05:56:11.286912 139846906525440 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.4970661699771881, loss=5.4157609939575195
I0308 06:02:38.814781 139846898132736 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.2732942998409271, loss=5.399616718292236
I0308 06:09:56.606319 139846906525440 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.37716424465179443, loss=5.40311336517334
I0308 06:15:11.181669 140017254528832 spec.py:321] Evaluating on the training split.
I0308 06:15:48.420522 140017254528832 spec.py:333] Evaluating on the validation split.
I0308 06:16:33.431876 140017254528832 spec.py:349] Evaluating on the test split.
I0308 06:16:56.013494 140017254528832 submission_runner.py:413] Time since start: 60439.44s, 	Step: 69410, 	{'train/ctc_loss': Array(5.403235, dtype=float32), 'train/wer': 0.9398940095257262, 'validation/ctc_loss': Array(5.3822827, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3642, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 56232.551755428314, 'total_duration': 60439.435509204865, 'accumulated_submission_time': 56232.551755428314, 'accumulated_eval_time': 4202.0318121910095, 'accumulated_logging_time': 2.033263921737671}
I0308 06:16:56.055621 139846906525440 logging_writer.py:48] [69410] accumulated_eval_time=4202.031812, accumulated_logging_time=2.033264, accumulated_submission_time=56232.551755, global_step=69410, preemption_count=0, score=56232.551755, test/ctc_loss=5.364200115203857, test/num_examples=2472, test/wer=0.899580, total_duration=60439.435509, train/ctc_loss=5.403234958648682, train/wer=0.939894, validation/ctc_loss=5.382282733917236, validation/num_examples=5348, validation/wer=0.896618
I0308 06:18:05.115447 139846898132736 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.43375903367996216, loss=5.437784194946289
I0308 06:24:54.255109 139846906525440 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.3769674301147461, loss=5.403590679168701
I0308 06:31:20.889686 139846906525440 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.385553240776062, loss=5.399128437042236
I0308 06:38:32.617402 139846898132736 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.5123043060302734, loss=5.411776065826416
I0308 06:40:56.528425 140017254528832 spec.py:321] Evaluating on the training split.
I0308 06:41:33.705897 140017254528832 spec.py:333] Evaluating on the validation split.
I0308 06:42:18.332796 140017254528832 spec.py:349] Evaluating on the test split.
I0308 06:42:40.839584 140017254528832 submission_runner.py:413] Time since start: 61984.26s, 	Step: 71179, 	{'train/ctc_loss': Array(5.414238, dtype=float32), 'train/wer': 0.9404934527301506, 'validation/ctc_loss': Array(5.3822856, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3642035, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 57672.94034910202, 'total_duration': 61984.26207089424, 'accumulated_submission_time': 57672.94034910202, 'accumulated_eval_time': 4306.3378846645355, 'accumulated_logging_time': 2.0926036834716797}
I0308 06:42:40.874250 139846906525440 logging_writer.py:48] [71179] accumulated_eval_time=4306.337885, accumulated_logging_time=2.092604, accumulated_submission_time=57672.940349, global_step=71179, preemption_count=0, score=57672.940349, test/ctc_loss=5.364203453063965, test/num_examples=2472, test/wer=0.899580, total_duration=61984.262071, train/ctc_loss=5.414237976074219, train/wer=0.940493, validation/ctc_loss=5.3822855949401855, validation/num_examples=5348, validation/wer=0.896618
I0308 06:46:45.211113 139846898132736 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.493381142616272, loss=5.400367736816406
I0308 06:53:41.067377 139846906525440 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.5126936435699463, loss=5.392576217651367
I0308 07:00:12.472294 139846906525440 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.5815618634223938, loss=5.3826093673706055
I0308 07:06:41.550747 140017254528832 spec.py:321] Evaluating on the training split.
I0308 07:07:19.097012 140017254528832 spec.py:333] Evaluating on the validation split.
I0308 07:08:03.919572 140017254528832 spec.py:349] Evaluating on the test split.
I0308 07:08:27.157239 140017254528832 submission_runner.py:413] Time since start: 63530.58s, 	Step: 72941, 	{'train/ctc_loss': Array(5.410898, dtype=float32), 'train/wer': 0.939210620049414, 'validation/ctc_loss': Array(5.382282, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3642, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 59113.53583741188, 'total_duration': 63530.578117609024, 'accumulated_submission_time': 59113.53583741188, 'accumulated_eval_time': 4411.937520027161, 'accumulated_logging_time': 2.141740560531616}
I0308 07:08:27.200048 139846906525440 logging_writer.py:48] [72941] accumulated_eval_time=4411.937520, accumulated_logging_time=2.141741, accumulated_submission_time=59113.535837, global_step=72941, preemption_count=0, score=59113.535837, test/ctc_loss=5.364200115203857, test/num_examples=2472, test/wer=0.899580, total_duration=63530.578118, train/ctc_loss=5.410898208618164, train/wer=0.939211, validation/ctc_loss=5.38228178024292, validation/num_examples=5348, validation/wer=0.896618
I0308 07:09:12.661384 139846898132736 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.5170212984085083, loss=5.410888195037842
I0308 07:15:37.119532 139846906525440 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.9298620820045471, loss=5.382929801940918
I0308 07:22:53.172554 139846898132736 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.6016197800636292, loss=5.406298637390137
I0308 07:29:34.017762 139846906525440 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.5573897361755371, loss=5.397802829742432
I0308 07:32:27.572043 140017254528832 spec.py:321] Evaluating on the training split.
I0308 07:33:04.856343 140017254528832 spec.py:333] Evaluating on the validation split.
I0308 07:33:49.061362 140017254528832 spec.py:349] Evaluating on the test split.
I0308 07:34:11.948129 140017254528832 submission_runner.py:413] Time since start: 65075.37s, 	Step: 74712, 	{'train/ctc_loss': Array(5.4066095, dtype=float32), 'train/wer': 0.9391004400944759, 'validation/ctc_loss': Array(5.3822885, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3642063, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 60553.82614278793, 'total_duration': 65075.370559453964, 'accumulated_submission_time': 60553.82614278793, 'accumulated_eval_time': 4516.308297872543, 'accumulated_logging_time': 2.1996893882751465}
I0308 07:34:11.984563 139846906525440 logging_writer.py:48] [74712] accumulated_eval_time=4516.308298, accumulated_logging_time=2.199689, accumulated_submission_time=60553.826143, global_step=74712, preemption_count=0, score=60553.826143, test/ctc_loss=5.364206314086914, test/num_examples=2472, test/wer=0.899580, total_duration=65075.370559, train/ctc_loss=5.406609535217285, train/wer=0.939100, validation/ctc_loss=5.382288455963135, validation/num_examples=5348, validation/wer=0.896618
I0308 07:37:51.278180 139846898132736 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.41678375005722046, loss=5.439233303070068
I0308 07:42:46.497491 139846906525440 logging_writer.py:48] [75357] global_step=75357, preemption_count=0, score=61068.276282
I0308 07:42:47.347454 140017254528832 checkpoints.py:490] Saving checkpoint at step: 75357
I0308 07:42:48.837914 140017254528832 checkpoints.py:422] Saved checkpoint at /experiment_runs/variants_target_setting/study_0/librispeech_conformer_attention_temperature_jax/trial_1/checkpoint_75357
I0308 07:42:48.870009 140017254528832 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_attention_temperature_jax/trial_1/checkpoint_75357.
I0308 07:42:52.429827 140017254528832 submission_runner.py:588] Tuning trial 1/1
I0308 07:42:52.430082 140017254528832 submission_runner.py:589] Hyperparameters: Hyperparameters(learning_rate=0.001308209823469072, beta1=0.9731333693827139, beta2=0.9981232922116359, warmup_steps=9999, weight_decay=0.16375311233774334)
I0308 07:42:52.453578 140017254528832 submission_runner.py:590] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.646616, dtype=float32), 'train/wer': 0.9441569664535743, 'validation/ctc_loss': Array(30.636509, dtype=float32), 'validation/wer': 0.8989737103797175, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.752209, dtype=float32), 'test/wer': 0.9018138240610972, 'test/num_examples': 2472, 'score': 61.52786350250244, 'total_duration': 203.23771166801453, 'accumulated_submission_time': 61.52786350250244, 'accumulated_eval_time': 141.70978260040283, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1761, {'train/ctc_loss': Array(5.9389296, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(5.964965, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.9309573, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1501.9751040935516, 'total_duration': 1745.8570942878723, 'accumulated_submission_time': 1501.9751040935516, 'accumulated_eval_time': 243.77579021453857, 'accumulated_logging_time': 0.04176211357116699, 'global_step': 1761, 'preemption_count': 0}), (3570, {'train/ctc_loss': Array(5.678523, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': Array(5.6549478, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.644139, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2941.9806768894196, 'total_duration': 3288.545496702194, 'accumulated_submission_time': 2941.9806768894196, 'accumulated_eval_time': 346.3394613265991, 'accumulated_logging_time': 0.08922410011291504, 'global_step': 3570, 'preemption_count': 0}), (5339, {'train/ctc_loss': Array(7.206813, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': Array(9.036783, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(8.352105, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4381.889694213867, 'total_duration': 4831.627207994461, 'accumulated_submission_time': 4381.889694213867, 'accumulated_eval_time': 449.3937108516693, 'accumulated_logging_time': 0.13728642463684082, 'global_step': 5339, 'preemption_count': 0}), (7099, {'train/ctc_loss': Array(7.6976004, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': Array(9.880598, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(9.57103, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 5822.1394028663635, 'total_duration': 6376.600991487503, 'accumulated_submission_time': 5822.1394028663635, 'accumulated_eval_time': 553.9952628612518, 'accumulated_logging_time': 0.1885666847229004, 'global_step': 7099, 'preemption_count': 0}), (8863, {'train/ctc_loss': Array(5.786989, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': Array(5.6355996, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.645039, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7262.676467657089, 'total_duration': 7921.123960733414, 'accumulated_submission_time': 7262.676467657089, 'accumulated_eval_time': 657.8572702407837, 'accumulated_logging_time': 0.2408149242401123, 'global_step': 8863, 'preemption_count': 0}), (10653, {'train/ctc_loss': Array(5.8362217, dtype=float32), 'train/wer': 0.9285253212839623, 'validation/ctc_loss': Array(5.6233115, dtype=float32), 'validation/wer': 0.8941270745435763, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.6422524, dtype=float32), 'test/wer': 0.8957812849105274, 'test/num_examples': 2472, 'score': 8702.776113510132, 'total_duration': 9466.500397920609, 'accumulated_submission_time': 8702.776113510132, 'accumulated_eval_time': 763.006599187851, 'accumulated_logging_time': 0.29567575454711914, 'global_step': 10653, 'preemption_count': 0}), (12417, {'train/ctc_loss': Array(5.510566, dtype=float32), 'train/wer': 0.9294503647927169, 'validation/ctc_loss': Array(5.462328, dtype=float32), 'validation/wer': 0.894300858298657, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4465885, dtype=float32), 'test/wer': 0.8958219080697906, 'test/num_examples': 2472, 'score': 10143.150477647781, 'total_duration': 11013.738177537918, 'accumulated_submission_time': 10143.150477647781, 'accumulated_eval_time': 869.7494282722473, 'accumulated_logging_time': 0.3447258472442627, 'global_step': 12417, 'preemption_count': 0}), (14203, {'train/ctc_loss': Array(5.4679193, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': Array(5.435099, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.412527, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 11583.75929760933, 'total_duration': 12557.975610017776, 'accumulated_submission_time': 11583.75929760933, 'accumulated_eval_time': 973.2495701313019, 'accumulated_logging_time': 0.40009021759033203, 'global_step': 14203, 'preemption_count': 0}), (15983, {'train/ctc_loss': Array(8.226454, dtype=float32), 'train/wer': 0.9427990785714666, 'validation/ctc_loss': Array(7.8200235, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(7.9349136, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 13023.681303739548, 'total_duration': 14102.13569688797, 'accumulated_submission_time': 13023.681303739548, 'accumulated_eval_time': 1077.3698818683624, 'accumulated_logging_time': 0.4471440315246582, 'global_step': 15983, 'preemption_count': 0}), (17771, {'train/ctc_loss': Array(9.104383, dtype=float32), 'train/wer': 0.9423383225986367, 'validation/ctc_loss': Array(8.864255, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(8.938903, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14464.052798748016, 'total_duration': 15646.482597112656, 'accumulated_submission_time': 14464.052798748016, 'accumulated_eval_time': 1181.2217242717743, 'accumulated_logging_time': 0.4993119239807129, 'global_step': 17771, 'preemption_count': 0}), (19522, {'train/ctc_loss': Array(5.4958663, dtype=float32), 'train/wer': 0.9431396916893625, 'validation/ctc_loss': Array(5.467418, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4494553, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15904.257827997208, 'total_duration': 17190.606981039047, 'accumulated_submission_time': 15904.257827997208, 'accumulated_eval_time': 1285.0141394138336, 'accumulated_logging_time': 0.5547926425933838, 'global_step': 19522, 'preemption_count': 0}), (21288, {'train/ctc_loss': Array(6.357177, dtype=float32), 'train/wer': 0.9432716912443612, 'validation/ctc_loss': Array(6.344222, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.3780193, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 17344.80734229088, 'total_duration': 18735.375351190567, 'accumulated_submission_time': 17344.80734229088, 'accumulated_eval_time': 1389.1090364456177, 'accumulated_logging_time': 0.6050007343292236, 'global_step': 21288, 'preemption_count': 0}), (23095, {'train/ctc_loss': Array(5.62643, dtype=float32), 'train/wer': 0.944685667249717, 'validation/ctc_loss': Array(5.537226, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.536327, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 18785.39287829399, 'total_duration': 20280.319013118744, 'accumulated_submission_time': 18785.39287829399, 'accumulated_eval_time': 1493.3436286449432, 'accumulated_logging_time': 0.6555466651916504, 'global_step': 23095, 'preemption_count': 0}), (24866, {'train/ctc_loss': Array(5.5369515, dtype=float32), 'train/wer': 0.9432456399645285, 'validation/ctc_loss': Array(5.5097246, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4994817, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 20225.317187309265, 'total_duration': 21824.630343437195, 'accumulated_submission_time': 20225.317187309265, 'accumulated_eval_time': 1597.6108083724976, 'accumulated_logging_time': 0.7038488388061523, 'global_step': 24866, 'preemption_count': 0}), (26645, {'train/ctc_loss': Array(5.503868, dtype=float32), 'train/wer': 0.9439109001278072, 'validation/ctc_loss': Array(5.458417, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.44211, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 21665.257386922836, 'total_duration': 23368.220731019974, 'accumulated_submission_time': 21665.257386922836, 'accumulated_eval_time': 1701.1358575820923, 'accumulated_logging_time': 0.7563469409942627, 'global_step': 26645, 'preemption_count': 0}), (28440, {'train/ctc_loss': Array(5.4748054, dtype=float32), 'train/wer': 0.9450143703143059, 'validation/ctc_loss': Array(5.4371943, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.417845, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 23105.754850387573, 'total_duration': 24912.41552066803, 'accumulated_submission_time': 23105.754850387573, 'accumulated_eval_time': 1804.7141313552856, 'accumulated_logging_time': 0.8027734756469727, 'global_step': 28440, 'preemption_count': 0}), (30239, {'train/ctc_loss': Array(5.4860835, dtype=float32), 'train/wer': 0.9417576703068122, 'validation/ctc_loss': Array(5.4399314, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.421852, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 24546.3284304142, 'total_duration': 26456.92817234993, 'accumulated_submission_time': 24546.3284304142, 'accumulated_eval_time': 1908.5130717754364, 'accumulated_logging_time': 0.8684427738189697, 'global_step': 30239, 'preemption_count': 0}), (32004, {'train/ctc_loss': Array(5.515235, dtype=float32), 'train/wer': 0.9416600198590335, 'validation/ctc_loss': Array(5.506643, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4905233, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 25986.90826368332, 'total_duration': 28001.848637342453, 'accumulated_submission_time': 25986.90826368332, 'accumulated_eval_time': 2012.7334079742432, 'accumulated_logging_time': 0.9174127578735352, 'global_step': 32004, 'preemption_count': 0}), (33791, {'train/ctc_loss': Array(5.5589013, dtype=float32), 'train/wer': 0.9447677853176417, 'validation/ctc_loss': Array(5.448199, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4482307, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 27427.07439470291, 'total_duration': 29546.137358427048, 'accumulated_submission_time': 27427.07439470291, 'accumulated_eval_time': 2116.731162548065, 'accumulated_logging_time': 0.9689841270446777, 'global_step': 33791, 'preemption_count': 0}), (35586, {'train/ctc_loss': Array(5.4749928, dtype=float32), 'train/wer': 0.9427091658940503, 'validation/ctc_loss': Array(5.4578133, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4331236, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 28867.101755142212, 'total_duration': 31089.976769685745, 'accumulated_submission_time': 28867.101755142212, 'accumulated_eval_time': 2220.417268514633, 'accumulated_logging_time': 1.0206682682037354, 'global_step': 35586, 'preemption_count': 0}), (37388, {'train/ctc_loss': Array(5.6269917, dtype=float32), 'train/wer': 0.9448971433842748, 'validation/ctc_loss': Array(5.5650015, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.562459, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 30307.23932147026, 'total_duration': 32634.37336921692, 'accumulated_submission_time': 30307.23932147026, 'accumulated_eval_time': 2324.5490913391113, 'accumulated_logging_time': 1.0720014572143555, 'global_step': 37388, 'preemption_count': 0}), (39162, {'train/ctc_loss': Array(5.609433, dtype=float32), 'train/wer': 0.9432324554919642, 'validation/ctc_loss': Array(5.6505947, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.625185, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 31747.398077964783, 'total_duration': 34178.32664036751, 'accumulated_submission_time': 31747.398077964783, 'accumulated_eval_time': 2428.2190799713135, 'accumulated_logging_time': 1.1256511211395264, 'global_step': 39162, 'preemption_count': 0}), (40963, {'train/ctc_loss': Array(5.8990645, dtype=float32), 'train/wer': 0.941680272071945, 'validation/ctc_loss': Array(6.010586, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.000703, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 33187.64439082146, 'total_duration': 35723.08069252968, 'accumulated_submission_time': 33187.64439082146, 'accumulated_eval_time': 2532.6028385162354, 'accumulated_logging_time': 1.1763088703155518, 'global_step': 40963, 'preemption_count': 0}), (42779, {'train/ctc_loss': Array(5.428235, dtype=float32), 'train/wer': 0.940312095793757, 'validation/ctc_loss': Array(5.4031405, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3880925, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 34628.318608284, 'total_duration': 37267.20153093338, 'accumulated_submission_time': 34628.318608284, 'accumulated_eval_time': 2635.9222514629364, 'accumulated_logging_time': 1.2293226718902588, 'global_step': 42779, 'preemption_count': 0}), (44557, {'train/ctc_loss': Array(5.4203744, dtype=float32), 'train/wer': 0.9371047844119075, 'validation/ctc_loss': Array(5.401307, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.382077, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 36068.27321147919, 'total_duration': 38812.989600896835, 'accumulated_submission_time': 36068.27321147919, 'accumulated_eval_time': 2741.6304364204407, 'accumulated_logging_time': 1.280510425567627, 'global_step': 44557, 'preemption_count': 0}), (46307, {'train/ctc_loss': Array(5.480328, dtype=float32), 'train/wer': 0.9366967129626904, 'validation/ctc_loss': Array(5.5026855, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4791045, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 37508.74952530861, 'total_duration': 40357.14412713051, 'accumulated_submission_time': 37508.74952530861, 'accumulated_eval_time': 2845.1845121383667, 'accumulated_logging_time': 1.3334717750549316, 'global_step': 46307, 'preemption_count': 0}), (48081, {'train/ctc_loss': Array(5.549283, dtype=float32), 'train/wer': 0.9401422956587576, 'validation/ctc_loss': Array(5.516625, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.4945273, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 38949.04762458801, 'total_duration': 41901.40736937523, 'accumulated_submission_time': 38949.04762458801, 'accumulated_eval_time': 2949.0222511291504, 'accumulated_logging_time': 1.3872625827789307, 'global_step': 48081, 'preemption_count': 0}), (49883, {'train/ctc_loss': Array(5.442956, dtype=float32), 'train/wer': 0.9415328062295403, 'validation/ctc_loss': Array(5.413966, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3947864, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 40389.62878370285, 'total_duration': 43447.31326317787, 'accumulated_submission_time': 40389.62878370285, 'accumulated_eval_time': 3054.2239389419556, 'accumulated_logging_time': 1.436805009841919, 'global_step': 49883, 'preemption_count': 0}), (51644, {'train/ctc_loss': Array(5.4087667, dtype=float32), 'train/wer': 0.941605522275386, 'validation/ctc_loss': Array(5.388261, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3714447, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 41829.744005680084, 'total_duration': 44991.162705898285, 'accumulated_submission_time': 41829.744005680084, 'accumulated_eval_time': 3157.832223176956, 'accumulated_logging_time': 1.4919836521148682, 'global_step': 51644, 'preemption_count': 0}), (53410, {'train/ctc_loss': Array(5.411853, dtype=float32), 'train/wer': 0.9422779591135544, 'validation/ctc_loss': Array(5.382742, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.365683, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 43270.42512345314, 'total_duration': 46536.05523133278, 'accumulated_submission_time': 43270.42512345314, 'accumulated_eval_time': 3261.918598175049, 'accumulated_logging_time': 1.544386386871338, 'global_step': 53410, 'preemption_count': 0}), (55191, {'train/ctc_loss': Array(5.421169, dtype=float32), 'train/wer': 0.9416786903741633, 'validation/ctc_loss': Array(5.384265, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.36615, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 44710.594696998596, 'total_duration': 48080.70483779907, 'accumulated_submission_time': 44710.594696998596, 'accumulated_eval_time': 3366.2702553272247, 'accumulated_logging_time': 1.6008696556091309, 'global_step': 55191, 'preemption_count': 0}), (56984, {'train/ctc_loss': Array(5.4246964, dtype=float32), 'train/wer': 0.9406376811594203, 'validation/ctc_loss': Array(5.384198, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.366017, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 46150.76512384415, 'total_duration': 49624.93389749527, 'accumulated_submission_time': 46150.76512384415, 'accumulated_eval_time': 3470.200798034668, 'accumulated_logging_time': 1.6559462547302246, 'global_step': 56984, 'preemption_count': 0}), (58729, {'train/ctc_loss': Array(5.421024, dtype=float32), 'train/wer': 0.9384636516603139, 'validation/ctc_loss': Array(5.382616, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3641405, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 47591.339581012726, 'total_duration': 51169.72738194466, 'accumulated_submission_time': 47591.339581012726, 'accumulated_eval_time': 3574.296988248825, 'accumulated_logging_time': 1.7087180614471436, 'global_step': 58729, 'preemption_count': 0}), (60509, {'train/ctc_loss': Array(5.4091215, dtype=float32), 'train/wer': 0.9414500266829305, 'validation/ctc_loss': Array(5.3822837, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.364134, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 49031.55167889595, 'total_duration': 52715.30888700485, 'accumulated_submission_time': 49031.55167889595, 'accumulated_eval_time': 3679.5397565364838, 'accumulated_logging_time': 1.7614917755126953, 'global_step': 60509, 'preemption_count': 0}), (62303, {'train/ctc_loss': Array(5.416031, dtype=float32), 'train/wer': 0.9397403994888694, 'validation/ctc_loss': Array(5.3822937, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3641806, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 50471.970804691315, 'total_duration': 54260.792806625366, 'accumulated_submission_time': 50471.970804691315, 'accumulated_eval_time': 3784.479830980301, 'accumulated_logging_time': 1.8119473457336426, 'global_step': 62303, 'preemption_count': 0}), (64070, {'train/ctc_loss': Array(5.4193106, dtype=float32), 'train/wer': 0.9386609465565241, 'validation/ctc_loss': Array(5.3822846, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.364198, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 51911.893756866455, 'total_duration': 55804.953489780426, 'accumulated_submission_time': 51911.893756866455, 'accumulated_eval_time': 3888.5770647525787, 'accumulated_logging_time': 1.881136178970337, 'global_step': 64070, 'preemption_count': 0}), (65836, {'train/ctc_loss': Array(5.4082317, dtype=float32), 'train/wer': 0.9400080346382181, 'validation/ctc_loss': Array(5.3822803, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.364198, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 53351.90572834015, 'total_duration': 57349.386419057846, 'accumulated_submission_time': 53351.90572834015, 'accumulated_eval_time': 3992.872305393219, 'accumulated_logging_time': 1.9347491264343262, 'global_step': 65836, 'preemption_count': 0}), (67618, {'train/ctc_loss': Array(5.4043484, dtype=float32), 'train/wer': 0.938782830496991, 'validation/ctc_loss': Array(5.382287, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3642063, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 54792.06509613991, 'total_duration': 58893.9998357296, 'accumulated_submission_time': 54792.06509613991, 'accumulated_eval_time': 4097.205637931824, 'accumulated_logging_time': 1.983492136001587, 'global_step': 67618, 'preemption_count': 0}), (69410, {'train/ctc_loss': Array(5.403235, dtype=float32), 'train/wer': 0.9398940095257262, 'validation/ctc_loss': Array(5.3822827, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3642, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 56232.551755428314, 'total_duration': 60439.435509204865, 'accumulated_submission_time': 56232.551755428314, 'accumulated_eval_time': 4202.0318121910095, 'accumulated_logging_time': 2.033263921737671, 'global_step': 69410, 'preemption_count': 0}), (71179, {'train/ctc_loss': Array(5.414238, dtype=float32), 'train/wer': 0.9404934527301506, 'validation/ctc_loss': Array(5.3822856, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3642035, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 57672.94034910202, 'total_duration': 61984.26207089424, 'accumulated_submission_time': 57672.94034910202, 'accumulated_eval_time': 4306.3378846645355, 'accumulated_logging_time': 2.0926036834716797, 'global_step': 71179, 'preemption_count': 0}), (72941, {'train/ctc_loss': Array(5.410898, dtype=float32), 'train/wer': 0.939210620049414, 'validation/ctc_loss': Array(5.382282, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3642, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 59113.53583741188, 'total_duration': 63530.578117609024, 'accumulated_submission_time': 59113.53583741188, 'accumulated_eval_time': 4411.937520027161, 'accumulated_logging_time': 2.141740560531616, 'global_step': 72941, 'preemption_count': 0}), (74712, {'train/ctc_loss': Array(5.4066095, dtype=float32), 'train/wer': 0.9391004400944759, 'validation/ctc_loss': Array(5.3822885, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(5.3642063, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 60553.82614278793, 'total_duration': 65075.370559453964, 'accumulated_submission_time': 60553.82614278793, 'accumulated_eval_time': 4516.308297872543, 'accumulated_logging_time': 2.1996893882751465, 'global_step': 74712, 'preemption_count': 0})], 'global_step': 75357}
I0308 07:42:52.453816 140017254528832 submission_runner.py:591] Timing: 61068.27628207207
I0308 07:42:52.453879 140017254528832 submission_runner.py:593] Total number of evals: 43
I0308 07:42:52.453951 140017254528832 submission_runner.py:594] ====================
I0308 07:42:52.459062 140017254528832 submission_runner.py:678] Final librispeech_conformer_attention_temperature score: 61068.27628207207
