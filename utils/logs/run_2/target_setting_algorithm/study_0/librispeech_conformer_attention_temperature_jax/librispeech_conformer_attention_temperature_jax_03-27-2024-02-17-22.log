python3 submission_runner.py --framework=jax --workload=librispeech_conformer_attention_temperature --submission_path=reference_algorithms/target_setting_algorithms/jax_adamw.py --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --rng_seed=1706992344 --max_global_steps=80000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab --tuning_ruleset=external --tuning_search_space=reference_algorithms/target_setting_algorithms/librispeech_conformer_attention_temperature/tuning_search_space.json --num_tuning_trials=1 2>&1 | tee -a /logs/librispeech_conformer_attention_temperature_jax_03-27-2024-02-17-22.log
I0327 02:17:46.327191 140406042187584 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/librispeech_conformer_attention_temperature_jax.
I0327 02:17:47.479959 140406042187584 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter CUDA Host
I0327 02:17:47.480709 140406042187584 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0327 02:17:47.480854 140406042187584 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0327 02:17:47.487626 140406042187584 submission_runner.py:557] Using RNG seed 1706992344
I0327 02:17:48.774506 140406042187584 submission_runner.py:566] --- Tuning run 1/1 ---
I0327 02:17:48.774717 140406042187584 submission_runner.py:571] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/librispeech_conformer_attention_temperature_jax/trial_1.
I0327 02:17:48.775119 140406042187584 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_attention_temperature_jax/trial_1/hparams.json.
I0327 02:17:48.953316 140406042187584 submission_runner.py:211] Initializing dataset.
I0327 02:17:48.953531 140406042187584 submission_runner.py:222] Initializing model.
I0327 02:17:53.588081 140406042187584 submission_runner.py:264] Initializing optimizer.
I0327 02:17:54.758564 140406042187584 submission_runner.py:271] Initializing metrics bundle.
I0327 02:17:54.758759 140406042187584 submission_runner.py:289] Initializing checkpoint and logger.
I0327 02:17:54.759948 140406042187584 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/librispeech_conformer_attention_temperature_jax/trial_1 with prefix checkpoint_
I0327 02:17:54.760100 140406042187584 submission_runner.py:309] Saving meta data to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_attention_temperature_jax/trial_1/meta_data_0.json.
I0327 02:17:54.760574 140406042187584 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0327 02:17:54.760696 140406042187584 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0327 02:17:55.017973 140406042187584 logger_utils.py:220] Unable to record git information. Continuing without it.
I0327 02:17:55.256227 140406042187584 submission_runner.py:313] Saving flags to /experiment_runs/variants_target_setting/study_0/librispeech_conformer_attention_temperature_jax/trial_1/flags_0.json.
I0327 02:17:55.272855 140406042187584 submission_runner.py:323] Starting training loop.
I0327 02:17:55.555659 140406042187584 input_pipeline.py:20] Loading split = train-clean-100
I0327 02:17:55.621804 140406042187584 input_pipeline.py:20] Loading split = train-clean-360
I0327 02:17:56.009023 140406042187584 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0327 02:18:54.627281 140228586612480 logging_writer.py:48] [0] global_step=0, grad_norm=53.91887664794922, loss=31.47677230834961
I0327 02:18:54.664630 140406042187584 spec.py:321] Evaluating on the training split.
I0327 02:18:54.827584 140406042187584 input_pipeline.py:20] Loading split = train-clean-100
I0327 02:18:54.861366 140406042187584 input_pipeline.py:20] Loading split = train-clean-360
I0327 02:18:55.229217 140406042187584 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0327 02:19:50.000183 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 02:19:50.112110 140406042187584 input_pipeline.py:20] Loading split = dev-clean
I0327 02:19:50.117702 140406042187584 input_pipeline.py:20] Loading split = dev-other
I0327 02:20:47.328461 140406042187584 spec.py:349] Evaluating on the test split.
I0327 02:20:47.439332 140406042187584 input_pipeline.py:20] Loading split = test-clean
I0327 02:21:19.953998 140406042187584 submission_runner.py:422] Time since start: 204.68s, 	Step: 1, 	{'train/ctc_loss': Array(31.331608, dtype=float32), 'train/wer': 0.9451200466443163, 'validation/ctc_loss': Array(30.673698, dtype=float32), 'validation/wer': 0.9064850304604304, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.753916, dtype=float32), 'test/wer': 0.9097150285377694, 'test/num_examples': 2472, 'score': 59.39172101020813, 'total_duration': 204.67879557609558, 'accumulated_submission_time': 59.39172101020813, 'accumulated_eval_time': 145.28702449798584, 'accumulated_logging_time': 0}
I0327 02:21:19.981483 140219873490688 logging_writer.py:48] [1] accumulated_eval_time=145.287024, accumulated_logging_time=0, accumulated_submission_time=59.391721, global_step=1, preemption_count=0, score=59.391721, test/ctc_loss=30.753915786743164, test/num_examples=2472, test/wer=0.909715, total_duration=204.678796, train/ctc_loss=31.331607818603516, train/wer=0.945120, validation/ctc_loss=30.67369842529297, validation/num_examples=5348, validation/wer=0.906485
I0327 02:21:42.426519 140234416137984 logging_writer.py:48] [1] global_step=1, grad_norm=54.7853889465332, loss=32.241939544677734
I0327 02:21:43.336082 140234424530688 logging_writer.py:48] [2] global_step=2, grad_norm=60.85182189941406, loss=31.78856658935547
I0327 02:21:44.172403 140234416137984 logging_writer.py:48] [3] global_step=3, grad_norm=60.265281677246094, loss=31.835315704345703
I0327 02:21:45.012326 140234424530688 logging_writer.py:48] [4] global_step=4, grad_norm=51.199954986572266, loss=32.223846435546875
I0327 02:21:45.912360 140234416137984 logging_writer.py:48] [5] global_step=5, grad_norm=54.41377258300781, loss=31.413957595825195
I0327 02:21:46.817427 140234424530688 logging_writer.py:48] [6] global_step=6, grad_norm=54.4508056640625, loss=30.788476943969727
I0327 02:21:47.723352 140234416137984 logging_writer.py:48] [7] global_step=7, grad_norm=62.23462677001953, loss=31.84909439086914
I0327 02:21:48.632313 140234424530688 logging_writer.py:48] [8] global_step=8, grad_norm=63.266998291015625, loss=31.586515426635742
I0327 02:21:49.536736 140234416137984 logging_writer.py:48] [9] global_step=9, grad_norm=66.5223159790039, loss=31.395383834838867
I0327 02:21:50.440473 140234424530688 logging_writer.py:48] [10] global_step=10, grad_norm=67.82760620117188, loss=30.358938217163086
I0327 02:21:51.340587 140234416137984 logging_writer.py:48] [11] global_step=11, grad_norm=74.91828918457031, loss=31.11846351623535
I0327 02:21:52.274797 140234424530688 logging_writer.py:48] [12] global_step=12, grad_norm=79.54319763183594, loss=30.56407928466797
I0327 02:21:53.187816 140234416137984 logging_writer.py:48] [13] global_step=13, grad_norm=87.043701171875, loss=29.8964786529541
I0327 02:21:54.099603 140234424530688 logging_writer.py:48] [14] global_step=14, grad_norm=107.0121841430664, loss=30.244821548461914
I0327 02:21:55.007649 140234416137984 logging_writer.py:48] [15] global_step=15, grad_norm=125.86470031738281, loss=29.733890533447266
I0327 02:21:55.956747 140234424530688 logging_writer.py:48] [16] global_step=16, grad_norm=140.00473022460938, loss=28.635400772094727
I0327 02:21:56.873973 140234416137984 logging_writer.py:48] [17] global_step=17, grad_norm=169.94302368164062, loss=28.565921783447266
I0327 02:21:57.773711 140234424530688 logging_writer.py:48] [18] global_step=18, grad_norm=191.1325225830078, loss=27.155752182006836
I0327 02:21:58.658546 140234416137984 logging_writer.py:48] [19] global_step=19, grad_norm=200.41468811035156, loss=26.184614181518555
I0327 02:21:59.551152 140234424530688 logging_writer.py:48] [20] global_step=20, grad_norm=200.18295288085938, loss=24.48876190185547
I0327 02:22:00.449113 140234416137984 logging_writer.py:48] [21] global_step=21, grad_norm=198.39407348632812, loss=22.801660537719727
I0327 02:22:01.344859 140234424530688 logging_writer.py:48] [22] global_step=22, grad_norm=204.07162475585938, loss=21.84637451171875
I0327 02:22:02.234656 140234416137984 logging_writer.py:48] [23] global_step=23, grad_norm=192.6178741455078, loss=20.16675567626953
I0327 02:22:03.155034 140234424530688 logging_writer.py:48] [24] global_step=24, grad_norm=187.9259033203125, loss=18.645849227905273
I0327 02:22:04.054517 140234416137984 logging_writer.py:48] [25] global_step=25, grad_norm=182.017822265625, loss=17.186466217041016
I0327 02:22:04.953288 140234424530688 logging_writer.py:48] [26] global_step=26, grad_norm=166.1407012939453, loss=15.231428146362305
I0327 02:22:05.848054 140234416137984 logging_writer.py:48] [27] global_step=27, grad_norm=160.54917907714844, loss=14.283297538757324
I0327 02:22:06.745283 140234424530688 logging_writer.py:48] [28] global_step=28, grad_norm=145.2268524169922, loss=12.78059196472168
I0327 02:22:07.650050 140234416137984 logging_writer.py:48] [29] global_step=29, grad_norm=131.40542602539062, loss=11.798730850219727
I0327 02:22:08.554128 140234424530688 logging_writer.py:48] [30] global_step=30, grad_norm=113.21601867675781, loss=10.608637809753418
I0327 02:22:09.452193 140234416137984 logging_writer.py:48] [31] global_step=31, grad_norm=100.46919250488281, loss=9.946602821350098
I0327 02:22:10.356295 140234424530688 logging_writer.py:48] [32] global_step=32, grad_norm=85.14630889892578, loss=9.285802841186523
I0327 02:22:11.265713 140234416137984 logging_writer.py:48] [33] global_step=33, grad_norm=74.89466857910156, loss=8.86287784576416
I0327 02:22:12.169152 140234424530688 logging_writer.py:48] [34] global_step=34, grad_norm=60.10866928100586, loss=8.36248779296875
I0327 02:22:13.061208 140234416137984 logging_writer.py:48] [35] global_step=35, grad_norm=48.37105178833008, loss=8.017853736877441
I0327 02:22:13.968362 140234424530688 logging_writer.py:48] [36] global_step=36, grad_norm=40.128089904785156, loss=7.797854900360107
I0327 02:22:14.870013 140234416137984 logging_writer.py:48] [37] global_step=37, grad_norm=33.290618896484375, loss=7.663711071014404
I0327 02:22:15.779854 140234424530688 logging_writer.py:48] [38] global_step=38, grad_norm=24.927568435668945, loss=7.476781845092773
I0327 02:22:16.687414 140234416137984 logging_writer.py:48] [39] global_step=39, grad_norm=20.77593421936035, loss=7.401798725128174
I0327 02:22:17.587782 140234424530688 logging_writer.py:48] [40] global_step=40, grad_norm=17.747636795043945, loss=7.370049953460693
I0327 02:22:18.480102 140234416137984 logging_writer.py:48] [41] global_step=41, grad_norm=12.357192993164062, loss=7.24031925201416
I0327 02:22:19.392574 140234424530688 logging_writer.py:48] [42] global_step=42, grad_norm=10.163105964660645, loss=7.2310566902160645
I0327 02:22:20.304514 140234416137984 logging_writer.py:48] [43] global_step=43, grad_norm=9.473859786987305, loss=7.218228340148926
I0327 02:22:21.214651 140234424530688 logging_writer.py:48] [44] global_step=44, grad_norm=8.726435661315918, loss=7.227696418762207
I0327 02:22:22.137727 140234416137984 logging_writer.py:48] [45] global_step=45, grad_norm=6.6003947257995605, loss=7.183043003082275
I0327 02:22:23.044277 140234424530688 logging_writer.py:48] [46] global_step=46, grad_norm=5.318282127380371, loss=7.140445709228516
I0327 02:22:23.957925 140234416137984 logging_writer.py:48] [47] global_step=47, grad_norm=6.623309135437012, loss=7.167844772338867
I0327 02:22:24.865546 140234424530688 logging_writer.py:48] [48] global_step=48, grad_norm=6.4031805992126465, loss=7.156707286834717
I0327 02:22:25.774632 140234416137984 logging_writer.py:48] [49] global_step=49, grad_norm=5.390851020812988, loss=7.128708362579346
I0327 02:22:26.687078 140234424530688 logging_writer.py:48] [50] global_step=50, grad_norm=4.9519362449646, loss=7.1012797355651855
I0327 02:22:27.597537 140234416137984 logging_writer.py:48] [51] global_step=51, grad_norm=4.858403205871582, loss=7.099968910217285
I0327 02:22:28.500736 140234424530688 logging_writer.py:48] [52] global_step=52, grad_norm=4.9931488037109375, loss=7.070437431335449
I0327 02:22:29.403038 140234416137984 logging_writer.py:48] [53] global_step=53, grad_norm=4.396590232849121, loss=7.061151504516602
I0327 02:22:30.313486 140234424530688 logging_writer.py:48] [54] global_step=54, grad_norm=4.884206771850586, loss=7.07697868347168
I0327 02:22:31.224961 140234416137984 logging_writer.py:48] [55] global_step=55, grad_norm=4.147730827331543, loss=7.006655693054199
I0327 02:22:32.138056 140234424530688 logging_writer.py:48] [56] global_step=56, grad_norm=4.152077674865723, loss=7.012355804443359
I0327 02:22:33.038554 140234416137984 logging_writer.py:48] [57] global_step=57, grad_norm=4.527945041656494, loss=7.018322467803955
I0327 02:22:33.943052 140234424530688 logging_writer.py:48] [58] global_step=58, grad_norm=4.38419771194458, loss=7.020827770233154
I0327 02:22:34.852463 140234416137984 logging_writer.py:48] [59] global_step=59, grad_norm=3.958439826965332, loss=7.010612487792969
I0327 02:22:35.757937 140234424530688 logging_writer.py:48] [60] global_step=60, grad_norm=3.9086198806762695, loss=6.991506099700928
I0327 02:22:36.661252 140234416137984 logging_writer.py:48] [61] global_step=61, grad_norm=4.478835582733154, loss=6.986907958984375
I0327 02:22:37.569754 140234424530688 logging_writer.py:48] [62] global_step=62, grad_norm=3.8235623836517334, loss=6.980496883392334
I0327 02:22:38.473290 140234416137984 logging_writer.py:48] [63] global_step=63, grad_norm=3.8920578956604004, loss=6.972895622253418
I0327 02:22:39.375986 140234424530688 logging_writer.py:48] [64] global_step=64, grad_norm=4.5007243156433105, loss=6.9728827476501465
I0327 02:22:40.306134 140234416137984 logging_writer.py:48] [65] global_step=65, grad_norm=4.097631931304932, loss=6.954410552978516
I0327 02:22:41.211300 140234424530688 logging_writer.py:48] [66] global_step=66, grad_norm=3.8804004192352295, loss=6.927919864654541
I0327 02:22:42.112036 140234416137984 logging_writer.py:48] [67] global_step=67, grad_norm=4.069989204406738, loss=6.929276466369629
I0327 02:22:43.016411 140234424530688 logging_writer.py:48] [68] global_step=68, grad_norm=3.761495590209961, loss=6.895094871520996
I0327 02:22:43.919170 140234416137984 logging_writer.py:48] [69] global_step=69, grad_norm=3.6995935440063477, loss=6.922958850860596
I0327 02:22:44.827130 140234424530688 logging_writer.py:48] [70] global_step=70, grad_norm=3.973830223083496, loss=6.907037258148193
I0327 02:22:45.738225 140234416137984 logging_writer.py:48] [71] global_step=71, grad_norm=3.792210102081299, loss=6.89626407623291
I0327 02:22:46.642976 140234424530688 logging_writer.py:48] [72] global_step=72, grad_norm=3.9307632446289062, loss=6.885485649108887
I0327 02:22:47.545524 140234416137984 logging_writer.py:48] [73] global_step=73, grad_norm=3.536053419113159, loss=6.875877857208252
I0327 02:22:48.456750 140234424530688 logging_writer.py:48] [74] global_step=74, grad_norm=3.382131576538086, loss=6.859299659729004
I0327 02:22:49.367225 140234416137984 logging_writer.py:48] [75] global_step=75, grad_norm=3.8179125785827637, loss=6.853476524353027
I0327 02:22:50.287210 140234424530688 logging_writer.py:48] [76] global_step=76, grad_norm=3.8158206939697266, loss=6.850275993347168
I0327 02:22:51.187273 140234416137984 logging_writer.py:48] [77] global_step=77, grad_norm=3.5847952365875244, loss=6.850305557250977
I0327 02:22:52.096392 140234424530688 logging_writer.py:48] [78] global_step=78, grad_norm=3.255598306655884, loss=6.80948543548584
I0327 02:22:53.003847 140234416137984 logging_writer.py:48] [79] global_step=79, grad_norm=3.4444735050201416, loss=6.837802886962891
I0327 02:22:53.915282 140234424530688 logging_writer.py:48] [80] global_step=80, grad_norm=3.2473437786102295, loss=6.813376426696777
I0327 02:22:54.833039 140234416137984 logging_writer.py:48] [81] global_step=81, grad_norm=3.262094259262085, loss=6.800297260284424
I0327 02:22:55.731967 140234424530688 logging_writer.py:48] [82] global_step=82, grad_norm=3.1326770782470703, loss=6.787019729614258
I0327 02:22:56.641381 140234416137984 logging_writer.py:48] [83] global_step=83, grad_norm=3.3057689666748047, loss=6.783987522125244
I0327 02:22:57.553406 140234424530688 logging_writer.py:48] [84] global_step=84, grad_norm=3.609693765640259, loss=6.786221027374268
I0327 02:22:58.463627 140234416137984 logging_writer.py:48] [85] global_step=85, grad_norm=3.4121055603027344, loss=6.784107685089111
I0327 02:22:59.373189 140234424530688 logging_writer.py:48] [86] global_step=86, grad_norm=3.3726186752319336, loss=6.756728649139404
I0327 02:23:00.282310 140234416137984 logging_writer.py:48] [87] global_step=87, grad_norm=3.208096742630005, loss=6.744716167449951
I0327 02:23:01.195134 140234424530688 logging_writer.py:48] [88] global_step=88, grad_norm=3.007735252380371, loss=6.754623889923096
I0327 02:23:02.101134 140234416137984 logging_writer.py:48] [89] global_step=89, grad_norm=2.77030873298645, loss=6.737191200256348
I0327 02:23:03.046530 140234424530688 logging_writer.py:48] [90] global_step=90, grad_norm=2.782562494277954, loss=6.731991767883301
I0327 02:23:03.954353 140234416137984 logging_writer.py:48] [91] global_step=91, grad_norm=2.92398738861084, loss=6.720067024230957
I0327 02:23:04.865202 140234424530688 logging_writer.py:48] [92] global_step=92, grad_norm=2.9104297161102295, loss=6.709112644195557
I0327 02:23:05.771340 140234416137984 logging_writer.py:48] [93] global_step=93, grad_norm=2.820587635040283, loss=6.697323322296143
I0327 02:23:06.694905 140234424530688 logging_writer.py:48] [94] global_step=94, grad_norm=3.107537269592285, loss=6.698222637176514
I0327 02:23:07.599339 140234416137984 logging_writer.py:48] [95] global_step=95, grad_norm=3.348900318145752, loss=6.690446853637695
I0327 02:23:08.505279 140234424530688 logging_writer.py:48] [96] global_step=96, grad_norm=2.63852858543396, loss=6.66919469833374
I0327 02:23:09.415718 140234416137984 logging_writer.py:48] [97] global_step=97, grad_norm=2.6254146099090576, loss=6.668227195739746
I0327 02:23:10.329115 140234424530688 logging_writer.py:48] [98] global_step=98, grad_norm=2.664494752883911, loss=6.668162822723389
I0327 02:23:11.237036 140234416137984 logging_writer.py:48] [99] global_step=99, grad_norm=2.559898614883423, loss=6.632085800170898
I0327 02:23:12.165881 140234424530688 logging_writer.py:48] [100] global_step=100, grad_norm=2.5139176845550537, loss=6.654201507568359
I0327 02:28:37.656801 140234416137984 logging_writer.py:48] [500] global_step=500, grad_norm=7.321263790130615, loss=5.841272354125977
I0327 02:36:18.819053 140234424530688 logging_writer.py:48] [1000] global_step=1000, grad_norm=1.975119948387146, loss=5.7829365730285645
I0327 02:43:08.172728 140234491672320 logging_writer.py:48] [1500] global_step=1500, grad_norm=1.649586796760559, loss=5.5523834228515625
I0327 02:45:19.986716 140406042187584 spec.py:321] Evaluating on the training split.
I0327 02:45:55.573875 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 02:46:40.828135 140406042187584 spec.py:349] Evaluating on the test split.
I0327 02:47:02.910182 140406042187584 submission_runner.py:422] Time since start: 1747.63s, 	Step: 1649, 	{'train/ctc_loss': Array(6.258007, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(6.381837, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.350325, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1499.3281831741333, 'total_duration': 1747.6310703754425, 'accumulated_submission_time': 1499.3281831741333, 'accumulated_eval_time': 248.20428919792175, 'accumulated_logging_time': 0.04201531410217285}
I0327 02:47:02.944450 140235218712320 logging_writer.py:48] [1649] accumulated_eval_time=248.204289, accumulated_logging_time=0.042015, accumulated_submission_time=1499.328183, global_step=1649, preemption_count=0, score=1499.328183, test/ctc_loss=6.350325107574463, test/num_examples=2472, test/wer=0.899580, total_duration=1747.631070, train/ctc_loss=6.258007049560547, train/wer=0.944636, validation/ctc_loss=6.381836891174316, validation/num_examples=5348, validation/wer=0.896618
I0327 02:51:55.068348 140235210319616 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.9494476318359375, loss=5.530045032501221
I0327 02:58:47.428776 140234563352320 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.3251345157623291, loss=5.488722801208496
I0327 03:06:24.813079 140234554959616 logging_writer.py:48] [3000] global_step=3000, grad_norm=1.0650224685668945, loss=4.601537227630615
I0327 03:11:02.942787 140406042187584 spec.py:321] Evaluating on the training split.
I0327 03:11:51.034518 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 03:12:39.556635 140406042187584 spec.py:349] Evaluating on the test split.
I0327 03:13:04.543331 140406042187584 submission_runner.py:422] Time since start: 3309.26s, 	Step: 3341, 	{'train/ctc_loss': Array(3.233811, dtype=float32), 'train/wer': 0.6867645100285007, 'validation/ctc_loss': Array(3.4814465, dtype=float32), 'validation/wer': 0.6947584888537031, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.2425442, dtype=float32), 'test/wer': 0.6666666666666666, 'test/num_examples': 2472, 'score': 2939.2526803016663, 'total_duration': 3309.2614362239838, 'accumulated_submission_time': 2939.2526803016663, 'accumulated_eval_time': 369.7960579395294, 'accumulated_logging_time': 0.09071874618530273}
I0327 03:13:04.579561 140235802392320 logging_writer.py:48] [3341] accumulated_eval_time=369.796058, accumulated_logging_time=0.090719, accumulated_submission_time=2939.252680, global_step=3341, preemption_count=0, score=2939.252680, test/ctc_loss=3.242544174194336, test/num_examples=2472, test/wer=0.666667, total_duration=3309.261436, train/ctc_loss=3.2338109016418457, train/wer=0.686765, validation/ctc_loss=3.4814465045928955, validation/num_examples=5348, validation/wer=0.694758
I0327 03:15:08.497199 140235793999616 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.9693132042884827, loss=3.322148084640503
I0327 03:22:36.604247 140235802392320 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.9114371538162231, loss=2.847669839859009
I0327 03:29:38.684284 140234563352320 logging_writer.py:48] [4500] global_step=4500, grad_norm=1.582686424255371, loss=2.6396265029907227
I0327 03:37:04.736751 140406042187584 spec.py:321] Evaluating on the training split.
I0327 03:37:57.760438 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 03:38:48.163729 140406042187584 spec.py:349] Evaluating on the test split.
I0327 03:39:14.304720 140406042187584 submission_runner.py:422] Time since start: 4879.03s, 	Step: 4986, 	{'train/ctc_loss': Array(1.1162223, dtype=float32), 'train/wer': 0.35403324003352465, 'validation/ctc_loss': Array(1.4812187, dtype=float32), 'validation/wer': 0.4057754134605173, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.1754295, dtype=float32), 'test/wer': 0.351187211829464, 'test/num_examples': 2472, 'score': 4379.337501049042, 'total_duration': 4879.025956392288, 'accumulated_submission_time': 4379.337501049042, 'accumulated_eval_time': 499.3581876754761, 'accumulated_logging_time': 0.14188861846923828}
I0327 03:39:14.340690 140235474712320 logging_writer.py:48] [4986] accumulated_eval_time=499.358188, accumulated_logging_time=0.141889, accumulated_submission_time=4379.337501, global_step=4986, preemption_count=0, score=4379.337501, test/ctc_loss=1.1754294633865356, test/num_examples=2472, test/wer=0.351187, total_duration=4879.025956, train/ctc_loss=1.1162222623825073, train/wer=0.354033, validation/ctc_loss=1.481218695640564, validation/num_examples=5348, validation/wer=0.405775
I0327 03:39:26.056942 140235466319616 logging_writer.py:48] [5000] global_step=5000, grad_norm=1.0568767786026, loss=2.371950387954712
I0327 03:46:04.572057 140234604312320 logging_writer.py:48] [5500] global_step=5500, grad_norm=1.0992885828018188, loss=2.350198745727539
I0327 03:53:45.107874 140234595919616 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.7904625535011292, loss=2.136011838912964
I0327 04:00:49.369385 140235802392320 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.9815120100975037, loss=2.040560245513916
I0327 04:03:14.846513 140406042187584 spec.py:321] Evaluating on the training split.
I0327 04:04:10.780102 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 04:05:02.612700 140406042187584 spec.py:349] Evaluating on the test split.
I0327 04:05:29.631346 140406042187584 submission_runner.py:422] Time since start: 6454.35s, 	Step: 6667, 	{'train/ctc_loss': Array(0.7248324, dtype=float32), 'train/wer': 0.24071359004702111, 'validation/ctc_loss': Array(1.0661641, dtype=float32), 'validation/wer': 0.3064579974318623, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.77094764, dtype=float32), 'test/wer': 0.24564824406394084, 'test/num_examples': 2472, 'score': 5819.764954805374, 'total_duration': 6454.353077173233, 'accumulated_submission_time': 5819.764954805374, 'accumulated_eval_time': 634.1376528739929, 'accumulated_logging_time': 0.19594025611877441}
I0327 04:05:29.666676 140235802392320 logging_writer.py:48] [6667] accumulated_eval_time=634.137653, accumulated_logging_time=0.195940, accumulated_submission_time=5819.764955, global_step=6667, preemption_count=0, score=5819.764955, test/ctc_loss=0.7709476351737976, test/num_examples=2472, test/wer=0.245648, total_duration=6454.353077, train/ctc_loss=0.7248324155807495, train/wer=0.240714, validation/ctc_loss=1.0661641359329224, validation/num_examples=5348, validation/wer=0.306458
I0327 04:10:09.130090 140235793999616 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.8689921498298645, loss=1.9960211515426636
I0327 04:17:14.501457 140235802392320 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.5867196917533875, loss=1.8955800533294678
I0327 04:24:51.777389 140235793999616 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.606662929058075, loss=1.9202189445495605
I0327 04:29:29.977092 140406042187584 spec.py:321] Evaluating on the training split.
I0327 04:30:22.364387 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 04:31:12.296296 140406042187584 spec.py:349] Evaluating on the test split.
I0327 04:31:37.411294 140406042187584 submission_runner.py:422] Time since start: 8022.13s, 	Step: 8309, 	{'train/ctc_loss': Array(0.5645023, dtype=float32), 'train/wer': 0.19665572202048315, 'validation/ctc_loss': Array(0.8911399, dtype=float32), 'validation/wer': 0.2630506772739122, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6137322, dtype=float32), 'test/wer': 0.20157211626348182, 'test/num_examples': 2472, 'score': 7260.00180888176, 'total_duration': 8022.13317990303, 'accumulated_submission_time': 7260.00180888176, 'accumulated_eval_time': 761.5667946338654, 'accumulated_logging_time': 0.24703526496887207}
I0327 04:31:37.453554 140235003672320 logging_writer.py:48] [8309] accumulated_eval_time=761.566795, accumulated_logging_time=0.247035, accumulated_submission_time=7260.001809, global_step=8309, preemption_count=0, score=7260.001809, test/ctc_loss=0.6137322187423706, test/num_examples=2472, test/wer=0.201572, total_duration=8022.133180, train/ctc_loss=0.5645022988319397, train/wer=0.196656, validation/ctc_loss=0.8911399245262146, validation/num_examples=5348, validation/wer=0.263051
I0327 04:34:05.695435 140234995279616 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.7841035723686218, loss=1.845881462097168
I0327 04:40:48.671605 140235003672320 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.6690154671669006, loss=1.867059350013733
I0327 04:47:46.994766 140235003672320 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.7161885499954224, loss=1.705935001373291
I0327 04:54:40.293348 140234995279616 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.7616551518440247, loss=1.7698121070861816
I0327 04:55:37.856688 140406042187584 spec.py:321] Evaluating on the training split.
I0327 04:56:31.259916 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 04:57:21.154387 140406042187584 spec.py:349] Evaluating on the test split.
I0327 04:57:46.840753 140406042187584 submission_runner.py:422] Time since start: 9591.56s, 	Step: 10067, 	{'train/ctc_loss': Array(0.4868831, dtype=float32), 'train/wer': 0.16960165589940837, 'validation/ctc_loss': Array(0.7654884, dtype=float32), 'validation/wer': 0.22954903115556544, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50620687, dtype=float32), 'test/wer': 0.17145004366989622, 'test/num_examples': 2472, 'score': 8700.331828832626, 'total_duration': 9591.559700727463, 'accumulated_submission_time': 8700.331828832626, 'accumulated_eval_time': 890.5427072048187, 'accumulated_logging_time': 0.30434226989746094}
I0327 04:57:46.899543 140235802392320 logging_writer.py:48] [10067] accumulated_eval_time=890.542707, accumulated_logging_time=0.304342, accumulated_submission_time=8700.331829, global_step=10067, preemption_count=0, score=8700.331829, test/ctc_loss=0.5062068700790405, test/num_examples=2472, test/wer=0.171450, total_duration=9591.559701, train/ctc_loss=0.48688310384750366, train/wer=0.169602, validation/ctc_loss=0.7654883861541748, validation/num_examples=5348, validation/wer=0.229549
I0327 05:03:25.653840 140235044632320 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.5502166748046875, loss=1.7003748416900635
I0327 05:10:21.883858 140235036239616 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.5897229313850403, loss=1.6701407432556152
I0327 05:17:31.821661 140235044632320 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.6605705618858337, loss=1.6923398971557617
I0327 05:21:47.231598 140406042187584 spec.py:321] Evaluating on the training split.
I0327 05:22:41.702332 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 05:23:30.410115 140406042187584 spec.py:349] Evaluating on the test split.
I0327 05:23:55.163839 140406042187584 submission_runner.py:422] Time since start: 11159.89s, 	Step: 11832, 	{'train/ctc_loss': Array(0.43912536, dtype=float32), 'train/wer': 0.15616957011890328, 'validation/ctc_loss': Array(0.7563776, dtype=float32), 'validation/wer': 0.2247506685847244, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4932181, dtype=float32), 'test/wer': 0.16580342453232588, 'test/num_examples': 2472, 'score': 10140.590170145035, 'total_duration': 11159.885578393936, 'accumulated_submission_time': 10140.590170145035, 'accumulated_eval_time': 1018.4696464538574, 'accumulated_logging_time': 0.37738537788391113}
I0327 05:23:55.209628 140234460952320 logging_writer.py:48] [11832] accumulated_eval_time=1018.469646, accumulated_logging_time=0.377385, accumulated_submission_time=10140.590170, global_step=11832, preemption_count=0, score=10140.590170, test/ctc_loss=0.4932180941104889, test/num_examples=2472, test/wer=0.165803, total_duration=11159.885578, train/ctc_loss=0.4391253590583801, train/wer=0.156170, validation/ctc_loss=0.7563775777816772, validation/num_examples=5348, validation/wer=0.224751
I0327 05:26:05.706732 140234452559616 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.5208517909049988, loss=1.7254836559295654
I0327 05:32:34.997588 140234460952320 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.5781623721122742, loss=1.7401797771453857
I0327 05:39:00.569343 140234452559616 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.585448682308197, loss=1.552300214767456
I0327 05:45:54.145387 140234460952320 logging_writer.py:48] [13500] global_step=13500, grad_norm=1.0401381254196167, loss=1.588830590248108
I0327 05:47:55.795994 140406042187584 spec.py:321] Evaluating on the training split.
I0327 05:48:47.681594 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 05:49:36.335933 140406042187584 spec.py:349] Evaluating on the test split.
I0327 05:50:00.760075 140406042187584 submission_runner.py:422] Time since start: 12725.48s, 	Step: 13659, 	{'train/ctc_loss': Array(0.35610482, dtype=float32), 'train/wer': 0.13272836007176472, 'validation/ctc_loss': Array(0.6884118, dtype=float32), 'validation/wer': 0.20827983046429227, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44046697, dtype=float32), 'test/wer': 0.1493307334511405, 'test/num_examples': 2472, 'score': 11581.088110208511, 'total_duration': 12725.48209643364, 'accumulated_submission_time': 11581.088110208511, 'accumulated_eval_time': 1143.428656578064, 'accumulated_logging_time': 0.4524505138397217}
I0327 05:50:00.792037 140235587352320 logging_writer.py:48] [13659] accumulated_eval_time=1143.428657, accumulated_logging_time=0.452451, accumulated_submission_time=11581.088110, global_step=13659, preemption_count=0, score=11581.088110, test/ctc_loss=0.440466970205307, test/num_examples=2472, test/wer=0.149331, total_duration=12725.482096, train/ctc_loss=0.3561048209667206, train/wer=0.132728, validation/ctc_loss=0.6884117722511292, validation/num_examples=5348, validation/wer=0.208280
I0327 05:54:24.440099 140235578959616 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.8103086352348328, loss=1.5656088590621948
I0327 06:01:19.499155 140235259672320 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.5514602065086365, loss=1.5437294244766235
I0327 06:07:45.663928 140235251279616 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.7660632133483887, loss=1.5367897748947144
I0327 06:14:01.444884 140406042187584 spec.py:321] Evaluating on the training split.
I0327 06:14:51.740274 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 06:15:41.117387 140406042187584 spec.py:349] Evaluating on the test split.
I0327 06:16:05.761450 140406042187584 submission_runner.py:422] Time since start: 14290.48s, 	Step: 15446, 	{'train/ctc_loss': Array(0.31225345, dtype=float32), 'train/wer': 0.1149444855268688, 'validation/ctc_loss': Array(0.63623434, dtype=float32), 'validation/wer': 0.19359510315996795, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40065524, dtype=float32), 'test/wer': 0.13519387402758312, 'test/num_examples': 2472, 'score': 13021.669342517853, 'total_duration': 14290.482184648514, 'accumulated_submission_time': 13021.669342517853, 'accumulated_eval_time': 1267.7388746738434, 'accumulated_logging_time': 0.49801135063171387}
I0327 06:16:05.796744 140235259672320 logging_writer.py:48] [15446] accumulated_eval_time=1267.738875, accumulated_logging_time=0.498011, accumulated_submission_time=13021.669343, global_step=15446, preemption_count=0, score=13021.669343, test/ctc_loss=0.40065523982048035, test/num_examples=2472, test/wer=0.135194, total_duration=14290.482185, train/ctc_loss=0.3122534453868866, train/wer=0.114944, validation/ctc_loss=0.6362343430519104, validation/num_examples=5348, validation/wer=0.193595
I0327 06:16:51.338395 140234425112320 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.6163938045501709, loss=1.5930023193359375
I0327 06:23:19.840942 140234416719616 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.7524815797805786, loss=1.5257405042648315
I0327 06:30:30.637025 140234425112320 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.5093381404876709, loss=1.515957236289978
I0327 06:36:56.346282 140234416719616 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.5578960180282593, loss=1.483333945274353
I0327 06:40:06.108587 140406042187584 spec.py:321] Evaluating on the training split.
I0327 06:40:57.885792 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 06:41:46.595032 140406042187584 spec.py:349] Evaluating on the test split.
I0327 06:42:11.537863 140406042187584 submission_runner.py:422] Time since start: 15856.26s, 	Step: 17229, 	{'train/ctc_loss': Array(0.3181728, dtype=float32), 'train/wer': 0.12069159690454612, 'validation/ctc_loss': Array(0.64264226, dtype=float32), 'validation/wer': 0.19314133446614595, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39903724, dtype=float32), 'test/wer': 0.1338329981922694, 'test/num_examples': 2472, 'score': 14461.909377336502, 'total_duration': 15856.259784698486, 'accumulated_submission_time': 14461.909377336502, 'accumulated_eval_time': 1393.162992477417, 'accumulated_logging_time': 0.5475101470947266}
I0327 06:42:11.577182 140234425112320 logging_writer.py:48] [17229] accumulated_eval_time=1393.162992, accumulated_logging_time=0.547510, accumulated_submission_time=14461.909377, global_step=17229, preemption_count=0, score=14461.909377, test/ctc_loss=0.39903724193573, test/num_examples=2472, test/wer=0.133833, total_duration=15856.259785, train/ctc_loss=0.318172812461853, train/wer=0.120692, validation/ctc_loss=0.6426422595977783, validation/num_examples=5348, validation/wer=0.193141
I0327 06:45:41.137248 140234416719616 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.6469214558601379, loss=1.501107096672058
I0327 06:52:10.114378 140235587352320 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.8350098133087158, loss=1.4886418581008911
I0327 06:59:06.126260 140235578959616 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.5558322072029114, loss=1.5060566663742065
I0327 07:05:37.628945 140234425112320 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.613404393196106, loss=1.522764801979065
I0327 07:06:12.058943 140406042187584 spec.py:321] Evaluating on the training split.
I0327 07:07:03.532991 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 07:07:52.130545 140406042187584 spec.py:349] Evaluating on the test split.
I0327 07:08:17.085365 140406042187584 submission_runner.py:422] Time since start: 17421.81s, 	Step: 19046, 	{'train/ctc_loss': Array(0.296525, dtype=float32), 'train/wer': 0.10833707924588169, 'validation/ctc_loss': Array(0.5774814, dtype=float32), 'validation/wer': 0.1760912171621113, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35609692, dtype=float32), 'test/wer': 0.12186947778928767, 'test/num_examples': 2472, 'score': 15902.316984415054, 'total_duration': 17421.807481765747, 'accumulated_submission_time': 15902.316984415054, 'accumulated_eval_time': 1518.1844382286072, 'accumulated_logging_time': 0.6014482975006104}
I0327 07:08:17.120582 140235372312320 logging_writer.py:48] [19046] accumulated_eval_time=1518.184438, accumulated_logging_time=0.601448, accumulated_submission_time=15902.316984, global_step=19046, preemption_count=0, score=15902.316984, test/ctc_loss=0.35609692335128784, test/num_examples=2472, test/wer=0.121869, total_duration=17421.807482, train/ctc_loss=0.2965250015258789, train/wer=0.108337, validation/ctc_loss=0.5774813890457153, validation/num_examples=5348, validation/wer=0.176091
I0327 07:14:12.426968 140235363919616 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.5036680102348328, loss=1.4630203247070312
I0327 07:20:47.980570 140235044632320 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.5997361540794373, loss=1.3837279081344604
I0327 07:27:55.769997 140235036239616 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.787744402885437, loss=1.4675695896148682
I0327 07:32:17.380255 140406042187584 spec.py:321] Evaluating on the training split.
I0327 07:33:09.827694 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 07:33:58.972254 140406042187584 spec.py:349] Evaluating on the test split.
I0327 07:34:23.884977 140406042187584 submission_runner.py:422] Time since start: 18988.61s, 	Step: 20823, 	{'train/ctc_loss': Array(0.2906837, dtype=float32), 'train/wer': 0.10649220460600944, 'validation/ctc_loss': Array(0.5690938, dtype=float32), 'validation/wer': 0.17209419079525376, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34827888, dtype=float32), 'test/wer': 0.11928990717608108, 'test/num_examples': 2472, 'score': 17342.500242233276, 'total_duration': 18988.60622382164, 'accumulated_submission_time': 17342.500242233276, 'accumulated_eval_time': 1644.6834568977356, 'accumulated_logging_time': 0.6532752513885498}
I0327 07:34:23.916162 140235802392320 logging_writer.py:48] [20823] accumulated_eval_time=1644.683457, accumulated_logging_time=0.653275, accumulated_submission_time=17342.500242, global_step=20823, preemption_count=0, score=17342.500242, test/ctc_loss=0.34827888011932373, test/num_examples=2472, test/wer=0.119290, total_duration=18988.606224, train/ctc_loss=0.29068368673324585, train/wer=0.106492, validation/ctc_loss=0.5690938234329224, validation/num_examples=5348, validation/wer=0.172094
I0327 07:36:40.975932 140235793999616 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.561418890953064, loss=1.4680697917938232
I0327 07:43:47.655398 140235802392320 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.783621609210968, loss=1.5505790710449219
I0327 07:50:30.783340 140235147032320 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.5261864066123962, loss=1.4234833717346191
I0327 07:57:33.955279 140235138639616 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.5239651799201965, loss=1.4366651773452759
I0327 07:58:23.954034 140406042187584 spec.py:321] Evaluating on the training split.
I0327 07:59:16.766342 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 08:00:05.229717 140406042187584 spec.py:349] Evaluating on the test split.
I0327 08:00:29.788783 140406042187584 submission_runner.py:422] Time since start: 20554.51s, 	Step: 22560, 	{'train/ctc_loss': Array(0.2656108, dtype=float32), 'train/wer': 0.09865212470418767, 'validation/ctc_loss': Array(0.54345155, dtype=float32), 'validation/wer': 0.16567384651032566, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33042258, dtype=float32), 'test/wer': 0.11218085430503931, 'test/num_examples': 2472, 'score': 18782.465693712234, 'total_duration': 20554.51053571701, 'accumulated_submission_time': 18782.465693712234, 'accumulated_eval_time': 1770.5128767490387, 'accumulated_logging_time': 0.6983656883239746}
I0327 08:00:29.829836 140235802392320 logging_writer.py:48] [22560] accumulated_eval_time=1770.512877, accumulated_logging_time=0.698366, accumulated_submission_time=18782.465694, global_step=22560, preemption_count=0, score=18782.465694, test/ctc_loss=0.330422580242157, test/num_examples=2472, test/wer=0.112181, total_duration=20554.510536, train/ctc_loss=0.26561081409454346, train/wer=0.098652, validation/ctc_loss=0.5434515476226807, validation/num_examples=5348, validation/wer=0.165674
I0327 08:06:13.612151 140235802392320 logging_writer.py:48] [23000] global_step=23000, grad_norm=1.087755799293518, loss=1.3784734010696411
I0327 08:13:19.475985 140235793999616 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.42869633436203003, loss=1.378319501876831
I0327 08:20:03.617449 140235802392320 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.48051872849464417, loss=1.406927466392517
I0327 08:24:30.779765 140406042187584 spec.py:321] Evaluating on the training split.
I0327 08:25:23.031579 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 08:26:11.652155 140406042187584 spec.py:349] Evaluating on the test split.
I0327 08:26:36.308279 140406042187584 submission_runner.py:422] Time since start: 22121.03s, 	Step: 24340, 	{'train/ctc_loss': Array(0.2391195, dtype=float32), 'train/wer': 0.08944934757822727, 'validation/ctc_loss': Array(0.5295098, dtype=float32), 'validation/wer': 0.1602286221844618, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3209627, dtype=float32), 'test/wer': 0.10901224788251783, 'test/num_examples': 2472, 'score': 20223.338948726654, 'total_duration': 22121.02773118019, 'accumulated_submission_time': 20223.338948726654, 'accumulated_eval_time': 1896.033747434616, 'accumulated_logging_time': 0.7552995681762695}
I0327 08:26:36.347300 140235510552320 logging_writer.py:48] [24340] accumulated_eval_time=1896.033747, accumulated_logging_time=0.755300, accumulated_submission_time=20223.338949, global_step=24340, preemption_count=0, score=20223.338949, test/ctc_loss=0.32096269726753235, test/num_examples=2472, test/wer=0.109012, total_duration=22121.027731, train/ctc_loss=0.2391194999217987, train/wer=0.089449, validation/ctc_loss=0.5295097827911377, validation/num_examples=5348, validation/wer=0.160229
I0327 08:28:40.343734 140235502159616 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.7144882082939148, loss=1.3531832695007324
I0327 08:35:09.006690 140235182872320 logging_writer.py:48] [25000] global_step=25000, grad_norm=1.5606038570404053, loss=1.3718425035476685
I0327 08:41:56.323137 140235174479616 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.6028954982757568, loss=1.3962241411209106
I0327 08:48:50.753703 140235510552320 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.5746764540672302, loss=1.3886438608169556
I0327 08:50:36.728398 140406042187584 spec.py:321] Evaluating on the training split.
I0327 08:51:28.900048 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 08:52:18.657327 140406042187584 spec.py:349] Evaluating on the test split.
I0327 08:52:43.565385 140406042187584 submission_runner.py:422] Time since start: 23688.29s, 	Step: 26139, 	{'train/ctc_loss': Array(0.22598623, dtype=float32), 'train/wer': 0.0868358590469235, 'validation/ctc_loss': Array(0.524366, dtype=float32), 'validation/wer': 0.15879973353157553, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30867016, dtype=float32), 'test/wer': 0.10618893831373266, 'test/num_examples': 2472, 'score': 21663.641766548157, 'total_duration': 23688.287184000015, 'accumulated_submission_time': 21663.641766548157, 'accumulated_eval_time': 2022.8654391765594, 'accumulated_logging_time': 0.811131477355957}
I0327 08:52:43.604942 140235510552320 logging_writer.py:48] [26139] accumulated_eval_time=2022.865439, accumulated_logging_time=0.811131, accumulated_submission_time=21663.641767, global_step=26139, preemption_count=0, score=21663.641767, test/ctc_loss=0.30867016315460205, test/num_examples=2472, test/wer=0.106189, total_duration=23688.287184, train/ctc_loss=0.22598622739315033, train/wer=0.086836, validation/ctc_loss=0.524366021156311, validation/num_examples=5348, validation/wer=0.158800
I0327 08:57:24.961534 140235502159616 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.5435500144958496, loss=1.386715054512024
I0327 09:04:23.969527 140235182872320 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.5286281108856201, loss=1.3760637044906616
I0327 09:11:13.406304 140235174479616 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.5291776061058044, loss=1.3857526779174805
I0327 09:16:43.585279 140406042187584 spec.py:321] Evaluating on the training split.
I0327 09:17:36.514433 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 09:18:26.075190 140406042187584 spec.py:349] Evaluating on the test split.
I0327 09:18:50.878593 140406042187584 submission_runner.py:422] Time since start: 25255.60s, 	Step: 27884, 	{'train/ctc_loss': Array(0.21838745, dtype=float32), 'train/wer': 0.0825040144828221, 'validation/ctc_loss': Array(0.515495, dtype=float32), 'validation/wer': 0.15555577010340133, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30463678, dtype=float32), 'test/wer': 0.10297970873194809, 'test/num_examples': 2472, 'score': 23103.543451070786, 'total_duration': 25255.599942207336, 'accumulated_submission_time': 23103.543451070786, 'accumulated_eval_time': 2150.1531643867493, 'accumulated_logging_time': 0.8675537109375}
I0327 09:18:50.910241 140235182872320 logging_writer.py:48] [27884] accumulated_eval_time=2150.153164, accumulated_logging_time=0.867554, accumulated_submission_time=23103.543451, global_step=27884, preemption_count=0, score=23103.543451, test/ctc_loss=0.30463677644729614, test/num_examples=2472, test/wer=0.102980, total_duration=25255.599942, train/ctc_loss=0.2183874547481537, train/wer=0.082504, validation/ctc_loss=0.5154950022697449, validation/num_examples=5348, validation/wer=0.155556
I0327 09:20:21.090492 140235174479616 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.5708181858062744, loss=1.3451476097106934
I0327 09:26:55.372470 140235182872320 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.5579081177711487, loss=1.3512862920761108
I0327 09:33:57.393495 140235510552320 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.5113661289215088, loss=1.3239134550094604
I0327 09:40:31.989740 140235502159616 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.8341867923736572, loss=1.3326644897460938
I0327 09:42:51.404718 140406042187584 spec.py:321] Evaluating on the training split.
I0327 09:43:43.433357 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 09:44:32.523814 140406042187584 spec.py:349] Evaluating on the test split.
I0327 09:44:57.272210 140406042187584 submission_runner.py:422] Time since start: 26821.99s, 	Step: 29663, 	{'train/ctc_loss': Array(0.22144225, dtype=float32), 'train/wer': 0.0828241462991853, 'validation/ctc_loss': Array(0.49088266, dtype=float32), 'validation/wer': 0.14791893953290788, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28876853, dtype=float32), 'test/wer': 0.09916113176121708, 'test/num_examples': 2472, 'score': 24543.958880662918, 'total_duration': 26821.993340969086, 'accumulated_submission_time': 24543.958880662918, 'accumulated_eval_time': 2276.0146985054016, 'accumulated_logging_time': 0.9153838157653809}
I0327 09:44:57.308226 140235510552320 logging_writer.py:48] [29663] accumulated_eval_time=2276.014699, accumulated_logging_time=0.915384, accumulated_submission_time=24543.958881, global_step=29663, preemption_count=0, score=24543.958881, test/ctc_loss=0.2887685298919678, test/num_examples=2472, test/wer=0.099161, total_duration=26821.993341, train/ctc_loss=0.22144225239753723, train/wer=0.082824, validation/ctc_loss=0.49088266491889954, validation/num_examples=5348, validation/wer=0.147919
I0327 09:49:20.778419 140235510552320 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.7472134828567505, loss=1.2952202558517456
I0327 09:56:01.436813 140235502159616 logging_writer.py:48] [30500] global_step=30500, grad_norm=1.0335731506347656, loss=1.3339165449142456
I0327 10:03:08.995131 140235510552320 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.48935702443122864, loss=1.2812159061431885
I0327 10:08:57.581724 140406042187584 spec.py:321] Evaluating on the training split.
I0327 10:09:49.133116 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 10:10:38.345951 140406042187584 spec.py:349] Evaluating on the test split.
I0327 10:11:03.102257 140406042187584 submission_runner.py:422] Time since start: 28387.82s, 	Step: 31448, 	{'train/ctc_loss': Array(0.21021047, dtype=float32), 'train/wer': 0.07750991595179572, 'validation/ctc_loss': Array(0.49241236, dtype=float32), 'validation/wer': 0.1481023779410487, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28568774, dtype=float32), 'test/wer': 0.09704872747953608, 'test/num_examples': 2472, 'score': 25984.15348649025, 'total_duration': 28387.823632001877, 'accumulated_submission_time': 25984.15348649025, 'accumulated_eval_time': 2401.529518842697, 'accumulated_logging_time': 0.9679491519927979}
I0327 10:11:03.146211 140234921748224 logging_writer.py:48] [31448] accumulated_eval_time=2401.529519, accumulated_logging_time=0.967949, accumulated_submission_time=25984.153486, global_step=31448, preemption_count=0, score=25984.153486, test/ctc_loss=0.28568774461746216, test/num_examples=2472, test/wer=0.097049, total_duration=28387.823632, train/ctc_loss=0.21021047234535217, train/wer=0.077510, validation/ctc_loss=0.49241235852241516, validation/num_examples=5348, validation/wer=0.148102
I0327 10:11:43.961564 140234913355520 logging_writer.py:48] [31500] global_step=31500, grad_norm=1.0899758338928223, loss=1.3301883935928345
I0327 10:18:35.169901 140234921748224 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.5252581238746643, loss=1.2446686029434204
I0327 10:25:09.476984 140234913355520 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.6817660927772522, loss=1.3151897192001343
I0327 10:32:27.983286 140234594068224 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.5504847168922424, loss=1.248807430267334
I0327 10:35:03.636112 140406042187584 spec.py:321] Evaluating on the training split.
I0327 10:35:55.936197 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 10:36:44.896600 140406042187584 spec.py:349] Evaluating on the test split.
I0327 10:37:09.622301 140406042187584 submission_runner.py:422] Time since start: 29954.34s, 	Step: 33203, 	{'train/ctc_loss': Array(0.21211581, dtype=float32), 'train/wer': 0.07885179458465395, 'validation/ctc_loss': Array(0.47753543, dtype=float32), 'validation/wer': 0.14282128271720557, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2814209, dtype=float32), 'test/wer': 0.09453009160522413, 'test/num_examples': 2472, 'score': 27424.557148456573, 'total_duration': 29954.343950748444, 'accumulated_submission_time': 27424.557148456573, 'accumulated_eval_time': 2527.510297060013, 'accumulated_logging_time': 1.0379853248596191}
I0327 10:37:09.658070 140234594068224 logging_writer.py:48] [33203] accumulated_eval_time=2527.510297, accumulated_logging_time=1.037985, accumulated_submission_time=27424.557148, global_step=33203, preemption_count=0, score=27424.557148, test/ctc_loss=0.28142088651657104, test/num_examples=2472, test/wer=0.094530, total_duration=29954.343951, train/ctc_loss=0.2121158093214035, train/wer=0.078852, validation/ctc_loss=0.4775354266166687, validation/num_examples=5348, validation/wer=0.142821
I0327 10:40:59.453459 140234585675520 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.5389612317085266, loss=1.280419945716858
I0327 10:48:13.793571 140234921748224 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.7441859841346741, loss=1.2312153577804565
I0327 10:54:39.707524 140234913355520 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.754983127117157, loss=1.2736425399780273
I0327 11:01:10.469656 140406042187584 spec.py:321] Evaluating on the training split.
I0327 11:02:02.719253 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 11:02:52.084480 140406042187584 spec.py:349] Evaluating on the test split.
I0327 11:03:17.168735 140406042187584 submission_runner.py:422] Time since start: 31521.89s, 	Step: 34948, 	{'train/ctc_loss': Array(0.19727528, dtype=float32), 'train/wer': 0.07296162494538402, 'validation/ctc_loss': Array(0.45192993, dtype=float32), 'validation/wer': 0.13746295026888208, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2662599, dtype=float32), 'test/wer': 0.091930209412386, 'test/num_examples': 2472, 'score': 28865.29248404503, 'total_duration': 31521.889628887177, 'accumulated_submission_time': 28865.29248404503, 'accumulated_eval_time': 2654.2031893730164, 'accumulated_logging_time': 1.089148759841919}
I0327 11:03:17.208079 140234921748224 logging_writer.py:48] [34948] accumulated_eval_time=2654.203189, accumulated_logging_time=1.089149, accumulated_submission_time=28865.292484, global_step=34948, preemption_count=0, score=28865.292484, test/ctc_loss=0.26625990867614746, test/num_examples=2472, test/wer=0.091930, total_duration=31521.889629, train/ctc_loss=0.1972752809524536, train/wer=0.072962, validation/ctc_loss=0.4519299268722534, validation/num_examples=5348, validation/wer=0.137463
I0327 11:03:58.002352 140234913355520 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.7867964506149292, loss=1.2824008464813232
I0327 11:10:31.607230 140234921748224 logging_writer.py:48] [35500] global_step=35500, grad_norm=2.2043521404266357, loss=1.3078906536102295
I0327 11:17:53.006211 140234913355520 logging_writer.py:48] [36000] global_step=36000, grad_norm=1.0332614183425903, loss=1.2435840368270874
I0327 11:24:27.662006 140234594068224 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.9616210460662842, loss=1.193361759185791
I0327 11:27:17.221881 140406042187584 spec.py:321] Evaluating on the training split.
I0327 11:28:10.919742 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 11:29:00.498669 140406042187584 spec.py:349] Evaluating on the test split.
I0327 11:29:26.070293 140406042187584 submission_runner.py:422] Time since start: 33090.79s, 	Step: 36698, 	{'train/ctc_loss': Array(0.15301058, dtype=float32), 'train/wer': 0.059951006016625154, 'validation/ctc_loss': Array(0.45113835, dtype=float32), 'validation/wer': 0.13586027786091506, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26068655, dtype=float32), 'test/wer': 0.08817256718054964, 'test/num_examples': 2472, 'score': 30305.226006507874, 'total_duration': 33090.79103398323, 'accumulated_submission_time': 30305.226006507874, 'accumulated_eval_time': 2783.0452823638916, 'accumulated_logging_time': 1.1461570262908936}
I0327 11:29:26.104258 140234788632320 logging_writer.py:48] [36698] accumulated_eval_time=2783.045282, accumulated_logging_time=1.146157, accumulated_submission_time=30305.226007, global_step=36698, preemption_count=0, score=30305.226007, test/ctc_loss=0.2606865465641022, test/num_examples=2472, test/wer=0.088173, total_duration=33090.791034, train/ctc_loss=0.15301057696342468, train/wer=0.059951, validation/ctc_loss=0.45113834738731384, validation/num_examples=5348, validation/wer=0.135860
I0327 11:33:19.818790 140234780239616 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.6333849430084229, loss=1.2593268156051636
I0327 11:39:52.910735 140234460952320 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.9675902724266052, loss=1.2448230981826782
I0327 11:46:59.852748 140234452559616 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.6497098207473755, loss=1.2517709732055664
I0327 11:53:26.164023 140406042187584 spec.py:321] Evaluating on the training split.
I0327 11:54:19.167675 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 11:55:08.925160 140406042187584 spec.py:349] Evaluating on the test split.
I0327 11:55:34.096255 140406042187584 submission_runner.py:422] Time since start: 34658.82s, 	Step: 38483, 	{'train/ctc_loss': Array(0.17294581, dtype=float32), 'train/wer': 0.06504614474879095, 'validation/ctc_loss': Array(0.43772656, dtype=float32), 'validation/wer': 0.1324618399837802, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25223765, dtype=float32), 'test/wer': 0.08500396075802816, 'test/num_examples': 2472, 'score': 31745.207817554474, 'total_duration': 34658.81834125519, 'accumulated_submission_time': 31745.207817554474, 'accumulated_eval_time': 2910.9727005958557, 'accumulated_logging_time': 1.1947596073150635}
I0327 11:55:34.134241 140235218712320 logging_writer.py:48] [38483] accumulated_eval_time=2910.972701, accumulated_logging_time=1.194760, accumulated_submission_time=31745.207818, global_step=38483, preemption_count=0, score=31745.207818, test/ctc_loss=0.2522376477718353, test/num_examples=2472, test/wer=0.085004, total_duration=34658.818341, train/ctc_loss=0.1729458123445511, train/wer=0.065046, validation/ctc_loss=0.43772655725479126, validation/num_examples=5348, validation/wer=0.132462
I0327 11:55:48.013319 140235210319616 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.5221267342567444, loss=1.2705096006393433
I0327 12:02:36.714944 140235218712320 logging_writer.py:48] [39000] global_step=39000, grad_norm=1.9843603372573853, loss=1.2297887802124023
I0327 12:09:20.349801 140235218712320 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.5560914278030396, loss=1.2547239065170288
I0327 12:16:22.898879 140235210319616 logging_writer.py:48] [40000] global_step=40000, grad_norm=1.040624976158142, loss=1.2093318700790405
I0327 12:19:34.792584 140406042187584 spec.py:321] Evaluating on the training split.
I0327 12:20:26.274173 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 12:21:16.483530 140406042187584 spec.py:349] Evaluating on the test split.
I0327 12:21:41.670186 140406042187584 submission_runner.py:422] Time since start: 36226.39s, 	Step: 40225, 	{'train/ctc_loss': Array(0.20648734, dtype=float32), 'train/wer': 0.07711959558045811, 'validation/ctc_loss': Array(0.42702603, dtype=float32), 'validation/wer': 0.12834895778020217, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24681981, dtype=float32), 'test/wer': 0.08268844068003169, 'test/num_examples': 2472, 'score': 33185.78887987137, 'total_duration': 36226.39206290245, 'accumulated_submission_time': 33185.78887987137, 'accumulated_eval_time': 3037.8453941345215, 'accumulated_logging_time': 1.2474281787872314}
I0327 12:21:41.704313 140235218712320 logging_writer.py:48] [40225] accumulated_eval_time=3037.845394, accumulated_logging_time=1.247428, accumulated_submission_time=33185.788880, global_step=40225, preemption_count=0, score=33185.788880, test/ctc_loss=0.24681980907917023, test/num_examples=2472, test/wer=0.082688, total_duration=36226.392063, train/ctc_loss=0.20648734271526337, train/wer=0.077120, validation/ctc_loss=0.42702603340148926, validation/num_examples=5348, validation/wer=0.128349
I0327 12:25:14.311729 140235210319616 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.5663771629333496, loss=1.2208669185638428
I0327 12:31:56.655833 140235218712320 logging_writer.py:48] [41000] global_step=41000, grad_norm=1.002103567123413, loss=1.2342058420181274
I0327 12:38:46.555711 140235218712320 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.5512645244598389, loss=1.18478262424469
I0327 12:45:36.304945 140235210319616 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.6193720698356628, loss=1.166514277458191
I0327 12:45:41.889135 140406042187584 spec.py:321] Evaluating on the training split.
I0327 12:46:33.106168 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 12:47:22.793990 140406042187584 spec.py:349] Evaluating on the test split.
I0327 12:47:48.132373 140406042187584 submission_runner.py:422] Time since start: 37792.85s, 	Step: 42008, 	{'train/ctc_loss': Array(0.2051479, dtype=float32), 'train/wer': 0.07630303602258282, 'validation/ctc_loss': Array(0.4145394, dtype=float32), 'validation/wer': 0.12533670602546898, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.238132, dtype=float32), 'test/wer': 0.07994637742977272, 'test/num_examples': 2472, 'score': 34625.891954660416, 'total_duration': 37792.85226535797, 'accumulated_submission_time': 34625.891954660416, 'accumulated_eval_time': 3164.0814225673676, 'accumulated_logging_time': 1.298886775970459}
I0327 12:47:48.170971 140235218712320 logging_writer.py:48] [42008] accumulated_eval_time=3164.081423, accumulated_logging_time=1.298887, accumulated_submission_time=34625.891955, global_step=42008, preemption_count=0, score=34625.891955, test/ctc_loss=0.23813199996948242, test/num_examples=2472, test/wer=0.079946, total_duration=37792.852265, train/ctc_loss=0.2051479071378708, train/wer=0.076303, validation/ctc_loss=0.4145393967628479, validation/num_examples=5348, validation/wer=0.125337
I0327 12:54:11.248242 140235218712320 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.5771852731704712, loss=1.1668447256088257
I0327 13:01:10.469457 140235210319616 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.6560109853744507, loss=1.130040168762207
I0327 13:08:05.269910 140235218712320 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.7418633699417114, loss=1.1184295415878296
I0327 13:11:48.164306 140406042187584 spec.py:321] Evaluating on the training split.
I0327 13:12:37.750332 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 13:13:27.543293 140406042187584 spec.py:349] Evaluating on the test split.
I0327 13:13:52.584203 140406042187584 submission_runner.py:422] Time since start: 39357.30s, 	Step: 43787, 	{'train/ctc_loss': Array(0.2362001, dtype=float32), 'train/wer': 0.08832940433223556, 'validation/ctc_loss': Array(0.41255334, dtype=float32), 'validation/wer': 0.1231740637400195, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23624404, dtype=float32), 'test/wer': 0.07880892897040603, 'test/num_examples': 2472, 'score': 36065.80715060234, 'total_duration': 39357.30459356308, 'accumulated_submission_time': 36065.80715060234, 'accumulated_eval_time': 3288.4946336746216, 'accumulated_logging_time': 1.3524503707885742}
I0327 13:13:52.625820 140235802392320 logging_writer.py:48] [43787] accumulated_eval_time=3288.494634, accumulated_logging_time=1.352450, accumulated_submission_time=36065.807151, global_step=43787, preemption_count=0, score=36065.807151, test/ctc_loss=0.23624403774738312, test/num_examples=2472, test/wer=0.078809, total_duration=39357.304594, train/ctc_loss=0.23620009422302246, train/wer=0.088329, validation/ctc_loss=0.4125533401966095, validation/num_examples=5348, validation/wer=0.123174
I0327 13:16:37.802761 140235793999616 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.9100778102874756, loss=1.224600076675415
I0327 13:23:27.987525 140235802392320 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.6212896704673767, loss=1.1684314012527466
I0327 13:30:17.968576 140235793999616 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.623548150062561, loss=1.1711968183517456
I0327 13:37:21.136980 140235802392320 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.8228602409362793, loss=1.1591356992721558
I0327 13:37:53.217705 140406042187584 spec.py:321] Evaluating on the training split.
I0327 13:38:42.595065 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 13:39:32.622060 140406042187584 spec.py:349] Evaluating on the test split.
I0327 13:39:57.677036 140406042187584 submission_runner.py:422] Time since start: 40922.40s, 	Step: 45543, 	{'train/ctc_loss': Array(0.20127806, dtype=float32), 'train/wer': 0.07341226618937997, 'validation/ctc_loss': Array(0.40122727, dtype=float32), 'validation/wer': 0.12035490504648716, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22649619, dtype=float32), 'test/wer': 0.07541689517193752, 'test/num_examples': 2472, 'score': 37506.3197286129, 'total_duration': 40922.39886689186, 'accumulated_submission_time': 37506.3197286129, 'accumulated_eval_time': 3412.9487121105194, 'accumulated_logging_time': 1.4097650051116943}
I0327 13:39:57.710905 140235003672320 logging_writer.py:48] [45543] accumulated_eval_time=3412.948712, accumulated_logging_time=1.409765, accumulated_submission_time=37506.319729, global_step=45543, preemption_count=0, score=37506.319729, test/ctc_loss=0.22649618983268738, test/num_examples=2472, test/wer=0.075417, total_duration=40922.398867, train/ctc_loss=0.20127806067466736, train/wer=0.073412, validation/ctc_loss=0.40122726559638977, validation/num_examples=5348, validation/wer=0.120355
I0327 13:45:59.120147 140234995279616 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.8075248599052429, loss=1.16770601272583
I0327 13:53:02.757239 140235003672320 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.6320574283599854, loss=1.0930147171020508
I0327 13:59:42.887210 140234995279616 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.5310903787612915, loss=1.1199980974197388
I0327 14:03:58.365149 140406042187584 spec.py:321] Evaluating on the training split.
I0327 14:04:49.941665 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 14:05:39.807632 140406042187584 spec.py:349] Evaluating on the test split.
I0327 14:06:05.410697 140406042187584 submission_runner.py:422] Time since start: 42490.13s, 	Step: 47289, 	{'train/ctc_loss': Array(0.18256974, dtype=float32), 'train/wer': 0.06944428980214445, 'validation/ctc_loss': Array(0.39311105, dtype=float32), 'validation/wer': 0.11752609169989477, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2217817, dtype=float32), 'test/wer': 0.07440131619036013, 'test/num_examples': 2472, 'score': 38946.892574071884, 'total_duration': 42490.132388830185, 'accumulated_submission_time': 38946.892574071884, 'accumulated_eval_time': 3539.9888577461243, 'accumulated_logging_time': 1.4624249935150146}
I0327 14:06:05.453156 140234158159616 logging_writer.py:48] [47289] accumulated_eval_time=3539.988858, accumulated_logging_time=1.462425, accumulated_submission_time=38946.892574, global_step=47289, preemption_count=0, score=38946.892574, test/ctc_loss=0.22178170084953308, test/num_examples=2472, test/wer=0.074401, total_duration=42490.132389, train/ctc_loss=0.1825697422027588, train/wer=0.069444, validation/ctc_loss=0.39311105012893677, validation/num_examples=5348, validation/wer=0.117526
I0327 14:08:52.189276 140234158159616 logging_writer.py:48] [47500] global_step=47500, grad_norm=1.3863192796707153, loss=1.1465147733688354
I0327 14:15:35.192475 140234149766912 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.6947247982025146, loss=1.1425756216049194
I0327 14:22:49.013667 140234158159616 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.9993613958358765, loss=1.1540777683258057
I0327 14:29:23.063096 140234149766912 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.8103046417236328, loss=1.140170693397522
I0327 14:30:05.589256 140406042187584 spec.py:321] Evaluating on the training split.
I0327 14:30:57.199983 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 14:31:46.689552 140406042187584 spec.py:349] Evaluating on the test split.
I0327 14:32:11.733353 140406042187584 submission_runner.py:422] Time since start: 44056.45s, 	Step: 49051, 	{'train/ctc_loss': Array(0.15362187, dtype=float32), 'train/wer': 0.059070789241862334, 'validation/ctc_loss': Array(0.38932952, dtype=float32), 'validation/wer': 0.1158654913735675, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21907575, dtype=float32), 'test/wer': 0.07202486137346902, 'test/num_examples': 2472, 'score': 40386.9514503479, 'total_duration': 44056.453432798386, 'accumulated_submission_time': 40386.9514503479, 'accumulated_eval_time': 3666.125949382782, 'accumulated_logging_time': 1.519129991531372}
I0327 14:32:11.776802 140234158159616 logging_writer.py:48] [49051] accumulated_eval_time=3666.125949, accumulated_logging_time=1.519130, accumulated_submission_time=40386.951450, global_step=49051, preemption_count=0, score=40386.951450, test/ctc_loss=0.2190757542848587, test/num_examples=2472, test/wer=0.072025, total_duration=44056.453433, train/ctc_loss=0.1536218672990799, train/wer=0.059071, validation/ctc_loss=0.3893295228481293, validation/num_examples=5348, validation/wer=0.115865
I0327 14:38:05.112395 140234158159616 logging_writer.py:48] [49500] global_step=49500, grad_norm=1.1469142436981201, loss=1.111514687538147
I0327 14:44:38.257510 140234149766912 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.5622278451919556, loss=1.1271785497665405
I0327 14:51:57.784167 140234158159616 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.8751571774482727, loss=1.0944342613220215
I0327 14:56:12.067052 140406042187584 spec.py:321] Evaluating on the training split.
I0327 14:57:03.619626 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 14:57:53.370610 140406042187584 spec.py:349] Evaluating on the test split.
I0327 14:58:18.528904 140406042187584 submission_runner.py:422] Time since start: 45623.25s, 	Step: 50831, 	{'train/ctc_loss': Array(0.17177925, dtype=float32), 'train/wer': 0.06501490818836998, 'validation/ctc_loss': Array(0.38417214, dtype=float32), 'validation/wer': 0.11419523639418018, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21522643, dtype=float32), 'test/wer': 0.07143582556415412, 'test/num_examples': 2472, 'score': 41827.16106724739, 'total_duration': 45623.25108194351, 'accumulated_submission_time': 41827.16106724739, 'accumulated_eval_time': 3792.582904100418, 'accumulated_logging_time': 1.5789299011230469}
I0327 14:58:18.566124 140234373199616 logging_writer.py:48] [50831] accumulated_eval_time=3792.582904, accumulated_logging_time=1.578930, accumulated_submission_time=41827.161067, global_step=50831, preemption_count=0, score=41827.161067, test/ctc_loss=0.2152264267206192, test/num_examples=2472, test/wer=0.071436, total_duration=45623.251082, train/ctc_loss=0.17177924513816833, train/wer=0.065015, validation/ctc_loss=0.38417214155197144, validation/num_examples=5348, validation/wer=0.114195
I0327 15:00:29.479077 140234364806912 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.6117901802062988, loss=1.0720722675323486
I0327 15:07:33.209900 140234373199616 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.59162437915802, loss=1.0989089012145996
I0327 15:14:01.666694 140234364806912 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.8786659240722656, loss=1.0942859649658203
I0327 15:21:20.675151 140234373199616 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.7467918992042542, loss=1.075315237045288
I0327 15:22:19.022604 140406042187584 spec.py:321] Evaluating on the training split.
I0327 15:23:11.681806 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 15:24:02.745234 140406042187584 spec.py:349] Evaluating on the test split.
I0327 15:24:28.047305 140406042187584 submission_runner.py:422] Time since start: 47192.77s, 	Step: 52570, 	{'train/ctc_loss': Array(0.15486632, dtype=float32), 'train/wer': 0.058913846418141215, 'validation/ctc_loss': Array(0.37977183, dtype=float32), 'validation/wer': 0.11201328480261062, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21341266, dtype=float32), 'test/wer': 0.06956716023805172, 'test/num_examples': 2472, 'score': 43267.5399684906, 'total_duration': 47192.76909375191, 'accumulated_submission_time': 43267.5399684906, 'accumulated_eval_time': 3921.6024510860443, 'accumulated_logging_time': 1.6307737827301025}
I0327 15:24:28.087462 140234373199616 logging_writer.py:48] [52570] accumulated_eval_time=3921.602451, accumulated_logging_time=1.630774, accumulated_submission_time=43267.539968, global_step=52570, preemption_count=0, score=43267.539968, test/ctc_loss=0.21341265738010406, test/num_examples=2472, test/wer=0.069567, total_duration=47192.769094, train/ctc_loss=0.1548663228750229, train/wer=0.058914, validation/ctc_loss=0.3797718286514282, validation/num_examples=5348, validation/wer=0.112013
I0327 15:30:00.770936 140234364806912 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.9968075752258301, loss=1.0774195194244385
I0327 15:37:03.271403 140234373199616 logging_writer.py:48] [53500] global_step=53500, grad_norm=1.454361915588379, loss=1.1418046951293945
I0327 15:43:38.006601 140234373199616 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.5361612439155579, loss=1.0734093189239502
I0327 15:48:28.477983 140406042187584 spec.py:321] Evaluating on the training split.
I0327 15:49:20.376485 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 15:50:10.321248 140406042187584 spec.py:349] Evaluating on the test split.
I0327 15:50:35.153495 140406042187584 submission_runner.py:422] Time since start: 48759.87s, 	Step: 54345, 	{'train/ctc_loss': Array(0.1578446, dtype=float32), 'train/wer': 0.06065372807255449, 'validation/ctc_loss': Array(0.3755843, dtype=float32), 'validation/wer': 0.11206155806791084, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20987432, dtype=float32), 'test/wer': 0.06899843600836837, 'test/num_examples': 2472, 'score': 44707.850329875946, 'total_duration': 48759.87359333038, 'accumulated_submission_time': 44707.850329875946, 'accumulated_eval_time': 4048.2709839344025, 'accumulated_logging_time': 1.6867382526397705}
I0327 15:50:35.198135 140234373199616 logging_writer.py:48] [54345] accumulated_eval_time=4048.270984, accumulated_logging_time=1.686738, accumulated_submission_time=44707.850330, global_step=54345, preemption_count=0, score=44707.850330, test/ctc_loss=0.20987431704998016, test/num_examples=2472, test/wer=0.068998, total_duration=48759.873593, train/ctc_loss=0.15784460306167603, train/wer=0.060654, validation/ctc_loss=0.37558430433273315, validation/num_examples=5348, validation/wer=0.112062
I0327 15:52:35.402497 140234364806912 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.8088212609291077, loss=1.059517741203308
I0327 15:59:04.776689 140234045519616 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.7980443835258484, loss=1.0717594623565674
I0327 16:06:17.042359 140234037126912 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.609747588634491, loss=1.1135050058364868
I0327 16:12:59.274552 140234045519616 logging_writer.py:48] [56000] global_step=56000, grad_norm=1.4558683633804321, loss=1.1134504079818726
I0327 16:14:35.837829 140406042187584 spec.py:321] Evaluating on the training split.
I0327 16:15:27.545770 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 16:16:16.801311 140406042187584 spec.py:349] Evaluating on the test split.
I0327 16:16:41.720283 140406042187584 submission_runner.py:422] Time since start: 50326.44s, 	Step: 56125, 	{'train/ctc_loss': Array(0.154804, dtype=float32), 'train/wer': 0.057971014492753624, 'validation/ctc_loss': Array(0.3748947, dtype=float32), 'validation/wer': 0.11103816484354635, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20994216, dtype=float32), 'test/wer': 0.06806410334531716, 'test/num_examples': 2472, 'score': 46148.409047842026, 'total_duration': 50326.44254684448, 'accumulated_submission_time': 46148.409047842026, 'accumulated_eval_time': 4174.148626804352, 'accumulated_logging_time': 1.7477538585662842}
I0327 16:16:41.762109 140235587352320 logging_writer.py:48] [56125] accumulated_eval_time=4174.148627, accumulated_logging_time=1.747754, accumulated_submission_time=46148.409048, global_step=56125, preemption_count=0, score=46148.409048, test/ctc_loss=0.20994216203689575, test/num_examples=2472, test/wer=0.068064, total_duration=50326.442547, train/ctc_loss=0.15480400621891022, train/wer=0.057971, validation/ctc_loss=0.3748947083950043, validation/num_examples=5348, validation/wer=0.111038
I0327 16:21:34.834670 140235578959616 logging_writer.py:48] [56500] global_step=56500, grad_norm=1.07061767578125, loss=1.0279074907302856
I0327 16:28:18.933466 140235587352320 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.7895989418029785, loss=1.0249040126800537
I0327 16:35:14.592927 140235578959616 logging_writer.py:48] [57500] global_step=57500, grad_norm=1.808050513267517, loss=1.0411120653152466
I0327 16:40:41.894786 140406042187584 spec.py:321] Evaluating on the training split.
I0327 16:41:32.726012 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 16:42:23.135549 140406042187584 spec.py:349] Evaluating on the test split.
I0327 16:42:48.397646 140406042187584 submission_runner.py:422] Time since start: 51893.12s, 	Step: 57899, 	{'train/ctc_loss': Array(0.16507912, dtype=float32), 'train/wer': 0.06262376520952806, 'validation/ctc_loss': Array(0.37280464, dtype=float32), 'validation/wer': 0.11026579259874296, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20841596, dtype=float32), 'test/wer': 0.06828753072126419, 'test/num_examples': 2472, 'score': 47588.45893073082, 'total_duration': 51893.119094610214, 'accumulated_submission_time': 47588.45893073082, 'accumulated_eval_time': 4300.646054029465, 'accumulated_logging_time': 1.8065762519836426}
I0327 16:42:48.436530 140235259672320 logging_writer.py:48] [57899] accumulated_eval_time=4300.646054, accumulated_logging_time=1.806576, accumulated_submission_time=47588.458931, global_step=57899, preemption_count=0, score=47588.458931, test/ctc_loss=0.2084159553050995, test/num_examples=2472, test/wer=0.068288, total_duration=51893.119095, train/ctc_loss=0.16507911682128906, train/wer=0.062624, validation/ctc_loss=0.3728046417236328, validation/num_examples=5348, validation/wer=0.110266
I0327 16:44:07.205461 140235251279616 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.935669481754303, loss=1.0357000827789307
I0327 16:50:54.380225 140235259672320 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.7988073229789734, loss=1.0685564279556274
I0327 16:57:44.181299 140235587352320 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.8009056448936462, loss=1.0399667024612427
I0327 17:04:36.681912 140235578959616 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.9515138864517212, loss=1.0365321636199951
I0327 17:06:48.838304 140406042187584 spec.py:321] Evaluating on the training split.
I0327 17:07:41.399997 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 17:08:31.283914 140406042187584 spec.py:349] Evaluating on the test split.
I0327 17:08:56.296306 140406042187584 submission_runner.py:422] Time since start: 53461.02s, 	Step: 59654, 	{'train/ctc_loss': Array(0.1756524, dtype=float32), 'train/wer': 0.06469793833520296, 'validation/ctc_loss': Array(0.37235644, dtype=float32), 'validation/wer': 0.11000511696612182, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20838574, dtype=float32), 'test/wer': 0.06840940019905348, 'test/num_examples': 2472, 'score': 49028.78290057182, 'total_duration': 53461.01828098297, 'accumulated_submission_time': 49028.78290057182, 'accumulated_eval_time': 4428.098953485489, 'accumulated_logging_time': 1.8600318431854248}
I0327 17:08:56.335312 140235587352320 logging_writer.py:48] [59654] accumulated_eval_time=4428.098953, accumulated_logging_time=1.860032, accumulated_submission_time=49028.782901, global_step=59654, preemption_count=0, score=49028.782901, test/ctc_loss=0.20838573575019836, test/num_examples=2472, test/wer=0.068409, total_duration=53461.018281, train/ctc_loss=0.1756523996591568, train/wer=0.064698, validation/ctc_loss=0.37235644459724426, validation/num_examples=5348, validation/wer=0.110005
I0327 17:13:27.225325 140235259672320 logging_writer.py:48] [60000] global_step=60000, grad_norm=1.359001636505127, loss=1.0437242984771729
I0327 17:20:20.247378 140235251279616 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.7102457284927368, loss=1.0495283603668213
I0327 17:27:13.575222 140235259672320 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.7166847586631775, loss=1.0899772644042969
I0327 17:32:57.350847 140406042187584 spec.py:321] Evaluating on the training split.
I0327 17:33:49.057572 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 17:34:38.962521 140406042187584 spec.py:349] Evaluating on the test split.
I0327 17:35:04.322347 140406042187584 submission_runner.py:422] Time since start: 55029.04s, 	Step: 61428, 	{'train/ctc_loss': Array(0.15365617, dtype=float32), 'train/wer': 0.05921155872396709, 'validation/ctc_loss': Array(0.3723714, dtype=float32), 'validation/wer': 0.11003408092530195, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20837489, dtype=float32), 'test/wer': 0.06834846546015884, 'test/num_examples': 2472, 'score': 50469.71608543396, 'total_duration': 55029.04315876961, 'accumulated_submission_time': 50469.71608543396, 'accumulated_eval_time': 4555.0641849040985, 'accumulated_logging_time': 1.915844440460205}
I0327 17:35:04.366232 140235044632320 logging_writer.py:48] [61428] accumulated_eval_time=4555.064185, accumulated_logging_time=1.915844, accumulated_submission_time=50469.716085, global_step=61428, preemption_count=0, score=50469.716085, test/ctc_loss=0.20837488770484924, test/num_examples=2472, test/wer=0.068348, total_duration=55029.043159, train/ctc_loss=0.15365616977214813, train/wer=0.059212, validation/ctc_loss=0.3723714053630829, validation/num_examples=5348, validation/wer=0.110034
I0327 17:36:00.650286 140235036239616 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.8963697552680969, loss=1.096970796585083
I0327 17:42:31.103423 140235587352320 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.6371318697929382, loss=1.0589075088500977
I0327 17:49:17.194685 140235578959616 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.8010609745979309, loss=1.0769495964050293
I0327 17:56:18.070602 140234716952320 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.6068140864372253, loss=1.0502525568008423
I0327 17:59:05.041315 140406042187584 spec.py:321] Evaluating on the training split.
I0327 17:59:55.368081 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 18:00:45.705429 140406042187584 spec.py:349] Evaluating on the test split.
I0327 18:01:11.149084 140406042187584 submission_runner.py:422] Time since start: 56595.87s, 	Step: 63218, 	{'train/ctc_loss': Array(0.16204578, dtype=float32), 'train/wer': 0.06031217196841769, 'validation/ctc_loss': Array(0.3723746, dtype=float32), 'validation/wer': 0.11000511696612182, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20837016, dtype=float32), 'test/wer': 0.06832815388052729, 'test/num_examples': 2472, 'score': 51910.30851364136, 'total_duration': 56595.87143230438, 'accumulated_submission_time': 51910.30851364136, 'accumulated_eval_time': 4681.167226076126, 'accumulated_logging_time': 1.9789986610412598}
I0327 18:01:11.188346 140235802392320 logging_writer.py:48] [63218] accumulated_eval_time=4681.167226, accumulated_logging_time=1.978999, accumulated_submission_time=51910.308514, global_step=63218, preemption_count=0, score=51910.308514, test/ctc_loss=0.2083701640367508, test/num_examples=2472, test/wer=0.068328, total_duration=56595.871432, train/ctc_loss=0.16204577684402466, train/wer=0.060312, validation/ctc_loss=0.37237459421157837, validation/num_examples=5348, validation/wer=0.110005
I0327 18:04:49.392390 140235793999616 logging_writer.py:48] [63500] global_step=63500, grad_norm=1.082963466644287, loss=1.0632636547088623
I0327 18:11:48.002974 140235147032320 logging_writer.py:48] [64000] global_step=64000, grad_norm=1.6593146324157715, loss=1.0196821689605713
I0327 18:18:28.213313 140235138639616 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.6608604788780212, loss=1.075587511062622
I0327 18:25:11.336656 140406042187584 spec.py:321] Evaluating on the training split.
I0327 18:26:02.640806 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 18:26:51.877070 140406042187584 spec.py:349] Evaluating on the test split.
I0327 18:27:17.275478 140406042187584 submission_runner.py:422] Time since start: 58162.00s, 	Step: 64972, 	{'train/ctc_loss': Array(0.15897346, dtype=float32), 'train/wer': 0.05968508681872963, 'validation/ctc_loss': Array(0.37237266, dtype=float32), 'validation/wer': 0.11000511696612182, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20837186, dtype=float32), 'test/wer': 0.06838908861942193, 'test/num_examples': 2472, 'score': 53350.37599372864, 'total_duration': 58161.99714946747, 'accumulated_submission_time': 53350.37599372864, 'accumulated_eval_time': 4807.100838661194, 'accumulated_logging_time': 2.034437417984009}
I0327 18:27:17.316696 140234931992320 logging_writer.py:48] [64972] accumulated_eval_time=4807.100839, accumulated_logging_time=2.034437, accumulated_submission_time=53350.375994, global_step=64972, preemption_count=0, score=53350.375994, test/ctc_loss=0.2083718627691269, test/num_examples=2472, test/wer=0.068389, total_duration=58161.997149, train/ctc_loss=0.15897345542907715, train/wer=0.059685, validation/ctc_loss=0.37237265706062317, validation/num_examples=5348, validation/wer=0.110005
I0327 18:27:39.660255 140234923599616 logging_writer.py:48] [65000] global_step=65000, grad_norm=1.3609991073608398, loss=1.075300931930542
I0327 18:34:05.495347 140234931992320 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.7732268571853638, loss=1.0673414468765259
I0327 18:41:14.544394 140234931992320 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.9412103295326233, loss=1.063977837562561
I0327 18:47:51.319469 140234923599616 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.8386645913124084, loss=1.092491865158081
I0327 18:51:17.665849 140406042187584 spec.py:321] Evaluating on the training split.
I0327 18:52:08.746572 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 18:52:58.826119 140406042187584 spec.py:349] Evaluating on the test split.
I0327 18:53:24.243219 140406042187584 submission_runner.py:422] Time since start: 59728.96s, 	Step: 66736, 	{'train/ctc_loss': Array(0.16451569, dtype=float32), 'train/wer': 0.061530315366352194, 'validation/ctc_loss': Array(0.37238693, dtype=float32), 'validation/wer': 0.10999546231306179, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20837137, dtype=float32), 'test/wer': 0.06838908861942193, 'test/num_examples': 2472, 'score': 54790.64636230469, 'total_duration': 59728.964972019196, 'accumulated_submission_time': 54790.64636230469, 'accumulated_eval_time': 4933.6728801727295, 'accumulated_logging_time': 2.0890610218048096}
I0327 18:53:24.285917 140235802392320 logging_writer.py:48] [66736] accumulated_eval_time=4933.672880, accumulated_logging_time=2.089061, accumulated_submission_time=54790.646362, global_step=66736, preemption_count=0, score=54790.646362, test/ctc_loss=0.2083713710308075, test/num_examples=2472, test/wer=0.068389, total_duration=59728.964972, train/ctc_loss=0.1645156890153885, train/wer=0.061530, validation/ctc_loss=0.3723869323730469, validation/num_examples=5348, validation/wer=0.109995
I0327 18:56:51.732963 140235802392320 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.6474193930625916, loss=1.0044279098510742
I0327 19:03:23.975282 140235793999616 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.7104071974754333, loss=1.0357513427734375
I0327 19:10:38.325309 140235802392320 logging_writer.py:48] [68000] global_step=68000, grad_norm=1.707167625427246, loss=1.0676072835922241
I0327 19:17:06.101589 140235793999616 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.5802466869354248, loss=1.0924149751663208
I0327 19:17:24.957556 140406042187584 spec.py:321] Evaluating on the training split.
I0327 19:18:16.720405 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 19:19:06.685931 140406042187584 spec.py:349] Evaluating on the test split.
I0327 19:19:31.822066 140406042187584 submission_runner.py:422] Time since start: 61296.54s, 	Step: 68523, 	{'train/ctc_loss': Array(0.16200455, dtype=float32), 'train/wer': 0.060407862078218286, 'validation/ctc_loss': Array(0.37237912, dtype=float32), 'validation/wer': 0.10997615300694169, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2083734, dtype=float32), 'test/wer': 0.06836877703979038, 'test/num_examples': 2472, 'score': 56231.2382543087, 'total_duration': 61296.541285037994, 'accumulated_submission_time': 56231.2382543087, 'accumulated_eval_time': 5060.529519081116, 'accumulated_logging_time': 2.145503044128418}
I0327 19:19:31.867066 140235587352320 logging_writer.py:48] [68523] accumulated_eval_time=5060.529519, accumulated_logging_time=2.145503, accumulated_submission_time=56231.238254, global_step=68523, preemption_count=0, score=56231.238254, test/ctc_loss=0.20837339758872986, test/num_examples=2472, test/wer=0.068369, total_duration=61296.541285, train/ctc_loss=0.16200454533100128, train/wer=0.060408, validation/ctc_loss=0.3723791241645813, validation/num_examples=5348, validation/wer=0.109976
I0327 19:25:53.290004 140235578959616 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.5654764175415039, loss=1.0681402683258057
I0327 19:32:23.319864 140235587352320 logging_writer.py:48] [69500] global_step=69500, grad_norm=1.0337309837341309, loss=1.0791895389556885
I0327 19:39:36.804247 140235578959616 logging_writer.py:48] [70000] global_step=70000, grad_norm=1.1001819372177124, loss=1.044814944267273
I0327 19:43:32.283786 140406042187584 spec.py:321] Evaluating on the training split.
I0327 19:44:23.643829 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 19:45:13.508325 140406042187584 spec.py:349] Evaluating on the test split.
I0327 19:45:38.577176 140406042187584 submission_runner.py:422] Time since start: 62863.30s, 	Step: 70298, 	{'train/ctc_loss': Array(0.16449541, dtype=float32), 'train/wer': 0.062578174291312, 'validation/ctc_loss': Array(0.3723822, dtype=float32), 'validation/wer': 0.10999546231306179, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20837566, dtype=float32), 'test/wer': 0.06834846546015884, 'test/num_examples': 2472, 'score': 57671.5746421814, 'total_duration': 62863.2994260788, 'accumulated_submission_time': 57671.5746421814, 'accumulated_eval_time': 5186.81823182106, 'accumulated_logging_time': 2.205833673477173}
I0327 19:45:38.628895 140235295512320 logging_writer.py:48] [70298] accumulated_eval_time=5186.818232, accumulated_logging_time=2.205834, accumulated_submission_time=57671.574642, global_step=70298, preemption_count=0, score=57671.574642, test/ctc_loss=0.20837566256523132, test/num_examples=2472, test/wer=0.068348, total_duration=62863.299426, train/ctc_loss=0.16449540853500366, train/wer=0.062578, validation/ctc_loss=0.37238219380378723, validation/num_examples=5348, validation/wer=0.109995
I0327 19:48:15.363658 140235287119616 logging_writer.py:48] [70500] global_step=70500, grad_norm=2.222839117050171, loss=1.0349187850952148
I0327 19:55:04.719964 140235295512320 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.7634116411209106, loss=1.0291860103607178
I0327 20:01:40.032227 140235295512320 logging_writer.py:48] [71500] global_step=71500, grad_norm=1.1779470443725586, loss=1.0514227151870728
I0327 20:08:45.333691 140235287119616 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.9221535921096802, loss=1.0838840007781982
I0327 20:09:39.279781 140406042187584 spec.py:321] Evaluating on the training split.
I0327 20:10:29.956053 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 20:11:19.838851 140406042187584 spec.py:349] Evaluating on the test split.
I0327 20:11:45.020843 140406042187584 submission_runner.py:422] Time since start: 64429.74s, 	Step: 72063, 	{'train/ctc_loss': Array(0.1460689, dtype=float32), 'train/wer': 0.055949749255115255, 'validation/ctc_loss': Array(0.3723792, dtype=float32), 'validation/wer': 0.10999546231306179, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20837334, dtype=float32), 'test/wer': 0.06836877703979038, 'test/num_examples': 2472, 'score': 59112.14495778084, 'total_duration': 64429.74206542969, 'accumulated_submission_time': 59112.14495778084, 'accumulated_eval_time': 5312.5534245967865, 'accumulated_logging_time': 2.2725188732147217}
I0327 20:11:45.060984 140234491668224 logging_writer.py:48] [72063] accumulated_eval_time=5312.553425, accumulated_logging_time=2.272519, accumulated_submission_time=59112.144958, global_step=72063, preemption_count=0, score=59112.144958, test/ctc_loss=0.20837333798408508, test/num_examples=2472, test/wer=0.068369, total_duration=64429.742065, train/ctc_loss=0.14606890082359314, train/wer=0.055950, validation/ctc_loss=0.37237921357154846, validation/num_examples=5348, validation/wer=0.109995
I0327 20:17:26.229560 140234491668224 logging_writer.py:48] [72500] global_step=72500, grad_norm=1.4078844785690308, loss=0.9764647483825684
I0327 20:24:30.684703 140234483275520 logging_writer.py:48] [73000] global_step=73000, grad_norm=1.3490524291992188, loss=1.0782707929611206
I0327 20:31:11.135834 140234491668224 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.560973584651947, loss=1.0717636346817017
I0327 20:35:45.647209 140406042187584 spec.py:321] Evaluating on the training split.
I0327 20:36:36.677806 140406042187584 spec.py:333] Evaluating on the validation split.
I0327 20:37:26.593547 140406042187584 spec.py:349] Evaluating on the test split.
I0327 20:37:52.142519 140406042187584 submission_runner.py:422] Time since start: 65996.86s, 	Step: 73834, 	{'train/ctc_loss': Array(0.1430181, dtype=float32), 'train/wer': 0.05336074719772533, 'validation/ctc_loss': Array(0.37238505, dtype=float32), 'validation/wer': 0.1100244262722419, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20837466, dtype=float32), 'test/wer': 0.06838908861942193, 'test/num_examples': 2472, 'score': 60552.652999162674, 'total_duration': 65996.86452555656, 'accumulated_submission_time': 60552.652999162674, 'accumulated_eval_time': 5439.043668746948, 'accumulated_logging_time': 2.3270580768585205}
I0327 20:37:52.187551 140234491668224 logging_writer.py:48] [73834] accumulated_eval_time=5439.043669, accumulated_logging_time=2.327058, accumulated_submission_time=60552.652999, global_step=73834, preemption_count=0, score=60552.652999, test/ctc_loss=0.20837466418743134, test/num_examples=2472, test/wer=0.068389, total_duration=65996.864526, train/ctc_loss=0.14301809668540955, train/wer=0.053361, validation/ctc_loss=0.37238505482673645, validation/num_examples=5348, validation/wer=0.110024
I0327 20:40:01.052759 140234483275520 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.668321430683136, loss=1.0973680019378662
I0327 20:46:28.270699 140234491668224 logging_writer.py:48] [74499] global_step=74499, preemption_count=0, score=61068.673444
I0327 20:46:31.562356 140406042187584 submission_runner.py:596] Tuning trial 1/1
I0327 20:46:31.562653 140406042187584 submission_runner.py:597] Hyperparameters: Hyperparameters(learning_rate=0.0007852999990476642, beta1=0.6994142393023162, beta2=0.9918636824608852, warmup_steps=6000, weight_decay=0.07286322158086678)
I0327 20:46:31.586379 140406042187584 submission_runner.py:598] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(31.331608, dtype=float32), 'train/wer': 0.9451200466443163, 'validation/ctc_loss': Array(30.673698, dtype=float32), 'validation/wer': 0.9064850304604304, 'validation/num_examples': 5348, 'test/ctc_loss': Array(30.753916, dtype=float32), 'test/wer': 0.9097150285377694, 'test/num_examples': 2472, 'score': 59.39172101020813, 'total_duration': 204.67879557609558, 'accumulated_submission_time': 59.39172101020813, 'accumulated_eval_time': 145.28702449798584, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1649, {'train/ctc_loss': Array(6.258007, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(6.381837, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(6.350325, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1499.3281831741333, 'total_duration': 1747.6310703754425, 'accumulated_submission_time': 1499.3281831741333, 'accumulated_eval_time': 248.20428919792175, 'accumulated_logging_time': 0.04201531410217285, 'global_step': 1649, 'preemption_count': 0}), (3341, {'train/ctc_loss': Array(3.233811, dtype=float32), 'train/wer': 0.6867645100285007, 'validation/ctc_loss': Array(3.4814465, dtype=float32), 'validation/wer': 0.6947584888537031, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3.2425442, dtype=float32), 'test/wer': 0.6666666666666666, 'test/num_examples': 2472, 'score': 2939.2526803016663, 'total_duration': 3309.2614362239838, 'accumulated_submission_time': 2939.2526803016663, 'accumulated_eval_time': 369.7960579395294, 'accumulated_logging_time': 0.09071874618530273, 'global_step': 3341, 'preemption_count': 0}), (4986, {'train/ctc_loss': Array(1.1162223, dtype=float32), 'train/wer': 0.35403324003352465, 'validation/ctc_loss': Array(1.4812187, dtype=float32), 'validation/wer': 0.4057754134605173, 'validation/num_examples': 5348, 'test/ctc_loss': Array(1.1754295, dtype=float32), 'test/wer': 0.351187211829464, 'test/num_examples': 2472, 'score': 4379.337501049042, 'total_duration': 4879.025956392288, 'accumulated_submission_time': 4379.337501049042, 'accumulated_eval_time': 499.3581876754761, 'accumulated_logging_time': 0.14188861846923828, 'global_step': 4986, 'preemption_count': 0}), (6667, {'train/ctc_loss': Array(0.7248324, dtype=float32), 'train/wer': 0.24071359004702111, 'validation/ctc_loss': Array(1.0661641, dtype=float32), 'validation/wer': 0.3064579974318623, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.77094764, dtype=float32), 'test/wer': 0.24564824406394084, 'test/num_examples': 2472, 'score': 5819.764954805374, 'total_duration': 6454.353077173233, 'accumulated_submission_time': 5819.764954805374, 'accumulated_eval_time': 634.1376528739929, 'accumulated_logging_time': 0.19594025611877441, 'global_step': 6667, 'preemption_count': 0}), (8309, {'train/ctc_loss': Array(0.5645023, dtype=float32), 'train/wer': 0.19665572202048315, 'validation/ctc_loss': Array(0.8911399, dtype=float32), 'validation/wer': 0.2630506772739122, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.6137322, dtype=float32), 'test/wer': 0.20157211626348182, 'test/num_examples': 2472, 'score': 7260.00180888176, 'total_duration': 8022.13317990303, 'accumulated_submission_time': 7260.00180888176, 'accumulated_eval_time': 761.5667946338654, 'accumulated_logging_time': 0.24703526496887207, 'global_step': 8309, 'preemption_count': 0}), (10067, {'train/ctc_loss': Array(0.4868831, dtype=float32), 'train/wer': 0.16960165589940837, 'validation/ctc_loss': Array(0.7654884, dtype=float32), 'validation/wer': 0.22954903115556544, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.50620687, dtype=float32), 'test/wer': 0.17145004366989622, 'test/num_examples': 2472, 'score': 8700.331828832626, 'total_duration': 9591.559700727463, 'accumulated_submission_time': 8700.331828832626, 'accumulated_eval_time': 890.5427072048187, 'accumulated_logging_time': 0.30434226989746094, 'global_step': 10067, 'preemption_count': 0}), (11832, {'train/ctc_loss': Array(0.43912536, dtype=float32), 'train/wer': 0.15616957011890328, 'validation/ctc_loss': Array(0.7563776, dtype=float32), 'validation/wer': 0.2247506685847244, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.4932181, dtype=float32), 'test/wer': 0.16580342453232588, 'test/num_examples': 2472, 'score': 10140.590170145035, 'total_duration': 11159.885578393936, 'accumulated_submission_time': 10140.590170145035, 'accumulated_eval_time': 1018.4696464538574, 'accumulated_logging_time': 0.37738537788391113, 'global_step': 11832, 'preemption_count': 0}), (13659, {'train/ctc_loss': Array(0.35610482, dtype=float32), 'train/wer': 0.13272836007176472, 'validation/ctc_loss': Array(0.6884118, dtype=float32), 'validation/wer': 0.20827983046429227, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.44046697, dtype=float32), 'test/wer': 0.1493307334511405, 'test/num_examples': 2472, 'score': 11581.088110208511, 'total_duration': 12725.48209643364, 'accumulated_submission_time': 11581.088110208511, 'accumulated_eval_time': 1143.428656578064, 'accumulated_logging_time': 0.4524505138397217, 'global_step': 13659, 'preemption_count': 0}), (15446, {'train/ctc_loss': Array(0.31225345, dtype=float32), 'train/wer': 0.1149444855268688, 'validation/ctc_loss': Array(0.63623434, dtype=float32), 'validation/wer': 0.19359510315996795, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.40065524, dtype=float32), 'test/wer': 0.13519387402758312, 'test/num_examples': 2472, 'score': 13021.669342517853, 'total_duration': 14290.482184648514, 'accumulated_submission_time': 13021.669342517853, 'accumulated_eval_time': 1267.7388746738434, 'accumulated_logging_time': 0.49801135063171387, 'global_step': 15446, 'preemption_count': 0}), (17229, {'train/ctc_loss': Array(0.3181728, dtype=float32), 'train/wer': 0.12069159690454612, 'validation/ctc_loss': Array(0.64264226, dtype=float32), 'validation/wer': 0.19314133446614595, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.39903724, dtype=float32), 'test/wer': 0.1338329981922694, 'test/num_examples': 2472, 'score': 14461.909377336502, 'total_duration': 15856.259784698486, 'accumulated_submission_time': 14461.909377336502, 'accumulated_eval_time': 1393.162992477417, 'accumulated_logging_time': 0.5475101470947266, 'global_step': 17229, 'preemption_count': 0}), (19046, {'train/ctc_loss': Array(0.296525, dtype=float32), 'train/wer': 0.10833707924588169, 'validation/ctc_loss': Array(0.5774814, dtype=float32), 'validation/wer': 0.1760912171621113, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.35609692, dtype=float32), 'test/wer': 0.12186947778928767, 'test/num_examples': 2472, 'score': 15902.316984415054, 'total_duration': 17421.807481765747, 'accumulated_submission_time': 15902.316984415054, 'accumulated_eval_time': 1518.1844382286072, 'accumulated_logging_time': 0.6014482975006104, 'global_step': 19046, 'preemption_count': 0}), (20823, {'train/ctc_loss': Array(0.2906837, dtype=float32), 'train/wer': 0.10649220460600944, 'validation/ctc_loss': Array(0.5690938, dtype=float32), 'validation/wer': 0.17209419079525376, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.34827888, dtype=float32), 'test/wer': 0.11928990717608108, 'test/num_examples': 2472, 'score': 17342.500242233276, 'total_duration': 18988.60622382164, 'accumulated_submission_time': 17342.500242233276, 'accumulated_eval_time': 1644.6834568977356, 'accumulated_logging_time': 0.6532752513885498, 'global_step': 20823, 'preemption_count': 0}), (22560, {'train/ctc_loss': Array(0.2656108, dtype=float32), 'train/wer': 0.09865212470418767, 'validation/ctc_loss': Array(0.54345155, dtype=float32), 'validation/wer': 0.16567384651032566, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.33042258, dtype=float32), 'test/wer': 0.11218085430503931, 'test/num_examples': 2472, 'score': 18782.465693712234, 'total_duration': 20554.51053571701, 'accumulated_submission_time': 18782.465693712234, 'accumulated_eval_time': 1770.5128767490387, 'accumulated_logging_time': 0.6983656883239746, 'global_step': 22560, 'preemption_count': 0}), (24340, {'train/ctc_loss': Array(0.2391195, dtype=float32), 'train/wer': 0.08944934757822727, 'validation/ctc_loss': Array(0.5295098, dtype=float32), 'validation/wer': 0.1602286221844618, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.3209627, dtype=float32), 'test/wer': 0.10901224788251783, 'test/num_examples': 2472, 'score': 20223.338948726654, 'total_duration': 22121.02773118019, 'accumulated_submission_time': 20223.338948726654, 'accumulated_eval_time': 1896.033747434616, 'accumulated_logging_time': 0.7552995681762695, 'global_step': 24340, 'preemption_count': 0}), (26139, {'train/ctc_loss': Array(0.22598623, dtype=float32), 'train/wer': 0.0868358590469235, 'validation/ctc_loss': Array(0.524366, dtype=float32), 'validation/wer': 0.15879973353157553, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30867016, dtype=float32), 'test/wer': 0.10618893831373266, 'test/num_examples': 2472, 'score': 21663.641766548157, 'total_duration': 23688.287184000015, 'accumulated_submission_time': 21663.641766548157, 'accumulated_eval_time': 2022.8654391765594, 'accumulated_logging_time': 0.811131477355957, 'global_step': 26139, 'preemption_count': 0}), (27884, {'train/ctc_loss': Array(0.21838745, dtype=float32), 'train/wer': 0.0825040144828221, 'validation/ctc_loss': Array(0.515495, dtype=float32), 'validation/wer': 0.15555577010340133, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.30463678, dtype=float32), 'test/wer': 0.10297970873194809, 'test/num_examples': 2472, 'score': 23103.543451070786, 'total_duration': 25255.599942207336, 'accumulated_submission_time': 23103.543451070786, 'accumulated_eval_time': 2150.1531643867493, 'accumulated_logging_time': 0.8675537109375, 'global_step': 27884, 'preemption_count': 0}), (29663, {'train/ctc_loss': Array(0.22144225, dtype=float32), 'train/wer': 0.0828241462991853, 'validation/ctc_loss': Array(0.49088266, dtype=float32), 'validation/wer': 0.14791893953290788, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28876853, dtype=float32), 'test/wer': 0.09916113176121708, 'test/num_examples': 2472, 'score': 24543.958880662918, 'total_duration': 26821.993340969086, 'accumulated_submission_time': 24543.958880662918, 'accumulated_eval_time': 2276.0146985054016, 'accumulated_logging_time': 0.9153838157653809, 'global_step': 29663, 'preemption_count': 0}), (31448, {'train/ctc_loss': Array(0.21021047, dtype=float32), 'train/wer': 0.07750991595179572, 'validation/ctc_loss': Array(0.49241236, dtype=float32), 'validation/wer': 0.1481023779410487, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.28568774, dtype=float32), 'test/wer': 0.09704872747953608, 'test/num_examples': 2472, 'score': 25984.15348649025, 'total_duration': 28387.823632001877, 'accumulated_submission_time': 25984.15348649025, 'accumulated_eval_time': 2401.529518842697, 'accumulated_logging_time': 0.9679491519927979, 'global_step': 31448, 'preemption_count': 0}), (33203, {'train/ctc_loss': Array(0.21211581, dtype=float32), 'train/wer': 0.07885179458465395, 'validation/ctc_loss': Array(0.47753543, dtype=float32), 'validation/wer': 0.14282128271720557, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2814209, dtype=float32), 'test/wer': 0.09453009160522413, 'test/num_examples': 2472, 'score': 27424.557148456573, 'total_duration': 29954.343950748444, 'accumulated_submission_time': 27424.557148456573, 'accumulated_eval_time': 2527.510297060013, 'accumulated_logging_time': 1.0379853248596191, 'global_step': 33203, 'preemption_count': 0}), (34948, {'train/ctc_loss': Array(0.19727528, dtype=float32), 'train/wer': 0.07296162494538402, 'validation/ctc_loss': Array(0.45192993, dtype=float32), 'validation/wer': 0.13746295026888208, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2662599, dtype=float32), 'test/wer': 0.091930209412386, 'test/num_examples': 2472, 'score': 28865.29248404503, 'total_duration': 31521.889628887177, 'accumulated_submission_time': 28865.29248404503, 'accumulated_eval_time': 2654.2031893730164, 'accumulated_logging_time': 1.089148759841919, 'global_step': 34948, 'preemption_count': 0}), (36698, {'train/ctc_loss': Array(0.15301058, dtype=float32), 'train/wer': 0.059951006016625154, 'validation/ctc_loss': Array(0.45113835, dtype=float32), 'validation/wer': 0.13586027786091506, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.26068655, dtype=float32), 'test/wer': 0.08817256718054964, 'test/num_examples': 2472, 'score': 30305.226006507874, 'total_duration': 33090.79103398323, 'accumulated_submission_time': 30305.226006507874, 'accumulated_eval_time': 2783.0452823638916, 'accumulated_logging_time': 1.1461570262908936, 'global_step': 36698, 'preemption_count': 0}), (38483, {'train/ctc_loss': Array(0.17294581, dtype=float32), 'train/wer': 0.06504614474879095, 'validation/ctc_loss': Array(0.43772656, dtype=float32), 'validation/wer': 0.1324618399837802, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.25223765, dtype=float32), 'test/wer': 0.08500396075802816, 'test/num_examples': 2472, 'score': 31745.207817554474, 'total_duration': 34658.81834125519, 'accumulated_submission_time': 31745.207817554474, 'accumulated_eval_time': 2910.9727005958557, 'accumulated_logging_time': 1.1947596073150635, 'global_step': 38483, 'preemption_count': 0}), (40225, {'train/ctc_loss': Array(0.20648734, dtype=float32), 'train/wer': 0.07711959558045811, 'validation/ctc_loss': Array(0.42702603, dtype=float32), 'validation/wer': 0.12834895778020217, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.24681981, dtype=float32), 'test/wer': 0.08268844068003169, 'test/num_examples': 2472, 'score': 33185.78887987137, 'total_duration': 36226.39206290245, 'accumulated_submission_time': 33185.78887987137, 'accumulated_eval_time': 3037.8453941345215, 'accumulated_logging_time': 1.2474281787872314, 'global_step': 40225, 'preemption_count': 0}), (42008, {'train/ctc_loss': Array(0.2051479, dtype=float32), 'train/wer': 0.07630303602258282, 'validation/ctc_loss': Array(0.4145394, dtype=float32), 'validation/wer': 0.12533670602546898, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.238132, dtype=float32), 'test/wer': 0.07994637742977272, 'test/num_examples': 2472, 'score': 34625.891954660416, 'total_duration': 37792.85226535797, 'accumulated_submission_time': 34625.891954660416, 'accumulated_eval_time': 3164.0814225673676, 'accumulated_logging_time': 1.298886775970459, 'global_step': 42008, 'preemption_count': 0}), (43787, {'train/ctc_loss': Array(0.2362001, dtype=float32), 'train/wer': 0.08832940433223556, 'validation/ctc_loss': Array(0.41255334, dtype=float32), 'validation/wer': 0.1231740637400195, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.23624404, dtype=float32), 'test/wer': 0.07880892897040603, 'test/num_examples': 2472, 'score': 36065.80715060234, 'total_duration': 39357.30459356308, 'accumulated_submission_time': 36065.80715060234, 'accumulated_eval_time': 3288.4946336746216, 'accumulated_logging_time': 1.3524503707885742, 'global_step': 43787, 'preemption_count': 0}), (45543, {'train/ctc_loss': Array(0.20127806, dtype=float32), 'train/wer': 0.07341226618937997, 'validation/ctc_loss': Array(0.40122727, dtype=float32), 'validation/wer': 0.12035490504648716, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.22649619, dtype=float32), 'test/wer': 0.07541689517193752, 'test/num_examples': 2472, 'score': 37506.3197286129, 'total_duration': 40922.39886689186, 'accumulated_submission_time': 37506.3197286129, 'accumulated_eval_time': 3412.9487121105194, 'accumulated_logging_time': 1.4097650051116943, 'global_step': 45543, 'preemption_count': 0}), (47289, {'train/ctc_loss': Array(0.18256974, dtype=float32), 'train/wer': 0.06944428980214445, 'validation/ctc_loss': Array(0.39311105, dtype=float32), 'validation/wer': 0.11752609169989477, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2217817, dtype=float32), 'test/wer': 0.07440131619036013, 'test/num_examples': 2472, 'score': 38946.892574071884, 'total_duration': 42490.132388830185, 'accumulated_submission_time': 38946.892574071884, 'accumulated_eval_time': 3539.9888577461243, 'accumulated_logging_time': 1.4624249935150146, 'global_step': 47289, 'preemption_count': 0}), (49051, {'train/ctc_loss': Array(0.15362187, dtype=float32), 'train/wer': 0.059070789241862334, 'validation/ctc_loss': Array(0.38932952, dtype=float32), 'validation/wer': 0.1158654913735675, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21907575, dtype=float32), 'test/wer': 0.07202486137346902, 'test/num_examples': 2472, 'score': 40386.9514503479, 'total_duration': 44056.453432798386, 'accumulated_submission_time': 40386.9514503479, 'accumulated_eval_time': 3666.125949382782, 'accumulated_logging_time': 1.519129991531372, 'global_step': 49051, 'preemption_count': 0}), (50831, {'train/ctc_loss': Array(0.17177925, dtype=float32), 'train/wer': 0.06501490818836998, 'validation/ctc_loss': Array(0.38417214, dtype=float32), 'validation/wer': 0.11419523639418018, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21522643, dtype=float32), 'test/wer': 0.07143582556415412, 'test/num_examples': 2472, 'score': 41827.16106724739, 'total_duration': 45623.25108194351, 'accumulated_submission_time': 41827.16106724739, 'accumulated_eval_time': 3792.582904100418, 'accumulated_logging_time': 1.5789299011230469, 'global_step': 50831, 'preemption_count': 0}), (52570, {'train/ctc_loss': Array(0.15486632, dtype=float32), 'train/wer': 0.058913846418141215, 'validation/ctc_loss': Array(0.37977183, dtype=float32), 'validation/wer': 0.11201328480261062, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.21341266, dtype=float32), 'test/wer': 0.06956716023805172, 'test/num_examples': 2472, 'score': 43267.5399684906, 'total_duration': 47192.76909375191, 'accumulated_submission_time': 43267.5399684906, 'accumulated_eval_time': 3921.6024510860443, 'accumulated_logging_time': 1.6307737827301025, 'global_step': 52570, 'preemption_count': 0}), (54345, {'train/ctc_loss': Array(0.1578446, dtype=float32), 'train/wer': 0.06065372807255449, 'validation/ctc_loss': Array(0.3755843, dtype=float32), 'validation/wer': 0.11206155806791084, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20987432, dtype=float32), 'test/wer': 0.06899843600836837, 'test/num_examples': 2472, 'score': 44707.850329875946, 'total_duration': 48759.87359333038, 'accumulated_submission_time': 44707.850329875946, 'accumulated_eval_time': 4048.2709839344025, 'accumulated_logging_time': 1.6867382526397705, 'global_step': 54345, 'preemption_count': 0}), (56125, {'train/ctc_loss': Array(0.154804, dtype=float32), 'train/wer': 0.057971014492753624, 'validation/ctc_loss': Array(0.3748947, dtype=float32), 'validation/wer': 0.11103816484354635, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20994216, dtype=float32), 'test/wer': 0.06806410334531716, 'test/num_examples': 2472, 'score': 46148.409047842026, 'total_duration': 50326.44254684448, 'accumulated_submission_time': 46148.409047842026, 'accumulated_eval_time': 4174.148626804352, 'accumulated_logging_time': 1.7477538585662842, 'global_step': 56125, 'preemption_count': 0}), (57899, {'train/ctc_loss': Array(0.16507912, dtype=float32), 'train/wer': 0.06262376520952806, 'validation/ctc_loss': Array(0.37280464, dtype=float32), 'validation/wer': 0.11026579259874296, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20841596, dtype=float32), 'test/wer': 0.06828753072126419, 'test/num_examples': 2472, 'score': 47588.45893073082, 'total_duration': 51893.119094610214, 'accumulated_submission_time': 47588.45893073082, 'accumulated_eval_time': 4300.646054029465, 'accumulated_logging_time': 1.8065762519836426, 'global_step': 57899, 'preemption_count': 0}), (59654, {'train/ctc_loss': Array(0.1756524, dtype=float32), 'train/wer': 0.06469793833520296, 'validation/ctc_loss': Array(0.37235644, dtype=float32), 'validation/wer': 0.11000511696612182, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20838574, dtype=float32), 'test/wer': 0.06840940019905348, 'test/num_examples': 2472, 'score': 49028.78290057182, 'total_duration': 53461.01828098297, 'accumulated_submission_time': 49028.78290057182, 'accumulated_eval_time': 4428.098953485489, 'accumulated_logging_time': 1.8600318431854248, 'global_step': 59654, 'preemption_count': 0}), (61428, {'train/ctc_loss': Array(0.15365617, dtype=float32), 'train/wer': 0.05921155872396709, 'validation/ctc_loss': Array(0.3723714, dtype=float32), 'validation/wer': 0.11003408092530195, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20837489, dtype=float32), 'test/wer': 0.06834846546015884, 'test/num_examples': 2472, 'score': 50469.71608543396, 'total_duration': 55029.04315876961, 'accumulated_submission_time': 50469.71608543396, 'accumulated_eval_time': 4555.0641849040985, 'accumulated_logging_time': 1.915844440460205, 'global_step': 61428, 'preemption_count': 0}), (63218, {'train/ctc_loss': Array(0.16204578, dtype=float32), 'train/wer': 0.06031217196841769, 'validation/ctc_loss': Array(0.3723746, dtype=float32), 'validation/wer': 0.11000511696612182, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20837016, dtype=float32), 'test/wer': 0.06832815388052729, 'test/num_examples': 2472, 'score': 51910.30851364136, 'total_duration': 56595.87143230438, 'accumulated_submission_time': 51910.30851364136, 'accumulated_eval_time': 4681.167226076126, 'accumulated_logging_time': 1.9789986610412598, 'global_step': 63218, 'preemption_count': 0}), (64972, {'train/ctc_loss': Array(0.15897346, dtype=float32), 'train/wer': 0.05968508681872963, 'validation/ctc_loss': Array(0.37237266, dtype=float32), 'validation/wer': 0.11000511696612182, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20837186, dtype=float32), 'test/wer': 0.06838908861942193, 'test/num_examples': 2472, 'score': 53350.37599372864, 'total_duration': 58161.99714946747, 'accumulated_submission_time': 53350.37599372864, 'accumulated_eval_time': 4807.100838661194, 'accumulated_logging_time': 2.034437417984009, 'global_step': 64972, 'preemption_count': 0}), (66736, {'train/ctc_loss': Array(0.16451569, dtype=float32), 'train/wer': 0.061530315366352194, 'validation/ctc_loss': Array(0.37238693, dtype=float32), 'validation/wer': 0.10999546231306179, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20837137, dtype=float32), 'test/wer': 0.06838908861942193, 'test/num_examples': 2472, 'score': 54790.64636230469, 'total_duration': 59728.964972019196, 'accumulated_submission_time': 54790.64636230469, 'accumulated_eval_time': 4933.6728801727295, 'accumulated_logging_time': 2.0890610218048096, 'global_step': 66736, 'preemption_count': 0}), (68523, {'train/ctc_loss': Array(0.16200455, dtype=float32), 'train/wer': 0.060407862078218286, 'validation/ctc_loss': Array(0.37237912, dtype=float32), 'validation/wer': 0.10997615300694169, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.2083734, dtype=float32), 'test/wer': 0.06836877703979038, 'test/num_examples': 2472, 'score': 56231.2382543087, 'total_duration': 61296.541285037994, 'accumulated_submission_time': 56231.2382543087, 'accumulated_eval_time': 5060.529519081116, 'accumulated_logging_time': 2.145503044128418, 'global_step': 68523, 'preemption_count': 0}), (70298, {'train/ctc_loss': Array(0.16449541, dtype=float32), 'train/wer': 0.062578174291312, 'validation/ctc_loss': Array(0.3723822, dtype=float32), 'validation/wer': 0.10999546231306179, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20837566, dtype=float32), 'test/wer': 0.06834846546015884, 'test/num_examples': 2472, 'score': 57671.5746421814, 'total_duration': 62863.2994260788, 'accumulated_submission_time': 57671.5746421814, 'accumulated_eval_time': 5186.81823182106, 'accumulated_logging_time': 2.205833673477173, 'global_step': 70298, 'preemption_count': 0}), (72063, {'train/ctc_loss': Array(0.1460689, dtype=float32), 'train/wer': 0.055949749255115255, 'validation/ctc_loss': Array(0.3723792, dtype=float32), 'validation/wer': 0.10999546231306179, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20837334, dtype=float32), 'test/wer': 0.06836877703979038, 'test/num_examples': 2472, 'score': 59112.14495778084, 'total_duration': 64429.74206542969, 'accumulated_submission_time': 59112.14495778084, 'accumulated_eval_time': 5312.5534245967865, 'accumulated_logging_time': 2.2725188732147217, 'global_step': 72063, 'preemption_count': 0}), (73834, {'train/ctc_loss': Array(0.1430181, dtype=float32), 'train/wer': 0.05336074719772533, 'validation/ctc_loss': Array(0.37238505, dtype=float32), 'validation/wer': 0.1100244262722419, 'validation/num_examples': 5348, 'test/ctc_loss': Array(0.20837466, dtype=float32), 'test/wer': 0.06838908861942193, 'test/num_examples': 2472, 'score': 60552.652999162674, 'total_duration': 65996.86452555656, 'accumulated_submission_time': 60552.652999162674, 'accumulated_eval_time': 5439.043668746948, 'accumulated_logging_time': 2.3270580768585205, 'global_step': 73834, 'preemption_count': 0})], 'global_step': 74499}
I0327 20:46:31.586811 140406042187584 submission_runner.py:599] Timing: 61068.67344355583
I0327 20:46:31.586879 140406042187584 submission_runner.py:601] Total number of evals: 43
I0327 20:46:31.586927 140406042187584 submission_runner.py:602] ====================
I0327 20:46:31.593744 140406042187584 submission_runner.py:686] Final librispeech_conformer_attention_temperature score: 0
