python3 submission_runner.py --framework=jax --workload=librispeech_deepspeech_no_resnet --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --data_dir=/data/librispeech --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --rng_seed=2181967211 --max_global_steps=48000 --librispeech_tokenizer_vocab_path=/data/librispeech/spm_model.vocab --tuning_ruleset=external --tuning_search_space=reference_algorithms/target_setting_algorithms/imagenet_resnet/tuning_search_space.json --num_tuning_trials=1 2>&1 | tee -a /logs/librispeech_deepspeech_no_resnet_jax_03-27-2024-02-15-01.log
I0327 02:15:21.411106 140317300651840 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_no_resnet_jax.
I0327 02:15:22.476803 140317300651840 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0327 02:15:22.477513 140317300651840 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0327 02:15:22.477644 140317300651840 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0327 02:15:22.484309 140317300651840 submission_runner.py:557] Using RNG seed 2181967211
I0327 02:15:23.639670 140317300651840 submission_runner.py:566] --- Tuning run 1/1 ---
I0327 02:15:23.639878 140317300651840 submission_runner.py:571] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_no_resnet_jax/trial_1.
I0327 02:15:23.640246 140317300651840 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_no_resnet_jax/trial_1/hparams.json.
I0327 02:15:23.821322 140317300651840 submission_runner.py:211] Initializing dataset.
I0327 02:15:23.821537 140317300651840 submission_runner.py:222] Initializing model.
I0327 02:15:26.388666 140317300651840 submission_runner.py:264] Initializing optimizer.
I0327 02:15:27.051344 140317300651840 submission_runner.py:271] Initializing metrics bundle.
I0327 02:15:27.051517 140317300651840 submission_runner.py:289] Initializing checkpoint and logger.
I0327 02:15:27.052468 140317300651840 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_no_resnet_jax/trial_1 with prefix checkpoint_
I0327 02:15:27.052631 140317300651840 submission_runner.py:309] Saving meta data to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_no_resnet_jax/trial_1/meta_data_0.json.
I0327 02:15:27.052848 140317300651840 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0327 02:15:27.052915 140317300651840 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0327 02:15:27.300599 140317300651840 logger_utils.py:220] Unable to record git information. Continuing without it.
I0327 02:15:27.526844 140317300651840 submission_runner.py:313] Saving flags to /experiment_runs/variants_target_setting/study_0/librispeech_deepspeech_no_resnet_jax/trial_1/flags_0.json.
I0327 02:15:27.539749 140317300651840 submission_runner.py:323] Starting training loop.
I0327 02:15:27.823977 140317300651840 input_pipeline.py:20] Loading split = train-clean-100
I0327 02:15:27.888667 140317300651840 input_pipeline.py:20] Loading split = train-clean-360
I0327 02:15:28.041721 140317300651840 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/interpreters/mlir.py:582: UserWarning: Some donated buffers were not usable: ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]), ShapedArray(float32[512]).
See an explanation at https://jax.readthedocs.io/en/latest/faq.html#buffer-donation.
  warnings.warn(f"Some donated buffers were not usable: {', '.join(unused_donations)}.\n{msg}")
I0327 02:16:09.868083 140153586779904 logging_writer.py:48] [0] global_step=0, grad_norm=15.843111038208008, loss=33.982425689697266
I0327 02:16:09.897616 140317300651840 spec.py:321] Evaluating on the training split.
I0327 02:16:10.148039 140317300651840 input_pipeline.py:20] Loading split = train-clean-100
I0327 02:16:10.182182 140317300651840 input_pipeline.py:20] Loading split = train-clean-360
I0327 02:16:10.506239 140317300651840 input_pipeline.py:20] Loading split = train-other-500
/usr/local/lib/python3.8/dist-packages/jax/_src/ops/scatter.py:92: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=float32 to dtype=int32. In future JAX releases this will result in an error.
  warnings.warn("scatter inputs have incompatible types: cannot safely cast "
I0327 02:17:00.199861 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 02:17:00.392469 140317300651840 input_pipeline.py:20] Loading split = dev-clean
I0327 02:17:00.397788 140317300651840 input_pipeline.py:20] Loading split = dev-other
I0327 02:17:53.122139 140317300651840 spec.py:349] Evaluating on the test split.
I0327 02:17:53.314306 140317300651840 input_pipeline.py:20] Loading split = test-clean
I0327 02:18:23.218833 140317300651840 submission_runner.py:422] Time since start: 175.68s, 	Step: 1, 	{'train/ctc_loss': Array(30.544735, dtype=float32), 'train/wer': 0.9437925577327531, 'validation/ctc_loss': Array(29.510908, dtype=float32), 'validation/wer': 0.8979985904206532, 'validation/num_examples': 5348, 'test/ctc_loss': Array(29.550442, dtype=float32), 'test/wer': 0.9005951292832044, 'test/num_examples': 2472, 'score': 42.35780739784241, 'total_duration': 175.67673444747925, 'accumulated_submission_time': 42.35780739784241, 'accumulated_eval_time': 133.3188726902008, 'accumulated_logging_time': 0}
I0327 02:18:23.243538 140146284422912 logging_writer.py:48] [1] accumulated_eval_time=133.318873, accumulated_logging_time=0, accumulated_submission_time=42.357807, global_step=1, preemption_count=0, score=42.357807, test/ctc_loss=29.55044174194336, test/num_examples=2472, test/wer=0.900595, total_duration=175.676734, train/ctc_loss=30.544734954833984, train/wer=0.943793, validation/ctc_loss=29.510908126831055, validation/num_examples=5348, validation/wer=0.897999
I0327 02:18:32.174038 140160120817408 logging_writer.py:48] [1] global_step=1, grad_norm=15.98091983795166, loss=33.8848876953125
I0327 02:18:33.097223 140160129210112 logging_writer.py:48] [2] global_step=2, grad_norm=22.920623779296875, loss=32.20151901245117
I0327 02:18:33.922924 140160120817408 logging_writer.py:48] [3] global_step=3, grad_norm=19.919382095336914, loss=19.431406021118164
I0327 02:18:34.744394 140160129210112 logging_writer.py:48] [4] global_step=4, grad_norm=9.474971771240234, loss=11.788002967834473
I0327 02:18:35.624691 140160120817408 logging_writer.py:48] [5] global_step=5, grad_norm=19.284591674804688, loss=11.519659042358398
I0327 02:18:36.513574 140160129210112 logging_writer.py:48] [6] global_step=6, grad_norm=9.470664024353027, loss=10.572250366210938
I0327 02:18:37.401305 140160120817408 logging_writer.py:48] [7] global_step=7, grad_norm=7.381765842437744, loss=9.286012649536133
I0327 02:18:38.278837 140160129210112 logging_writer.py:48] [8] global_step=8, grad_norm=3.223419427871704, loss=8.205039978027344
I0327 02:18:39.182171 140160120817408 logging_writer.py:48] [9] global_step=9, grad_norm=3.4111311435699463, loss=7.659331798553467
I0327 02:18:40.083235 140160129210112 logging_writer.py:48] [10] global_step=10, grad_norm=1.997462511062622, loss=7.417550086975098
I0327 02:18:40.968030 140160120817408 logging_writer.py:48] [11] global_step=11, grad_norm=2.452591896057129, loss=7.070533275604248
I0327 02:18:41.845001 140160129210112 logging_writer.py:48] [12] global_step=12, grad_norm=1.1602832078933716, loss=6.75848388671875
I0327 02:18:42.730310 140160120817408 logging_writer.py:48] [13] global_step=13, grad_norm=1.2814860343933105, loss=6.561640739440918
I0327 02:18:43.620758 140160129210112 logging_writer.py:48] [14] global_step=14, grad_norm=6.12524938583374, loss=6.506199359893799
I0327 02:18:44.502311 140160120817408 logging_writer.py:48] [15] global_step=15, grad_norm=5.0952677726745605, loss=7.366734027862549
I0327 02:18:45.393674 140160129210112 logging_writer.py:48] [16] global_step=16, grad_norm=5.284039497375488, loss=6.962468147277832
I0327 02:18:46.290345 140160120817408 logging_writer.py:48] [17] global_step=17, grad_norm=3.1449191570281982, loss=6.808556079864502
I0327 02:18:47.174356 140160129210112 logging_writer.py:48] [18] global_step=18, grad_norm=4.321041107177734, loss=6.737371921539307
I0327 02:18:48.051501 140160120817408 logging_writer.py:48] [19] global_step=19, grad_norm=7.260697841644287, loss=6.681948184967041
I0327 02:18:48.931174 140160129210112 logging_writer.py:48] [20] global_step=20, grad_norm=4.205528736114502, loss=6.428950786590576
I0327 02:18:49.813523 140160120817408 logging_writer.py:48] [21] global_step=21, grad_norm=5.682578086853027, loss=6.403193950653076
I0327 02:18:50.701838 140160129210112 logging_writer.py:48] [22] global_step=22, grad_norm=6.6949076652526855, loss=6.61583948135376
I0327 02:18:51.572815 140160120817408 logging_writer.py:48] [23] global_step=23, grad_norm=9.557608604431152, loss=6.573909759521484
I0327 02:18:52.479637 140160129210112 logging_writer.py:48] [24] global_step=24, grad_norm=3.7918758392333984, loss=6.347446918487549
I0327 02:18:53.372509 140160120817408 logging_writer.py:48] [25] global_step=25, grad_norm=8.013572692871094, loss=8.237857818603516
I0327 02:18:54.260136 140160129210112 logging_writer.py:48] [26] global_step=26, grad_norm=5.445017337799072, loss=7.2062506675720215
I0327 02:18:55.133570 140160120817408 logging_writer.py:48] [27] global_step=27, grad_norm=3.7686667442321777, loss=6.469796657562256
I0327 02:18:56.052729 140160129210112 logging_writer.py:48] [28] global_step=28, grad_norm=9.43764877319336, loss=6.8155412673950195
I0327 02:18:56.925157 140160120817408 logging_writer.py:48] [29] global_step=29, grad_norm=6.994302749633789, loss=6.874835968017578
I0327 02:18:57.813619 140160129210112 logging_writer.py:48] [30] global_step=30, grad_norm=15.51275634765625, loss=7.170680999755859
I0327 02:18:58.707053 140160120817408 logging_writer.py:48] [31] global_step=31, grad_norm=6.746740341186523, loss=7.201681137084961
I0327 02:18:59.609893 140160129210112 logging_writer.py:48] [32] global_step=32, grad_norm=3.969456672668457, loss=6.738631725311279
I0327 02:19:00.495780 140160120817408 logging_writer.py:48] [33] global_step=33, grad_norm=5.028317928314209, loss=6.568204879760742
I0327 02:19:01.383683 140160129210112 logging_writer.py:48] [34] global_step=34, grad_norm=4.017605304718018, loss=6.495601177215576
I0327 02:19:02.327590 140160120817408 logging_writer.py:48] [35] global_step=35, grad_norm=3.4818270206451416, loss=6.357728004455566
I0327 02:19:03.299128 140160129210112 logging_writer.py:48] [36] global_step=36, grad_norm=2.7125473022460938, loss=6.31571626663208
I0327 02:19:04.186356 140160120817408 logging_writer.py:48] [37] global_step=37, grad_norm=4.223293781280518, loss=6.289220333099365
I0327 02:19:05.072230 140160129210112 logging_writer.py:48] [38] global_step=38, grad_norm=3.9350907802581787, loss=6.315864562988281
I0327 02:19:05.964035 140160120817408 logging_writer.py:48] [39] global_step=39, grad_norm=4.580883502960205, loss=6.287966251373291
I0327 02:19:06.850443 140160129210112 logging_writer.py:48] [40] global_step=40, grad_norm=4.678206920623779, loss=6.277890682220459
I0327 02:19:07.729083 140160120817408 logging_writer.py:48] [41] global_step=41, grad_norm=4.2242350578308105, loss=6.218087196350098
I0327 02:19:08.602852 140160129210112 logging_writer.py:48] [42] global_step=42, grad_norm=3.2377147674560547, loss=6.160719871520996
I0327 02:19:09.496999 140160120817408 logging_writer.py:48] [43] global_step=43, grad_norm=2.6985721588134766, loss=6.165489196777344
I0327 02:19:10.399496 140160129210112 logging_writer.py:48] [44] global_step=44, grad_norm=2.6190273761749268, loss=6.156717300415039
I0327 02:19:11.274130 140160120817408 logging_writer.py:48] [45] global_step=45, grad_norm=3.0770766735076904, loss=6.068680763244629
I0327 02:19:12.164911 140160129210112 logging_writer.py:48] [46] global_step=46, grad_norm=2.887510061264038, loss=6.11047887802124
I0327 02:19:13.054929 140160120817408 logging_writer.py:48] [47] global_step=47, grad_norm=3.263343572616577, loss=6.086339950561523
I0327 02:19:13.938570 140160129210112 logging_writer.py:48] [48] global_step=48, grad_norm=3.626046895980835, loss=6.127608299255371
I0327 02:19:14.824923 140160120817408 logging_writer.py:48] [49] global_step=49, grad_norm=6.293215274810791, loss=6.217011451721191
I0327 02:19:15.704571 140160129210112 logging_writer.py:48] [50] global_step=50, grad_norm=6.853604316711426, loss=6.4567060470581055
I0327 02:19:16.579060 140160120817408 logging_writer.py:48] [51] global_step=51, grad_norm=5.9436936378479, loss=6.29495906829834
I0327 02:19:17.453087 140160129210112 logging_writer.py:48] [52] global_step=52, grad_norm=5.60163688659668, loss=6.366611480712891
I0327 02:19:18.327402 140160120817408 logging_writer.py:48] [53] global_step=53, grad_norm=5.934588432312012, loss=6.306732177734375
I0327 02:19:19.215039 140160129210112 logging_writer.py:48] [54] global_step=54, grad_norm=5.295957565307617, loss=6.294957637786865
I0327 02:19:20.102230 140160120817408 logging_writer.py:48] [55] global_step=55, grad_norm=5.093864440917969, loss=6.23574161529541
I0327 02:19:20.978927 140160129210112 logging_writer.py:48] [56] global_step=56, grad_norm=4.833794116973877, loss=6.263338088989258
I0327 02:19:21.852406 140160120817408 logging_writer.py:48] [57] global_step=57, grad_norm=5.289267063140869, loss=6.236802577972412
I0327 02:19:22.732423 140160129210112 logging_writer.py:48] [58] global_step=58, grad_norm=5.509936809539795, loss=6.332846641540527
I0327 02:19:23.614312 140160120817408 logging_writer.py:48] [59] global_step=59, grad_norm=6.856075763702393, loss=6.300739288330078
I0327 02:19:24.507873 140160129210112 logging_writer.py:48] [60] global_step=60, grad_norm=6.393962383270264, loss=6.387014389038086
I0327 02:19:25.383852 140160120817408 logging_writer.py:48] [61] global_step=61, grad_norm=5.536211013793945, loss=6.241743564605713
I0327 02:19:26.264330 140160129210112 logging_writer.py:48] [62] global_step=62, grad_norm=4.481723308563232, loss=6.21946907043457
I0327 02:19:27.162525 140160120817408 logging_writer.py:48] [63] global_step=63, grad_norm=3.793217182159424, loss=6.133687973022461
I0327 02:19:28.048627 140160129210112 logging_writer.py:48] [64] global_step=64, grad_norm=3.039454698562622, loss=6.110002040863037
I0327 02:19:28.930404 140160120817408 logging_writer.py:48] [65] global_step=65, grad_norm=2.659999132156372, loss=6.022739410400391
I0327 02:19:29.814154 140160129210112 logging_writer.py:48] [66] global_step=66, grad_norm=1.8807514905929565, loss=6.0102128982543945
I0327 02:19:30.690489 140160120817408 logging_writer.py:48] [67] global_step=67, grad_norm=1.601298213005066, loss=5.978905200958252
I0327 02:19:31.565655 140160129210112 logging_writer.py:48] [68] global_step=68, grad_norm=0.9871496558189392, loss=5.975335597991943
I0327 02:19:32.452699 140160120817408 logging_writer.py:48] [69] global_step=69, grad_norm=0.7850171327590942, loss=5.942162036895752
I0327 02:19:33.338926 140160129210112 logging_writer.py:48] [70] global_step=70, grad_norm=0.8761060833930969, loss=5.9107160568237305
I0327 02:19:34.220221 140160120817408 logging_writer.py:48] [71] global_step=71, grad_norm=1.3383817672729492, loss=5.922813415527344
I0327 02:19:35.099785 140160129210112 logging_writer.py:48] [72] global_step=72, grad_norm=1.3994415998458862, loss=5.924833297729492
I0327 02:19:36.001245 140160120817408 logging_writer.py:48] [73] global_step=73, grad_norm=2.460158109664917, loss=5.94848108291626
I0327 02:19:36.893484 140160129210112 logging_writer.py:48] [74] global_step=74, grad_norm=2.912999153137207, loss=5.947165012359619
I0327 02:19:37.768937 140160120817408 logging_writer.py:48] [75] global_step=75, grad_norm=3.043856620788574, loss=5.946996688842773
I0327 02:19:38.662105 140160129210112 logging_writer.py:48] [76] global_step=76, grad_norm=3.1469039916992188, loss=5.944021701812744
I0327 02:19:39.551320 140160120817408 logging_writer.py:48] [77] global_step=77, grad_norm=3.682415723800659, loss=5.983339786529541
I0327 02:19:40.435700 140160129210112 logging_writer.py:48] [78] global_step=78, grad_norm=3.764463186264038, loss=6.002013206481934
I0327 02:19:41.309628 140160120817408 logging_writer.py:48] [79] global_step=79, grad_norm=3.927255868911743, loss=5.992153167724609
I0327 02:19:42.187566 140160129210112 logging_writer.py:48] [80] global_step=80, grad_norm=3.6771492958068848, loss=6.0115838050842285
I0327 02:19:43.094054 140160120817408 logging_writer.py:48] [81] global_step=81, grad_norm=3.1498863697052, loss=5.962759971618652
I0327 02:19:44.008833 140160129210112 logging_writer.py:48] [82] global_step=82, grad_norm=2.505608081817627, loss=5.935365200042725
I0327 02:19:44.892066 140160120817408 logging_writer.py:48] [83] global_step=83, grad_norm=2.805737018585205, loss=5.970019340515137
I0327 02:19:45.773230 140160129210112 logging_writer.py:48] [84] global_step=84, grad_norm=2.805971622467041, loss=5.978198528289795
I0327 02:19:46.678776 140160120817408 logging_writer.py:48] [85] global_step=85, grad_norm=2.795271873474121, loss=5.95274543762207
I0327 02:19:47.565468 140160129210112 logging_writer.py:48] [86] global_step=86, grad_norm=2.678046941757202, loss=5.959078311920166
I0327 02:19:48.436921 140160120817408 logging_writer.py:48] [87] global_step=87, grad_norm=2.4410016536712646, loss=5.941924095153809
I0327 02:19:49.328921 140160129210112 logging_writer.py:48] [88] global_step=88, grad_norm=2.0964033603668213, loss=5.924478054046631
I0327 02:19:50.220728 140160120817408 logging_writer.py:48] [89] global_step=89, grad_norm=1.937444806098938, loss=5.893260478973389
I0327 02:19:51.107919 140160129210112 logging_writer.py:48] [90] global_step=90, grad_norm=1.613450288772583, loss=5.8738484382629395
I0327 02:19:51.988431 140160120817408 logging_writer.py:48] [91] global_step=91, grad_norm=1.4427424669265747, loss=5.858381748199463
I0327 02:19:52.869051 140160129210112 logging_writer.py:48] [92] global_step=92, grad_norm=0.9628574848175049, loss=5.865846633911133
I0327 02:19:53.748102 140160120817408 logging_writer.py:48] [93] global_step=93, grad_norm=0.7504244446754456, loss=5.854879379272461
I0327 02:19:54.642840 140160129210112 logging_writer.py:48] [94] global_step=94, grad_norm=0.5211437940597534, loss=5.87406587600708
I0327 02:19:55.524094 140160120817408 logging_writer.py:48] [95] global_step=95, grad_norm=0.31095877289772034, loss=5.872107982635498
I0327 02:19:56.431627 140160129210112 logging_writer.py:48] [96] global_step=96, grad_norm=0.24809662997722626, loss=5.857244968414307
I0327 02:19:57.336263 140160120817408 logging_writer.py:48] [97] global_step=97, grad_norm=0.2733785808086395, loss=5.869302272796631
I0327 02:19:58.209703 140160129210112 logging_writer.py:48] [98] global_step=98, grad_norm=0.33053135871887207, loss=5.8514814376831055
I0327 02:19:59.094922 140160120817408 logging_writer.py:48] [99] global_step=99, grad_norm=0.5163567662239075, loss=5.843420028686523
I0327 02:19:59.986720 140160129210112 logging_writer.py:48] [100] global_step=100, grad_norm=0.806537389755249, loss=5.849474906921387
I0327 02:25:04.128697 140160120817408 logging_writer.py:48] [500] global_step=500, grad_norm=136.52102661132812, loss=374.4059143066406
I0327 02:31:19.481686 140160129210112 logging_writer.py:48] [1000] global_step=1000, grad_norm=242.65306091308594, loss=1025.46630859375
I0327 02:37:36.987322 140160818140928 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.0, loss=1859.9114990234375
I0327 02:42:23.938616 140317300651840 spec.py:321] Evaluating on the training split.
I0327 02:42:59.184335 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 02:43:41.863443 140317300651840 spec.py:349] Evaluating on the test split.
I0327 02:44:03.175307 140317300651840 submission_runner.py:422] Time since start: 1715.63s, 	Step: 1884, 	{'train/ctc_loss': Array(1767.5358, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(3353.434, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3185.5388, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1482.9771449565887, 'total_duration': 1715.630652666092, 'accumulated_submission_time': 1482.9771449565887, 'accumulated_eval_time': 232.55072259902954, 'accumulated_logging_time': 0.03709816932678223}
I0327 02:44:03.205753 140160388060928 logging_writer.py:48] [1884] accumulated_eval_time=232.550723, accumulated_logging_time=0.037098, accumulated_submission_time=1482.977145, global_step=1884, preemption_count=0, score=1482.977145, test/ctc_loss=3185.538818359375, test/num_examples=2472, test/wer=0.899580, total_duration=1715.630653, train/ctc_loss=1767.5357666015625, train/wer=0.944636, validation/ctc_loss=3353.43408203125, validation/num_examples=5348, validation/wer=0.896618
I0327 02:45:30.957488 140160379668224 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.11327391862869263, loss=1849.953125
I0327 02:51:48.316044 140160388060928 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.0, loss=1776.9017333984375
I0327 02:58:06.518810 140160379668224 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.0, loss=1891.3795166015625
I0327 03:04:27.394763 140161473500928 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.0, loss=1856.0010986328125
I0327 03:08:03.588901 140317300651840 spec.py:321] Evaluating on the training split.
I0327 03:08:39.249680 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 03:09:21.651356 140317300651840 spec.py:349] Evaluating on the test split.
I0327 03:09:43.116649 140317300651840 submission_runner.py:422] Time since start: 3255.57s, 	Step: 3789, 	{'train/ctc_loss': Array(1761.5707, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2923.2749993801117, 'total_duration': 3255.5716688632965, 'accumulated_submission_time': 2923.2749993801117, 'accumulated_eval_time': 332.073290348053, 'accumulated_logging_time': 0.08661532402038574}
I0327 03:09:43.151670 140161473500928 logging_writer.py:48] [3789] accumulated_eval_time=332.073290, accumulated_logging_time=0.086615, accumulated_submission_time=2923.274999, global_step=3789, preemption_count=0, score=2923.274999, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=3255.571669, train/ctc_loss=1761.5706787109375, train/wer=0.942722, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0327 03:12:21.847302 140161465108224 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.0, loss=1776.4085693359375
I0327 03:18:38.945158 140161473500928 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.0, loss=1853.9847412109375
I0327 03:24:59.676223 140161465108224 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.0, loss=1927.842529296875
I0327 03:31:26.097388 140161473500928 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.0, loss=1857.3477783203125
I0327 03:33:43.543648 140317300651840 spec.py:321] Evaluating on the training split.
I0327 03:34:19.696677 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 03:35:03.300678 140317300651840 spec.py:349] Evaluating on the test split.
I0327 03:35:25.036000 140317300651840 submission_runner.py:422] Time since start: 4797.49s, 	Step: 5684, 	{'train/ctc_loss': Array(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4363.585999965668, 'total_duration': 4797.491341590881, 'accumulated_submission_time': 4363.585999965668, 'accumulated_eval_time': 433.56079840660095, 'accumulated_logging_time': 0.1356813907623291}
I0327 03:35:25.065144 140161473500928 logging_writer.py:48] [5684] accumulated_eval_time=433.560798, accumulated_logging_time=0.135681, accumulated_submission_time=4363.586000, global_step=5684, preemption_count=0, score=4363.586000, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=4797.491342, train/ctc_loss=1741.2979736328125, train/wer=0.943324, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0327 03:39:22.337586 140161465108224 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.0, loss=1894.0384521484375
I0327 03:45:39.484754 140161473500928 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.0, loss=1838.8035888671875
I0327 03:52:00.173861 140161465108224 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.0, loss=1890.8206787109375
I0327 03:58:29.620737 140161473500928 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.0, loss=1812.1202392578125
I0327 03:59:25.672370 140317300651840 spec.py:321] Evaluating on the training split.
I0327 04:00:02.093569 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 04:00:45.221981 140317300651840 spec.py:349] Evaluating on the test split.
I0327 04:01:07.449852 140317300651840 submission_runner.py:422] Time since start: 6339.90s, 	Step: 7576, 	{'train/ctc_loss': Array(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 5804.109092712402, 'total_duration': 6339.904704332352, 'accumulated_submission_time': 5804.109092712402, 'accumulated_eval_time': 535.3329384326935, 'accumulated_logging_time': 0.1791532039642334}
I0327 04:01:07.480693 140161473500928 logging_writer.py:48] [7576] accumulated_eval_time=535.332938, accumulated_logging_time=0.179153, accumulated_submission_time=5804.109093, global_step=7576, preemption_count=0, score=5804.109093, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=6339.904704, train/ctc_loss=1724.861328125, train/wer=0.943700, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0327 04:06:26.189024 140161465108224 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.0, loss=1762.9517822265625
I0327 04:12:49.920860 140159937496832 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.0, loss=1791.4500732421875
I0327 04:19:08.960696 140159929104128 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.0, loss=1844.3675537109375
I0327 04:25:07.777504 140317300651840 spec.py:321] Evaluating on the training split.
I0327 04:25:43.738648 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 04:26:26.443465 140317300651840 spec.py:349] Evaluating on the test split.
I0327 04:26:48.305202 140317300651840 submission_runner.py:422] Time since start: 7880.76s, 	Step: 9455, 	{'train/ctc_loss': Array(1832.9288, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7244.324284791946, 'total_duration': 7880.7595911026, 'accumulated_submission_time': 7244.324284791946, 'accumulated_eval_time': 635.8550357818604, 'accumulated_logging_time': 0.22446107864379883}
I0327 04:26:48.335532 140160388060928 logging_writer.py:48] [9455] accumulated_eval_time=635.855036, accumulated_logging_time=0.224461, accumulated_submission_time=7244.324285, global_step=9455, preemption_count=0, score=7244.324285, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=7880.759591, train/ctc_loss=1832.9288330078125, train/wer=0.941551, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0327 04:27:23.016919 140160379668224 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.0, loss=1868.1910400390625
I0327 04:33:38.028446 140160388060928 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.0, loss=1817.13671875
I0327 04:40:11.809193 140159732700928 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.0, loss=1799.7625732421875
I0327 04:46:31.003467 140159724308224 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.0, loss=1888.310302734375
I0327 04:50:48.728149 140317300651840 spec.py:321] Evaluating on the training split.
I0327 04:51:24.835388 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 04:52:08.486043 140317300651840 spec.py:349] Evaluating on the test split.
I0327 04:52:30.534886 140317300651840 submission_runner.py:422] Time since start: 9422.99s, 	Step: 11317, 	{'train/ctc_loss': Array(1752.8004, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 8684.634976148605, 'total_duration': 9422.989792108536, 'accumulated_submission_time': 8684.634976148605, 'accumulated_eval_time': 737.6564910411835, 'accumulated_logging_time': 0.269711971282959}
I0327 04:52:30.565347 140160388060928 logging_writer.py:48] [11317] accumulated_eval_time=737.656491, accumulated_logging_time=0.269712, accumulated_submission_time=8684.634976, global_step=11317, preemption_count=0, score=8684.634976, test/ctc_loss=3189.86083984375, test/num_examples=2472, test/wer=0.899580, total_duration=9422.989792, train/ctc_loss=1752.8004150390625, train/wer=0.942641, validation/ctc_loss=3357.92236328125, validation/num_examples=5348, validation/wer=0.896618
I0327 04:54:51.710328 140161473500928 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.0, loss=1798.3642578125
I0327 05:01:13.739594 140161465108224 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.0, loss=1811.343505859375
I0327 05:07:57.445399 140160388060928 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.0, loss=1906.7269287109375
I0327 05:14:15.332858 140160379668224 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.0, loss=1828.5511474609375
I0327 05:16:30.669739 140317300651840 spec.py:321] Evaluating on the training split.
I0327 05:17:07.729375 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 05:17:51.475777 140317300651840 spec.py:349] Evaluating on the test split.
I0327 05:18:13.434423 140317300651840 submission_runner.py:422] Time since start: 10965.89s, 	Step: 13166, 	{'train/ctc_loss': Array(1746.1039, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 10124.65800499916, 'total_duration': 10965.888332128525, 'accumulated_submission_time': 10124.65800499916, 'accumulated_eval_time': 840.4148964881897, 'accumulated_logging_time': 0.3143177032470703}
I0327 05:18:13.467816 140160818140928 logging_writer.py:48] [13166] accumulated_eval_time=840.414896, accumulated_logging_time=0.314318, accumulated_submission_time=10124.658005, global_step=13166, preemption_count=0, score=10124.658005, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=10965.888332, train/ctc_loss=1746.1038818359375, train/wer=0.942824, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 05:22:27.823891 140161473500928 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.0, loss=1787.315185546875
I0327 05:28:47.833406 140161465108224 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.0, loss=1819.5831298828125
I0327 05:35:35.514591 140160818140928 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.0, loss=1829.07373046875
I0327 05:41:50.650384 140160809748224 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.0, loss=1824.9013671875
I0327 05:42:13.657713 140317300651840 spec.py:321] Evaluating on the training split.
I0327 05:42:50.641429 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 05:43:34.311610 140317300651840 spec.py:349] Evaluating on the test split.
I0327 05:43:56.117148 140317300651840 submission_runner.py:422] Time since start: 12508.57s, 	Step: 15030, 	{'train/ctc_loss': Array(1733.7323, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 11564.76735496521, 'total_duration': 12508.570070266724, 'accumulated_submission_time': 11564.76735496521, 'accumulated_eval_time': 942.8670687675476, 'accumulated_logging_time': 0.3611013889312744}
I0327 05:43:56.152349 140160818140928 logging_writer.py:48] [15030] accumulated_eval_time=942.867069, accumulated_logging_time=0.361101, accumulated_submission_time=11564.767355, global_step=15030, preemption_count=0, score=11564.767355, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=12508.570070, train/ctc_loss=1733.7322998046875, train/wer=0.944086, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 05:49:52.489587 140160818140928 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.0, loss=1851.0279541015625
I0327 05:56:07.195152 140160809748224 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.0, loss=1905.591552734375
I0327 06:02:59.177965 140161473500928 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.0, loss=1868.0469970703125
I0327 06:07:56.174370 140317300651840 spec.py:321] Evaluating on the training split.
I0327 06:08:33.281033 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 06:09:16.748636 140317300651840 spec.py:349] Evaluating on the test split.
I0327 06:09:38.734320 140317300651840 submission_runner.py:422] Time since start: 14051.19s, 	Step: 16898, 	{'train/ctc_loss': Array(1786.8575, dtype=float32), 'train/wer': 0.9427990785714666, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 13004.706295013428, 'total_duration': 14051.189279079437, 'accumulated_submission_time': 13004.706295013428, 'accumulated_eval_time': 1045.4217836856842, 'accumulated_logging_time': 0.41051316261291504}
I0327 06:09:38.766069 140161473500928 logging_writer.py:48] [16898] accumulated_eval_time=1045.421784, accumulated_logging_time=0.410513, accumulated_submission_time=13004.706295, global_step=16898, preemption_count=0, score=13004.706295, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=14051.189279, train/ctc_loss=1786.8575439453125, train/wer=0.942799, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 06:10:55.874432 140161465108224 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.0, loss=1864.6453857421875
I0327 06:17:25.231367 140161473500928 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.0, loss=1805.0850830078125
I0327 06:23:43.666599 140161473500928 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.0, loss=1791.81884765625
I0327 06:30:34.476957 140161465108224 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.0, loss=1916.4332275390625
I0327 06:33:39.129042 140317300651840 spec.py:321] Evaluating on the training split.
I0327 06:34:16.002612 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 06:34:59.399404 140317300651840 spec.py:349] Evaluating on the test split.
I0327 06:35:21.983962 140317300651840 submission_runner.py:422] Time since start: 15594.44s, 	Step: 18740, 	{'train/ctc_loss': Array(1755.9307, dtype=float32), 'train/wer': 0.9423383225986367, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14444.9840426445, 'total_duration': 15594.438996076584, 'accumulated_submission_time': 14444.9840426445, 'accumulated_eval_time': 1148.2717576026917, 'accumulated_logging_time': 0.4578583240509033}
I0327 06:35:22.018089 140161473500928 logging_writer.py:48] [18740] accumulated_eval_time=1148.271758, accumulated_logging_time=0.457858, accumulated_submission_time=14444.984043, global_step=18740, preemption_count=0, score=14444.984043, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=15594.438996, train/ctc_loss=1755.9306640625, train/wer=0.942338, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 06:38:37.808211 140161465108224 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.0, loss=1849.0224609375
I0327 06:45:16.477941 140161473500928 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.0, loss=1817.5162353515625
I0327 06:51:39.199726 140161473500928 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.0, loss=1857.070556640625
I0327 06:58:21.344925 140161465108224 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.0, loss=1858.823486328125
I0327 06:59:22.405688 140317300651840 spec.py:321] Evaluating on the training split.
I0327 06:59:59.480447 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 07:00:43.337742 140317300651840 spec.py:349] Evaluating on the test split.
I0327 07:01:05.598301 140317300651840 submission_runner.py:422] Time since start: 17138.05s, 	Step: 20576, 	{'train/ctc_loss': Array(1731.2422, dtype=float32), 'train/wer': 0.9431396916893625, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15885.290096282959, 'total_duration': 17138.053234815598, 'accumulated_submission_time': 15885.290096282959, 'accumulated_eval_time': 1251.4591183662415, 'accumulated_logging_time': 0.5051019191741943}
I0327 07:01:05.629988 140160459740928 logging_writer.py:48] [20576] accumulated_eval_time=1251.459118, accumulated_logging_time=0.505102, accumulated_submission_time=15885.290096, global_step=20576, preemption_count=0, score=15885.290096, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=17138.053235, train/ctc_loss=1731.2421875, train/wer=0.943140, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 07:06:28.495852 140160459740928 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.0, loss=1807.3790283203125
I0327 07:13:12.534805 140160451348224 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.0, loss=1787.315185546875
I0327 07:19:41.003772 140159804380928 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.0, loss=1820.618408203125
I0327 07:25:06.192595 140317300651840 spec.py:321] Evaluating on the training split.
I0327 07:25:42.857621 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 07:26:27.110994 140317300651840 spec.py:349] Evaluating on the test split.
I0327 07:26:49.182022 140317300651840 submission_runner.py:422] Time since start: 18681.64s, 	Step: 22410, 	{'train/ctc_loss': Array(1763.6094, dtype=float32), 'train/wer': 0.9432716912443612, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 17325.769654989243, 'total_duration': 18681.63670516014, 'accumulated_submission_time': 17325.769654989243, 'accumulated_eval_time': 1354.4430437088013, 'accumulated_logging_time': 0.5506594181060791}
I0327 07:26:49.221883 140159804380928 logging_writer.py:48] [22410] accumulated_eval_time=1354.443044, accumulated_logging_time=0.550659, accumulated_submission_time=17325.769655, global_step=22410, preemption_count=0, score=17325.769655, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=18681.636705, train/ctc_loss=1763.609375, train/wer=0.943272, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 07:27:57.373716 140159795988224 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.0, loss=1890.53369140625
I0327 07:34:15.511279 140160459740928 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.0, loss=1804.0672607421875
I0327 07:40:54.849102 140160451348224 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.0, loss=1855.5897216796875
I0327 07:47:26.921243 140159804380928 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.0, loss=1822.4329833984375
I0327 07:50:49.734664 140317300651840 spec.py:321] Evaluating on the training split.
I0327 07:51:26.899292 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 07:52:11.827802 140317300651840 spec.py:349] Evaluating on the test split.
I0327 07:52:34.041047 140317300651840 submission_runner.py:422] Time since start: 20226.49s, 	Step: 24267, 	{'train/ctc_loss': Array(1739.3417, dtype=float32), 'train/wer': 0.944685667249717, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 18766.195712566376, 'total_duration': 20226.49410033226, 'accumulated_submission_time': 18766.195712566376, 'accumulated_eval_time': 1458.7423107624054, 'accumulated_logging_time': 0.6058189868927002}
I0327 07:52:34.076488 140161258460928 logging_writer.py:48] [24267] accumulated_eval_time=1458.742311, accumulated_logging_time=0.605819, accumulated_submission_time=18766.195713, global_step=24267, preemption_count=0, score=18766.195713, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=20226.494100, train/ctc_loss=1739.3416748046875, train/wer=0.944686, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 07:55:29.874939 140161250068224 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.0, loss=1832.6090087890625
I0327 08:01:55.937597 140160603100928 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.0, loss=1848.3548583984375
I0327 08:08:33.549168 140160594708224 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.0, loss=1903.8909912109375
I0327 08:15:12.705531 140161258460928 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.0, loss=1786.9407958984375
I0327 08:16:34.162560 140317300651840 spec.py:321] Evaluating on the training split.
I0327 08:17:11.242249 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 08:17:55.615122 140317300651840 spec.py:349] Evaluating on the test split.
I0327 08:18:17.931114 140317300651840 submission_runner.py:422] Time since start: 21770.39s, 	Step: 26110, 	{'train/ctc_loss': Array(1769.4664, dtype=float32), 'train/wer': 0.9432456399645285, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 20206.198328971863, 'total_duration': 21770.385784864426, 'accumulated_submission_time': 20206.198328971863, 'accumulated_eval_time': 1562.5053470134735, 'accumulated_logging_time': 0.6552648544311523}
I0327 08:18:17.963325 140161258460928 logging_writer.py:48] [26110] accumulated_eval_time=1562.505347, accumulated_logging_time=0.655265, accumulated_submission_time=20206.198329, global_step=26110, preemption_count=0, score=20206.198329, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=21770.385785, train/ctc_loss=1769.4664306640625, train/wer=0.943246, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 08:23:13.953716 140161250068224 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.0, loss=1776.031494140625
I0327 08:29:55.143604 140160603100928 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.0, loss=1847.2879638671875
I0327 08:36:24.934677 140160594708224 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.0, loss=1855.052001953125
I0327 08:42:18.627094 140317300651840 spec.py:321] Evaluating on the training split.
I0327 08:42:55.506344 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 08:43:39.313559 140317300651840 spec.py:349] Evaluating on the test split.
I0327 08:44:01.522232 140317300651840 submission_runner.py:422] Time since start: 23313.98s, 	Step: 27934, 	{'train/ctc_loss': Array(1736.9141, dtype=float32), 'train/wer': 0.9439109001278072, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 21646.778349161148, 'total_duration': 23313.977199316025, 'accumulated_submission_time': 21646.778349161148, 'accumulated_eval_time': 1665.3954215049744, 'accumulated_logging_time': 0.7016565799713135}
I0327 08:44:01.559159 140161258460928 logging_writer.py:48] [27934] accumulated_eval_time=1665.395422, accumulated_logging_time=0.701657, accumulated_submission_time=21646.778349, global_step=27934, preemption_count=0, score=21646.778349, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=23313.977199, train/ctc_loss=1736.9140625, train/wer=0.943911, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 08:44:51.942233 140161250068224 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.0, loss=1881.9169921875
I0327 08:51:09.908226 140161258460928 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.0, loss=1879.8441162109375
I0327 08:57:57.447232 140161258460928 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.0, loss=1794.45654296875
I0327 09:04:24.187370 140161250068224 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.0, loss=1793.576416015625
I0327 09:08:01.827078 140317300651840 spec.py:321] Evaluating on the training split.
I0327 09:08:38.884799 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 09:09:23.337814 140317300651840 spec.py:349] Evaluating on the test split.
I0327 09:09:45.209534 140317300651840 submission_runner.py:422] Time since start: 24857.66s, 	Step: 29762, 	{'train/ctc_loss': Array(1715.2401, dtype=float32), 'train/wer': 0.9450143703143059, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 23086.956217050552, 'total_duration': 24857.6637737751, 'accumulated_submission_time': 23086.956217050552, 'accumulated_eval_time': 1768.7719390392303, 'accumulated_logging_time': 0.7576029300689697}
I0327 09:09:45.242350 140160674780928 logging_writer.py:48] [29762] accumulated_eval_time=1768.771939, accumulated_logging_time=0.757603, accumulated_submission_time=23086.956217, global_step=29762, preemption_count=0, score=23086.956217, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=24857.663774, train/ctc_loss=1715.2401123046875, train/wer=0.945014, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 09:12:47.196477 140160674780928 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.0, loss=1803.5589599609375
I0327 09:19:14.076059 140160666388224 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.0, loss=1832.6090087890625
I0327 09:26:07.673700 140160674780928 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.0, loss=1860.7149658203125
I0327 09:32:31.913911 140160666388224 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.0, loss=1896.134765625
I0327 09:33:45.644587 140317300651840 spec.py:321] Evaluating on the training split.
I0327 09:34:26.004091 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 09:35:09.843472 140317300651840 spec.py:349] Evaluating on the test split.
I0327 09:35:31.988601 140317300651840 submission_runner.py:422] Time since start: 26404.44s, 	Step: 31590, 	{'train/ctc_loss': Array(1783.4905, dtype=float32), 'train/wer': 0.9417576703068122, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 24527.2738301754, 'total_duration': 26404.442217588425, 'accumulated_submission_time': 24527.2738301754, 'accumulated_eval_time': 1875.1094052791595, 'accumulated_logging_time': 0.804293155670166}
I0327 09:35:32.020895 140160674780928 logging_writer.py:48] [31590] accumulated_eval_time=1875.109405, accumulated_logging_time=0.804293, accumulated_submission_time=24527.273830, global_step=31590, preemption_count=0, score=24527.273830, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=26404.442218, train/ctc_loss=1783.490478515625, train/wer=0.941758, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 09:40:43.434114 140160019420928 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.0, loss=1785.943603515625
I0327 09:46:59.613534 140160011028224 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.0, loss=1834.5789794921875
I0327 09:53:56.358643 140160019420928 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.0, loss=1777.88134765625
I0327 09:59:32.651772 140317300651840 spec.py:321] Evaluating on the training split.
I0327 10:00:10.092138 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 10:00:54.857064 140317300651840 spec.py:349] Evaluating on the test split.
I0327 10:01:17.396629 140317300651840 submission_runner.py:422] Time since start: 27949.85s, 	Step: 33451, 	{'train/ctc_loss': Array(1824.6843, dtype=float32), 'train/wer': 0.9416600198590335, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 25967.817270994186, 'total_duration': 27949.851816892624, 'accumulated_submission_time': 25967.817270994186, 'accumulated_eval_time': 1979.8492782115936, 'accumulated_logging_time': 0.8511202335357666}
I0327 10:01:17.433995 140160080852736 logging_writer.py:48] [33451] accumulated_eval_time=1979.849278, accumulated_logging_time=0.851120, accumulated_submission_time=25967.817271, global_step=33451, preemption_count=0, score=25967.817271, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=27949.851817, train/ctc_loss=1824.684326171875, train/wer=0.941660, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 10:01:54.837870 140160072460032 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.0, loss=1823.6014404296875
I0327 10:08:24.580476 140159538132736 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.0, loss=1893.050048828125
I0327 10:14:40.476727 140159529740032 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.0, loss=1767.8140869140625
I0327 10:21:30.775325 140159538132736 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.0, loss=1834.71044921875
I0327 10:25:17.536437 140317300651840 spec.py:321] Evaluating on the training split.
I0327 10:25:54.303030 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 10:26:38.231104 140317300651840 spec.py:349] Evaluating on the test split.
I0327 10:27:00.323695 140317300651840 submission_runner.py:422] Time since start: 29492.78s, 	Step: 35298, 	{'train/ctc_loss': Array(1692.767, dtype=float32), 'train/wer': 0.9447677853176417, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 27407.83339715004, 'total_duration': 29492.778332948685, 'accumulated_submission_time': 27407.83339715004, 'accumulated_eval_time': 2082.6311688423157, 'accumulated_logging_time': 0.9028010368347168}
I0327 10:27:00.354997 140159538132736 logging_writer.py:48] [35298] accumulated_eval_time=2082.631169, accumulated_logging_time=0.902801, accumulated_submission_time=27407.833397, global_step=35298, preemption_count=0, score=27407.833397, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=29492.778333, train/ctc_loss=1692.7669677734375, train/wer=0.944768, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 10:29:32.487725 140159529740032 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.0, loss=1844.0941162109375
I0327 10:36:07.656161 140159538132736 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.0, loss=1824.6412353515625
I0327 10:42:29.054455 140159538132736 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.0, loss=1772.219970703125
I0327 10:49:13.857193 140159529740032 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.0, loss=1858.5535888671875
I0327 10:51:00.627810 140317300651840 spec.py:321] Evaluating on the training split.
I0327 10:51:37.220200 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 10:52:21.090089 140317300651840 spec.py:349] Evaluating on the test split.
I0327 10:52:43.568630 140317300651840 submission_runner.py:422] Time since start: 31036.02s, 	Step: 37132, 	{'train/ctc_loss': Array(1787.1725, dtype=float32), 'train/wer': 0.9427091658940503, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 28848.01889371872, 'total_duration': 31036.02292251587, 'accumulated_submission_time': 28848.01889371872, 'accumulated_eval_time': 2185.566300392151, 'accumulated_logging_time': 0.950528621673584}
I0327 10:52:43.604248 140161043420928 logging_writer.py:48] [37132] accumulated_eval_time=2185.566300, accumulated_logging_time=0.950529, accumulated_submission_time=28848.018894, global_step=37132, preemption_count=0, score=28848.018894, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=31036.022923, train/ctc_loss=1787.1724853515625, train/wer=0.942709, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 10:57:19.358523 140161035028224 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.0, loss=1892.2105712890625
I0327 11:03:51.236659 140161043420928 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.0, loss=1841.8387451171875
I0327 11:10:16.469916 140161043420928 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.0, loss=1754.84765625
I0327 11:16:44.258049 140317300651840 spec.py:321] Evaluating on the training split.
I0327 11:17:24.608010 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 11:18:09.266598 140317300651840 spec.py:349] Evaluating on the test split.
I0327 11:18:31.451624 140317300651840 submission_runner.py:422] Time since start: 32583.91s, 	Step: 38983, 	{'train/ctc_loss': Array(1714.3213, dtype=float32), 'train/wer': 0.9448971433842748, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 30288.587795495987, 'total_duration': 32583.906710147858, 'accumulated_submission_time': 30288.587795495987, 'accumulated_eval_time': 2292.7547812461853, 'accumulated_logging_time': 0.9997653961181641}
I0327 11:18:31.486873 140161473500928 logging_writer.py:48] [38983] accumulated_eval_time=2292.754781, accumulated_logging_time=0.999765, accumulated_submission_time=30288.587795, global_step=38983, preemption_count=0, score=30288.587795, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=32583.906710, train/ctc_loss=1714.3212890625, train/wer=0.944897, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 11:18:45.100246 140161465108224 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.0, loss=1749.3314208984375
I0327 11:25:03.627017 140160818140928 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.0, loss=1878.18896484375
I0327 11:31:44.132802 140160809748224 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.0, loss=1871.1875
I0327 11:38:15.531370 140161473500928 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.0, loss=1767.692138671875
I0327 11:42:31.512586 140317300651840 spec.py:321] Evaluating on the training split.
I0327 11:43:08.532847 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 11:43:52.735349 140317300651840 spec.py:349] Evaluating on the test split.
I0327 11:44:15.026723 140317300651840 submission_runner.py:422] Time since start: 34127.48s, 	Step: 40831, 	{'train/ctc_loss': Array(1760.6809, dtype=float32), 'train/wer': 0.9432324554919642, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 31728.521372795105, 'total_duration': 34127.480954408646, 'accumulated_submission_time': 31728.521372795105, 'accumulated_eval_time': 2396.263006925583, 'accumulated_logging_time': 1.0554406642913818}
I0327 11:44:15.063313 140161473500928 logging_writer.py:48] [40831] accumulated_eval_time=2396.263007, accumulated_logging_time=1.055441, accumulated_submission_time=31728.521373, global_step=40831, preemption_count=0, score=31728.521373, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=34127.480954, train/ctc_loss=1760.680908203125, train/wer=0.943232, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 11:46:22.210227 140161465108224 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.0, loss=1856.66650390625
I0327 11:52:39.884927 140161473500928 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.0, loss=1856.935791015625
I0327 11:59:16.618982 140161465108224 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.0, loss=1865.868408203125
I0327 12:05:53.965127 140161473500928 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.0, loss=1844.625732421875
I0327 12:08:15.590355 140317300651840 spec.py:321] Evaluating on the training split.
I0327 12:08:52.867191 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 12:09:37.288154 140317300651840 spec.py:349] Evaluating on the test split.
I0327 12:09:59.760672 140317300651840 submission_runner.py:422] Time since start: 35672.22s, 	Step: 42690, 	{'train/ctc_loss': Array(1852.6161, dtype=float32), 'train/wer': 0.941680272071945, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 33168.95918941498, 'total_duration': 35672.21574687958, 'accumulated_submission_time': 33168.95918941498, 'accumulated_eval_time': 2500.428219795227, 'accumulated_logging_time': 1.1089749336242676}
I0327 12:09:59.793354 140161043420928 logging_writer.py:48] [42690] accumulated_eval_time=2500.428220, accumulated_logging_time=1.108975, accumulated_submission_time=33168.959189, global_step=42690, preemption_count=0, score=33168.959189, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=35672.215747, train/ctc_loss=1852.6160888671875, train/wer=0.941680, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 12:13:53.220462 140161035028224 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.0, loss=1924.356689453125
I0327 12:20:29.324827 140160715740928 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.0, loss=1850.49267578125
I0327 12:27:01.459698 140160707348224 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.0, loss=1835.2366943359375
I0327 12:33:43.838077 140161043420928 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.0, loss=1786.8162841796875
I0327 12:34:00.256671 140317300651840 spec.py:321] Evaluating on the training split.
I0327 12:34:37.159032 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 12:35:21.190434 140317300651840 spec.py:349] Evaluating on the test split.
I0327 12:35:43.409740 140317300651840 submission_runner.py:422] Time since start: 37215.86s, 	Step: 44523, 	{'train/ctc_loss': Array(1901.3655, dtype=float32), 'train/wer': 0.940312095793757, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 34609.33559679985, 'total_duration': 37215.86471271515, 'accumulated_submission_time': 34609.33559679985, 'accumulated_eval_time': 2603.5760900974274, 'accumulated_logging_time': 1.1572954654693604}
I0327 12:35:43.441199 140161258460928 logging_writer.py:48] [44523] accumulated_eval_time=2603.576090, accumulated_logging_time=1.157295, accumulated_submission_time=34609.335597, global_step=44523, preemption_count=0, score=34609.335597, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=37215.864713, train/ctc_loss=1901.365478515625, train/wer=0.940312, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 12:41:44.973530 140161250068224 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.0, loss=1774.4310302734375
I0327 12:48:28.484589 140161258460928 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.0, loss=1779.61181640625
I0327 12:54:54.763408 140161250068224 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.0, loss=1783.206787109375
I0327 12:59:43.581440 140317300651840 spec.py:321] Evaluating on the training split.
I0327 13:00:20.032125 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 13:01:04.491850 140317300651840 spec.py:349] Evaluating on the test split.
I0327 13:01:26.939595 140317300651840 submission_runner.py:422] Time since start: 38759.39s, 	Step: 46350, 	{'train/ctc_loss': Array(1959.8617, dtype=float32), 'train/wer': 0.9371047844119075, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 36049.388392448425, 'total_duration': 38759.39369678497, 'accumulated_submission_time': 36049.388392448425, 'accumulated_eval_time': 2706.9282174110413, 'accumulated_logging_time': 1.2041244506835938}
I0327 13:01:26.973788 140161258460928 logging_writer.py:48] [46350] accumulated_eval_time=2706.928217, accumulated_logging_time=1.204124, accumulated_submission_time=36049.388392, global_step=46350, preemption_count=0, score=36049.388392, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=38759.393697, train/ctc_loss=1959.8616943359375, train/wer=0.937105, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 13:03:23.653729 140161258460928 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.0, loss=1798.1116943359375
I0327 13:09:53.698520 140161250068224 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.0, loss=1886.493408203125
I0327 13:16:45.125912 140161258460928 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.0, loss=1832.7403564453125
I0327 13:23:08.888897 140317300651840 spec.py:321] Evaluating on the training split.
I0327 13:23:46.078127 140317300651840 spec.py:333] Evaluating on the validation split.
I0327 13:24:30.301775 140317300651840 spec.py:349] Evaluating on the test split.
I0327 13:24:52.620933 140317300651840 submission_runner.py:422] Time since start: 40165.07s, 	Step: 48000, 	{'train/ctc_loss': Array(2009.269, dtype=float32), 'train/wer': 0.9366967129626904, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 37351.22464346886, 'total_duration': 40165.07485938072, 'accumulated_submission_time': 37351.22464346886, 'accumulated_eval_time': 2810.654004096985, 'accumulated_logging_time': 1.2512035369873047}
I0327 13:24:52.666420 140160674780928 logging_writer.py:48] [48000] accumulated_eval_time=2810.654004, accumulated_logging_time=1.251204, accumulated_submission_time=37351.224643, global_step=48000, preemption_count=0, score=37351.224643, test/ctc_loss=3189.847412109375, test/num_examples=2472, test/wer=0.899580, total_duration=40165.074859, train/ctc_loss=2009.26904296875, train/wer=0.936697, validation/ctc_loss=3357.908935546875, validation/num_examples=5348, validation/wer=0.896618
I0327 13:24:52.693944 140160666388224 logging_writer.py:48] [48000] global_step=48000, preemption_count=0, score=37351.224643
I0327 13:24:53.927617 140317300651840 submission_runner.py:596] Tuning trial 1/1
I0327 13:24:53.927881 140317300651840 submission_runner.py:597] Hyperparameters: Hyperparameters(learning_rate=4.131896390902391, beta1=0.9274758113254791, beta2=0.9978504782314613, warmup_steps=6999, decay_steps_factor=0.9007765761611038, end_factor=0.001, weight_decay=5.6687777311501786e-06, label_smoothing=0.2)
I0327 13:24:53.939092 140317300651840 submission_runner.py:598] Metrics: {'eval_results': [(1, {'train/ctc_loss': Array(30.544735, dtype=float32), 'train/wer': 0.9437925577327531, 'validation/ctc_loss': Array(29.510908, dtype=float32), 'validation/wer': 0.8979985904206532, 'validation/num_examples': 5348, 'test/ctc_loss': Array(29.550442, dtype=float32), 'test/wer': 0.9005951292832044, 'test/num_examples': 2472, 'score': 42.35780739784241, 'total_duration': 175.67673444747925, 'accumulated_submission_time': 42.35780739784241, 'accumulated_eval_time': 133.3188726902008, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (1884, {'train/ctc_loss': Array(1767.5358, dtype=float32), 'train/wer': 0.944635537887994, 'validation/ctc_loss': Array(3353.434, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3185.5388, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 1482.9771449565887, 'total_duration': 1715.630652666092, 'accumulated_submission_time': 1482.9771449565887, 'accumulated_eval_time': 232.55072259902954, 'accumulated_logging_time': 0.03709816932678223, 'global_step': 1884, 'preemption_count': 0}), (3789, {'train/ctc_loss': Array(1761.5707, dtype=float32), 'train/wer': 0.9427216791412514, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 2923.2749993801117, 'total_duration': 3255.5716688632965, 'accumulated_submission_time': 2923.2749993801117, 'accumulated_eval_time': 332.073290348053, 'accumulated_logging_time': 0.08661532402038574, 'global_step': 3789, 'preemption_count': 0}), (5684, {'train/ctc_loss': Array(1741.298, dtype=float32), 'train/wer': 0.9433243196230056, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 4363.585999965668, 'total_duration': 4797.491341590881, 'accumulated_submission_time': 4363.585999965668, 'accumulated_eval_time': 433.56079840660095, 'accumulated_logging_time': 0.1356813907623291, 'global_step': 5684, 'preemption_count': 0}), (7576, {'train/ctc_loss': Array(1724.8613, dtype=float32), 'train/wer': 0.9437003215030003, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 5804.109092712402, 'total_duration': 6339.904704332352, 'accumulated_submission_time': 5804.109092712402, 'accumulated_eval_time': 535.3329384326935, 'accumulated_logging_time': 0.1791532039642334, 'global_step': 7576, 'preemption_count': 0}), (9455, {'train/ctc_loss': Array(1832.9288, dtype=float32), 'train/wer': 0.9415512405140359, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 7244.324284791946, 'total_duration': 7880.7595911026, 'accumulated_submission_time': 7244.324284791946, 'accumulated_eval_time': 635.8550357818604, 'accumulated_logging_time': 0.22446107864379883, 'global_step': 9455, 'preemption_count': 0}), (11317, {'train/ctc_loss': Array(1752.8004, dtype=float32), 'train/wer': 0.9426410101839947, 'validation/ctc_loss': Array(3357.9224, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8608, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 8684.634976148605, 'total_duration': 9422.989792108536, 'accumulated_submission_time': 8684.634976148605, 'accumulated_eval_time': 737.6564910411835, 'accumulated_logging_time': 0.269711971282959, 'global_step': 11317, 'preemption_count': 0}), (13166, {'train/ctc_loss': Array(1746.1039, dtype=float32), 'train/wer': 0.9428243251866505, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 10124.65800499916, 'total_duration': 10965.888332128525, 'accumulated_submission_time': 10124.65800499916, 'accumulated_eval_time': 840.4148964881897, 'accumulated_logging_time': 0.3143177032470703, 'global_step': 13166, 'preemption_count': 0}), (15030, {'train/ctc_loss': Array(1733.7323, dtype=float32), 'train/wer': 0.9440859096700382, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 11564.76735496521, 'total_duration': 12508.570070266724, 'accumulated_submission_time': 11564.76735496521, 'accumulated_eval_time': 942.8670687675476, 'accumulated_logging_time': 0.3611013889312744, 'global_step': 15030, 'preemption_count': 0}), (16898, {'train/ctc_loss': Array(1786.8575, dtype=float32), 'train/wer': 0.9427990785714666, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 13004.706295013428, 'total_duration': 14051.189279079437, 'accumulated_submission_time': 13004.706295013428, 'accumulated_eval_time': 1045.4217836856842, 'accumulated_logging_time': 0.41051316261291504, 'global_step': 16898, 'preemption_count': 0}), (18740, {'train/ctc_loss': Array(1755.9307, dtype=float32), 'train/wer': 0.9423383225986367, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 14444.9840426445, 'total_duration': 15594.438996076584, 'accumulated_submission_time': 14444.9840426445, 'accumulated_eval_time': 1148.2717576026917, 'accumulated_logging_time': 0.4578583240509033, 'global_step': 18740, 'preemption_count': 0}), (20576, {'train/ctc_loss': Array(1731.2422, dtype=float32), 'train/wer': 0.9431396916893625, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 15885.290096282959, 'total_duration': 17138.053234815598, 'accumulated_submission_time': 15885.290096282959, 'accumulated_eval_time': 1251.4591183662415, 'accumulated_logging_time': 0.5051019191741943, 'global_step': 20576, 'preemption_count': 0}), (22410, {'train/ctc_loss': Array(1763.6094, dtype=float32), 'train/wer': 0.9432716912443612, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 17325.769654989243, 'total_duration': 18681.63670516014, 'accumulated_submission_time': 17325.769654989243, 'accumulated_eval_time': 1354.4430437088013, 'accumulated_logging_time': 0.5506594181060791, 'global_step': 22410, 'preemption_count': 0}), (24267, {'train/ctc_loss': Array(1739.3417, dtype=float32), 'train/wer': 0.944685667249717, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 18766.195712566376, 'total_duration': 20226.49410033226, 'accumulated_submission_time': 18766.195712566376, 'accumulated_eval_time': 1458.7423107624054, 'accumulated_logging_time': 0.6058189868927002, 'global_step': 24267, 'preemption_count': 0}), (26110, {'train/ctc_loss': Array(1769.4664, dtype=float32), 'train/wer': 0.9432456399645285, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 20206.198328971863, 'total_duration': 21770.385784864426, 'accumulated_submission_time': 20206.198328971863, 'accumulated_eval_time': 1562.5053470134735, 'accumulated_logging_time': 0.6552648544311523, 'global_step': 26110, 'preemption_count': 0}), (27934, {'train/ctc_loss': Array(1736.9141, dtype=float32), 'train/wer': 0.9439109001278072, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 21646.778349161148, 'total_duration': 23313.977199316025, 'accumulated_submission_time': 21646.778349161148, 'accumulated_eval_time': 1665.3954215049744, 'accumulated_logging_time': 0.7016565799713135, 'global_step': 27934, 'preemption_count': 0}), (29762, {'train/ctc_loss': Array(1715.2401, dtype=float32), 'train/wer': 0.9450143703143059, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 23086.956217050552, 'total_duration': 24857.6637737751, 'accumulated_submission_time': 23086.956217050552, 'accumulated_eval_time': 1768.7719390392303, 'accumulated_logging_time': 0.7576029300689697, 'global_step': 29762, 'preemption_count': 0}), (31590, {'train/ctc_loss': Array(1783.4905, dtype=float32), 'train/wer': 0.9417576703068122, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 24527.2738301754, 'total_duration': 26404.442217588425, 'accumulated_submission_time': 24527.2738301754, 'accumulated_eval_time': 1875.1094052791595, 'accumulated_logging_time': 0.804293155670166, 'global_step': 31590, 'preemption_count': 0}), (33451, {'train/ctc_loss': Array(1824.6843, dtype=float32), 'train/wer': 0.9416600198590335, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 25967.817270994186, 'total_duration': 27949.851816892624, 'accumulated_submission_time': 25967.817270994186, 'accumulated_eval_time': 1979.8492782115936, 'accumulated_logging_time': 0.8511202335357666, 'global_step': 33451, 'preemption_count': 0}), (35298, {'train/ctc_loss': Array(1692.767, dtype=float32), 'train/wer': 0.9447677853176417, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 27407.83339715004, 'total_duration': 29492.778332948685, 'accumulated_submission_time': 27407.83339715004, 'accumulated_eval_time': 2082.6311688423157, 'accumulated_logging_time': 0.9028010368347168, 'global_step': 35298, 'preemption_count': 0}), (37132, {'train/ctc_loss': Array(1787.1725, dtype=float32), 'train/wer': 0.9427091658940503, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 28848.01889371872, 'total_duration': 31036.02292251587, 'accumulated_submission_time': 28848.01889371872, 'accumulated_eval_time': 2185.566300392151, 'accumulated_logging_time': 0.950528621673584, 'global_step': 37132, 'preemption_count': 0}), (38983, {'train/ctc_loss': Array(1714.3213, dtype=float32), 'train/wer': 0.9448971433842748, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 30288.587795495987, 'total_duration': 32583.906710147858, 'accumulated_submission_time': 30288.587795495987, 'accumulated_eval_time': 2292.7547812461853, 'accumulated_logging_time': 0.9997653961181641, 'global_step': 38983, 'preemption_count': 0}), (40831, {'train/ctc_loss': Array(1760.6809, dtype=float32), 'train/wer': 0.9432324554919642, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 31728.521372795105, 'total_duration': 34127.480954408646, 'accumulated_submission_time': 31728.521372795105, 'accumulated_eval_time': 2396.263006925583, 'accumulated_logging_time': 1.0554406642913818, 'global_step': 40831, 'preemption_count': 0}), (42690, {'train/ctc_loss': Array(1852.6161, dtype=float32), 'train/wer': 0.941680272071945, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 33168.95918941498, 'total_duration': 35672.21574687958, 'accumulated_submission_time': 33168.95918941498, 'accumulated_eval_time': 2500.428219795227, 'accumulated_logging_time': 1.1089749336242676, 'global_step': 42690, 'preemption_count': 0}), (44523, {'train/ctc_loss': Array(1901.3655, dtype=float32), 'train/wer': 0.940312095793757, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 34609.33559679985, 'total_duration': 37215.86471271515, 'accumulated_submission_time': 34609.33559679985, 'accumulated_eval_time': 2603.5760900974274, 'accumulated_logging_time': 1.1572954654693604, 'global_step': 44523, 'preemption_count': 0}), (46350, {'train/ctc_loss': Array(1959.8617, dtype=float32), 'train/wer': 0.9371047844119075, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 36049.388392448425, 'total_duration': 38759.39369678497, 'accumulated_submission_time': 36049.388392448425, 'accumulated_eval_time': 2706.9282174110413, 'accumulated_logging_time': 1.2041244506835938, 'global_step': 46350, 'preemption_count': 0}), (48000, {'train/ctc_loss': Array(2009.269, dtype=float32), 'train/wer': 0.9366967129626904, 'validation/ctc_loss': Array(3357.909, dtype=float32), 'validation/wer': 0.8966179750330672, 'validation/num_examples': 5348, 'test/ctc_loss': Array(3189.8474, dtype=float32), 'test/wer': 0.899579550301627, 'test/num_examples': 2472, 'score': 37351.22464346886, 'total_duration': 40165.07485938072, 'accumulated_submission_time': 37351.22464346886, 'accumulated_eval_time': 2810.654004096985, 'accumulated_logging_time': 1.2512035369873047, 'global_step': 48000, 'preemption_count': 0})], 'global_step': 48000}
I0327 13:24:53.939280 140317300651840 submission_runner.py:599] Timing: 37351.22464346886
I0327 13:24:53.939349 140317300651840 submission_runner.py:601] Total number of evals: 27
I0327 13:24:53.939406 140317300651840 submission_runner.py:602] ====================
I0327 13:24:53.942728 140317300651840 submission_runner.py:686] Final librispeech_deepspeech_no_resnet score: 0
