python3 submission_runner.py --framework=jax --workload=wmt_attention_temp --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --rng_seed=2250445898 --max_global_steps=133333 --tuning_ruleset=external --tuning_search_space=reference_algorithms/target_setting_algorithms/wmt_attention_temp/tuning_search_space.json --num_tuning_trials=1 2>&1 | tee -a /logs/wmt_attention_temp_jax_03-26-2024-02-18-45.log
I0326 02:19:09.257041 140257815279424 logger_utils.py:61] Removing existing experiment directory /experiment_runs/variants_target_setting/study_0/wmt_attention_temp_jax because --overwrite was set.
I0326 02:19:09.281193 140257815279424 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/wmt_attention_temp_jax.
I0326 02:19:10.291187 140257815279424 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0326 02:19:10.291886 140257815279424 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0326 02:19:10.292008 140257815279424 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0326 02:19:10.297829 140257815279424 submission_runner.py:554] Using RNG seed 2250445898
I0326 02:19:11.391870 140257815279424 submission_runner.py:563] --- Tuning run 1/1 ---
I0326 02:19:11.392050 140257815279424 submission_runner.py:568] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/wmt_attention_temp_jax/trial_1.
I0326 02:19:11.392214 140257815279424 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/wmt_attention_temp_jax/trial_1/hparams.json.
I0326 02:19:11.572094 140257815279424 submission_runner.py:209] Initializing dataset.
I0326 02:19:11.593474 140257815279424 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0326 02:19:11.614543 140257815279424 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0326 02:19:11.759031 140257815279424 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0326 02:19:13.762553 140257815279424 submission_runner.py:220] Initializing model.
I0326 02:19:22.605278 140257815279424 submission_runner.py:262] Initializing optimizer.
I0326 02:19:23.559290 140257815279424 submission_runner.py:269] Initializing metrics bundle.
I0326 02:19:23.559467 140257815279424 submission_runner.py:287] Initializing checkpoint and logger.
I0326 02:19:23.560504 140257815279424 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/wmt_attention_temp_jax/trial_1 with prefix checkpoint_
I0326 02:19:23.560632 140257815279424 submission_runner.py:307] Saving meta data to /experiment_runs/variants_target_setting/study_0/wmt_attention_temp_jax/trial_1/meta_data_0.json.
I0326 02:19:23.560833 140257815279424 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0326 02:19:23.560897 140257815279424 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0326 02:19:23.841174 140257815279424 logger_utils.py:220] Unable to record git information. Continuing without it.
I0326 02:19:24.089185 140257815279424 submission_runner.py:311] Saving flags to /experiment_runs/variants_target_setting/study_0/wmt_attention_temp_jax/trial_1/flags_0.json.
I0326 02:19:24.098647 140257815279424 submission_runner.py:321] Starting training loop.
I0326 02:20:02.038073 140092404397824 logging_writer.py:48] [0] global_step=0, grad_norm=3.8800957202911377, loss=10.9512939453125
I0326 02:20:02.052867 140257815279424 spec.py:321] Evaluating on the training split.
I0326 02:20:02.056125 140257815279424 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0326 02:20:02.058991 140257815279424 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0326 02:20:02.094307 140257815279424 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0326 02:20:10.075783 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 02:25:02.241833 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 02:25:02.245708 140257815279424 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0326 02:25:02.249591 140257815279424 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0326 02:25:02.284350 140257815279424 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0326 02:25:09.152544 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 02:29:51.721095 140257815279424 spec.py:349] Evaluating on the test split.
I0326 02:29:51.723656 140257815279424 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0326 02:29:51.726690 140257815279424 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0326 02:29:51.759365 140257815279424 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0326 02:29:54.567840 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 02:34:36.855698 140257815279424 submission_runner.py:420] Time since start: 912.76s, 	Step: 1, 	{'train/accuracy': 0.0006656643818132579, 'train/loss': 11.016271591186523, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.048277854919434, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.050202369689941, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 37.95417833328247, 'total_duration': 912.7569696903229, 'accumulated_submission_time': 37.95417833328247, 'accumulated_eval_time': 874.8027560710907, 'accumulated_logging_time': 0}
I0326 02:34:36.877470 140088116741888 logging_writer.py:48] [1] accumulated_eval_time=874.802756, accumulated_logging_time=0, accumulated_submission_time=37.954178, global_step=1, preemption_count=0, score=37.954178, test/accuracy=0.000709, test/bleu=0.000000, test/loss=11.050202, test/num_examples=3003, total_duration=912.756970, train/accuracy=0.000666, train/bleu=0.000000, train/loss=11.016272, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=11.048278, validation/num_examples=3000
I0326 02:34:37.252835 140088108349184 logging_writer.py:48] [1] global_step=1, grad_norm=3.810023307800293, loss=10.949250221252441
I0326 02:34:37.624984 140088116741888 logging_writer.py:48] [2] global_step=2, grad_norm=3.9242653846740723, loss=10.947556495666504
I0326 02:34:37.996084 140088108349184 logging_writer.py:48] [3] global_step=3, grad_norm=3.85127592086792, loss=10.94140625
I0326 02:34:38.364364 140088116741888 logging_writer.py:48] [4] global_step=4, grad_norm=3.860316038131714, loss=10.940237998962402
I0326 02:34:38.732593 140088108349184 logging_writer.py:48] [5] global_step=5, grad_norm=3.886275053024292, loss=10.93589973449707
I0326 02:34:39.104809 140088116741888 logging_writer.py:48] [6] global_step=6, grad_norm=3.872824192047119, loss=10.933978080749512
I0326 02:34:39.473886 140088108349184 logging_writer.py:48] [7] global_step=7, grad_norm=3.8668148517608643, loss=10.929248809814453
I0326 02:34:39.845412 140088116741888 logging_writer.py:48] [8] global_step=8, grad_norm=3.886298894882202, loss=10.934144973754883
I0326 02:34:40.213753 140088108349184 logging_writer.py:48] [9] global_step=9, grad_norm=3.8552558422088623, loss=10.925869941711426
I0326 02:34:40.584167 140088116741888 logging_writer.py:48] [10] global_step=10, grad_norm=3.853433132171631, loss=10.926674842834473
I0326 02:34:40.954728 140088108349184 logging_writer.py:48] [11] global_step=11, grad_norm=3.889247179031372, loss=10.905633926391602
I0326 02:34:41.323373 140088116741888 logging_writer.py:48] [12] global_step=12, grad_norm=3.87041974067688, loss=10.896669387817383
I0326 02:34:41.692099 140088108349184 logging_writer.py:48] [13] global_step=13, grad_norm=3.8703489303588867, loss=10.90511703491211
I0326 02:34:42.061311 140088116741888 logging_writer.py:48] [14] global_step=14, grad_norm=3.7983999252319336, loss=10.900300025939941
I0326 02:34:42.430863 140088108349184 logging_writer.py:48] [15] global_step=15, grad_norm=3.855543375015259, loss=10.866813659667969
I0326 02:34:42.802469 140088116741888 logging_writer.py:48] [16] global_step=16, grad_norm=3.8869717121124268, loss=10.877819061279297
I0326 02:34:43.172806 140088108349184 logging_writer.py:48] [17] global_step=17, grad_norm=3.815659761428833, loss=10.864459037780762
I0326 02:34:43.542054 140088116741888 logging_writer.py:48] [18] global_step=18, grad_norm=3.8278207778930664, loss=10.851326942443848
I0326 02:34:43.911984 140088108349184 logging_writer.py:48] [19] global_step=19, grad_norm=3.814760208129883, loss=10.841156005859375
I0326 02:34:44.278194 140088116741888 logging_writer.py:48] [20] global_step=20, grad_norm=3.807809829711914, loss=10.829072952270508
I0326 02:34:44.643974 140088108349184 logging_writer.py:48] [21] global_step=21, grad_norm=3.770752429962158, loss=10.824993133544922
I0326 02:34:45.013453 140088116741888 logging_writer.py:48] [22] global_step=22, grad_norm=3.824307441711426, loss=10.817566871643066
I0326 02:34:45.382025 140088108349184 logging_writer.py:48] [23] global_step=23, grad_norm=3.78084659576416, loss=10.802170753479004
I0326 02:34:45.752525 140088116741888 logging_writer.py:48] [24] global_step=24, grad_norm=3.7537975311279297, loss=10.787947654724121
I0326 02:34:46.120441 140088108349184 logging_writer.py:48] [25] global_step=25, grad_norm=3.7716033458709717, loss=10.775052070617676
I0326 02:34:46.487152 140088116741888 logging_writer.py:48] [26] global_step=26, grad_norm=3.682880401611328, loss=10.770214080810547
I0326 02:34:46.855280 140088108349184 logging_writer.py:48] [27] global_step=27, grad_norm=3.7616868019104004, loss=10.740083694458008
I0326 02:34:47.224864 140088116741888 logging_writer.py:48] [28] global_step=28, grad_norm=3.7126340866088867, loss=10.734021186828613
I0326 02:34:47.592572 140088108349184 logging_writer.py:48] [29] global_step=29, grad_norm=3.6885106563568115, loss=10.729747772216797
I0326 02:34:47.961014 140088116741888 logging_writer.py:48] [30] global_step=30, grad_norm=3.710517168045044, loss=10.691324234008789
I0326 02:34:48.329404 140088108349184 logging_writer.py:48] [31] global_step=31, grad_norm=3.6276021003723145, loss=10.686700820922852
I0326 02:34:48.698066 140088116741888 logging_writer.py:48] [32] global_step=32, grad_norm=3.703366994857788, loss=10.658164978027344
I0326 02:34:49.066386 140088108349184 logging_writer.py:48] [33] global_step=33, grad_norm=3.602020263671875, loss=10.658914566040039
I0326 02:34:49.433209 140088116741888 logging_writer.py:48] [34] global_step=34, grad_norm=3.5899152755737305, loss=10.635047912597656
I0326 02:34:49.800952 140088108349184 logging_writer.py:48] [35] global_step=35, grad_norm=3.5552730560302734, loss=10.62564754486084
I0326 02:34:50.168373 140088116741888 logging_writer.py:48] [36] global_step=36, grad_norm=3.530679225921631, loss=10.6056489944458
I0326 02:34:50.533831 140088108349184 logging_writer.py:48] [37] global_step=37, grad_norm=3.5009381771087646, loss=10.593342781066895
I0326 02:34:50.899888 140088116741888 logging_writer.py:48] [38] global_step=38, grad_norm=3.451314926147461, loss=10.580708503723145
I0326 02:34:51.265033 140088108349184 logging_writer.py:48] [39] global_step=39, grad_norm=3.51631498336792, loss=10.537444114685059
I0326 02:34:51.632313 140088116741888 logging_writer.py:48] [40] global_step=40, grad_norm=3.446382522583008, loss=10.534125328063965
I0326 02:34:51.996899 140088108349184 logging_writer.py:48] [41] global_step=41, grad_norm=3.3979783058166504, loss=10.525579452514648
I0326 02:34:52.362274 140088116741888 logging_writer.py:48] [42] global_step=42, grad_norm=3.3552210330963135, loss=10.483878135681152
I0326 02:34:52.727939 140088108349184 logging_writer.py:48] [43] global_step=43, grad_norm=3.348698377609253, loss=10.468945503234863
I0326 02:34:53.093655 140088116741888 logging_writer.py:48] [44] global_step=44, grad_norm=3.3651041984558105, loss=10.436838150024414
I0326 02:34:53.462784 140088108349184 logging_writer.py:48] [45] global_step=45, grad_norm=3.2778329849243164, loss=10.421599388122559
I0326 02:34:53.831278 140088116741888 logging_writer.py:48] [46] global_step=46, grad_norm=3.2986834049224854, loss=10.39305305480957
I0326 02:34:54.198820 140088108349184 logging_writer.py:48] [47] global_step=47, grad_norm=3.208148717880249, loss=10.398852348327637
I0326 02:34:54.566959 140088116741888 logging_writer.py:48] [48] global_step=48, grad_norm=3.1963443756103516, loss=10.372408866882324
I0326 02:34:54.936593 140088108349184 logging_writer.py:48] [49] global_step=49, grad_norm=3.1979997158050537, loss=10.334796905517578
I0326 02:34:55.304546 140088116741888 logging_writer.py:48] [50] global_step=50, grad_norm=3.1333751678466797, loss=10.324444770812988
I0326 02:34:55.673541 140088108349184 logging_writer.py:48] [51] global_step=51, grad_norm=3.068119764328003, loss=10.305676460266113
I0326 02:34:56.042215 140088116741888 logging_writer.py:48] [52] global_step=52, grad_norm=3.031684160232544, loss=10.291914939880371
I0326 02:34:56.408238 140088108349184 logging_writer.py:48] [53] global_step=53, grad_norm=3.022332191467285, loss=10.267256736755371
I0326 02:34:56.775043 140088116741888 logging_writer.py:48] [54] global_step=54, grad_norm=2.9600679874420166, loss=10.247432708740234
I0326 02:34:57.141356 140088108349184 logging_writer.py:48] [55] global_step=55, grad_norm=2.903671979904175, loss=10.235847473144531
I0326 02:34:57.508769 140088116741888 logging_writer.py:48] [56] global_step=56, grad_norm=2.9056050777435303, loss=10.192111015319824
I0326 02:34:57.875802 140088108349184 logging_writer.py:48] [57] global_step=57, grad_norm=2.845250129699707, loss=10.189227104187012
I0326 02:34:58.241453 140088116741888 logging_writer.py:48] [58] global_step=58, grad_norm=2.829180955886841, loss=10.148463249206543
I0326 02:34:58.607820 140088108349184 logging_writer.py:48] [59] global_step=59, grad_norm=2.7656409740448, loss=10.139732360839844
I0326 02:34:58.975908 140088116741888 logging_writer.py:48] [60] global_step=60, grad_norm=2.737321138381958, loss=10.107616424560547
I0326 02:34:59.345225 140088108349184 logging_writer.py:48] [61] global_step=61, grad_norm=2.712822675704956, loss=10.099327087402344
I0326 02:34:59.712669 140088116741888 logging_writer.py:48] [62] global_step=62, grad_norm=2.6405670642852783, loss=10.089604377746582
I0326 02:35:00.079275 140088108349184 logging_writer.py:48] [63] global_step=63, grad_norm=2.616631031036377, loss=10.060397148132324
I0326 02:35:00.448550 140088116741888 logging_writer.py:48] [64] global_step=64, grad_norm=2.5578994750976562, loss=10.044028282165527
I0326 02:35:00.815611 140088108349184 logging_writer.py:48] [65] global_step=65, grad_norm=2.5377416610717773, loss=10.021968841552734
I0326 02:35:01.182889 140088116741888 logging_writer.py:48] [66] global_step=66, grad_norm=2.506741762161255, loss=9.994073867797852
I0326 02:35:01.551536 140088108349184 logging_writer.py:48] [67] global_step=67, grad_norm=2.4809346199035645, loss=9.96907901763916
I0326 02:35:01.918477 140088116741888 logging_writer.py:48] [68] global_step=68, grad_norm=2.4208152294158936, loss=9.930444717407227
I0326 02:35:02.285384 140088108349184 logging_writer.py:48] [69] global_step=69, grad_norm=2.4122419357299805, loss=9.928404808044434
I0326 02:35:02.652948 140088116741888 logging_writer.py:48] [70] global_step=70, grad_norm=2.338801622390747, loss=9.894303321838379
I0326 02:35:03.022379 140088108349184 logging_writer.py:48] [71] global_step=71, grad_norm=2.286320209503174, loss=9.903447151184082
I0326 02:35:03.392841 140088116741888 logging_writer.py:48] [72] global_step=72, grad_norm=2.265310764312744, loss=9.869525909423828
I0326 02:35:03.762340 140088108349184 logging_writer.py:48] [73] global_step=73, grad_norm=2.2282769680023193, loss=9.838810920715332
I0326 02:35:04.130252 140088116741888 logging_writer.py:48] [74] global_step=74, grad_norm=2.1872103214263916, loss=9.818666458129883
I0326 02:35:04.498677 140088108349184 logging_writer.py:48] [75] global_step=75, grad_norm=2.130690336227417, loss=9.819818496704102
I0326 02:35:04.866821 140088116741888 logging_writer.py:48] [76] global_step=76, grad_norm=2.113067865371704, loss=9.771283149719238
I0326 02:35:05.231655 140088108349184 logging_writer.py:48] [77] global_step=77, grad_norm=2.0902814865112305, loss=9.76298713684082
I0326 02:35:05.599415 140088116741888 logging_writer.py:48] [78] global_step=78, grad_norm=2.041818857192993, loss=9.751138687133789
I0326 02:35:05.966659 140088108349184 logging_writer.py:48] [79] global_step=79, grad_norm=2.00959849357605, loss=9.732842445373535
I0326 02:35:06.336988 140088116741888 logging_writer.py:48] [80] global_step=80, grad_norm=1.9839190244674683, loss=9.718035697937012
I0326 02:35:06.703601 140088108349184 logging_writer.py:48] [81] global_step=81, grad_norm=1.9531586170196533, loss=9.691228866577148
I0326 02:35:07.072549 140088116741888 logging_writer.py:48] [82] global_step=82, grad_norm=1.9078370332717896, loss=9.689153671264648
I0326 02:35:07.438760 140088108349184 logging_writer.py:48] [83] global_step=83, grad_norm=1.8591071367263794, loss=9.683403968811035
I0326 02:35:07.803855 140088116741888 logging_writer.py:48] [84] global_step=84, grad_norm=1.867098093032837, loss=9.631091117858887
I0326 02:35:08.169391 140088108349184 logging_writer.py:48] [85] global_step=85, grad_norm=1.819674015045166, loss=9.611385345458984
I0326 02:35:08.535210 140088116741888 logging_writer.py:48] [86] global_step=86, grad_norm=1.7765597105026245, loss=9.59956169128418
I0326 02:35:08.903449 140088108349184 logging_writer.py:48] [87] global_step=87, grad_norm=1.7593814134597778, loss=9.578513145446777
I0326 02:35:09.270438 140088116741888 logging_writer.py:48] [88] global_step=88, grad_norm=1.7265989780426025, loss=9.559545516967773
I0326 02:35:09.638314 140088108349184 logging_writer.py:48] [89] global_step=89, grad_norm=1.6976059675216675, loss=9.554398536682129
I0326 02:35:10.003280 140088116741888 logging_writer.py:48] [90] global_step=90, grad_norm=1.6522825956344604, loss=9.55985164642334
I0326 02:35:10.369561 140088108349184 logging_writer.py:48] [91] global_step=91, grad_norm=1.6394742727279663, loss=9.532973289489746
I0326 02:35:10.738397 140088116741888 logging_writer.py:48] [92] global_step=92, grad_norm=1.6210922002792358, loss=9.51297664642334
I0326 02:35:11.106359 140088108349184 logging_writer.py:48] [93] global_step=93, grad_norm=1.5871466398239136, loss=9.48934555053711
I0326 02:35:11.473256 140088116741888 logging_writer.py:48] [94] global_step=94, grad_norm=1.5714761018753052, loss=9.468873977661133
I0326 02:35:11.838387 140088108349184 logging_writer.py:48] [95] global_step=95, grad_norm=1.5306615829467773, loss=9.458343505859375
I0326 02:35:12.206343 140088116741888 logging_writer.py:48] [96] global_step=96, grad_norm=1.5370100736618042, loss=9.421087265014648
I0326 02:35:12.571939 140088108349184 logging_writer.py:48] [97] global_step=97, grad_norm=1.4935635328292847, loss=9.426209449768066
I0326 02:35:12.939099 140088116741888 logging_writer.py:48] [98] global_step=98, grad_norm=1.458921194076538, loss=9.413516998291016
I0326 02:35:13.309249 140088108349184 logging_writer.py:48] [99] global_step=99, grad_norm=1.4229958057403564, loss=9.429396629333496
I0326 02:35:13.676029 140088116741888 logging_writer.py:48] [100] global_step=100, grad_norm=1.3979119062423706, loss=9.416817665100098
I0326 02:37:36.206194 140088108349184 logging_writer.py:48] [500] global_step=500, grad_norm=0.11570870131254196, loss=8.324926376342773
I0326 02:40:34.527624 140088116741888 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.1449757069349289, loss=7.681800842285156
I0326 02:43:32.952259 140088108349184 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.1997401863336563, loss=6.863441467285156
I0326 02:46:31.420967 140088116741888 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.2587220370769501, loss=6.220836162567139
I0326 02:48:37.165712 140257815279424 spec.py:321] Evaluating on the training split.
I0326 02:48:40.160047 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 02:53:22.283157 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 02:53:24.984559 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 02:58:05.326804 140257815279424 spec.py:349] Evaluating on the test split.
I0326 02:58:08.010382 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 03:02:47.914319 140257815279424 submission_runner.py:420] Time since start: 2603.82s, 	Step: 2354, 	{'train/accuracy': 0.23909994959831238, 'train/loss': 5.794710159301758, 'train/bleu': 2.9965542897400064, 'validation/accuracy': 0.2231590449810028, 'validation/loss': 5.979231357574463, 'validation/bleu': 1.0355133685575952, 'validation/num_examples': 3000, 'test/accuracy': 0.20591482520103455, 'test/loss': 6.264630317687988, 'test/bleu': 0.9448569837909995, 'test/num_examples': 3003, 'score': 878.1650412082672, 'total_duration': 2603.815593481064, 'accumulated_submission_time': 878.1650412082672, 'accumulated_eval_time': 1725.5513274669647, 'accumulated_logging_time': 0.03208470344543457}
I0326 03:02:47.929012 140088108349184 logging_writer.py:48] [2354] accumulated_eval_time=1725.551327, accumulated_logging_time=0.032085, accumulated_submission_time=878.165041, global_step=2354, preemption_count=0, score=878.165041, test/accuracy=0.205915, test/bleu=0.944857, test/loss=6.264630, test/num_examples=3003, total_duration=2603.815593, train/accuracy=0.239100, train/bleu=2.996554, train/loss=5.794710, validation/accuracy=0.223159, validation/bleu=1.035513, validation/loss=5.979231, validation/num_examples=3000
I0326 03:03:40.136021 140088116741888 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.3335130214691162, loss=5.776446342468262
I0326 03:06:38.496977 140088108349184 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.37102681398391724, loss=5.365442752838135
I0326 03:09:37.014741 140088116741888 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.414823979139328, loss=5.031027317047119
I0326 03:12:35.618399 140088108349184 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.47357338666915894, loss=4.80819034576416
I0326 03:15:34.172417 140088116741888 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.5125444531440735, loss=4.504209518432617
I0326 03:16:48.202360 140257815279424 spec.py:321] Evaluating on the training split.
I0326 03:16:51.185703 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 03:20:56.372139 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 03:20:59.053508 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 03:25:03.071601 140257815279424 spec.py:349] Evaluating on the test split.
I0326 03:25:05.762783 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 03:29:00.017428 140257815279424 submission_runner.py:420] Time since start: 4175.92s, 	Step: 4709, 	{'train/accuracy': 0.3723996579647064, 'train/loss': 4.19680643081665, 'train/bleu': 10.72794150678018, 'validation/accuracy': 0.3527544438838959, 'validation/loss': 4.391270160675049, 'validation/bleu': 6.638556235953683, 'validation/num_examples': 3000, 'test/accuracy': 0.3339956998825073, 'test/loss': 4.64681339263916, 'test/bleu': 4.894400741660307, 'test/num_examples': 3003, 'score': 1718.3633179664612, 'total_duration': 4175.9186906814575, 'accumulated_submission_time': 1718.3633179664612, 'accumulated_eval_time': 2457.3663341999054, 'accumulated_logging_time': 0.055750370025634766}
I0326 03:29:00.032358 140088108349184 logging_writer.py:48] [4709] accumulated_eval_time=2457.366334, accumulated_logging_time=0.055750, accumulated_submission_time=1718.363318, global_step=4709, preemption_count=0, score=1718.363318, test/accuracy=0.333996, test/bleu=4.894401, test/loss=4.646813, test/num_examples=3003, total_duration=4175.918691, train/accuracy=0.372400, train/bleu=10.727942, train/loss=4.196806, validation/accuracy=0.352754, validation/bleu=6.638556, validation/loss=4.391270, validation/num_examples=3000
I0326 03:30:44.016952 140088116741888 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.5712779760360718, loss=4.095880508422852
I0326 03:33:42.651598 140088108349184 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6111771464347839, loss=3.9539361000061035
I0326 03:36:41.219553 140088116741888 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6113071441650391, loss=3.71917986869812
I0326 03:39:39.886297 140088108349184 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.6257744431495667, loss=3.3709661960601807
I0326 03:42:38.497813 140088116741888 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.6093923449516296, loss=3.409158229827881
I0326 03:43:00.365901 140257815279424 spec.py:321] Evaluating on the training split.
I0326 03:43:03.352838 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 03:45:42.709377 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 03:45:45.405030 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 03:48:43.446561 140257815279424 spec.py:349] Evaluating on the test split.
I0326 03:48:46.131643 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 03:51:10.925797 140257815279424 submission_runner.py:420] Time since start: 5506.83s, 	Step: 7063, 	{'train/accuracy': 0.49664491415023804, 'train/loss': 3.081625461578369, 'train/bleu': 20.618799445569337, 'validation/accuracy': 0.4871731102466583, 'validation/loss': 3.160738945007324, 'validation/bleu': 15.866710860775088, 'validation/num_examples': 3000, 'test/accuracy': 0.48168033361434937, 'test/loss': 3.2570042610168457, 'test/bleu': 14.23482273133436, 'test/num_examples': 3003, 'score': 2558.6197278499603, 'total_duration': 5506.827075481415, 'accumulated_submission_time': 2558.6197278499603, 'accumulated_eval_time': 2947.926177740097, 'accumulated_logging_time': 0.08072090148925781}
I0326 03:51:10.940909 140088108349184 logging_writer.py:48] [7063] accumulated_eval_time=2947.926178, accumulated_logging_time=0.080721, accumulated_submission_time=2558.619728, global_step=7063, preemption_count=0, score=2558.619728, test/accuracy=0.481680, test/bleu=14.234823, test/loss=3.257004, test/num_examples=3003, total_duration=5506.827075, train/accuracy=0.496645, train/bleu=20.618799, train/loss=3.081625, validation/accuracy=0.487173, validation/bleu=15.866711, validation/loss=3.160739, validation/num_examples=3000
I0326 03:53:47.053235 140088116741888 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.5688080787658691, loss=3.120121955871582
I0326 03:56:45.643729 140088108349184 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5702376961708069, loss=3.106193780899048
I0326 03:59:44.260435 140088116741888 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.49776530265808105, loss=2.946941614151001
I0326 04:02:42.850146 140088108349184 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.5137486457824707, loss=2.913447380065918
I0326 04:05:11.142680 140257815279424 spec.py:321] Evaluating on the training split.
I0326 04:05:14.132451 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 04:07:48.415369 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 04:07:51.100221 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 04:10:28.485003 140257815279424 spec.py:349] Evaluating on the test split.
I0326 04:10:31.176549 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 04:12:47.128978 140257815279424 submission_runner.py:420] Time since start: 6803.03s, 	Step: 9417, 	{'train/accuracy': 0.542782723903656, 'train/loss': 2.623689889907837, 'train/bleu': 24.78801555610238, 'validation/accuracy': 0.5464160442352295, 'validation/loss': 2.594118595123291, 'validation/bleu': 20.31014701984931, 'validation/num_examples': 3000, 'test/accuracy': 0.546139121055603, 'test/loss': 2.626765251159668, 'test/bleu': 18.94939669460022, 'test/num_examples': 3003, 'score': 3398.745558977127, 'total_duration': 6803.030266284943, 'accumulated_submission_time': 3398.745558977127, 'accumulated_eval_time': 3403.9124541282654, 'accumulated_logging_time': 0.1050713062286377}
I0326 04:12:47.144057 140088116741888 logging_writer.py:48] [9417] accumulated_eval_time=3403.912454, accumulated_logging_time=0.105071, accumulated_submission_time=3398.745559, global_step=9417, preemption_count=0, score=3398.745559, test/accuracy=0.546139, test/bleu=18.949397, test/loss=2.626765, test/num_examples=3003, total_duration=6803.030266, train/accuracy=0.542783, train/bleu=24.788016, train/loss=2.623690, validation/accuracy=0.546416, validation/bleu=20.310147, validation/loss=2.594119, validation/num_examples=3000
I0326 04:13:16.975205 140088108349184 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.4967745542526245, loss=2.6917214393615723
I0326 04:16:15.303809 140088116741888 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.48036226630210876, loss=2.6605207920074463
I0326 04:19:13.873042 140088108349184 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.4445868134498596, loss=2.605224847793579
I0326 04:22:12.489380 140088116741888 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.4624134302139282, loss=2.603783369064331
I0326 04:25:11.046899 140088108349184 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.42646467685699463, loss=2.550920248031616
I0326 04:26:47.163709 140257815279424 spec.py:321] Evaluating on the training split.
I0326 04:26:50.154908 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 04:29:17.002679 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 04:29:19.694856 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 04:31:45.956420 140257815279424 spec.py:349] Evaluating on the test split.
I0326 04:31:48.650994 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 04:34:02.994202 140257815279424 submission_runner.py:420] Time since start: 8078.90s, 	Step: 11771, 	{'train/accuracy': 0.5701367855072021, 'train/loss': 2.373084783554077, 'train/bleu': 26.71833475478846, 'validation/accuracy': 0.5803523659706116, 'validation/loss': 2.2887685298919678, 'validation/bleu': 22.777444797410663, 'validation/num_examples': 3000, 'test/accuracy': 0.5816512703895569, 'test/loss': 2.2855377197265625, 'test/bleu': 21.38527000165073, 'test/num_examples': 3003, 'score': 4238.6881992816925, 'total_duration': 8078.895473003387, 'accumulated_submission_time': 4238.6881992816925, 'accumulated_eval_time': 3839.7428874969482, 'accumulated_logging_time': 0.13013315200805664}
I0326 04:34:03.009324 140088116741888 logging_writer.py:48] [11771] accumulated_eval_time=3839.742887, accumulated_logging_time=0.130133, accumulated_submission_time=4238.688199, global_step=11771, preemption_count=0, score=4238.688199, test/accuracy=0.581651, test/bleu=21.385270, test/loss=2.285538, test/num_examples=3003, total_duration=8078.895473, train/accuracy=0.570137, train/bleu=26.718335, train/loss=2.373085, validation/accuracy=0.580352, validation/bleu=22.777445, validation/loss=2.288769, validation/num_examples=3000
I0326 04:35:24.772037 140088108349184 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.42034071683883667, loss=2.5616486072540283
I0326 04:38:23.274450 140088116741888 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.4320497512817383, loss=2.4202420711517334
I0326 04:41:21.868659 140088108349184 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.39352500438690186, loss=2.3821980953216553
I0326 04:44:20.449671 140088116741888 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.3935048282146454, loss=2.474263906478882
I0326 04:47:19.025850 140088108349184 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.3757416009902954, loss=2.3852896690368652
I0326 04:48:03.019738 140257815279424 spec.py:321] Evaluating on the training split.
I0326 04:48:06.001460 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 04:50:33.483789 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 04:50:36.167245 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 04:52:59.603060 140257815279424 spec.py:349] Evaluating on the test split.
I0326 04:53:02.290942 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 04:55:12.741503 140257815279424 submission_runner.py:420] Time since start: 9348.64s, 	Step: 14125, 	{'train/accuracy': 0.5916720628738403, 'train/loss': 2.1824207305908203, 'train/bleu': 28.196732035148745, 'validation/accuracy': 0.6031666994094849, 'validation/loss': 2.1089961528778076, 'validation/bleu': 24.17342051048301, 'validation/num_examples': 3000, 'test/accuracy': 0.6041136384010315, 'test/loss': 2.085299491882324, 'test/bleu': 22.705501317599904, 'test/num_examples': 3003, 'score': 5078.621715545654, 'total_duration': 9348.642786502838, 'accumulated_submission_time': 5078.621715545654, 'accumulated_eval_time': 4269.464607715607, 'accumulated_logging_time': 0.155181884765625}
I0326 04:55:12.758440 140088116741888 logging_writer.py:48] [14125] accumulated_eval_time=4269.464608, accumulated_logging_time=0.155182, accumulated_submission_time=5078.621716, global_step=14125, preemption_count=0, score=5078.621716, test/accuracy=0.604114, test/bleu=22.705501, test/loss=2.085299, test/num_examples=3003, total_duration=9348.642787, train/accuracy=0.591672, train/bleu=28.196732, train/loss=2.182421, validation/accuracy=0.603167, validation/bleu=24.173421, validation/loss=2.108996, validation/num_examples=3000
I0326 04:57:26.592635 140088108349184 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.3537024259567261, loss=2.2650697231292725
I0326 05:00:25.114514 140088116741888 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.36375692486763, loss=2.369795560836792
I0326 05:03:23.596328 140088108349184 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.33952757716178894, loss=2.2906596660614014
I0326 05:06:22.075351 140088116741888 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.33604905009269714, loss=2.217587947845459
I0326 05:09:12.757570 140257815279424 spec.py:321] Evaluating on the training split.
I0326 05:09:15.739480 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 05:11:43.741770 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 05:11:46.437227 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 05:14:12.440901 140257815279424 spec.py:349] Evaluating on the test split.
I0326 05:14:15.114172 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 05:16:30.168359 140257815279424 submission_runner.py:420] Time since start: 10626.07s, 	Step: 16480, 	{'train/accuracy': 0.602558970451355, 'train/loss': 2.0928454399108887, 'train/bleu': 29.180860091401485, 'validation/accuracy': 0.6138423681259155, 'validation/loss': 1.9973779916763306, 'validation/bleu': 24.98317953482843, 'validation/num_examples': 3000, 'test/accuracy': 0.6194294691085815, 'test/loss': 1.9588960409164429, 'test/bleu': 24.090562214757846, 'test/num_examples': 3003, 'score': 5918.54381942749, 'total_duration': 10626.06962442398, 'accumulated_submission_time': 5918.54381942749, 'accumulated_eval_time': 4706.8753480911255, 'accumulated_logging_time': 0.18241620063781738}
I0326 05:16:30.184693 140088108349184 logging_writer.py:48] [16480] accumulated_eval_time=4706.875348, accumulated_logging_time=0.182416, accumulated_submission_time=5918.543819, global_step=16480, preemption_count=0, score=5918.543819, test/accuracy=0.619429, test/bleu=24.090562, test/loss=1.958896, test/num_examples=3003, total_duration=10626.069624, train/accuracy=0.602559, train/bleu=29.180860, train/loss=2.092845, validation/accuracy=0.613842, validation/bleu=24.983180, validation/loss=1.997378, validation/num_examples=3000
I0326 05:16:37.659235 140088116741888 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.3276166617870331, loss=2.2202060222625732
I0326 05:19:35.746373 140088108349184 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.3282955288887024, loss=2.3443567752838135
I0326 05:22:34.279685 140088116741888 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.3157893121242523, loss=2.1899545192718506
I0326 05:25:32.772623 140088108349184 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.3055184781551361, loss=2.1258771419525146
I0326 05:28:31.226492 140088116741888 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.30846908688545227, loss=2.2367212772369385
I0326 05:30:30.177721 140257815279424 spec.py:321] Evaluating on the training split.
I0326 05:30:33.159398 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 05:33:04.496258 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 05:33:07.183298 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 05:35:25.751655 140257815279424 spec.py:349] Evaluating on the test split.
I0326 05:35:28.432426 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 05:37:36.168646 140257815279424 submission_runner.py:420] Time since start: 11892.07s, 	Step: 18835, 	{'train/accuracy': 0.619259238243103, 'train/loss': 1.9477331638336182, 'train/bleu': 30.11433571329848, 'validation/accuracy': 0.6248155832290649, 'validation/loss': 1.9138104915618896, 'validation/bleu': 25.60347878280189, 'validation/num_examples': 3000, 'test/accuracy': 0.6287025809288025, 'test/loss': 1.877466082572937, 'test/bleu': 24.52614101977529, 'test/num_examples': 3003, 'score': 6758.460902690887, 'total_duration': 11892.069929361343, 'accumulated_submission_time': 6758.460902690887, 'accumulated_eval_time': 5132.866238355637, 'accumulated_logging_time': 0.20859932899475098}
I0326 05:37:36.184302 140088108349184 logging_writer.py:48] [18835] accumulated_eval_time=5132.866238, accumulated_logging_time=0.208599, accumulated_submission_time=6758.460903, global_step=18835, preemption_count=0, score=6758.460903, test/accuracy=0.628703, test/bleu=24.526141, test/loss=1.877466, test/num_examples=3003, total_duration=11892.069929, train/accuracy=0.619259, train/bleu=30.114336, train/loss=1.947733, validation/accuracy=0.624816, validation/bleu=25.603479, validation/loss=1.913810, validation/num_examples=3000
I0326 05:38:35.205309 140088116741888 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.29804810881614685, loss=2.0558385848999023
I0326 05:41:33.670651 140088108349184 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.30052509903907776, loss=2.0819947719573975
I0326 05:44:32.173032 140088116741888 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.2790580093860626, loss=2.1009371280670166
I0326 05:47:30.695926 140088108349184 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.2773355543613434, loss=2.0916635990142822
I0326 05:50:29.298279 140088116741888 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.2718096673488617, loss=2.010159730911255
I0326 05:51:36.486661 140257815279424 spec.py:321] Evaluating on the training split.
I0326 05:51:39.479329 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 05:54:05.755324 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 05:54:08.453747 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 05:56:30.676377 140257815279424 spec.py:349] Evaluating on the test split.
I0326 05:56:33.370200 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 05:58:42.284777 140257815279424 submission_runner.py:420] Time since start: 13158.19s, 	Step: 21190, 	{'train/accuracy': 0.6186280846595764, 'train/loss': 1.9614083766937256, 'train/bleu': 29.658331311601806, 'validation/accuracy': 0.6329493522644043, 'validation/loss': 1.8543436527252197, 'validation/bleu': 26.341737414393798, 'validation/num_examples': 3000, 'test/accuracy': 0.6369182467460632, 'test/loss': 1.8068076372146606, 'test/bleu': 25.263544044158902, 'test/num_examples': 3003, 'score': 7598.609153270721, 'total_duration': 13158.18603682518, 'accumulated_submission_time': 7598.609153270721, 'accumulated_eval_time': 5558.664308309555, 'accumulated_logging_time': 0.31125497817993164}
I0326 05:58:42.301998 140088108349184 logging_writer.py:48] [21190] accumulated_eval_time=5558.664308, accumulated_logging_time=0.311255, accumulated_submission_time=7598.609153, global_step=21190, preemption_count=0, score=7598.609153, test/accuracy=0.636918, test/bleu=25.263544, test/loss=1.806808, test/num_examples=3003, total_duration=13158.186037, train/accuracy=0.618628, train/bleu=29.658331, train/loss=1.961408, validation/accuracy=0.632949, validation/bleu=26.341737, validation/loss=1.854344, validation/num_examples=3000
I0326 06:00:32.981710 140088116741888 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.2628001272678375, loss=2.0251641273498535
I0326 06:03:31.499502 140088108349184 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.2708604335784912, loss=2.1080527305603027
I0326 06:06:30.031871 140088116741888 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.2578728199005127, loss=2.0026772022247314
I0326 06:09:28.527819 140088108349184 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.2660975754261017, loss=2.1045594215393066
I0326 06:12:27.096712 140088116741888 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.2634488046169281, loss=2.0028181076049805
I0326 06:12:42.513409 140257815279424 spec.py:321] Evaluating on the training split.
I0326 06:12:45.501676 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 06:15:19.226449 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 06:15:21.915557 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 06:17:40.831283 140257815279424 spec.py:349] Evaluating on the test split.
I0326 06:17:43.521816 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 06:19:51.049012 140257815279424 submission_runner.py:420] Time since start: 14426.95s, 	Step: 23545, 	{'train/accuracy': 0.6223182678222656, 'train/loss': 1.9257683753967285, 'train/bleu': 30.742059609731815, 'validation/accuracy': 0.6388389468193054, 'validation/loss': 1.8090355396270752, 'validation/bleu': 26.658240310922874, 'validation/num_examples': 3000, 'test/accuracy': 0.6459706425666809, 'test/loss': 1.7570536136627197, 'test/bleu': 25.690809610209783, 'test/num_examples': 3003, 'score': 8438.74411201477, 'total_duration': 14426.950294733047, 'accumulated_submission_time': 8438.74411201477, 'accumulated_eval_time': 5987.1998591423035, 'accumulated_logging_time': 0.33873629570007324}
I0326 06:19:51.064942 140088108349184 logging_writer.py:48] [23545] accumulated_eval_time=5987.199859, accumulated_logging_time=0.338736, accumulated_submission_time=8438.744112, global_step=23545, preemption_count=0, score=8438.744112, test/accuracy=0.645971, test/bleu=25.690810, test/loss=1.757054, test/num_examples=3003, total_duration=14426.950295, train/accuracy=0.622318, train/bleu=30.742060, train/loss=1.925768, validation/accuracy=0.638839, validation/bleu=26.658240, validation/loss=1.809036, validation/num_examples=3000
I0326 06:22:33.466594 140088116741888 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.25265032052993774, loss=1.9996869564056396
I0326 06:25:31.897720 140088108349184 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.24761179089546204, loss=1.9731422662734985
I0326 06:28:30.371932 140088116741888 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.24946610629558563, loss=1.9470454454421997
I0326 06:31:28.867564 140088108349184 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.24153123795986176, loss=1.9208515882492065
I0326 06:33:51.404756 140257815279424 spec.py:321] Evaluating on the training split.
I0326 06:33:54.394003 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 06:36:20.608826 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 06:36:23.298922 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 06:38:42.398673 140257815279424 spec.py:349] Evaluating on the test split.
I0326 06:38:45.092746 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 06:40:54.600570 140257815279424 submission_runner.py:420] Time since start: 15690.50s, 	Step: 25901, 	{'train/accuracy': 0.6367673277854919, 'train/loss': 1.8128650188446045, 'train/bleu': 30.562781327433708, 'validation/accuracy': 0.6427942514419556, 'validation/loss': 1.7707115411758423, 'validation/bleu': 27.095397238687713, 'validation/num_examples': 3000, 'test/accuracy': 0.6497588753700256, 'test/loss': 1.708775520324707, 'test/bleu': 26.06388113648153, 'test/num_examples': 3003, 'score': 9279.006174087524, 'total_duration': 15690.501833438873, 'accumulated_submission_time': 9279.006174087524, 'accumulated_eval_time': 6410.395622491837, 'accumulated_logging_time': 0.3654816150665283}
I0326 06:40:54.616929 140088116741888 logging_writer.py:48] [25901] accumulated_eval_time=6410.395622, accumulated_logging_time=0.365482, accumulated_submission_time=9279.006174, global_step=25901, preemption_count=0, score=9279.006174, test/accuracy=0.649759, test/bleu=26.063881, test/loss=1.708776, test/num_examples=3003, total_duration=15690.501833, train/accuracy=0.636767, train/bleu=30.562781, train/loss=1.812865, validation/accuracy=0.642794, validation/bleu=27.095397, validation/loss=1.770712, validation/num_examples=3000
I0326 06:41:30.157626 140088108349184 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.2334764152765274, loss=1.8777610063552856
I0326 06:44:28.475202 140088116741888 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.24416956305503845, loss=1.9543358087539673
I0326 06:47:27.016220 140088108349184 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.2376164197921753, loss=1.8973373174667358
I0326 06:50:25.572194 140088116741888 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.24171920120716095, loss=2.023101329803467
I0326 06:53:24.224134 140088108349184 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.237216979265213, loss=1.9393635988235474
I0326 06:54:54.633452 140257815279424 spec.py:321] Evaluating on the training split.
I0326 06:54:57.617915 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 06:57:30.979957 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 06:57:33.665191 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 06:59:50.768200 140257815279424 spec.py:349] Evaluating on the test split.
I0326 06:59:53.452695 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 07:01:59.691364 140257815279424 submission_runner.py:420] Time since start: 16955.59s, 	Step: 28255, 	{'train/accuracy': 0.6323233246803284, 'train/loss': 1.8486928939819336, 'train/bleu': 31.546094148874186, 'validation/accuracy': 0.647034764289856, 'validation/loss': 1.738301396369934, 'validation/bleu': 27.224801526934705, 'validation/num_examples': 3000, 'test/accuracy': 0.6549416184425354, 'test/loss': 1.672187328338623, 'test/bleu': 26.580225343011396, 'test/num_examples': 3003, 'score': 10118.944992303848, 'total_duration': 16955.592629671097, 'accumulated_submission_time': 10118.944992303848, 'accumulated_eval_time': 6835.453477859497, 'accumulated_logging_time': 0.3930039405822754}
I0326 07:01:59.707911 140088116741888 logging_writer.py:48] [28255] accumulated_eval_time=6835.453478, accumulated_logging_time=0.393004, accumulated_submission_time=10118.944992, global_step=28255, preemption_count=0, score=10118.944992, test/accuracy=0.654942, test/bleu=26.580225, test/loss=1.672187, test/num_examples=3003, total_duration=16955.592630, train/accuracy=0.632323, train/bleu=31.546094, train/loss=1.848693, validation/accuracy=0.647035, validation/bleu=27.224802, validation/loss=1.738301, validation/num_examples=3000
I0326 07:03:27.175575 140088108349184 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.23010683059692383, loss=1.945885419845581
I0326 07:06:25.607647 140088116741888 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.23396478593349457, loss=1.8585096597671509
I0326 07:09:24.052163 140088108349184 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.2374383956193924, loss=1.9354690313339233
I0326 07:12:22.544737 140088116741888 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.23880086839199066, loss=1.9573675394058228
I0326 07:15:21.054670 140088108349184 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.2297813892364502, loss=1.8927479982376099
I0326 07:16:00.031884 140257815279424 spec.py:321] Evaluating on the training split.
I0326 07:16:03.008874 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 07:18:35.013257 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 07:18:37.701999 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 07:21:04.477498 140257815279424 spec.py:349] Evaluating on the test split.
I0326 07:21:07.175535 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 07:23:15.669155 140257815279424 submission_runner.py:420] Time since start: 18231.57s, 	Step: 30611, 	{'train/accuracy': 0.6344592571258545, 'train/loss': 1.8246880769729614, 'train/bleu': 31.079205482261536, 'validation/accuracy': 0.6497873663902283, 'validation/loss': 1.7120106220245361, 'validation/bleu': 27.56982375236608, 'validation/num_examples': 3000, 'test/accuracy': 0.6579629182815552, 'test/loss': 1.6442877054214478, 'test/bleu': 26.68151550804582, 'test/num_examples': 3003, 'score': 10959.192777395248, 'total_duration': 18231.570442676544, 'accumulated_submission_time': 10959.192777395248, 'accumulated_eval_time': 7271.090705394745, 'accumulated_logging_time': 0.4188506603240967}
I0326 07:23:15.685374 140088116741888 logging_writer.py:48] [30611] accumulated_eval_time=7271.090705, accumulated_logging_time=0.418851, accumulated_submission_time=10959.192777, global_step=30611, preemption_count=0, score=10959.192777, test/accuracy=0.657963, test/bleu=26.681516, test/loss=1.644288, test/num_examples=3003, total_duration=18231.570443, train/accuracy=0.634459, train/bleu=31.079205, train/loss=1.824688, validation/accuracy=0.649787, validation/bleu=27.569824, validation/loss=1.712011, validation/num_examples=3000
I0326 07:25:34.453242 140088108349184 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.21777909994125366, loss=1.8018457889556885
I0326 07:28:32.889041 140088116741888 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.22034761309623718, loss=1.8160998821258545
I0326 07:31:31.355313 140088108349184 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.2238846868276596, loss=1.9003249406814575
I0326 07:34:29.780046 140088116741888 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.21977518498897552, loss=1.8850802183151245
I0326 07:37:15.897800 140257815279424 spec.py:321] Evaluating on the training split.
I0326 07:37:18.887968 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 07:39:51.453709 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 07:39:54.138587 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 07:42:11.174691 140257815279424 spec.py:349] Evaluating on the test split.
I0326 07:42:13.857506 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 07:44:23.203663 140257815279424 submission_runner.py:420] Time since start: 19499.10s, 	Step: 32967, 	{'train/accuracy': 0.641680121421814, 'train/loss': 1.7624274492263794, 'train/bleu': 31.68453021303038, 'validation/accuracy': 0.654573380947113, 'validation/loss': 1.6891130208969116, 'validation/bleu': 28.027298562378714, 'validation/num_examples': 3000, 'test/accuracy': 0.6622392535209656, 'test/loss': 1.6180312633514404, 'test/bleu': 26.869818633674655, 'test/num_examples': 3003, 'score': 11799.329007387161, 'total_duration': 19499.104923725128, 'accumulated_submission_time': 11799.329007387161, 'accumulated_eval_time': 7698.396513223648, 'accumulated_logging_time': 0.4439373016357422}
I0326 07:44:23.220723 140088108349184 logging_writer.py:48] [32967] accumulated_eval_time=7698.396513, accumulated_logging_time=0.443937, accumulated_submission_time=11799.329007, global_step=32967, preemption_count=0, score=11799.329007, test/accuracy=0.662239, test/bleu=26.869819, test/loss=1.618031, test/num_examples=3003, total_duration=19499.104924, train/accuracy=0.641680, train/bleu=31.684530, train/loss=1.762427, validation/accuracy=0.654573, validation/bleu=28.027299, validation/loss=1.689113, validation/num_examples=3000
I0326 07:44:35.308894 140088116741888 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.22162756323814392, loss=1.8585944175720215
I0326 07:47:33.528148 140088108349184 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.21872691810131073, loss=1.7747886180877686
I0326 07:50:31.980440 140088116741888 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.229686439037323, loss=1.8741419315338135
I0326 07:53:30.411679 140088108349184 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.21802332997322083, loss=1.8952189683914185
I0326 07:56:28.921435 140088116741888 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.22160309553146362, loss=1.8204467296600342
I0326 07:58:23.222593 140257815279424 spec.py:321] Evaluating on the training split.
I0326 07:58:26.208305 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 08:00:48.253622 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 08:00:50.942695 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 08:03:06.907466 140257815279424 spec.py:349] Evaluating on the test split.
I0326 08:03:09.600287 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 08:05:18.743478 140257815279424 submission_runner.py:420] Time since start: 20754.64s, 	Step: 35322, 	{'train/accuracy': 0.6431238651275635, 'train/loss': 1.7652592658996582, 'train/bleu': 31.699655486503328, 'validation/accuracy': 0.6557885408401489, 'validation/loss': 1.6659809350967407, 'validation/bleu': 28.232595861105633, 'validation/num_examples': 3000, 'test/accuracy': 0.6649119853973389, 'test/loss': 1.592592716217041, 'test/bleu': 27.33828963506321, 'test/num_examples': 3003, 'score': 12639.25471663475, 'total_duration': 20754.64476943016, 'accumulated_submission_time': 12639.25471663475, 'accumulated_eval_time': 8113.917382717133, 'accumulated_logging_time': 0.470017671585083}
I0326 08:05:18.759911 140088108349184 logging_writer.py:48] [35322] accumulated_eval_time=8113.917383, accumulated_logging_time=0.470018, accumulated_submission_time=12639.254717, global_step=35322, preemption_count=0, score=12639.254717, test/accuracy=0.664912, test/bleu=27.338290, test/loss=1.592593, test/num_examples=3003, total_duration=20754.644769, train/accuracy=0.643124, train/bleu=31.699655, train/loss=1.765259, validation/accuracy=0.655789, validation/bleu=28.232596, validation/loss=1.665981, validation/num_examples=3000
I0326 08:06:22.354584 140088116741888 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.2213469296693802, loss=1.8953089714050293
I0326 08:09:20.831292 140088108349184 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.22025412321090698, loss=1.8339952230453491
I0326 08:12:19.369281 140088116741888 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.21907301247119904, loss=1.813657522201538
I0326 08:15:17.959062 140088108349184 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.21478518843650818, loss=1.881096363067627
I0326 08:18:16.558119 140088116741888 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.23550908267498016, loss=1.8399611711502075
I0326 08:19:19.090072 140257815279424 spec.py:321] Evaluating on the training split.
I0326 08:19:22.068359 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 08:22:11.949562 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 08:22:14.633203 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 08:24:44.196252 140257815279424 spec.py:349] Evaluating on the test split.
I0326 08:24:46.888652 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 08:26:56.427406 140257815279424 submission_runner.py:420] Time since start: 22052.33s, 	Step: 37677, 	{'train/accuracy': 0.6570726037025452, 'train/loss': 1.6629935503005981, 'train/bleu': 32.47941990390406, 'validation/accuracy': 0.6585162878036499, 'validation/loss': 1.6495169401168823, 'validation/bleu': 28.026113329746046, 'validation/num_examples': 3000, 'test/accuracy': 0.6668990850448608, 'test/loss': 1.5774426460266113, 'test/bleu': 27.161529678556956, 'test/num_examples': 3003, 'score': 13479.5070374012, 'total_duration': 22052.328681230545, 'accumulated_submission_time': 13479.5070374012, 'accumulated_eval_time': 8571.254666090012, 'accumulated_logging_time': 0.49652886390686035}
I0326 08:26:56.443421 140088108349184 logging_writer.py:48] [37677] accumulated_eval_time=8571.254666, accumulated_logging_time=0.496529, accumulated_submission_time=13479.507037, global_step=37677, preemption_count=0, score=13479.507037, test/accuracy=0.666899, test/bleu=27.161530, test/loss=1.577443, test/num_examples=3003, total_duration=22052.328681, train/accuracy=0.657073, train/bleu=32.479420, train/loss=1.662994, validation/accuracy=0.658516, validation/bleu=28.026113, validation/loss=1.649517, validation/num_examples=3000
I0326 08:28:51.675581 140088116741888 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.2213176190853119, loss=1.893908143043518
I0326 08:31:50.154536 140088108349184 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.22221535444259644, loss=1.9435372352600098
I0326 08:34:48.597321 140088116741888 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.22868098318576813, loss=1.8377765417099
I0326 08:37:47.047262 140088108349184 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.21073803305625916, loss=1.79573655128479
I0326 08:40:45.505129 140088116741888 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.22029553353786469, loss=1.8106948137283325
I0326 08:40:56.646948 140257815279424 spec.py:321] Evaluating on the training split.
I0326 08:40:59.637884 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 08:43:52.542310 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 08:43:55.229672 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 08:46:16.945832 140257815279424 spec.py:349] Evaluating on the test split.
I0326 08:46:19.639989 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 08:48:28.865438 140257815279424 submission_runner.py:420] Time since start: 23344.77s, 	Step: 40033, 	{'train/accuracy': 0.6479796171188354, 'train/loss': 1.7301517724990845, 'train/bleu': 32.19388530891541, 'validation/accuracy': 0.6622112393379211, 'validation/loss': 1.6316739320755005, 'validation/bleu': 28.438767871809628, 'validation/num_examples': 3000, 'test/accuracy': 0.6702225208282471, 'test/loss': 1.557000994682312, 'test/bleu': 27.443725232479164, 'test/num_examples': 3003, 'score': 14319.633382797241, 'total_duration': 23344.766710281372, 'accumulated_submission_time': 14319.633382797241, 'accumulated_eval_time': 9023.473095655441, 'accumulated_logging_time': 0.5230491161346436}
I0326 08:48:28.882153 140088108349184 logging_writer.py:48] [40033] accumulated_eval_time=9023.473096, accumulated_logging_time=0.523049, accumulated_submission_time=14319.633383, global_step=40033, preemption_count=0, score=14319.633383, test/accuracy=0.670223, test/bleu=27.443725, test/loss=1.557001, test/num_examples=3003, total_duration=23344.766710, train/accuracy=0.647980, train/bleu=32.193885, train/loss=1.730152, validation/accuracy=0.662211, validation/bleu=28.438768, validation/loss=1.631674, validation/num_examples=3000
I0326 08:51:15.486251 140088116741888 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.2128906548023224, loss=1.7898856401443481
I0326 08:54:13.875209 140088108349184 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.21633534133434296, loss=1.7562860250473022
I0326 08:57:12.330221 140088116741888 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.2187625616788864, loss=1.7802813053131104
I0326 09:00:10.778104 140088108349184 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.21726854145526886, loss=1.7529183626174927
I0326 09:02:28.964092 140257815279424 spec.py:321] Evaluating on the training split.
I0326 09:02:31.948865 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 09:05:01.205540 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 09:05:03.896321 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 09:07:27.947812 140257815279424 spec.py:349] Evaluating on the test split.
I0326 09:07:30.639626 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 09:09:33.142188 140257815279424 submission_runner.py:420] Time since start: 24609.04s, 	Step: 42389, 	{'train/accuracy': 0.6494043469429016, 'train/loss': 1.7194828987121582, 'train/bleu': 32.17564201220499, 'validation/accuracy': 0.6631287932395935, 'validation/loss': 1.622020959854126, 'validation/bleu': 28.502564160053694, 'validation/num_examples': 3000, 'test/accuracy': 0.6725814938545227, 'test/loss': 1.54263174533844, 'test/bleu': 27.770569045573712, 'test/num_examples': 3003, 'score': 15159.63986825943, 'total_duration': 24609.043479204178, 'accumulated_submission_time': 15159.63986825943, 'accumulated_eval_time': 9447.651156663895, 'accumulated_logging_time': 0.5486812591552734}
I0326 09:09:33.159126 140088116741888 logging_writer.py:48] [42389] accumulated_eval_time=9447.651157, accumulated_logging_time=0.548681, accumulated_submission_time=15159.639868, global_step=42389, preemption_count=0, score=15159.639868, test/accuracy=0.672581, test/bleu=27.770569, test/loss=1.542632, test/num_examples=3003, total_duration=24609.043479, train/accuracy=0.649404, train/bleu=32.175642, train/loss=1.719483, validation/accuracy=0.663129, validation/bleu=28.502564, validation/loss=1.622021, validation/num_examples=3000
I0326 09:10:12.919665 140088108349184 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.21454788744449615, loss=1.770171046257019
I0326 09:13:11.171221 140088116741888 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.21756237745285034, loss=1.807763934135437
I0326 09:16:09.706718 140088108349184 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.21628020703792572, loss=1.840427041053772
I0326 09:19:08.232500 140088116741888 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.2109081894159317, loss=1.8447984457015991
I0326 09:22:06.642480 140088108349184 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.21897350251674652, loss=1.8945866823196411
I0326 09:23:33.402600 140257815279424 spec.py:321] Evaluating on the training split.
I0326 09:23:36.389440 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 09:26:31.208700 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 09:26:33.880647 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 09:29:25.958325 140257815279424 spec.py:349] Evaluating on the test split.
I0326 09:29:28.643393 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 09:31:40.738769 140257815279424 submission_runner.py:420] Time since start: 25936.64s, 	Step: 44745, 	{'train/accuracy': 0.657556414604187, 'train/loss': 1.6528197526931763, 'train/bleu': 32.68821486326358, 'validation/accuracy': 0.6641207337379456, 'validation/loss': 1.6081554889678955, 'validation/bleu': 28.567016995392713, 'validation/num_examples': 3000, 'test/accuracy': 0.674487292766571, 'test/loss': 1.5296008586883545, 'test/bleu': 27.91581044042552, 'test/num_examples': 3003, 'score': 15999.807573318481, 'total_duration': 25936.64005255699, 'accumulated_submission_time': 15999.807573318481, 'accumulated_eval_time': 9934.987287282944, 'accumulated_logging_time': 0.5746548175811768}
I0326 09:31:40.756227 140088116741888 logging_writer.py:48] [44745] accumulated_eval_time=9934.987287, accumulated_logging_time=0.574655, accumulated_submission_time=15999.807573, global_step=44745, preemption_count=0, score=15999.807573, test/accuracy=0.674487, test/bleu=27.915810, test/loss=1.529601, test/num_examples=3003, total_duration=25936.640053, train/accuracy=0.657556, train/bleu=32.688215, train/loss=1.652820, validation/accuracy=0.664121, validation/bleu=28.567017, validation/loss=1.608155, validation/num_examples=3000
I0326 09:33:11.760752 140088108349184 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.2233269214630127, loss=1.7706630229949951
I0326 09:36:10.227569 140088116741888 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.22131888568401337, loss=1.7801614999771118
I0326 09:39:08.702422 140088108349184 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.21290095150470734, loss=1.775234580039978
I0326 09:42:07.149579 140088116741888 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.23743325471878052, loss=1.8083748817443848
I0326 09:45:05.595453 140088108349184 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.2092384546995163, loss=1.694362998008728
I0326 09:45:40.989578 140257815279424 spec.py:321] Evaluating on the training split.
I0326 09:45:43.972034 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 09:48:52.022985 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 09:48:54.705875 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 09:51:11.091376 140257815279424 spec.py:349] Evaluating on the test split.
I0326 09:51:13.788582 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 09:53:19.109145 140257815279424 submission_runner.py:420] Time since start: 27235.01s, 	Step: 47101, 	{'train/accuracy': 0.656990110874176, 'train/loss': 1.6636278629302979, 'train/bleu': 32.22773914432872, 'validation/accuracy': 0.6657077670097351, 'validation/loss': 1.5969038009643555, 'validation/bleu': 28.457516610878002, 'validation/num_examples': 3000, 'test/accuracy': 0.6777293682098389, 'test/loss': 1.5124273300170898, 'test/bleu': 28.254601899911435, 'test/num_examples': 3003, 'score': 16839.96423625946, 'total_duration': 27235.0104265213, 'accumulated_submission_time': 16839.96423625946, 'accumulated_eval_time': 10393.106807947159, 'accumulated_logging_time': 0.6021087169647217}
I0326 09:53:19.125947 140088116741888 logging_writer.py:48] [47101] accumulated_eval_time=10393.106808, accumulated_logging_time=0.602109, accumulated_submission_time=16839.964236, global_step=47101, preemption_count=0, score=16839.964236, test/accuracy=0.677729, test/bleu=28.254602, test/loss=1.512427, test/num_examples=3003, total_duration=27235.010427, train/accuracy=0.656990, train/bleu=32.227739, train/loss=1.663628, validation/accuracy=0.665708, validation/bleu=28.457517, validation/loss=1.596904, validation/num_examples=3000
I0326 09:55:41.491054 140088108349184 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.22038528323173523, loss=1.8331083059310913
I0326 09:58:39.906688 140088116741888 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.21275271475315094, loss=1.7876445055007935
I0326 10:01:38.328387 140088108349184 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.20917338132858276, loss=1.706091046333313
I0326 10:04:36.843494 140088116741888 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.21384082734584808, loss=1.7917156219482422
I0326 10:07:19.277721 140257815279424 spec.py:321] Evaluating on the training split.
I0326 10:07:22.261136 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 10:10:47.093795 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 10:10:49.773422 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 10:13:10.521731 140257815279424 spec.py:349] Evaluating on the test split.
I0326 10:13:13.208609 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 10:15:17.526495 140257815279424 submission_runner.py:420] Time since start: 28553.43s, 	Step: 49457, 	{'train/accuracy': 0.6551543474197388, 'train/loss': 1.664835810661316, 'train/bleu': 32.41724343726786, 'validation/accuracy': 0.6665013432502747, 'validation/loss': 1.585096836090088, 'validation/bleu': 28.82016060908621, 'validation/num_examples': 3000, 'test/accuracy': 0.6782174110412598, 'test/loss': 1.5012739896774292, 'test/bleu': 28.237425596124154, 'test/num_examples': 3003, 'score': 17680.039123773575, 'total_duration': 28553.427763938904, 'accumulated_submission_time': 17680.039123773575, 'accumulated_eval_time': 10871.355544090271, 'accumulated_logging_time': 0.6291701793670654}
I0326 10:15:17.543491 140088108349184 logging_writer.py:48] [49457] accumulated_eval_time=10871.355544, accumulated_logging_time=0.629170, accumulated_submission_time=17680.039124, global_step=49457, preemption_count=0, score=17680.039124, test/accuracy=0.678217, test/bleu=28.237426, test/loss=1.501274, test/num_examples=3003, total_duration=28553.427764, train/accuracy=0.655154, train/bleu=32.417243, train/loss=1.664836, validation/accuracy=0.666501, validation/bleu=28.820161, validation/loss=1.585097, validation/num_examples=3000
I0326 10:15:33.209119 140088116741888 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.2089170664548874, loss=1.781758189201355
I0326 10:18:31.375321 140088108349184 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.21908330917358398, loss=1.8179054260253906
I0326 10:21:29.818160 140088116741888 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.21098025143146515, loss=1.7031099796295166
I0326 10:24:28.308768 140088108349184 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.2134043276309967, loss=1.7589938640594482
I0326 10:27:26.772101 140088116741888 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.2086508870124817, loss=1.6588155031204224
I0326 10:29:17.866650 140257815279424 spec.py:321] Evaluating on the training split.
I0326 10:29:20.848635 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 10:32:25.837335 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 10:32:28.525372 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 10:35:00.148439 140257815279424 spec.py:349] Evaluating on the test split.
I0326 10:35:02.839346 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 10:37:09.154273 140257815279424 submission_runner.py:420] Time since start: 29865.06s, 	Step: 51813, 	{'train/accuracy': 0.6599031686782837, 'train/loss': 1.6309144496917725, 'train/bleu': 32.88231161081743, 'validation/accuracy': 0.6687827706336975, 'validation/loss': 1.573830008506775, 'validation/bleu': 28.834698072084503, 'validation/num_examples': 3000, 'test/accuracy': 0.6807855367660522, 'test/loss': 1.4898260831832886, 'test/bleu': 28.567021618836947, 'test/num_examples': 3003, 'score': 18520.285583257675, 'total_duration': 29865.055556058884, 'accumulated_submission_time': 18520.285583257675, 'accumulated_eval_time': 11342.643130540848, 'accumulated_logging_time': 0.6553468704223633}
I0326 10:37:09.171625 140088108349184 logging_writer.py:48] [51813] accumulated_eval_time=11342.643131, accumulated_logging_time=0.655347, accumulated_submission_time=18520.285583, global_step=51813, preemption_count=0, score=18520.285583, test/accuracy=0.680786, test/bleu=28.567022, test/loss=1.489826, test/num_examples=3003, total_duration=29865.055556, train/accuracy=0.659903, train/bleu=32.882312, train/loss=1.630914, validation/accuracy=0.668783, validation/bleu=28.834698, validation/loss=1.573830, validation/num_examples=3000
I0326 10:38:16.010473 140088116741888 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.21118687093257904, loss=1.6513867378234863
I0326 10:41:14.450267 140088108349184 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.21149763464927673, loss=1.7255644798278809
I0326 10:44:12.886973 140088116741888 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.22040393948554993, loss=1.7192976474761963
I0326 10:47:11.274713 140088108349184 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.21494758129119873, loss=1.7651498317718506
I0326 10:50:09.722710 140088116741888 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.21101151406764984, loss=1.7796275615692139
I0326 10:51:09.420720 140257815279424 spec.py:321] Evaluating on the training split.
I0326 10:51:12.423179 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 10:54:31.995094 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 10:54:34.666833 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 10:57:15.077455 140257815279424 spec.py:349] Evaluating on the test split.
I0326 10:57:17.750534 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 10:59:23.782285 140257815279424 submission_runner.py:420] Time since start: 31199.68s, 	Step: 54169, 	{'train/accuracy': 0.6590301990509033, 'train/loss': 1.636247992515564, 'train/bleu': 32.68305358451159, 'validation/accuracy': 0.6702706813812256, 'validation/loss': 1.5636259317398071, 'validation/bleu': 28.838455664827734, 'validation/num_examples': 3000, 'test/accuracy': 0.6809133887290955, 'test/loss': 1.476148247718811, 'test/bleu': 28.5247227921742, 'test/num_examples': 3003, 'score': 19360.457694768906, 'total_duration': 31199.683564424515, 'accumulated_submission_time': 19360.457694768906, 'accumulated_eval_time': 11837.004646778107, 'accumulated_logging_time': 0.682880163192749}
I0326 10:59:23.800007 140088108349184 logging_writer.py:48] [54169] accumulated_eval_time=11837.004647, accumulated_logging_time=0.682880, accumulated_submission_time=19360.457695, global_step=54169, preemption_count=0, score=19360.457695, test/accuracy=0.680913, test/bleu=28.524723, test/loss=1.476148, test/num_examples=3003, total_duration=31199.683564, train/accuracy=0.659030, train/bleu=32.683054, train/loss=1.636248, validation/accuracy=0.670271, validation/bleu=28.838456, validation/loss=1.563626, validation/num_examples=3000
I0326 11:01:21.894713 140088116741888 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.21439018845558167, loss=1.694654107093811
I0326 11:04:20.216379 140088108349184 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.21744735538959503, loss=1.7613290548324585
I0326 11:07:18.651439 140088116741888 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.21472887694835663, loss=1.6800328493118286
I0326 11:10:17.056904 140088108349184 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.21749211847782135, loss=1.7821546792984009
I0326 11:13:15.444470 140088116741888 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.22181664407253265, loss=1.6798911094665527
I0326 11:13:24.086076 140257815279424 spec.py:321] Evaluating on the training split.
I0326 11:13:27.065561 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 11:16:30.946496 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 11:16:33.626310 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 11:18:54.912677 140257815279424 spec.py:349] Evaluating on the test split.
I0326 11:18:57.599088 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 11:21:00.790303 140257815279424 submission_runner.py:420] Time since start: 32496.69s, 	Step: 56526, 	{'train/accuracy': 0.6727394461631775, 'train/loss': 1.547843337059021, 'train/bleu': 33.704396974588924, 'validation/accuracy': 0.67149817943573, 'validation/loss': 1.556463599205017, 'validation/bleu': 28.940380949052926, 'validation/num_examples': 3000, 'test/accuracy': 0.6832956075668335, 'test/loss': 1.4727809429168701, 'test/bleu': 28.48422535570874, 'test/num_examples': 3003, 'score': 20200.666444778442, 'total_duration': 32496.691576719284, 'accumulated_submission_time': 20200.666444778442, 'accumulated_eval_time': 12293.708816289902, 'accumulated_logging_time': 0.7112369537353516}
I0326 11:21:00.807513 140088108349184 logging_writer.py:48] [56526] accumulated_eval_time=12293.708816, accumulated_logging_time=0.711237, accumulated_submission_time=20200.666445, global_step=56526, preemption_count=0, score=20200.666445, test/accuracy=0.683296, test/bleu=28.484225, test/loss=1.472781, test/num_examples=3003, total_duration=32496.691577, train/accuracy=0.672739, train/bleu=33.704397, train/loss=1.547843, validation/accuracy=0.671498, validation/bleu=28.940381, validation/loss=1.556464, validation/num_examples=3000
I0326 11:23:49.874817 140088116741888 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.2164568454027176, loss=1.6943684816360474
I0326 11:26:48.334982 140088108349184 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.22066961228847504, loss=1.7352824211120605
I0326 11:29:46.696045 140088116741888 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.2180922031402588, loss=1.7001036405563354
I0326 11:32:45.075079 140088108349184 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.21516545116901398, loss=1.6970232725143433
I0326 11:35:01.094766 140257815279424 spec.py:321] Evaluating on the training split.
I0326 11:35:04.082057 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 11:38:32.492851 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 11:38:35.200809 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 11:41:06.208635 140257815279424 spec.py:349] Evaluating on the test split.
I0326 11:41:08.907201 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 11:43:33.620849 140257815279424 submission_runner.py:420] Time since start: 33849.52s, 	Step: 58883, 	{'train/accuracy': 0.6654917597770691, 'train/loss': 1.5997649431228638, 'train/bleu': 33.577646399954844, 'validation/accuracy': 0.6734076142311096, 'validation/loss': 1.5457199811935425, 'validation/bleu': 29.25365327177037, 'validation/num_examples': 3000, 'test/accuracy': 0.6844227910041809, 'test/loss': 1.4603431224822998, 'test/bleu': 28.452297195681055, 'test/num_examples': 3003, 'score': 21040.878185749054, 'total_duration': 33849.52210497856, 'accumulated_submission_time': 21040.878185749054, 'accumulated_eval_time': 12806.234851360321, 'accumulated_logging_time': 0.7371804714202881}
I0326 11:43:33.639030 140088116741888 logging_writer.py:48] [58883] accumulated_eval_time=12806.234851, accumulated_logging_time=0.737180, accumulated_submission_time=21040.878186, global_step=58883, preemption_count=0, score=21040.878186, test/accuracy=0.684423, test/bleu=28.452297, test/loss=1.460343, test/num_examples=3003, total_duration=33849.522105, train/accuracy=0.665492, train/bleu=33.577646, train/loss=1.599765, validation/accuracy=0.673408, validation/bleu=29.253653, validation/loss=1.545720, validation/num_examples=3000
I0326 11:44:15.516531 140088108349184 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.21732179820537567, loss=1.7281923294067383
I0326 11:47:13.695989 140088116741888 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.21932277083396912, loss=1.6666054725646973
I0326 11:50:12.138633 140088108349184 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.2142583280801773, loss=1.6823475360870361
I0326 11:53:10.562393 140088116741888 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.2115403562784195, loss=1.7361383438110352
I0326 11:56:08.959464 140088108349184 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.2149864286184311, loss=1.7154388427734375
I0326 11:57:33.960117 140257815279424 spec.py:321] Evaluating on the training split.
I0326 11:57:36.956724 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 12:01:20.691101 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 12:01:23.373468 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 12:03:50.199590 140257815279424 spec.py:349] Evaluating on the test split.
I0326 12:03:52.907136 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 12:05:59.239051 140257815279424 submission_runner.py:420] Time since start: 35195.14s, 	Step: 61240, 	{'train/accuracy': 0.6613894701004028, 'train/loss': 1.621449589729309, 'train/bleu': 32.875589064704464, 'validation/accuracy': 0.6731472611427307, 'validation/loss': 1.5452971458435059, 'validation/bleu': 29.22795765081323, 'validation/num_examples': 3000, 'test/accuracy': 0.685747504234314, 'test/loss': 1.4529186487197876, 'test/bleu': 28.606576618818305, 'test/num_examples': 3003, 'score': 21881.12133550644, 'total_duration': 35195.140328884125, 'accumulated_submission_time': 21881.12133550644, 'accumulated_eval_time': 13311.513763189316, 'accumulated_logging_time': 0.7662217617034912}
I0326 12:05:59.256748 140088116741888 logging_writer.py:48] [61240] accumulated_eval_time=13311.513763, accumulated_logging_time=0.766222, accumulated_submission_time=21881.121336, global_step=61240, preemption_count=0, score=21881.121336, test/accuracy=0.685748, test/bleu=28.606577, test/loss=1.452919, test/num_examples=3003, total_duration=35195.140329, train/accuracy=0.661389, train/bleu=32.875589, train/loss=1.621450, validation/accuracy=0.673147, validation/bleu=29.227958, validation/loss=1.545297, validation/num_examples=3000
I0326 12:07:32.110120 140088108349184 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.21606957912445068, loss=1.6425222158432007
I0326 12:10:30.468487 140088116741888 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.21722295880317688, loss=1.6613049507141113
I0326 12:13:28.931864 140088108349184 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.21673348546028137, loss=1.7008922100067139
I0326 12:16:27.324760 140088116741888 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.21770484745502472, loss=1.6674546003341675
I0326 12:19:25.735260 140088108349184 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.22619901597499847, loss=1.6627103090286255
I0326 12:19:59.369715 140257815279424 spec.py:321] Evaluating on the training split.
I0326 12:20:02.358952 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 12:23:21.283003 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 12:23:23.964871 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 12:25:40.932689 140257815279424 spec.py:349] Evaluating on the test split.
I0326 12:25:43.620519 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 12:27:41.400702 140257815279424 submission_runner.py:420] Time since start: 36497.30s, 	Step: 63596, 	{'train/accuracy': 0.6703936457633972, 'train/loss': 1.561401128768921, 'train/bleu': 33.186709795215776, 'validation/accuracy': 0.6751186847686768, 'validation/loss': 1.5343824625015259, 'validation/bleu': 29.201814838306824, 'validation/num_examples': 3000, 'test/accuracy': 0.6858869791030884, 'test/loss': 1.448622703552246, 'test/bleu': 28.8951092703082, 'test/num_examples': 3003, 'score': 22721.15820980072, 'total_duration': 36497.30195260048, 'accumulated_submission_time': 22721.15820980072, 'accumulated_eval_time': 13773.54467511177, 'accumulated_logging_time': 0.792844295501709}
I0326 12:27:41.418709 140088116741888 logging_writer.py:48] [63596] accumulated_eval_time=13773.544675, accumulated_logging_time=0.792844, accumulated_submission_time=22721.158210, global_step=63596, preemption_count=0, score=22721.158210, test/accuracy=0.685887, test/bleu=28.895109, test/loss=1.448623, test/num_examples=3003, total_duration=36497.301953, train/accuracy=0.670394, train/bleu=33.186710, train/loss=1.561401, validation/accuracy=0.675119, validation/bleu=29.201815, validation/loss=1.534382, validation/num_examples=3000
I0326 12:30:05.568556 140088108349184 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.22085008025169373, loss=1.7482036352157593
I0326 12:33:03.974102 140088116741888 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.22629909217357635, loss=1.7450426816940308
I0326 12:36:02.432052 140088108349184 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.21417993307113647, loss=1.6081687211990356
I0326 12:39:00.820918 140088116741888 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.21375921368598938, loss=1.610819935798645
I0326 12:41:41.510099 140257815279424 spec.py:321] Evaluating on the training split.
I0326 12:41:44.497463 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 12:45:00.949607 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 12:45:03.640741 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 12:47:18.223161 140257815279424 spec.py:349] Evaluating on the test split.
I0326 12:47:20.919188 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 12:49:18.713914 140257815279424 submission_runner.py:420] Time since start: 37794.62s, 	Step: 65952, 	{'train/accuracy': 0.6658357381820679, 'train/loss': 1.5820873975753784, 'train/bleu': 33.438667065409945, 'validation/accuracy': 0.6747467517852783, 'validation/loss': 1.529623031616211, 'validation/bleu': 29.082334854953462, 'validation/num_examples': 3000, 'test/accuracy': 0.6879205107688904, 'test/loss': 1.4367201328277588, 'test/bleu': 29.19896289156065, 'test/num_examples': 3003, 'score': 23561.172528982162, 'total_duration': 37794.61520290375, 'accumulated_submission_time': 23561.172528982162, 'accumulated_eval_time': 14230.748449802399, 'accumulated_logging_time': 0.8196899890899658}
I0326 12:49:18.731750 140088108349184 logging_writer.py:48] [65952] accumulated_eval_time=14230.748450, accumulated_logging_time=0.819690, accumulated_submission_time=23561.172529, global_step=65952, preemption_count=0, score=23561.172529, test/accuracy=0.687921, test/bleu=29.198963, test/loss=1.436720, test/num_examples=3003, total_duration=37794.615203, train/accuracy=0.665836, train/bleu=33.438667, train/loss=1.582087, validation/accuracy=0.674747, validation/bleu=29.082335, validation/loss=1.529623, validation/num_examples=3000
I0326 12:49:36.170766 140088116741888 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.21892352402210236, loss=1.5922904014587402
I0326 12:52:34.267122 140088108349184 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.23789764940738678, loss=1.6869101524353027
I0326 12:55:32.640213 140088116741888 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.22270208597183228, loss=1.6859471797943115
I0326 12:58:31.024936 140088108349184 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.21956397593021393, loss=1.652917742729187
I0326 13:01:29.469995 140088116741888 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.22464929521083832, loss=1.5725233554840088
I0326 13:03:18.735111 140257815279424 spec.py:321] Evaluating on the training split.
I0326 13:03:21.720253 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 13:06:59.992101 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 13:07:02.673663 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 13:09:24.862251 140257815279424 spec.py:349] Evaluating on the test split.
I0326 13:09:27.563381 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 13:11:28.922523 140257815279424 submission_runner.py:420] Time since start: 39124.82s, 	Step: 68308, 	{'train/accuracy': 0.6674439311027527, 'train/loss': 1.5769660472869873, 'train/bleu': 33.443686855980154, 'validation/accuracy': 0.6772265434265137, 'validation/loss': 1.5254470109939575, 'validation/bleu': 29.456049313556623, 'validation/num_examples': 3000, 'test/accuracy': 0.689175546169281, 'test/loss': 1.4308174848556519, 'test/bleu': 29.34665398865402, 'test/num_examples': 3003, 'score': 24401.09858083725, 'total_duration': 39124.82380437851, 'accumulated_submission_time': 24401.09858083725, 'accumulated_eval_time': 14720.935827493668, 'accumulated_logging_time': 0.8474724292755127}
I0326 13:11:28.940013 140088108349184 logging_writer.py:48] [68308] accumulated_eval_time=14720.935827, accumulated_logging_time=0.847472, accumulated_submission_time=24401.098581, global_step=68308, preemption_count=0, score=24401.098581, test/accuracy=0.689176, test/bleu=29.346654, test/loss=1.430817, test/num_examples=3003, total_duration=39124.823804, train/accuracy=0.667444, train/bleu=33.443687, train/loss=1.576966, validation/accuracy=0.677227, validation/bleu=29.456049, validation/loss=1.525447, validation/num_examples=3000
I0326 13:12:37.546021 140088116741888 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.22389493882656097, loss=1.7003624439239502
I0326 13:15:35.830658 140088108349184 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.2194688767194748, loss=1.6671528816223145
I0326 13:18:34.234335 140088116741888 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.2234192192554474, loss=1.6941518783569336
I0326 13:21:32.630874 140088108349184 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.22176441550254822, loss=1.6877027750015259
I0326 13:24:31.046198 140088116741888 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.21807818114757538, loss=1.581468939781189
I0326 13:25:28.924067 140257815279424 spec.py:321] Evaluating on the training split.
I0326 13:25:31.911252 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 13:28:32.368418 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 13:28:35.057873 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 13:31:09.377928 140257815279424 spec.py:349] Evaluating on the test split.
I0326 13:31:12.081780 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 13:33:18.031338 140257815279424 submission_runner.py:420] Time since start: 40433.93s, 	Step: 70664, 	{'train/accuracy': 0.6694921851158142, 'train/loss': 1.5606787204742432, 'train/bleu': 34.002941847398326, 'validation/accuracy': 0.6776109337806702, 'validation/loss': 1.5192512273788452, 'validation/bleu': 29.4744042274841, 'validation/num_examples': 3000, 'test/accuracy': 0.6896287202835083, 'test/loss': 1.4252281188964844, 'test/bleu': 29.037491151005273, 'test/num_examples': 3003, 'score': 25241.007563829422, 'total_duration': 40433.932607889175, 'accumulated_submission_time': 25241.007563829422, 'accumulated_eval_time': 15190.043044805527, 'accumulated_logging_time': 0.873542070388794}
I0326 13:33:18.049162 140088108349184 logging_writer.py:48] [70664] accumulated_eval_time=15190.043045, accumulated_logging_time=0.873542, accumulated_submission_time=25241.007564, global_step=70664, preemption_count=0, score=25241.007564, test/accuracy=0.689629, test/bleu=29.037491, test/loss=1.425228, test/num_examples=3003, total_duration=40433.932608, train/accuracy=0.669492, train/bleu=34.002942, train/loss=1.560679, validation/accuracy=0.677611, validation/bleu=29.474404, validation/loss=1.519251, validation/num_examples=3000
I0326 13:35:17.903202 140088116741888 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.22235029935836792, loss=1.667216181755066
I0326 13:38:16.328181 140088108349184 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.22622050344944, loss=1.6197879314422607
I0326 13:41:14.783577 140088116741888 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.22580665349960327, loss=1.6536462306976318
I0326 13:44:13.253191 140088108349184 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.22058944404125214, loss=1.5955959558486938
I0326 13:47:11.717867 140088116741888 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.22421154379844666, loss=1.662453532218933
I0326 13:47:18.216567 140257815279424 spec.py:321] Evaluating on the training split.
I0326 13:47:21.196161 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 13:51:16.546774 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 13:51:19.226538 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 13:54:07.690515 140257815279424 spec.py:349] Evaluating on the test split.
I0326 13:54:10.374513 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 13:56:14.956583 140257815279424 submission_runner.py:420] Time since start: 41810.86s, 	Step: 73020, 	{'train/accuracy': 0.6680378913879395, 'train/loss': 1.5732415914535522, 'train/bleu': 33.73546634671619, 'validation/accuracy': 0.6776357293128967, 'validation/loss': 1.5150974988937378, 'validation/bleu': 29.337378188980896, 'validation/num_examples': 3000, 'test/accuracy': 0.690395712852478, 'test/loss': 1.422254204750061, 'test/bleu': 29.0496697371159, 'test/num_examples': 3003, 'score': 26081.099046945572, 'total_duration': 41810.85783743858, 'accumulated_submission_time': 26081.099046945572, 'accumulated_eval_time': 15726.782981872559, 'accumulated_logging_time': 0.9002184867858887}
I0326 13:56:14.974452 140088108349184 logging_writer.py:48] [73020] accumulated_eval_time=15726.782982, accumulated_logging_time=0.900218, accumulated_submission_time=26081.099047, global_step=73020, preemption_count=0, score=26081.099047, test/accuracy=0.690396, test/bleu=29.049670, test/loss=1.422254, test/num_examples=3003, total_duration=41810.857837, train/accuracy=0.668038, train/bleu=33.735466, train/loss=1.573242, validation/accuracy=0.677636, validation/bleu=29.337378, validation/loss=1.515097, validation/num_examples=3000
I0326 13:59:06.287971 140088116741888 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.22416387498378754, loss=1.6836975812911987
I0326 14:02:04.707098 140088108349184 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.2244824916124344, loss=1.6476560831069946
I0326 14:05:03.133636 140088116741888 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.21935948729515076, loss=1.5669161081314087
I0326 14:08:01.541148 140088108349184 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.21989744901657104, loss=1.6279500722885132
I0326 14:10:15.112797 140257815279424 spec.py:321] Evaluating on the training split.
I0326 14:10:18.101167 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 14:13:28.834639 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 14:13:31.520210 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 14:16:00.995009 140257815279424 spec.py:349] Evaluating on the test split.
I0326 14:16:03.695855 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 14:18:14.551000 140257815279424 submission_runner.py:420] Time since start: 43130.45s, 	Step: 75376, 	{'train/accuracy': 0.678062915802002, 'train/loss': 1.5084115266799927, 'train/bleu': 34.37010738887513, 'validation/accuracy': 0.6786896586418152, 'validation/loss': 1.5128967761993408, 'validation/bleu': 29.442304474189754, 'validation/num_examples': 3000, 'test/accuracy': 0.6909418702125549, 'test/loss': 1.4211509227752686, 'test/bleu': 29.253641051429362, 'test/num_examples': 3003, 'score': 26921.161459684372, 'total_duration': 43130.45228648186, 'accumulated_submission_time': 26921.161459684372, 'accumulated_eval_time': 16206.221158504486, 'accumulated_logging_time': 0.9275047779083252}
I0326 14:18:14.568963 140088116741888 logging_writer.py:48] [75376] accumulated_eval_time=16206.221159, accumulated_logging_time=0.927505, accumulated_submission_time=26921.161460, global_step=75376, preemption_count=0, score=26921.161460, test/accuracy=0.690942, test/bleu=29.253641, test/loss=1.421151, test/num_examples=3003, total_duration=43130.452286, train/accuracy=0.678063, train/bleu=34.370107, train/loss=1.508412, validation/accuracy=0.678690, validation/bleu=29.442304, validation/loss=1.512897, validation/num_examples=3000
I0326 14:18:58.952857 140088108349184 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.22931598126888275, loss=1.6948962211608887
I0326 14:21:57.289223 140088116741888 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.23047548532485962, loss=1.694931983947754
I0326 14:24:55.728183 140088108349184 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.23347696661949158, loss=1.6450223922729492
I0326 14:27:54.140786 140088116741888 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.22362591326236725, loss=1.6301237344741821
I0326 14:30:52.571493 140088108349184 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.22931331396102905, loss=1.5890171527862549
I0326 14:32:14.709235 140257815279424 spec.py:321] Evaluating on the training split.
I0326 14:32:17.699430 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 14:35:58.841439 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 14:36:01.533558 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 14:38:42.665685 140257815279424 spec.py:349] Evaluating on the test split.
I0326 14:38:45.362812 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 14:41:02.821929 140257815279424 submission_runner.py:420] Time since start: 44498.72s, 	Step: 77732, 	{'train/accuracy': 0.6716535687446594, 'train/loss': 1.5411198139190674, 'train/bleu': 33.842845915802954, 'validation/accuracy': 0.67948317527771, 'validation/loss': 1.5097118616104126, 'validation/bleu': 29.653856831269366, 'validation/num_examples': 3000, 'test/accuracy': 0.6924176812171936, 'test/loss': 1.4167795181274414, 'test/bleu': 29.199592670309716, 'test/num_examples': 3003, 'score': 27761.225507974625, 'total_duration': 44498.723195552826, 'accumulated_submission_time': 27761.225507974625, 'accumulated_eval_time': 16734.333786725998, 'accumulated_logging_time': 0.9551715850830078}
I0326 14:41:02.840196 140088116741888 logging_writer.py:48] [77732] accumulated_eval_time=16734.333787, accumulated_logging_time=0.955172, accumulated_submission_time=27761.225508, global_step=77732, preemption_count=0, score=27761.225508, test/accuracy=0.692418, test/bleu=29.199593, test/loss=1.416780, test/num_examples=3003, total_duration=44498.723196, train/accuracy=0.671654, train/bleu=33.842846, train/loss=1.541120, validation/accuracy=0.679483, validation/bleu=29.653857, validation/loss=1.509712, validation/num_examples=3000
I0326 14:42:38.535631 140088108349184 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.23175227642059326, loss=1.6481049060821533
I0326 14:45:36.868709 140088116741888 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.23248161375522614, loss=1.623279333114624
I0326 14:48:35.294534 140088108349184 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.2256450653076172, loss=1.63463294506073
I0326 14:51:33.752019 140088116741888 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.23244807124137878, loss=1.6217111349105835
I0326 14:54:32.219578 140088108349184 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.22975656390190125, loss=1.706363320350647
I0326 14:55:02.957694 140257815279424 spec.py:321] Evaluating on the training split.
I0326 14:55:05.940752 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 14:58:43.222477 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 14:58:45.904945 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 15:01:12.688431 140257815279424 spec.py:349] Evaluating on the test split.
I0326 15:01:15.374509 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 15:03:16.155608 140257815279424 submission_runner.py:420] Time since start: 45832.06s, 	Step: 80088, 	{'train/accuracy': 0.6748469471931458, 'train/loss': 1.5335164070129395, 'train/bleu': 33.60797423619726, 'validation/accuracy': 0.6788756251335144, 'validation/loss': 1.5091896057128906, 'validation/bleu': 29.621779752797085, 'validation/num_examples': 3000, 'test/accuracy': 0.6921387910842896, 'test/loss': 1.413880705833435, 'test/bleu': 29.247403076136255, 'test/num_examples': 3003, 'score': 28601.267050027847, 'total_duration': 45832.05685567856, 'accumulated_submission_time': 28601.267050027847, 'accumulated_eval_time': 17227.53161740303, 'accumulated_logging_time': 0.9824159145355225}
I0326 15:03:16.174101 140088116741888 logging_writer.py:48] [80088] accumulated_eval_time=17227.531617, accumulated_logging_time=0.982416, accumulated_submission_time=28601.267050, global_step=80088, preemption_count=0, score=28601.267050, test/accuracy=0.692139, test/bleu=29.247403, test/loss=1.413881, test/num_examples=3003, total_duration=45832.056856, train/accuracy=0.674847, train/bleu=33.607974, train/loss=1.533516, validation/accuracy=0.678876, validation/bleu=29.621780, validation/loss=1.509190, validation/num_examples=3000
I0326 15:05:43.114613 140088108349184 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.22202953696250916, loss=1.57084059715271
I0326 15:08:41.522550 140088116741888 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.22593006491661072, loss=1.6004754304885864
I0326 15:11:39.933428 140088108349184 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.2205958068370819, loss=1.563261866569519
I0326 15:14:38.305685 140088116741888 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.22895874083042145, loss=1.633444905281067
I0326 15:17:16.445914 140257815279424 spec.py:321] Evaluating on the training split.
I0326 15:17:19.428196 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 15:21:02.147278 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 15:21:04.822379 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 15:23:39.293369 140257815279424 spec.py:349] Evaluating on the test split.
I0326 15:23:41.984576 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 15:25:45.444000 140257815279424 submission_runner.py:420] Time since start: 47181.35s, 	Step: 82445, 	{'train/accuracy': 0.6743976473808289, 'train/loss': 1.530717134475708, 'train/bleu': 34.55061700289805, 'validation/accuracy': 0.6797063946723938, 'validation/loss': 1.503297209739685, 'validation/bleu': 29.767430244775294, 'validation/num_examples': 3000, 'test/accuracy': 0.6929754614830017, 'test/loss': 1.4087291955947876, 'test/bleu': 29.301034914418047, 'test/num_examples': 3003, 'score': 29441.463753700256, 'total_duration': 47181.34526705742, 'accumulated_submission_time': 29441.463753700256, 'accumulated_eval_time': 17736.52965450287, 'accumulated_logging_time': 1.009713888168335}
I0326 15:25:45.462868 140088108349184 logging_writer.py:48] [82445] accumulated_eval_time=17736.529655, accumulated_logging_time=1.009714, accumulated_submission_time=29441.463754, global_step=82445, preemption_count=0, score=29441.463754, test/accuracy=0.692975, test/bleu=29.301035, test/loss=1.408729, test/num_examples=3003, total_duration=47181.345267, train/accuracy=0.674398, train/bleu=34.550617, train/loss=1.530717, validation/accuracy=0.679706, validation/bleu=29.767430, validation/loss=1.503297, validation/num_examples=3000
I0326 15:26:05.359312 140088116741888 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.22299335896968842, loss=1.6073848009109497
I0326 15:29:03.535869 140088108349184 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.2303125560283661, loss=1.5914850234985352
I0326 15:32:01.984265 140088116741888 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.22961804270744324, loss=1.591001033782959
I0326 15:35:00.398110 140088108349184 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.23608557879924774, loss=1.7456765174865723
I0326 15:37:58.781984 140088116741888 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.23139168322086334, loss=1.6711982488632202
I0326 15:39:45.515242 140257815279424 spec.py:321] Evaluating on the training split.
I0326 15:39:48.505409 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 15:43:19.619645 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 15:43:22.307761 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 15:45:46.826945 140257815279424 spec.py:349] Evaluating on the test split.
I0326 15:45:49.506822 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 15:47:46.128817 140257815279424 submission_runner.py:420] Time since start: 48502.03s, 	Step: 84801, 	{'train/accuracy': 0.6736806035041809, 'train/loss': 1.5392768383026123, 'train/bleu': 34.373158435533924, 'validation/accuracy': 0.6791608333587646, 'validation/loss': 1.5018540620803833, 'validation/bleu': 29.749724428666255, 'validation/num_examples': 3000, 'test/accuracy': 0.6930218935012817, 'test/loss': 1.4062680006027222, 'test/bleu': 29.30900212133315, 'test/num_examples': 3003, 'score': 30281.44095182419, 'total_duration': 48502.030096530914, 'accumulated_submission_time': 30281.44095182419, 'accumulated_eval_time': 18217.14319062233, 'accumulated_logging_time': 1.0376312732696533}
I0326 15:47:46.147105 140088108349184 logging_writer.py:48] [84801] accumulated_eval_time=18217.143191, accumulated_logging_time=1.037631, accumulated_submission_time=30281.440952, global_step=84801, preemption_count=0, score=30281.440952, test/accuracy=0.693022, test/bleu=29.309002, test/loss=1.406268, test/num_examples=3003, total_duration=48502.030097, train/accuracy=0.673681, train/bleu=34.373158, train/loss=1.539277, validation/accuracy=0.679161, validation/bleu=29.749724, validation/loss=1.501854, validation/num_examples=3000
I0326 15:48:57.221067 140088116741888 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.23187921941280365, loss=1.7117918729782104
I0326 15:51:55.610852 140088108349184 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.2300843447446823, loss=1.6242023706436157
I0326 15:54:54.006732 140088116741888 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.22761940956115723, loss=1.6456561088562012
I0326 15:57:52.333975 140088108349184 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.22376060485839844, loss=1.6141841411590576
I0326 16:00:50.731180 140088116741888 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.22665861248970032, loss=1.5776183605194092
I0326 16:01:46.453178 140257815279424 spec.py:321] Evaluating on the training split.
I0326 16:01:49.437501 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 16:05:31.101046 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 16:05:33.795965 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 16:07:57.840867 140257815279424 spec.py:349] Evaluating on the test split.
I0326 16:08:00.535687 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 16:10:02.518532 140257815279424 submission_runner.py:420] Time since start: 49838.42s, 	Step: 87158, 	{'train/accuracy': 0.6743996143341064, 'train/loss': 1.5298314094543457, 'train/bleu': 34.048574009989096, 'validation/accuracy': 0.6798179745674133, 'validation/loss': 1.4991397857666016, 'validation/bleu': 29.754207538581177, 'validation/num_examples': 3000, 'test/accuracy': 0.6935099959373474, 'test/loss': 1.4048857688903809, 'test/bleu': 29.401569654147707, 'test/num_examples': 3003, 'score': 31121.67044186592, 'total_duration': 49838.419803619385, 'accumulated_submission_time': 31121.67044186592, 'accumulated_eval_time': 18713.20849108696, 'accumulated_logging_time': 1.0662257671356201}
I0326 16:10:02.537344 140088108349184 logging_writer.py:48] [87158] accumulated_eval_time=18713.208491, accumulated_logging_time=1.066226, accumulated_submission_time=31121.670442, global_step=87158, preemption_count=0, score=31121.670442, test/accuracy=0.693510, test/bleu=29.401570, test/loss=1.404886, test/num_examples=3003, total_duration=49838.419804, train/accuracy=0.674400, train/bleu=34.048574, train/loss=1.529831, validation/accuracy=0.679818, validation/bleu=29.754208, validation/loss=1.499140, validation/num_examples=3000
I0326 16:12:04.568897 140088116741888 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.22197572886943817, loss=1.457288146018982
I0326 16:15:02.975777 140088108349184 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.23270723223686218, loss=1.7117615938186646
I0326 16:18:01.449791 140088116741888 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.22493427991867065, loss=1.539222240447998
I0326 16:20:59.882827 140088108349184 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.222564697265625, loss=1.6010116338729858
I0326 16:23:58.375342 140088116741888 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.2299453467130661, loss=1.584486722946167
I0326 16:24:02.738957 140257815279424 spec.py:321] Evaluating on the training split.
I0326 16:24:05.729032 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 16:27:52.789000 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 16:27:55.471302 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 16:30:25.501371 140257815279424 spec.py:349] Evaluating on the test split.
I0326 16:30:28.178456 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 16:32:39.881348 140257815279424 submission_runner.py:420] Time since start: 51195.78s, 	Step: 89514, 	{'train/accuracy': 0.6755579710006714, 'train/loss': 1.5158967971801758, 'train/bleu': 34.047552645039296, 'validation/accuracy': 0.6803139448165894, 'validation/loss': 1.4974576234817505, 'validation/bleu': 29.770309970047116, 'validation/num_examples': 3000, 'test/accuracy': 0.6933937668800354, 'test/loss': 1.404252052307129, 'test/bleu': 29.269887032234116, 'test/num_examples': 3003, 'score': 31961.795548677444, 'total_duration': 51195.782636642456, 'accumulated_submission_time': 31961.795548677444, 'accumulated_eval_time': 19230.350835561752, 'accumulated_logging_time': 1.0953691005706787}
I0326 16:32:39.900144 140088108349184 logging_writer.py:48] [89514] accumulated_eval_time=19230.350836, accumulated_logging_time=1.095369, accumulated_submission_time=31961.795549, global_step=89514, preemption_count=0, score=31961.795549, test/accuracy=0.693394, test/bleu=29.269887, test/loss=1.404252, test/num_examples=3003, total_duration=51195.782637, train/accuracy=0.675558, train/bleu=34.047553, train/loss=1.515897, validation/accuracy=0.680314, validation/bleu=29.770310, validation/loss=1.497458, validation/num_examples=3000
I0326 16:35:33.395606 140088116741888 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.23798054456710815, loss=1.6652566194534302
I0326 16:38:31.823836 140088108349184 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.22714336216449738, loss=1.5544222593307495
I0326 16:41:30.234206 140088116741888 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.22829779982566833, loss=1.6488512754440308
I0326 16:44:28.607700 140088108349184 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.23007185757160187, loss=1.571858525276184
I0326 16:46:39.971982 140257815279424 spec.py:321] Evaluating on the training split.
I0326 16:46:42.958157 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 16:50:23.857009 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 16:50:26.537853 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 16:52:53.782814 140257815279424 spec.py:349] Evaluating on the test split.
I0326 16:52:56.466537 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 16:54:54.762257 140257815279424 submission_runner.py:420] Time since start: 52530.66s, 	Step: 91870, 	{'train/accuracy': 0.6766529083251953, 'train/loss': 1.5187170505523682, 'train/bleu': 34.19179512597903, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4974420070648193, 'validation/bleu': 29.72557204059409, 'validation/num_examples': 3000, 'test/accuracy': 0.6943582892417908, 'test/loss': 1.4023356437683105, 'test/bleu': 29.39102388629618, 'test/num_examples': 3003, 'score': 32801.79234480858, 'total_duration': 52530.66354060173, 'accumulated_submission_time': 32801.79234480858, 'accumulated_eval_time': 19725.14108300209, 'accumulated_logging_time': 1.1229729652404785}
I0326 16:54:54.781402 140088116741888 logging_writer.py:48] [91870] accumulated_eval_time=19725.141083, accumulated_logging_time=1.122973, accumulated_submission_time=32801.792345, global_step=91870, preemption_count=0, score=32801.792345, test/accuracy=0.694358, test/bleu=29.391024, test/loss=1.402336, test/num_examples=3003, total_duration=52530.663541, train/accuracy=0.676653, train/bleu=34.191795, train/loss=1.518717, validation/accuracy=0.680202, validation/bleu=29.725572, validation/loss=1.497442, validation/num_examples=3000
I0326 16:55:41.264072 140088108349184 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.2340003103017807, loss=1.646338701248169
I0326 16:58:39.411079 140088116741888 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.2306266874074936, loss=1.5824047327041626
I0326 17:01:37.779058 140088108349184 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.23583275079727173, loss=1.6404337882995605
I0326 17:04:36.155505 140088116741888 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.21881425380706787, loss=1.5949269533157349
I0326 17:07:34.601061 140088108349184 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.2228102684020996, loss=1.5590569972991943
I0326 17:08:54.941912 140257815279424 spec.py:321] Evaluating on the training split.
I0326 17:08:57.921485 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 17:12:41.374122 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 17:12:44.064445 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 17:15:16.741913 140257815279424 spec.py:349] Evaluating on the test split.
I0326 17:15:19.433229 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 17:17:27.160702 140257815279424 submission_runner.py:420] Time since start: 53883.06s, 	Step: 94227, 	{'train/accuracy': 0.6747400164604187, 'train/loss': 1.523642659187317, 'train/bleu': 34.72360185432612, 'validation/accuracy': 0.6798303723335266, 'validation/loss': 1.496509075164795, 'validation/bleu': 29.861193927437423, 'validation/num_examples': 3000, 'test/accuracy': 0.6942304372787476, 'test/loss': 1.4020249843597412, 'test/bleu': 29.414385113390637, 'test/num_examples': 3003, 'score': 33641.87730574608, 'total_duration': 53883.06198430061, 'accumulated_submission_time': 33641.87730574608, 'accumulated_eval_time': 20237.35983300209, 'accumulated_logging_time': 1.1509370803833008}
I0326 17:17:27.180071 140088116741888 logging_writer.py:48] [94227] accumulated_eval_time=20237.359833, accumulated_logging_time=1.150937, accumulated_submission_time=33641.877306, global_step=94227, preemption_count=0, score=33641.877306, test/accuracy=0.694230, test/bleu=29.414385, test/loss=1.402025, test/num_examples=3003, total_duration=53883.061984, train/accuracy=0.674740, train/bleu=34.723602, train/loss=1.523643, validation/accuracy=0.679830, validation/bleu=29.861194, validation/loss=1.496509, validation/num_examples=3000
I0326 17:19:04.645898 140088108349184 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.23068086802959442, loss=1.5814077854156494
I0326 17:22:03.092036 140088116741888 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.22649671137332916, loss=1.5628563165664673
I0326 17:25:01.587811 140088108349184 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.22948382794857025, loss=1.6327781677246094
I0326 17:28:00.062729 140088116741888 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.22533635795116425, loss=1.5523734092712402
I0326 17:30:58.562651 140088108349184 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.22902263700962067, loss=1.6155105829238892
I0326 17:31:27.209058 140257815279424 spec.py:321] Evaluating on the training split.
I0326 17:31:30.194979 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 17:35:08.700047 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 17:35:11.379295 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 17:37:51.826386 140257815279424 spec.py:349] Evaluating on the test split.
I0326 17:37:54.525205 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 17:39:54.771842 140257815279424 submission_runner.py:420] Time since start: 55230.67s, 	Step: 96582, 	{'train/accuracy': 0.6793270111083984, 'train/loss': 1.5005772113800049, 'train/bleu': 34.57213673817869, 'validation/accuracy': 0.6799667477607727, 'validation/loss': 1.4966213703155518, 'validation/bleu': 29.816350818607244, 'validation/num_examples': 3000, 'test/accuracy': 0.6938701868057251, 'test/loss': 1.4020510911941528, 'test/bleu': 29.379294609935542, 'test/num_examples': 3003, 'score': 34481.83175301552, 'total_duration': 55230.67310833931, 'accumulated_submission_time': 34481.83175301552, 'accumulated_eval_time': 20744.92255783081, 'accumulated_logging_time': 1.1792380809783936}
I0326 17:39:54.790665 140088116741888 logging_writer.py:48] [96582] accumulated_eval_time=20744.922558, accumulated_logging_time=1.179238, accumulated_submission_time=34481.831753, global_step=96582, preemption_count=0, score=34481.831753, test/accuracy=0.693870, test/bleu=29.379295, test/loss=1.402051, test/num_examples=3003, total_duration=55230.673108, train/accuracy=0.679327, train/bleu=34.572137, train/loss=1.500577, validation/accuracy=0.679967, validation/bleu=29.816351, validation/loss=1.496621, validation/num_examples=3000
I0326 17:42:23.957650 140088108349184 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.23275400698184967, loss=1.6281733512878418
I0326 17:45:22.340325 140088116741888 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.22718286514282227, loss=1.510459303855896
I0326 17:48:20.801273 140088108349184 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.22934472560882568, loss=1.5847008228302002
I0326 17:51:19.183912 140088116741888 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.22646266222000122, loss=1.5924839973449707
I0326 17:53:55.112894 140257815279424 spec.py:321] Evaluating on the training split.
I0326 17:53:58.093657 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 17:57:44.396836 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 17:57:47.083451 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 18:00:31.016636 140257815279424 spec.py:349] Evaluating on the test split.
I0326 18:00:33.709335 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 18:02:34.177462 140257815279424 submission_runner.py:420] Time since start: 56590.08s, 	Step: 98939, 	{'train/accuracy': 0.6796087622642517, 'train/loss': 1.49925696849823, 'train/bleu': 34.22295377056319, 'validation/accuracy': 0.6802147626876831, 'validation/loss': 1.496536374092102, 'validation/bleu': 29.79260133975069, 'validation/num_examples': 3000, 'test/accuracy': 0.6941026449203491, 'test/loss': 1.401926875114441, 'test/bleu': 29.40612198300859, 'test/num_examples': 3003, 'score': 35322.07841825485, 'total_duration': 56590.07874560356, 'accumulated_submission_time': 35322.07841825485, 'accumulated_eval_time': 21263.987096071243, 'accumulated_logging_time': 1.2067761421203613}
I0326 18:02:34.197531 140088108349184 logging_writer.py:48] [98939] accumulated_eval_time=21263.987096, accumulated_logging_time=1.206776, accumulated_submission_time=35322.078418, global_step=98939, preemption_count=0, score=35322.078418, test/accuracy=0.694103, test/bleu=29.406122, test/loss=1.401927, test/num_examples=3003, total_duration=56590.078746, train/accuracy=0.679609, train/bleu=34.222954, train/loss=1.499257, validation/accuracy=0.680215, validation/bleu=29.792601, validation/loss=1.496536, validation/num_examples=3000
I0326 18:02:56.227592 140088116741888 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.2276596575975418, loss=1.5667648315429688
I0326 18:05:54.349739 140088108349184 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.232459157705307, loss=1.641715407371521
I0326 18:08:52.755179 140088116741888 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.22590655088424683, loss=1.5023692846298218
I0326 18:11:51.260483 140088108349184 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.23276574909687042, loss=1.617664098739624
I0326 18:14:49.709436 140088116741888 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.23320934176445007, loss=1.633983850479126
I0326 18:16:34.389394 140257815279424 spec.py:321] Evaluating on the training split.
I0326 18:16:37.371459 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 18:20:15.436551 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 18:20:18.125013 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 18:23:00.887800 140257815279424 spec.py:349] Evaluating on the test split.
I0326 18:23:03.586227 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 18:25:04.262522 140257815279424 submission_runner.py:420] Time since start: 57940.16s, 	Step: 101295, 	{'train/accuracy': 0.6781590580940247, 'train/loss': 1.5091168880462646, 'train/bleu': 34.24370911526101, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 36162.191653728485, 'total_duration': 57940.1638109684, 'accumulated_submission_time': 36162.191653728485, 'accumulated_eval_time': 21773.860198259354, 'accumulated_logging_time': 1.2375068664550781}
I0326 18:25:04.282250 140088108349184 logging_writer.py:48] [101295] accumulated_eval_time=21773.860198, accumulated_logging_time=1.237507, accumulated_submission_time=36162.191654, global_step=101295, preemption_count=0, score=36162.191654, test/accuracy=0.694114, test/bleu=29.393372, test/loss=1.401916, test/num_examples=3003, total_duration=57940.163811, train/accuracy=0.678159, train/bleu=34.243709, train/loss=1.509117, validation/accuracy=0.680202, validation/bleu=29.800453, validation/loss=1.496516, validation/num_examples=3000
I0326 18:26:17.499620 140088116741888 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.22959581017494202, loss=1.6229424476623535
I0326 18:29:15.793526 140088108349184 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.22491095960140228, loss=1.5911113023757935
I0326 18:32:14.175169 140088116741888 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.22755999863147736, loss=1.518200159072876
I0326 18:35:12.596684 140088108349184 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.2268940508365631, loss=1.578043818473816
I0326 18:38:11.049149 140088116741888 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.22718530893325806, loss=1.5723143815994263
I0326 18:39:04.280540 140257815279424 spec.py:321] Evaluating on the training split.
I0326 18:39:07.268661 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 18:42:44.047120 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 18:42:46.716987 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 18:45:29.486975 140257815279424 spec.py:349] Evaluating on the test split.
I0326 18:45:32.173246 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 18:47:32.271121 140257815279424 submission_runner.py:420] Time since start: 59288.17s, 	Step: 103651, 	{'train/accuracy': 0.674842894077301, 'train/loss': 1.5288186073303223, 'train/bleu': 34.416922127648505, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 37002.11167097092, 'total_duration': 59288.17239165306, 'accumulated_submission_time': 37002.11167097092, 'accumulated_eval_time': 22281.850743055344, 'accumulated_logging_time': 1.2679455280303955}
I0326 18:47:32.290836 140088108349184 logging_writer.py:48] [103651] accumulated_eval_time=22281.850743, accumulated_logging_time=1.267946, accumulated_submission_time=37002.111671, global_step=103651, preemption_count=0, score=37002.111671, test/accuracy=0.694114, test/bleu=29.393372, test/loss=1.401916, test/num_examples=3003, total_duration=59288.172392, train/accuracy=0.674843, train/bleu=34.416922, train/loss=1.528819, validation/accuracy=0.680202, validation/bleu=29.800453, validation/loss=1.496516, validation/num_examples=3000
I0326 18:49:36.852951 140088116741888 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.22936800122261047, loss=1.6292462348937988
I0326 18:52:35.207325 140088108349184 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.2259232997894287, loss=1.6493791341781616
I0326 18:55:33.529601 140088116741888 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.23031754791736603, loss=1.5990619659423828
I0326 18:58:31.894339 140088108349184 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.2364928126335144, loss=1.6163218021392822
I0326 19:01:30.273456 140088116741888 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.22697196900844574, loss=1.6313416957855225
I0326 19:01:32.489854 140257815279424 spec.py:321] Evaluating on the training split.
I0326 19:01:35.474933 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 19:05:22.104687 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 19:05:24.787708 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 19:08:07.510376 140257815279424 spec.py:349] Evaluating on the test split.
I0326 19:08:10.197108 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 19:10:10.355771 140257815279424 submission_runner.py:420] Time since start: 60646.26s, 	Step: 106008, 	{'train/accuracy': 0.6773278117179871, 'train/loss': 1.5113134384155273, 'train/bleu': 34.371530891826644, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 37842.233411073685, 'total_duration': 60646.25704789162, 'accumulated_submission_time': 37842.233411073685, 'accumulated_eval_time': 22799.716601848602, 'accumulated_logging_time': 1.2975380420684814}
I0326 19:10:10.376300 140088108349184 logging_writer.py:48] [106008] accumulated_eval_time=22799.716602, accumulated_logging_time=1.297538, accumulated_submission_time=37842.233411, global_step=106008, preemption_count=0, score=37842.233411, test/accuracy=0.694114, test/bleu=29.393372, test/loss=1.401916, test/num_examples=3003, total_duration=60646.257048, train/accuracy=0.677328, train/bleu=34.371531, train/loss=1.511313, validation/accuracy=0.680202, validation/bleu=29.800453, validation/loss=1.496516, validation/num_examples=3000
I0326 19:13:05.928456 140088116741888 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.2246718853712082, loss=1.5427497625350952
I0326 19:16:04.341262 140088108349184 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.2295980304479599, loss=1.6635630130767822
I0326 19:19:02.718617 140088116741888 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.23414278030395508, loss=1.561204433441162
I0326 19:22:01.123496 140088108349184 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.24208849668502808, loss=1.7098671197891235
I0326 19:24:10.387209 140257815279424 spec.py:321] Evaluating on the training split.
I0326 19:24:13.372603 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 19:28:13.879556 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 19:28:16.563462 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 19:31:00.151919 140257815279424 spec.py:349] Evaluating on the test split.
I0326 19:31:02.835122 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 19:33:03.639089 140257815279424 submission_runner.py:420] Time since start: 62019.54s, 	Step: 108364, 	{'train/accuracy': 0.6775650382041931, 'train/loss': 1.5132313966751099, 'train/bleu': 34.20403181864933, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 38682.16828632355, 'total_duration': 62019.54036283493, 'accumulated_submission_time': 38682.16828632355, 'accumulated_eval_time': 23332.9684278965, 'accumulated_logging_time': 1.3266677856445312}
I0326 19:33:03.659903 140088116741888 logging_writer.py:48] [108364] accumulated_eval_time=23332.968428, accumulated_logging_time=1.326668, accumulated_submission_time=38682.168286, global_step=108364, preemption_count=0, score=38682.168286, test/accuracy=0.694114, test/bleu=29.393372, test/loss=1.401916, test/num_examples=3003, total_duration=62019.540363, train/accuracy=0.677565, train/bleu=34.204032, train/loss=1.513231, validation/accuracy=0.680202, validation/bleu=29.800453, validation/loss=1.496516, validation/num_examples=3000
I0326 19:33:52.361825 140088108349184 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.22625762224197388, loss=1.6014353036880493
I0326 19:36:50.666933 140088116741888 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.23700885474681854, loss=1.586651086807251
I0326 19:39:48.999036 140088108349184 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.22698360681533813, loss=1.621686339378357
I0326 19:42:47.444154 140088116741888 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.22545947134494781, loss=1.5516833066940308
I0326 19:45:45.897873 140088108349184 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.22778593003749847, loss=1.5784021615982056
I0326 19:47:03.751136 140257815279424 spec.py:321] Evaluating on the training split.
I0326 19:47:06.730283 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 19:50:45.809194 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 19:50:48.489079 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 19:53:32.086513 140257815279424 spec.py:349] Evaluating on the test split.
I0326 19:53:34.783395 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 19:55:34.956524 140257815279424 submission_runner.py:420] Time since start: 63370.86s, 	Step: 110720, 	{'train/accuracy': 0.6771190762519836, 'train/loss': 1.515312671661377, 'train/bleu': 34.13090005895956, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 39522.18356657028, 'total_duration': 63370.85781121254, 'accumulated_submission_time': 39522.18356657028, 'accumulated_eval_time': 23844.173778533936, 'accumulated_logging_time': 1.3562805652618408}
I0326 19:55:34.976402 140088116741888 logging_writer.py:48] [110720] accumulated_eval_time=23844.173779, accumulated_logging_time=1.356281, accumulated_submission_time=39522.183567, global_step=110720, preemption_count=0, score=39522.183567, test/accuracy=0.694114, test/bleu=29.393372, test/loss=1.401916, test/num_examples=3003, total_duration=63370.857811, train/accuracy=0.677119, train/bleu=34.130900, train/loss=1.515313, validation/accuracy=0.680202, validation/bleu=29.800453, validation/loss=1.496516, validation/num_examples=3000
I0326 19:57:14.945931 140088108349184 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.23078002035617828, loss=1.6905134916305542
I0326 20:00:13.464768 140088116741888 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.2233271449804306, loss=1.6337741613388062
I0326 20:03:11.959877 140088108349184 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.2274610847234726, loss=1.5879963636398315
I0326 20:06:10.462625 140088116741888 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.23090824484825134, loss=1.6091173887252808
I0326 20:09:08.962499 140088108349184 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.22604645788669586, loss=1.6069961786270142
I0326 20:09:35.098394 140257815279424 spec.py:321] Evaluating on the training split.
I0326 20:09:38.089167 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 20:13:28.455770 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 20:13:31.136503 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 20:16:13.747463 140257815279424 spec.py:349] Evaluating on the test split.
I0326 20:16:16.449398 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 20:18:16.555100 140257815279424 submission_runner.py:420] Time since start: 64732.46s, 	Step: 113075, 	{'train/accuracy': 0.676160454750061, 'train/loss': 1.5152772665023804, 'train/bleu': 34.18633428085686, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 40362.22967219353, 'total_duration': 64732.45637965202, 'accumulated_submission_time': 40362.22967219353, 'accumulated_eval_time': 24365.630432605743, 'accumulated_logging_time': 1.3847291469573975}
I0326 20:18:16.574953 140088116741888 logging_writer.py:48] [113075] accumulated_eval_time=24365.630433, accumulated_logging_time=1.384729, accumulated_submission_time=40362.229672, global_step=113075, preemption_count=0, score=40362.229672, test/accuracy=0.694114, test/bleu=29.393372, test/loss=1.401916, test/num_examples=3003, total_duration=64732.456380, train/accuracy=0.676160, train/bleu=34.186334, train/loss=1.515277, validation/accuracy=0.680202, validation/bleu=29.800453, validation/loss=1.496516, validation/num_examples=3000
I0326 20:20:48.194509 140088108349184 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.22913652658462524, loss=1.621382236480713
I0326 20:23:46.615931 140088116741888 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.2283177524805069, loss=1.5802178382873535
I0326 20:26:45.072129 140088108349184 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.2265019565820694, loss=1.6442065238952637
I0326 20:29:43.545006 140088116741888 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.22135445475578308, loss=1.5943313837051392
I0326 20:32:16.779766 140257815279424 spec.py:321] Evaluating on the training split.
I0326 20:32:19.763069 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 20:36:11.584592 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 20:36:14.268026 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 20:38:56.960867 140257815279424 spec.py:349] Evaluating on the test split.
I0326 20:38:59.645167 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 20:41:00.484601 140257815279424 submission_runner.py:420] Time since start: 66096.39s, 	Step: 115431, 	{'train/accuracy': 0.6735895872116089, 'train/loss': 1.531890630722046, 'train/bleu': 34.70026898669226, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 41202.35964822769, 'total_duration': 66096.38588237762, 'accumulated_submission_time': 41202.35964822769, 'accumulated_eval_time': 24889.33523273468, 'accumulated_logging_time': 1.4133291244506836}
I0326 20:41:00.504740 140088108349184 logging_writer.py:48] [115431] accumulated_eval_time=24889.335233, accumulated_logging_time=1.413329, accumulated_submission_time=41202.359648, global_step=115431, preemption_count=0, score=41202.359648, test/accuracy=0.694114, test/bleu=29.393372, test/loss=1.401916, test/num_examples=3003, total_duration=66096.385882, train/accuracy=0.673590, train/bleu=34.700269, train/loss=1.531891, validation/accuracy=0.680202, validation/bleu=29.800453, validation/loss=1.496516, validation/num_examples=3000
I0326 20:41:25.400021 140088116741888 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.2308456152677536, loss=1.6096181869506836
I0326 20:44:23.584156 140088108349184 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.23308074474334717, loss=1.6492819786071777
I0326 20:47:21.947106 140088116741888 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.23117370903491974, loss=1.5908552408218384
I0326 20:50:20.514335 140088108349184 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.2353028506040573, loss=1.6404060125350952
I0326 20:53:18.912870 140088116741888 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.22703517973423004, loss=1.5552972555160522
I0326 20:55:00.659688 140257815279424 spec.py:321] Evaluating on the training split.
I0326 20:55:03.644969 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 20:59:10.867464 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 20:59:13.554836 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 21:01:56.248118 140257815279424 spec.py:349] Evaluating on the test split.
I0326 21:01:58.940453 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 21:03:59.061666 140257815279424 submission_runner.py:420] Time since start: 67474.96s, 	Step: 117787, 	{'train/accuracy': 0.6738109588623047, 'train/loss': 1.5302364826202393, 'train/bleu': 34.45853533681923, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 42042.437108039856, 'total_duration': 67474.96292757988, 'accumulated_submission_time': 42042.437108039856, 'accumulated_eval_time': 25427.737149000168, 'accumulated_logging_time': 1.444042682647705}
I0326 21:03:59.081981 140088108349184 logging_writer.py:48] [117787] accumulated_eval_time=25427.737149, accumulated_logging_time=1.444043, accumulated_submission_time=42042.437108, global_step=117787, preemption_count=0, score=42042.437108, test/accuracy=0.694114, test/bleu=29.393372, test/loss=1.401916, test/num_examples=3003, total_duration=67474.962928, train/accuracy=0.673811, train/bleu=34.458535, train/loss=1.530236, validation/accuracy=0.680202, validation/bleu=29.800453, validation/loss=1.496516, validation/num_examples=3000
I0326 21:05:15.236534 140088116741888 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.22633923590183258, loss=1.5915592908859253
I0326 21:08:13.614307 140088108349184 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.22999466955661774, loss=1.6082499027252197
I0326 21:11:12.082820 140088116741888 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.22017349302768707, loss=1.6128450632095337
I0326 21:14:10.558209 140088108349184 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.23150193691253662, loss=1.5925308465957642
I0326 21:17:09.041371 140088116741888 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.22839762270450592, loss=1.6051002740859985
I0326 21:17:59.098273 140257815279424 spec.py:321] Evaluating on the training split.
I0326 21:18:02.083447 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 21:22:03.357120 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 21:22:06.028693 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 21:24:48.736975 140257815279424 spec.py:349] Evaluating on the test split.
I0326 21:24:51.419951 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 21:26:51.534096 140257815279424 submission_runner.py:420] Time since start: 68847.44s, 	Step: 120142, 	{'train/accuracy': 0.6759756207466125, 'train/loss': 1.515751600265503, 'train/bleu': 34.22470415448941, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 42882.377626657486, 'total_duration': 68847.435379982, 'accumulated_submission_time': 42882.377626657486, 'accumulated_eval_time': 25960.172927379608, 'accumulated_logging_time': 1.4731831550598145}
I0326 21:26:51.554136 140088108349184 logging_writer.py:48] [120142] accumulated_eval_time=25960.172927, accumulated_logging_time=1.473183, accumulated_submission_time=42882.377627, global_step=120142, preemption_count=0, score=42882.377627, test/accuracy=0.694114, test/bleu=29.393372, test/loss=1.401916, test/num_examples=3003, total_duration=68847.435380, train/accuracy=0.675976, train/bleu=34.224704, train/loss=1.515752, validation/accuracy=0.680202, validation/bleu=29.800453, validation/loss=1.496516, validation/num_examples=3000
I0326 21:28:59.280650 140088116741888 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.23064763844013214, loss=1.597048044204712
I0326 21:31:57.641194 140088108349184 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.2266179323196411, loss=1.5924311876296997
I0326 21:34:56.029111 140088116741888 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.22752512991428375, loss=1.5780072212219238
I0326 21:37:54.432543 140088108349184 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.23015913367271423, loss=1.5800007581710815
I0326 21:40:51.810549 140257815279424 spec.py:321] Evaluating on the training split.
I0326 21:40:54.800220 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 21:44:47.547004 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 21:44:50.238374 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 21:47:32.885618 140257815279424 spec.py:349] Evaluating on the test split.
I0326 21:47:35.573350 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 21:49:35.749032 140257815279424 submission_runner.py:420] Time since start: 70211.65s, 	Step: 122499, 	{'train/accuracy': 0.6736052632331848, 'train/loss': 1.5344281196594238, 'train/bleu': 34.55278101370412, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 43722.55801129341, 'total_duration': 70211.65032339096, 'accumulated_submission_time': 43722.55801129341, 'accumulated_eval_time': 26484.111390829086, 'accumulated_logging_time': 1.502096176147461}
I0326 21:49:35.769563 140088116741888 logging_writer.py:48] [122499] accumulated_eval_time=26484.111391, accumulated_logging_time=1.502096, accumulated_submission_time=43722.558011, global_step=122499, preemption_count=0, score=43722.558011, test/accuracy=0.694114, test/bleu=29.393372, test/loss=1.401916, test/num_examples=3003, total_duration=70211.650323, train/accuracy=0.673605, train/bleu=34.552781, train/loss=1.534428, validation/accuracy=0.680202, validation/bleu=29.800453, validation/loss=1.496516, validation/num_examples=3000
I0326 21:49:36.501158 140088108349184 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.22532224655151367, loss=1.5682525634765625
I0326 21:52:34.698693 140088116741888 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.23366662859916687, loss=1.6735697984695435
I0326 21:55:33.249688 140088108349184 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.22839614748954773, loss=1.5633620023727417
I0326 21:58:31.813318 140088116741888 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.22730278968811035, loss=1.6098486185073853
I0326 22:01:30.322207 140088108349184 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.23427429795265198, loss=1.6193186044692993
I0326 22:03:36.051508 140257815279424 spec.py:321] Evaluating on the training split.
I0326 22:03:39.044650 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 22:07:33.761383 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 22:07:36.430780 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 22:10:19.105313 140257815279424 spec.py:349] Evaluating on the test split.
I0326 22:10:21.789378 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 22:12:22.197181 140257815279424 submission_runner.py:420] Time since start: 71578.10s, 	Step: 124854, 	{'train/accuracy': 0.6764072775840759, 'train/loss': 1.5210989713668823, 'train/bleu': 34.46103119602367, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 44562.76447367668, 'total_duration': 71578.09846949577, 'accumulated_submission_time': 44562.76447367668, 'accumulated_eval_time': 27010.25706577301, 'accumulated_logging_time': 1.5315184593200684}
I0326 22:12:22.218396 140088116741888 logging_writer.py:48] [124854] accumulated_eval_time=27010.257066, accumulated_logging_time=1.531518, accumulated_submission_time=44562.764474, global_step=124854, preemption_count=0, score=44562.764474, test/accuracy=0.694114, test/bleu=29.393372, test/loss=1.401916, test/num_examples=3003, total_duration=71578.098469, train/accuracy=0.676407, train/bleu=34.461031, train/loss=1.521099, validation/accuracy=0.680202, validation/bleu=29.800453, validation/loss=1.496516, validation/num_examples=3000
I0326 22:13:14.453880 140088108349184 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.2356695830821991, loss=1.6051884889602661
I0326 22:16:12.710100 140088116741888 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.2278449684381485, loss=1.634353518486023
I0326 22:19:11.107466 140088108349184 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.22413134574890137, loss=1.590828537940979
I0326 22:22:09.545956 140088116741888 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.2367161214351654, loss=1.6512378454208374
I0326 22:25:07.962901 140088108349184 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.22496826946735382, loss=1.501599907875061
I0326 22:26:22.300374 140257815279424 spec.py:321] Evaluating on the training split.
I0326 22:26:25.282789 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 22:30:32.335034 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 22:30:35.005590 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 22:33:17.702383 140257815279424 spec.py:349] Evaluating on the test split.
I0326 22:33:20.391217 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 22:35:20.700049 140257815279424 submission_runner.py:420] Time since start: 72956.60s, 	Step: 127210, 	{'train/accuracy': 0.6790623664855957, 'train/loss': 1.5042811632156372, 'train/bleu': 34.40183515742074, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 45402.769220113754, 'total_duration': 72956.60133194923, 'accumulated_submission_time': 45402.769220113754, 'accumulated_eval_time': 27548.65670323372, 'accumulated_logging_time': 1.562415361404419}
I0326 22:35:20.723012 140088116741888 logging_writer.py:48] [127210] accumulated_eval_time=27548.656703, accumulated_logging_time=1.562415, accumulated_submission_time=45402.769220, global_step=127210, preemption_count=0, score=45402.769220, test/accuracy=0.694114, test/bleu=29.393372, test/loss=1.401916, test/num_examples=3003, total_duration=72956.601332, train/accuracy=0.679062, train/bleu=34.401835, train/loss=1.504281, validation/accuracy=0.680202, validation/bleu=29.800453, validation/loss=1.496516, validation/num_examples=3000
I0326 22:37:04.176291 140088108349184 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.23139244318008423, loss=1.5595970153808594
I0326 22:40:02.584990 140088116741888 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.2261771708726883, loss=1.5879194736480713
I0326 22:43:01.123511 140088108349184 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.23205865919589996, loss=1.633662223815918
I0326 22:45:59.597944 140088116741888 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.2311931550502777, loss=1.6095637083053589
I0326 22:48:57.966804 140088108349184 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.2257930040359497, loss=1.6198253631591797
I0326 22:49:20.871967 140257815279424 spec.py:321] Evaluating on the training split.
I0326 22:49:23.856926 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 22:53:12.209192 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 22:53:14.911148 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 22:55:57.704047 140257815279424 spec.py:349] Evaluating on the test split.
I0326 22:56:00.401334 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 22:58:00.450267 140257815279424 submission_runner.py:420] Time since start: 74316.35s, 	Step: 129566, 	{'train/accuracy': 0.677972674369812, 'train/loss': 1.5072195529937744, 'train/bleu': 34.258766307568344, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 46242.84040975571, 'total_duration': 74316.35155272484, 'accumulated_submission_time': 46242.84040975571, 'accumulated_eval_time': 28068.23496031761, 'accumulated_logging_time': 1.595994472503662}
I0326 22:58:00.470921 140088116741888 logging_writer.py:48] [129566] accumulated_eval_time=28068.234960, accumulated_logging_time=1.595994, accumulated_submission_time=46242.840410, global_step=129566, preemption_count=0, score=46242.840410, test/accuracy=0.694114, test/bleu=29.393372, test/loss=1.401916, test/num_examples=3003, total_duration=74316.351553, train/accuracy=0.677973, train/bleu=34.258766, train/loss=1.507220, validation/accuracy=0.680202, validation/bleu=29.800453, validation/loss=1.496516, validation/num_examples=3000
I0326 23:00:35.341347 140088108349184 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.2277946025133133, loss=1.5419341325759888
I0326 23:03:33.816365 140088116741888 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.22557325661182404, loss=1.5993952751159668
I0326 23:06:32.267983 140088108349184 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.22606253623962402, loss=1.5805338621139526
I0326 23:09:30.736031 140088116741888 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.2282228022813797, loss=1.61867356300354
I0326 23:12:00.749111 140257815279424 spec.py:321] Evaluating on the training split.
I0326 23:12:03.733609 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 23:15:48.882363 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 23:15:51.567127 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 23:18:34.265657 140257815279424 spec.py:349] Evaluating on the test split.
I0326 23:18:36.944824 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 23:20:36.901468 140257815279424 submission_runner.py:420] Time since start: 75672.80s, 	Step: 131922, 	{'train/accuracy': 0.6743406057357788, 'train/loss': 1.5279185771942139, 'train/bleu': 34.241705504987344, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 47083.04276394844, 'total_duration': 75672.80275964737, 'accumulated_submission_time': 47083.04276394844, 'accumulated_eval_time': 28584.38727903366, 'accumulated_logging_time': 1.6250958442687988}
I0326 23:20:36.922619 140088108349184 logging_writer.py:48] [131922] accumulated_eval_time=28584.387279, accumulated_logging_time=1.625096, accumulated_submission_time=47083.042764, global_step=131922, preemption_count=0, score=47083.042764, test/accuracy=0.694114, test/bleu=29.393372, test/loss=1.401916, test/num_examples=3003, total_duration=75672.802760, train/accuracy=0.674341, train/bleu=34.241706, train/loss=1.527919, validation/accuracy=0.680202, validation/bleu=29.800453, validation/loss=1.496516, validation/num_examples=3000
I0326 23:21:04.956542 140088116741888 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.2313958704471588, loss=1.5602610111236572
I0326 23:24:03.096804 140088108349184 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.22381475567817688, loss=1.5905234813690186
I0326 23:27:01.474648 140088116741888 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.233027845621109, loss=1.6247555017471313
I0326 23:28:59.695000 140257815279424 spec.py:321] Evaluating on the training split.
I0326 23:29:02.679102 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 23:32:47.169503 140257815279424 spec.py:333] Evaluating on the validation split.
I0326 23:32:49.847080 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 23:35:32.496521 140257815279424 spec.py:349] Evaluating on the test split.
I0326 23:35:35.172226 140257815279424 workload.py:181] Translating evaluation dataset.
I0326 23:37:35.138042 140257815279424 submission_runner.py:420] Time since start: 76691.04s, 	Step: 133333, 	{'train/accuracy': 0.6771786212921143, 'train/loss': 1.514631986618042, 'train/bleu': 34.640933465596675, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 47585.765372276306, 'total_duration': 76691.0393230915, 'accumulated_submission_time': 47585.765372276306, 'accumulated_eval_time': 29099.83028960228, 'accumulated_logging_time': 1.6558685302734375}
I0326 23:37:35.158660 140088108349184 logging_writer.py:48] [133333] accumulated_eval_time=29099.830290, accumulated_logging_time=1.655869, accumulated_submission_time=47585.765372, global_step=133333, preemption_count=0, score=47585.765372, test/accuracy=0.694114, test/bleu=29.393372, test/loss=1.401916, test/num_examples=3003, total_duration=76691.039323, train/accuracy=0.677179, train/bleu=34.640933, train/loss=1.514632, validation/accuracy=0.680202, validation/bleu=29.800453, validation/loss=1.496516, validation/num_examples=3000
I0326 23:37:35.178712 140088116741888 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=47585.765372
I0326 23:37:36.369157 140257815279424 checkpoints.py:490] Saving checkpoint at step: 133333
I0326 23:37:40.830738 140257815279424 checkpoints.py:422] Saved checkpoint at /experiment_runs/variants_target_setting/study_0/wmt_attention_temp_jax/trial_1/checkpoint_133333
I0326 23:37:40.835919 140257815279424 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/variants_target_setting/study_0/wmt_attention_temp_jax/trial_1/checkpoint_133333.
I0326 23:37:40.873063 140257815279424 submission_runner.py:593] Tuning trial 1/1
I0326 23:37:40.873218 140257815279424 submission_runner.py:594] Hyperparameters: Hyperparameters(learning_rate=0.0003477912008450351, beta1=0.9936632117510711, beta2=0.9967873550453692, warmup_steps=9999, weight_decay=0.04120183162940475, label_smoothing=0.0)
I0326 23:37:40.878261 140257815279424 submission_runner.py:595] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006656643818132579, 'train/loss': 11.016271591186523, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 11.048277854919434, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 11.050202369689941, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 37.95417833328247, 'total_duration': 912.7569696903229, 'accumulated_submission_time': 37.95417833328247, 'accumulated_eval_time': 874.8027560710907, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2354, {'train/accuracy': 0.23909994959831238, 'train/loss': 5.794710159301758, 'train/bleu': 2.9965542897400064, 'validation/accuracy': 0.2231590449810028, 'validation/loss': 5.979231357574463, 'validation/bleu': 1.0355133685575952, 'validation/num_examples': 3000, 'test/accuracy': 0.20591482520103455, 'test/loss': 6.264630317687988, 'test/bleu': 0.9448569837909995, 'test/num_examples': 3003, 'score': 878.1650412082672, 'total_duration': 2603.815593481064, 'accumulated_submission_time': 878.1650412082672, 'accumulated_eval_time': 1725.5513274669647, 'accumulated_logging_time': 0.03208470344543457, 'global_step': 2354, 'preemption_count': 0}), (4709, {'train/accuracy': 0.3723996579647064, 'train/loss': 4.19680643081665, 'train/bleu': 10.72794150678018, 'validation/accuracy': 0.3527544438838959, 'validation/loss': 4.391270160675049, 'validation/bleu': 6.638556235953683, 'validation/num_examples': 3000, 'test/accuracy': 0.3339956998825073, 'test/loss': 4.64681339263916, 'test/bleu': 4.894400741660307, 'test/num_examples': 3003, 'score': 1718.3633179664612, 'total_duration': 4175.9186906814575, 'accumulated_submission_time': 1718.3633179664612, 'accumulated_eval_time': 2457.3663341999054, 'accumulated_logging_time': 0.055750370025634766, 'global_step': 4709, 'preemption_count': 0}), (7063, {'train/accuracy': 0.49664491415023804, 'train/loss': 3.081625461578369, 'train/bleu': 20.618799445569337, 'validation/accuracy': 0.4871731102466583, 'validation/loss': 3.160738945007324, 'validation/bleu': 15.866710860775088, 'validation/num_examples': 3000, 'test/accuracy': 0.48168033361434937, 'test/loss': 3.2570042610168457, 'test/bleu': 14.23482273133436, 'test/num_examples': 3003, 'score': 2558.6197278499603, 'total_duration': 5506.827075481415, 'accumulated_submission_time': 2558.6197278499603, 'accumulated_eval_time': 2947.926177740097, 'accumulated_logging_time': 0.08072090148925781, 'global_step': 7063, 'preemption_count': 0}), (9417, {'train/accuracy': 0.542782723903656, 'train/loss': 2.623689889907837, 'train/bleu': 24.78801555610238, 'validation/accuracy': 0.5464160442352295, 'validation/loss': 2.594118595123291, 'validation/bleu': 20.31014701984931, 'validation/num_examples': 3000, 'test/accuracy': 0.546139121055603, 'test/loss': 2.626765251159668, 'test/bleu': 18.94939669460022, 'test/num_examples': 3003, 'score': 3398.745558977127, 'total_duration': 6803.030266284943, 'accumulated_submission_time': 3398.745558977127, 'accumulated_eval_time': 3403.9124541282654, 'accumulated_logging_time': 0.1050713062286377, 'global_step': 9417, 'preemption_count': 0}), (11771, {'train/accuracy': 0.5701367855072021, 'train/loss': 2.373084783554077, 'train/bleu': 26.71833475478846, 'validation/accuracy': 0.5803523659706116, 'validation/loss': 2.2887685298919678, 'validation/bleu': 22.777444797410663, 'validation/num_examples': 3000, 'test/accuracy': 0.5816512703895569, 'test/loss': 2.2855377197265625, 'test/bleu': 21.38527000165073, 'test/num_examples': 3003, 'score': 4238.6881992816925, 'total_duration': 8078.895473003387, 'accumulated_submission_time': 4238.6881992816925, 'accumulated_eval_time': 3839.7428874969482, 'accumulated_logging_time': 0.13013315200805664, 'global_step': 11771, 'preemption_count': 0}), (14125, {'train/accuracy': 0.5916720628738403, 'train/loss': 2.1824207305908203, 'train/bleu': 28.196732035148745, 'validation/accuracy': 0.6031666994094849, 'validation/loss': 2.1089961528778076, 'validation/bleu': 24.17342051048301, 'validation/num_examples': 3000, 'test/accuracy': 0.6041136384010315, 'test/loss': 2.085299491882324, 'test/bleu': 22.705501317599904, 'test/num_examples': 3003, 'score': 5078.621715545654, 'total_duration': 9348.642786502838, 'accumulated_submission_time': 5078.621715545654, 'accumulated_eval_time': 4269.464607715607, 'accumulated_logging_time': 0.155181884765625, 'global_step': 14125, 'preemption_count': 0}), (16480, {'train/accuracy': 0.602558970451355, 'train/loss': 2.0928454399108887, 'train/bleu': 29.180860091401485, 'validation/accuracy': 0.6138423681259155, 'validation/loss': 1.9973779916763306, 'validation/bleu': 24.98317953482843, 'validation/num_examples': 3000, 'test/accuracy': 0.6194294691085815, 'test/loss': 1.9588960409164429, 'test/bleu': 24.090562214757846, 'test/num_examples': 3003, 'score': 5918.54381942749, 'total_duration': 10626.06962442398, 'accumulated_submission_time': 5918.54381942749, 'accumulated_eval_time': 4706.8753480911255, 'accumulated_logging_time': 0.18241620063781738, 'global_step': 16480, 'preemption_count': 0}), (18835, {'train/accuracy': 0.619259238243103, 'train/loss': 1.9477331638336182, 'train/bleu': 30.11433571329848, 'validation/accuracy': 0.6248155832290649, 'validation/loss': 1.9138104915618896, 'validation/bleu': 25.60347878280189, 'validation/num_examples': 3000, 'test/accuracy': 0.6287025809288025, 'test/loss': 1.877466082572937, 'test/bleu': 24.52614101977529, 'test/num_examples': 3003, 'score': 6758.460902690887, 'total_duration': 11892.069929361343, 'accumulated_submission_time': 6758.460902690887, 'accumulated_eval_time': 5132.866238355637, 'accumulated_logging_time': 0.20859932899475098, 'global_step': 18835, 'preemption_count': 0}), (21190, {'train/accuracy': 0.6186280846595764, 'train/loss': 1.9614083766937256, 'train/bleu': 29.658331311601806, 'validation/accuracy': 0.6329493522644043, 'validation/loss': 1.8543436527252197, 'validation/bleu': 26.341737414393798, 'validation/num_examples': 3000, 'test/accuracy': 0.6369182467460632, 'test/loss': 1.8068076372146606, 'test/bleu': 25.263544044158902, 'test/num_examples': 3003, 'score': 7598.609153270721, 'total_duration': 13158.18603682518, 'accumulated_submission_time': 7598.609153270721, 'accumulated_eval_time': 5558.664308309555, 'accumulated_logging_time': 0.31125497817993164, 'global_step': 21190, 'preemption_count': 0}), (23545, {'train/accuracy': 0.6223182678222656, 'train/loss': 1.9257683753967285, 'train/bleu': 30.742059609731815, 'validation/accuracy': 0.6388389468193054, 'validation/loss': 1.8090355396270752, 'validation/bleu': 26.658240310922874, 'validation/num_examples': 3000, 'test/accuracy': 0.6459706425666809, 'test/loss': 1.7570536136627197, 'test/bleu': 25.690809610209783, 'test/num_examples': 3003, 'score': 8438.74411201477, 'total_duration': 14426.950294733047, 'accumulated_submission_time': 8438.74411201477, 'accumulated_eval_time': 5987.1998591423035, 'accumulated_logging_time': 0.33873629570007324, 'global_step': 23545, 'preemption_count': 0}), (25901, {'train/accuracy': 0.6367673277854919, 'train/loss': 1.8128650188446045, 'train/bleu': 30.562781327433708, 'validation/accuracy': 0.6427942514419556, 'validation/loss': 1.7707115411758423, 'validation/bleu': 27.095397238687713, 'validation/num_examples': 3000, 'test/accuracy': 0.6497588753700256, 'test/loss': 1.708775520324707, 'test/bleu': 26.06388113648153, 'test/num_examples': 3003, 'score': 9279.006174087524, 'total_duration': 15690.501833438873, 'accumulated_submission_time': 9279.006174087524, 'accumulated_eval_time': 6410.395622491837, 'accumulated_logging_time': 0.3654816150665283, 'global_step': 25901, 'preemption_count': 0}), (28255, {'train/accuracy': 0.6323233246803284, 'train/loss': 1.8486928939819336, 'train/bleu': 31.546094148874186, 'validation/accuracy': 0.647034764289856, 'validation/loss': 1.738301396369934, 'validation/bleu': 27.224801526934705, 'validation/num_examples': 3000, 'test/accuracy': 0.6549416184425354, 'test/loss': 1.672187328338623, 'test/bleu': 26.580225343011396, 'test/num_examples': 3003, 'score': 10118.944992303848, 'total_duration': 16955.592629671097, 'accumulated_submission_time': 10118.944992303848, 'accumulated_eval_time': 6835.453477859497, 'accumulated_logging_time': 0.3930039405822754, 'global_step': 28255, 'preemption_count': 0}), (30611, {'train/accuracy': 0.6344592571258545, 'train/loss': 1.8246880769729614, 'train/bleu': 31.079205482261536, 'validation/accuracy': 0.6497873663902283, 'validation/loss': 1.7120106220245361, 'validation/bleu': 27.56982375236608, 'validation/num_examples': 3000, 'test/accuracy': 0.6579629182815552, 'test/loss': 1.6442877054214478, 'test/bleu': 26.68151550804582, 'test/num_examples': 3003, 'score': 10959.192777395248, 'total_duration': 18231.570442676544, 'accumulated_submission_time': 10959.192777395248, 'accumulated_eval_time': 7271.090705394745, 'accumulated_logging_time': 0.4188506603240967, 'global_step': 30611, 'preemption_count': 0}), (32967, {'train/accuracy': 0.641680121421814, 'train/loss': 1.7624274492263794, 'train/bleu': 31.68453021303038, 'validation/accuracy': 0.654573380947113, 'validation/loss': 1.6891130208969116, 'validation/bleu': 28.027298562378714, 'validation/num_examples': 3000, 'test/accuracy': 0.6622392535209656, 'test/loss': 1.6180312633514404, 'test/bleu': 26.869818633674655, 'test/num_examples': 3003, 'score': 11799.329007387161, 'total_duration': 19499.104923725128, 'accumulated_submission_time': 11799.329007387161, 'accumulated_eval_time': 7698.396513223648, 'accumulated_logging_time': 0.4439373016357422, 'global_step': 32967, 'preemption_count': 0}), (35322, {'train/accuracy': 0.6431238651275635, 'train/loss': 1.7652592658996582, 'train/bleu': 31.699655486503328, 'validation/accuracy': 0.6557885408401489, 'validation/loss': 1.6659809350967407, 'validation/bleu': 28.232595861105633, 'validation/num_examples': 3000, 'test/accuracy': 0.6649119853973389, 'test/loss': 1.592592716217041, 'test/bleu': 27.33828963506321, 'test/num_examples': 3003, 'score': 12639.25471663475, 'total_duration': 20754.64476943016, 'accumulated_submission_time': 12639.25471663475, 'accumulated_eval_time': 8113.917382717133, 'accumulated_logging_time': 0.470017671585083, 'global_step': 35322, 'preemption_count': 0}), (37677, {'train/accuracy': 0.6570726037025452, 'train/loss': 1.6629935503005981, 'train/bleu': 32.47941990390406, 'validation/accuracy': 0.6585162878036499, 'validation/loss': 1.6495169401168823, 'validation/bleu': 28.026113329746046, 'validation/num_examples': 3000, 'test/accuracy': 0.6668990850448608, 'test/loss': 1.5774426460266113, 'test/bleu': 27.161529678556956, 'test/num_examples': 3003, 'score': 13479.5070374012, 'total_duration': 22052.328681230545, 'accumulated_submission_time': 13479.5070374012, 'accumulated_eval_time': 8571.254666090012, 'accumulated_logging_time': 0.49652886390686035, 'global_step': 37677, 'preemption_count': 0}), (40033, {'train/accuracy': 0.6479796171188354, 'train/loss': 1.7301517724990845, 'train/bleu': 32.19388530891541, 'validation/accuracy': 0.6622112393379211, 'validation/loss': 1.6316739320755005, 'validation/bleu': 28.438767871809628, 'validation/num_examples': 3000, 'test/accuracy': 0.6702225208282471, 'test/loss': 1.557000994682312, 'test/bleu': 27.443725232479164, 'test/num_examples': 3003, 'score': 14319.633382797241, 'total_duration': 23344.766710281372, 'accumulated_submission_time': 14319.633382797241, 'accumulated_eval_time': 9023.473095655441, 'accumulated_logging_time': 0.5230491161346436, 'global_step': 40033, 'preemption_count': 0}), (42389, {'train/accuracy': 0.6494043469429016, 'train/loss': 1.7194828987121582, 'train/bleu': 32.17564201220499, 'validation/accuracy': 0.6631287932395935, 'validation/loss': 1.622020959854126, 'validation/bleu': 28.502564160053694, 'validation/num_examples': 3000, 'test/accuracy': 0.6725814938545227, 'test/loss': 1.54263174533844, 'test/bleu': 27.770569045573712, 'test/num_examples': 3003, 'score': 15159.63986825943, 'total_duration': 24609.043479204178, 'accumulated_submission_time': 15159.63986825943, 'accumulated_eval_time': 9447.651156663895, 'accumulated_logging_time': 0.5486812591552734, 'global_step': 42389, 'preemption_count': 0}), (44745, {'train/accuracy': 0.657556414604187, 'train/loss': 1.6528197526931763, 'train/bleu': 32.68821486326358, 'validation/accuracy': 0.6641207337379456, 'validation/loss': 1.6081554889678955, 'validation/bleu': 28.567016995392713, 'validation/num_examples': 3000, 'test/accuracy': 0.674487292766571, 'test/loss': 1.5296008586883545, 'test/bleu': 27.91581044042552, 'test/num_examples': 3003, 'score': 15999.807573318481, 'total_duration': 25936.64005255699, 'accumulated_submission_time': 15999.807573318481, 'accumulated_eval_time': 9934.987287282944, 'accumulated_logging_time': 0.5746548175811768, 'global_step': 44745, 'preemption_count': 0}), (47101, {'train/accuracy': 0.656990110874176, 'train/loss': 1.6636278629302979, 'train/bleu': 32.22773914432872, 'validation/accuracy': 0.6657077670097351, 'validation/loss': 1.5969038009643555, 'validation/bleu': 28.457516610878002, 'validation/num_examples': 3000, 'test/accuracy': 0.6777293682098389, 'test/loss': 1.5124273300170898, 'test/bleu': 28.254601899911435, 'test/num_examples': 3003, 'score': 16839.96423625946, 'total_duration': 27235.0104265213, 'accumulated_submission_time': 16839.96423625946, 'accumulated_eval_time': 10393.106807947159, 'accumulated_logging_time': 0.6021087169647217, 'global_step': 47101, 'preemption_count': 0}), (49457, {'train/accuracy': 0.6551543474197388, 'train/loss': 1.664835810661316, 'train/bleu': 32.41724343726786, 'validation/accuracy': 0.6665013432502747, 'validation/loss': 1.585096836090088, 'validation/bleu': 28.82016060908621, 'validation/num_examples': 3000, 'test/accuracy': 0.6782174110412598, 'test/loss': 1.5012739896774292, 'test/bleu': 28.237425596124154, 'test/num_examples': 3003, 'score': 17680.039123773575, 'total_duration': 28553.427763938904, 'accumulated_submission_time': 17680.039123773575, 'accumulated_eval_time': 10871.355544090271, 'accumulated_logging_time': 0.6291701793670654, 'global_step': 49457, 'preemption_count': 0}), (51813, {'train/accuracy': 0.6599031686782837, 'train/loss': 1.6309144496917725, 'train/bleu': 32.88231161081743, 'validation/accuracy': 0.6687827706336975, 'validation/loss': 1.573830008506775, 'validation/bleu': 28.834698072084503, 'validation/num_examples': 3000, 'test/accuracy': 0.6807855367660522, 'test/loss': 1.4898260831832886, 'test/bleu': 28.567021618836947, 'test/num_examples': 3003, 'score': 18520.285583257675, 'total_duration': 29865.055556058884, 'accumulated_submission_time': 18520.285583257675, 'accumulated_eval_time': 11342.643130540848, 'accumulated_logging_time': 0.6553468704223633, 'global_step': 51813, 'preemption_count': 0}), (54169, {'train/accuracy': 0.6590301990509033, 'train/loss': 1.636247992515564, 'train/bleu': 32.68305358451159, 'validation/accuracy': 0.6702706813812256, 'validation/loss': 1.5636259317398071, 'validation/bleu': 28.838455664827734, 'validation/num_examples': 3000, 'test/accuracy': 0.6809133887290955, 'test/loss': 1.476148247718811, 'test/bleu': 28.5247227921742, 'test/num_examples': 3003, 'score': 19360.457694768906, 'total_duration': 31199.683564424515, 'accumulated_submission_time': 19360.457694768906, 'accumulated_eval_time': 11837.004646778107, 'accumulated_logging_time': 0.682880163192749, 'global_step': 54169, 'preemption_count': 0}), (56526, {'train/accuracy': 0.6727394461631775, 'train/loss': 1.547843337059021, 'train/bleu': 33.704396974588924, 'validation/accuracy': 0.67149817943573, 'validation/loss': 1.556463599205017, 'validation/bleu': 28.940380949052926, 'validation/num_examples': 3000, 'test/accuracy': 0.6832956075668335, 'test/loss': 1.4727809429168701, 'test/bleu': 28.48422535570874, 'test/num_examples': 3003, 'score': 20200.666444778442, 'total_duration': 32496.691576719284, 'accumulated_submission_time': 20200.666444778442, 'accumulated_eval_time': 12293.708816289902, 'accumulated_logging_time': 0.7112369537353516, 'global_step': 56526, 'preemption_count': 0}), (58883, {'train/accuracy': 0.6654917597770691, 'train/loss': 1.5997649431228638, 'train/bleu': 33.577646399954844, 'validation/accuracy': 0.6734076142311096, 'validation/loss': 1.5457199811935425, 'validation/bleu': 29.25365327177037, 'validation/num_examples': 3000, 'test/accuracy': 0.6844227910041809, 'test/loss': 1.4603431224822998, 'test/bleu': 28.452297195681055, 'test/num_examples': 3003, 'score': 21040.878185749054, 'total_duration': 33849.52210497856, 'accumulated_submission_time': 21040.878185749054, 'accumulated_eval_time': 12806.234851360321, 'accumulated_logging_time': 0.7371804714202881, 'global_step': 58883, 'preemption_count': 0}), (61240, {'train/accuracy': 0.6613894701004028, 'train/loss': 1.621449589729309, 'train/bleu': 32.875589064704464, 'validation/accuracy': 0.6731472611427307, 'validation/loss': 1.5452971458435059, 'validation/bleu': 29.22795765081323, 'validation/num_examples': 3000, 'test/accuracy': 0.685747504234314, 'test/loss': 1.4529186487197876, 'test/bleu': 28.606576618818305, 'test/num_examples': 3003, 'score': 21881.12133550644, 'total_duration': 35195.140328884125, 'accumulated_submission_time': 21881.12133550644, 'accumulated_eval_time': 13311.513763189316, 'accumulated_logging_time': 0.7662217617034912, 'global_step': 61240, 'preemption_count': 0}), (63596, {'train/accuracy': 0.6703936457633972, 'train/loss': 1.561401128768921, 'train/bleu': 33.186709795215776, 'validation/accuracy': 0.6751186847686768, 'validation/loss': 1.5343824625015259, 'validation/bleu': 29.201814838306824, 'validation/num_examples': 3000, 'test/accuracy': 0.6858869791030884, 'test/loss': 1.448622703552246, 'test/bleu': 28.8951092703082, 'test/num_examples': 3003, 'score': 22721.15820980072, 'total_duration': 36497.30195260048, 'accumulated_submission_time': 22721.15820980072, 'accumulated_eval_time': 13773.54467511177, 'accumulated_logging_time': 0.792844295501709, 'global_step': 63596, 'preemption_count': 0}), (65952, {'train/accuracy': 0.6658357381820679, 'train/loss': 1.5820873975753784, 'train/bleu': 33.438667065409945, 'validation/accuracy': 0.6747467517852783, 'validation/loss': 1.529623031616211, 'validation/bleu': 29.082334854953462, 'validation/num_examples': 3000, 'test/accuracy': 0.6879205107688904, 'test/loss': 1.4367201328277588, 'test/bleu': 29.19896289156065, 'test/num_examples': 3003, 'score': 23561.172528982162, 'total_duration': 37794.61520290375, 'accumulated_submission_time': 23561.172528982162, 'accumulated_eval_time': 14230.748449802399, 'accumulated_logging_time': 0.8196899890899658, 'global_step': 65952, 'preemption_count': 0}), (68308, {'train/accuracy': 0.6674439311027527, 'train/loss': 1.5769660472869873, 'train/bleu': 33.443686855980154, 'validation/accuracy': 0.6772265434265137, 'validation/loss': 1.5254470109939575, 'validation/bleu': 29.456049313556623, 'validation/num_examples': 3000, 'test/accuracy': 0.689175546169281, 'test/loss': 1.4308174848556519, 'test/bleu': 29.34665398865402, 'test/num_examples': 3003, 'score': 24401.09858083725, 'total_duration': 39124.82380437851, 'accumulated_submission_time': 24401.09858083725, 'accumulated_eval_time': 14720.935827493668, 'accumulated_logging_time': 0.8474724292755127, 'global_step': 68308, 'preemption_count': 0}), (70664, {'train/accuracy': 0.6694921851158142, 'train/loss': 1.5606787204742432, 'train/bleu': 34.002941847398326, 'validation/accuracy': 0.6776109337806702, 'validation/loss': 1.5192512273788452, 'validation/bleu': 29.4744042274841, 'validation/num_examples': 3000, 'test/accuracy': 0.6896287202835083, 'test/loss': 1.4252281188964844, 'test/bleu': 29.037491151005273, 'test/num_examples': 3003, 'score': 25241.007563829422, 'total_duration': 40433.932607889175, 'accumulated_submission_time': 25241.007563829422, 'accumulated_eval_time': 15190.043044805527, 'accumulated_logging_time': 0.873542070388794, 'global_step': 70664, 'preemption_count': 0}), (73020, {'train/accuracy': 0.6680378913879395, 'train/loss': 1.5732415914535522, 'train/bleu': 33.73546634671619, 'validation/accuracy': 0.6776357293128967, 'validation/loss': 1.5150974988937378, 'validation/bleu': 29.337378188980896, 'validation/num_examples': 3000, 'test/accuracy': 0.690395712852478, 'test/loss': 1.422254204750061, 'test/bleu': 29.0496697371159, 'test/num_examples': 3003, 'score': 26081.099046945572, 'total_duration': 41810.85783743858, 'accumulated_submission_time': 26081.099046945572, 'accumulated_eval_time': 15726.782981872559, 'accumulated_logging_time': 0.9002184867858887, 'global_step': 73020, 'preemption_count': 0}), (75376, {'train/accuracy': 0.678062915802002, 'train/loss': 1.5084115266799927, 'train/bleu': 34.37010738887513, 'validation/accuracy': 0.6786896586418152, 'validation/loss': 1.5128967761993408, 'validation/bleu': 29.442304474189754, 'validation/num_examples': 3000, 'test/accuracy': 0.6909418702125549, 'test/loss': 1.4211509227752686, 'test/bleu': 29.253641051429362, 'test/num_examples': 3003, 'score': 26921.161459684372, 'total_duration': 43130.45228648186, 'accumulated_submission_time': 26921.161459684372, 'accumulated_eval_time': 16206.221158504486, 'accumulated_logging_time': 0.9275047779083252, 'global_step': 75376, 'preemption_count': 0}), (77732, {'train/accuracy': 0.6716535687446594, 'train/loss': 1.5411198139190674, 'train/bleu': 33.842845915802954, 'validation/accuracy': 0.67948317527771, 'validation/loss': 1.5097118616104126, 'validation/bleu': 29.653856831269366, 'validation/num_examples': 3000, 'test/accuracy': 0.6924176812171936, 'test/loss': 1.4167795181274414, 'test/bleu': 29.199592670309716, 'test/num_examples': 3003, 'score': 27761.225507974625, 'total_duration': 44498.723195552826, 'accumulated_submission_time': 27761.225507974625, 'accumulated_eval_time': 16734.333786725998, 'accumulated_logging_time': 0.9551715850830078, 'global_step': 77732, 'preemption_count': 0}), (80088, {'train/accuracy': 0.6748469471931458, 'train/loss': 1.5335164070129395, 'train/bleu': 33.60797423619726, 'validation/accuracy': 0.6788756251335144, 'validation/loss': 1.5091896057128906, 'validation/bleu': 29.621779752797085, 'validation/num_examples': 3000, 'test/accuracy': 0.6921387910842896, 'test/loss': 1.413880705833435, 'test/bleu': 29.247403076136255, 'test/num_examples': 3003, 'score': 28601.267050027847, 'total_duration': 45832.05685567856, 'accumulated_submission_time': 28601.267050027847, 'accumulated_eval_time': 17227.53161740303, 'accumulated_logging_time': 0.9824159145355225, 'global_step': 80088, 'preemption_count': 0}), (82445, {'train/accuracy': 0.6743976473808289, 'train/loss': 1.530717134475708, 'train/bleu': 34.55061700289805, 'validation/accuracy': 0.6797063946723938, 'validation/loss': 1.503297209739685, 'validation/bleu': 29.767430244775294, 'validation/num_examples': 3000, 'test/accuracy': 0.6929754614830017, 'test/loss': 1.4087291955947876, 'test/bleu': 29.301034914418047, 'test/num_examples': 3003, 'score': 29441.463753700256, 'total_duration': 47181.34526705742, 'accumulated_submission_time': 29441.463753700256, 'accumulated_eval_time': 17736.52965450287, 'accumulated_logging_time': 1.009713888168335, 'global_step': 82445, 'preemption_count': 0}), (84801, {'train/accuracy': 0.6736806035041809, 'train/loss': 1.5392768383026123, 'train/bleu': 34.373158435533924, 'validation/accuracy': 0.6791608333587646, 'validation/loss': 1.5018540620803833, 'validation/bleu': 29.749724428666255, 'validation/num_examples': 3000, 'test/accuracy': 0.6930218935012817, 'test/loss': 1.4062680006027222, 'test/bleu': 29.30900212133315, 'test/num_examples': 3003, 'score': 30281.44095182419, 'total_duration': 48502.030096530914, 'accumulated_submission_time': 30281.44095182419, 'accumulated_eval_time': 18217.14319062233, 'accumulated_logging_time': 1.0376312732696533, 'global_step': 84801, 'preemption_count': 0}), (87158, {'train/accuracy': 0.6743996143341064, 'train/loss': 1.5298314094543457, 'train/bleu': 34.048574009989096, 'validation/accuracy': 0.6798179745674133, 'validation/loss': 1.4991397857666016, 'validation/bleu': 29.754207538581177, 'validation/num_examples': 3000, 'test/accuracy': 0.6935099959373474, 'test/loss': 1.4048857688903809, 'test/bleu': 29.401569654147707, 'test/num_examples': 3003, 'score': 31121.67044186592, 'total_duration': 49838.419803619385, 'accumulated_submission_time': 31121.67044186592, 'accumulated_eval_time': 18713.20849108696, 'accumulated_logging_time': 1.0662257671356201, 'global_step': 87158, 'preemption_count': 0}), (89514, {'train/accuracy': 0.6755579710006714, 'train/loss': 1.5158967971801758, 'train/bleu': 34.047552645039296, 'validation/accuracy': 0.6803139448165894, 'validation/loss': 1.4974576234817505, 'validation/bleu': 29.770309970047116, 'validation/num_examples': 3000, 'test/accuracy': 0.6933937668800354, 'test/loss': 1.404252052307129, 'test/bleu': 29.269887032234116, 'test/num_examples': 3003, 'score': 31961.795548677444, 'total_duration': 51195.782636642456, 'accumulated_submission_time': 31961.795548677444, 'accumulated_eval_time': 19230.350835561752, 'accumulated_logging_time': 1.0953691005706787, 'global_step': 89514, 'preemption_count': 0}), (91870, {'train/accuracy': 0.6766529083251953, 'train/loss': 1.5187170505523682, 'train/bleu': 34.19179512597903, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4974420070648193, 'validation/bleu': 29.72557204059409, 'validation/num_examples': 3000, 'test/accuracy': 0.6943582892417908, 'test/loss': 1.4023356437683105, 'test/bleu': 29.39102388629618, 'test/num_examples': 3003, 'score': 32801.79234480858, 'total_duration': 52530.66354060173, 'accumulated_submission_time': 32801.79234480858, 'accumulated_eval_time': 19725.14108300209, 'accumulated_logging_time': 1.1229729652404785, 'global_step': 91870, 'preemption_count': 0}), (94227, {'train/accuracy': 0.6747400164604187, 'train/loss': 1.523642659187317, 'train/bleu': 34.72360185432612, 'validation/accuracy': 0.6798303723335266, 'validation/loss': 1.496509075164795, 'validation/bleu': 29.861193927437423, 'validation/num_examples': 3000, 'test/accuracy': 0.6942304372787476, 'test/loss': 1.4020249843597412, 'test/bleu': 29.414385113390637, 'test/num_examples': 3003, 'score': 33641.87730574608, 'total_duration': 53883.06198430061, 'accumulated_submission_time': 33641.87730574608, 'accumulated_eval_time': 20237.35983300209, 'accumulated_logging_time': 1.1509370803833008, 'global_step': 94227, 'preemption_count': 0}), (96582, {'train/accuracy': 0.6793270111083984, 'train/loss': 1.5005772113800049, 'train/bleu': 34.57213673817869, 'validation/accuracy': 0.6799667477607727, 'validation/loss': 1.4966213703155518, 'validation/bleu': 29.816350818607244, 'validation/num_examples': 3000, 'test/accuracy': 0.6938701868057251, 'test/loss': 1.4020510911941528, 'test/bleu': 29.379294609935542, 'test/num_examples': 3003, 'score': 34481.83175301552, 'total_duration': 55230.67310833931, 'accumulated_submission_time': 34481.83175301552, 'accumulated_eval_time': 20744.92255783081, 'accumulated_logging_time': 1.1792380809783936, 'global_step': 96582, 'preemption_count': 0}), (98939, {'train/accuracy': 0.6796087622642517, 'train/loss': 1.49925696849823, 'train/bleu': 34.22295377056319, 'validation/accuracy': 0.6802147626876831, 'validation/loss': 1.496536374092102, 'validation/bleu': 29.79260133975069, 'validation/num_examples': 3000, 'test/accuracy': 0.6941026449203491, 'test/loss': 1.401926875114441, 'test/bleu': 29.40612198300859, 'test/num_examples': 3003, 'score': 35322.07841825485, 'total_duration': 56590.07874560356, 'accumulated_submission_time': 35322.07841825485, 'accumulated_eval_time': 21263.987096071243, 'accumulated_logging_time': 1.2067761421203613, 'global_step': 98939, 'preemption_count': 0}), (101295, {'train/accuracy': 0.6781590580940247, 'train/loss': 1.5091168880462646, 'train/bleu': 34.24370911526101, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 36162.191653728485, 'total_duration': 57940.1638109684, 'accumulated_submission_time': 36162.191653728485, 'accumulated_eval_time': 21773.860198259354, 'accumulated_logging_time': 1.2375068664550781, 'global_step': 101295, 'preemption_count': 0}), (103651, {'train/accuracy': 0.674842894077301, 'train/loss': 1.5288186073303223, 'train/bleu': 34.416922127648505, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 37002.11167097092, 'total_duration': 59288.17239165306, 'accumulated_submission_time': 37002.11167097092, 'accumulated_eval_time': 22281.850743055344, 'accumulated_logging_time': 1.2679455280303955, 'global_step': 103651, 'preemption_count': 0}), (106008, {'train/accuracy': 0.6773278117179871, 'train/loss': 1.5113134384155273, 'train/bleu': 34.371530891826644, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 37842.233411073685, 'total_duration': 60646.25704789162, 'accumulated_submission_time': 37842.233411073685, 'accumulated_eval_time': 22799.716601848602, 'accumulated_logging_time': 1.2975380420684814, 'global_step': 106008, 'preemption_count': 0}), (108364, {'train/accuracy': 0.6775650382041931, 'train/loss': 1.5132313966751099, 'train/bleu': 34.20403181864933, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 38682.16828632355, 'total_duration': 62019.54036283493, 'accumulated_submission_time': 38682.16828632355, 'accumulated_eval_time': 23332.9684278965, 'accumulated_logging_time': 1.3266677856445312, 'global_step': 108364, 'preemption_count': 0}), (110720, {'train/accuracy': 0.6771190762519836, 'train/loss': 1.515312671661377, 'train/bleu': 34.13090005895956, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 39522.18356657028, 'total_duration': 63370.85781121254, 'accumulated_submission_time': 39522.18356657028, 'accumulated_eval_time': 23844.173778533936, 'accumulated_logging_time': 1.3562805652618408, 'global_step': 110720, 'preemption_count': 0}), (113075, {'train/accuracy': 0.676160454750061, 'train/loss': 1.5152772665023804, 'train/bleu': 34.18633428085686, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 40362.22967219353, 'total_duration': 64732.45637965202, 'accumulated_submission_time': 40362.22967219353, 'accumulated_eval_time': 24365.630432605743, 'accumulated_logging_time': 1.3847291469573975, 'global_step': 113075, 'preemption_count': 0}), (115431, {'train/accuracy': 0.6735895872116089, 'train/loss': 1.531890630722046, 'train/bleu': 34.70026898669226, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 41202.35964822769, 'total_duration': 66096.38588237762, 'accumulated_submission_time': 41202.35964822769, 'accumulated_eval_time': 24889.33523273468, 'accumulated_logging_time': 1.4133291244506836, 'global_step': 115431, 'preemption_count': 0}), (117787, {'train/accuracy': 0.6738109588623047, 'train/loss': 1.5302364826202393, 'train/bleu': 34.45853533681923, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 42042.437108039856, 'total_duration': 67474.96292757988, 'accumulated_submission_time': 42042.437108039856, 'accumulated_eval_time': 25427.737149000168, 'accumulated_logging_time': 1.444042682647705, 'global_step': 117787, 'preemption_count': 0}), (120142, {'train/accuracy': 0.6759756207466125, 'train/loss': 1.515751600265503, 'train/bleu': 34.22470415448941, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 42882.377626657486, 'total_duration': 68847.435379982, 'accumulated_submission_time': 42882.377626657486, 'accumulated_eval_time': 25960.172927379608, 'accumulated_logging_time': 1.4731831550598145, 'global_step': 120142, 'preemption_count': 0}), (122499, {'train/accuracy': 0.6736052632331848, 'train/loss': 1.5344281196594238, 'train/bleu': 34.55278101370412, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 43722.55801129341, 'total_duration': 70211.65032339096, 'accumulated_submission_time': 43722.55801129341, 'accumulated_eval_time': 26484.111390829086, 'accumulated_logging_time': 1.502096176147461, 'global_step': 122499, 'preemption_count': 0}), (124854, {'train/accuracy': 0.6764072775840759, 'train/loss': 1.5210989713668823, 'train/bleu': 34.46103119602367, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 44562.76447367668, 'total_duration': 71578.09846949577, 'accumulated_submission_time': 44562.76447367668, 'accumulated_eval_time': 27010.25706577301, 'accumulated_logging_time': 1.5315184593200684, 'global_step': 124854, 'preemption_count': 0}), (127210, {'train/accuracy': 0.6790623664855957, 'train/loss': 1.5042811632156372, 'train/bleu': 34.40183515742074, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 45402.769220113754, 'total_duration': 72956.60133194923, 'accumulated_submission_time': 45402.769220113754, 'accumulated_eval_time': 27548.65670323372, 'accumulated_logging_time': 1.562415361404419, 'global_step': 127210, 'preemption_count': 0}), (129566, {'train/accuracy': 0.677972674369812, 'train/loss': 1.5072195529937744, 'train/bleu': 34.258766307568344, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 46242.84040975571, 'total_duration': 74316.35155272484, 'accumulated_submission_time': 46242.84040975571, 'accumulated_eval_time': 28068.23496031761, 'accumulated_logging_time': 1.595994472503662, 'global_step': 129566, 'preemption_count': 0}), (131922, {'train/accuracy': 0.6743406057357788, 'train/loss': 1.5279185771942139, 'train/bleu': 34.241705504987344, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 47083.04276394844, 'total_duration': 75672.80275964737, 'accumulated_submission_time': 47083.04276394844, 'accumulated_eval_time': 28584.38727903366, 'accumulated_logging_time': 1.6250958442687988, 'global_step': 131922, 'preemption_count': 0}), (133333, {'train/accuracy': 0.6771786212921143, 'train/loss': 1.514631986618042, 'train/bleu': 34.640933465596675, 'validation/accuracy': 0.6802023649215698, 'validation/loss': 1.4965161085128784, 'validation/bleu': 29.800453262426707, 'validation/num_examples': 3000, 'test/accuracy': 0.6941142678260803, 'test/loss': 1.4019163846969604, 'test/bleu': 29.39337224763264, 'test/num_examples': 3003, 'score': 47585.765372276306, 'total_duration': 76691.0393230915, 'accumulated_submission_time': 47585.765372276306, 'accumulated_eval_time': 29099.83028960228, 'accumulated_logging_time': 1.6558685302734375, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0326 23:37:40.878413 140257815279424 submission_runner.py:596] Timing: 47585.765372276306
I0326 23:37:40.878462 140257815279424 submission_runner.py:598] Total number of evals: 58
I0326 23:37:40.878520 140257815279424 submission_runner.py:599] ====================
I0326 23:37:40.878659 140257815279424 submission_runner.py:683] Final wmt_attention_temp score: 0
