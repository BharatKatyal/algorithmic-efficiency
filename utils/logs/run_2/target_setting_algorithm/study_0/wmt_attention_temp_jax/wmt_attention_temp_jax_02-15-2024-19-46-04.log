python3 submission_runner.py --framework=jax --workload=wmt_attention_temp --submission_path=reference_algorithms/target_setting_algorithms/jax_nadamw.py --tuning_search_space=reference_algorithms/target_setting_algorithms/wmt_attention_temp/tuning_search_space.json --data_dir=/data/wmt --num_tuning_trials=1 --experiment_dir=/experiment_runs --experiment_name=variants_target_setting/study_0 --overwrite=true --save_checkpoints=false --num_tuning_trials=1 --rng_seed=3377931912 --max_global_steps=133333 2>&1 | tee -a /logs/wmt_attention_temp_jax_02-15-2024-19-46-04.log
I0215 19:46:29.316648 140586562139968 logger_utils.py:76] Creating experiment directory at /experiment_runs/variants_target_setting/study_0/wmt_attention_temp_jax.
I0215 19:46:30.331249 140586562139968 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: Interpreter Host CUDA
I0215 19:46:30.331971 140586562139968 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0215 19:46:30.332112 140586562139968 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
I0215 19:46:30.339128 140586562139968 submission_runner.py:542] Using RNG seed 3377931912
I0215 19:46:31.396827 140586562139968 submission_runner.py:551] --- Tuning run 1/1 ---
I0215 19:46:31.397026 140586562139968 submission_runner.py:556] Creating tuning directory at /experiment_runs/variants_target_setting/study_0/wmt_attention_temp_jax/trial_1.
I0215 19:46:31.397416 140586562139968 logger_utils.py:92] Saving hparams to /experiment_runs/variants_target_setting/study_0/wmt_attention_temp_jax/trial_1/hparams.json.
I0215 19:46:31.580183 140586562139968 submission_runner.py:206] Initializing dataset.
I0215 19:46:31.590933 140586562139968 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0215 19:46:31.595249 140586562139968 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0215 19:46:31.749873 140586562139968 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0215 19:46:33.754930 140586562139968 submission_runner.py:213] Initializing model.
I0215 19:46:42.607569 140586562139968 submission_runner.py:255] Initializing optimizer.
I0215 19:46:43.692275 140586562139968 submission_runner.py:262] Initializing metrics bundle.
I0215 19:46:43.692483 140586562139968 submission_runner.py:280] Initializing checkpoint and logger.
I0215 19:46:43.693605 140586562139968 checkpoints.py:915] Found no checkpoint files in /experiment_runs/variants_target_setting/study_0/wmt_attention_temp_jax/trial_1 with prefix checkpoint_
I0215 19:46:43.693749 140586562139968 submission_runner.py:300] Saving meta data to /experiment_runs/variants_target_setting/study_0/wmt_attention_temp_jax/trial_1/meta_data_0.json.
I0215 19:46:43.693959 140586562139968 logger_utils.py:257] Unable to record workload.train_mean information. Continuing without it.
I0215 19:46:43.694051 140586562139968 logger_utils.py:257] Unable to record workload.train_stddev information. Continuing without it.
fatal: detected dubious ownership in repository at '/algorithmic-efficiency'
To add an exception for this directory, call:

	git config --global --add safe.directory /algorithmic-efficiency
I0215 19:46:44.006738 140586562139968 logger_utils.py:220] Unable to record git information. Continuing without it.
I0215 19:46:44.296839 140586562139968 submission_runner.py:304] Saving flags to /experiment_runs/variants_target_setting/study_0/wmt_attention_temp_jax/trial_1/flags_0.json.
I0215 19:46:44.307175 140586562139968 submission_runner.py:314] Starting training loop.
I0215 19:47:23.398557 140421388818176 logging_writer.py:48] [0] global_step=0, grad_norm=3.771484375, loss=10.927717208862305
I0215 19:47:23.414348 140586562139968 spec.py:321] Evaluating on the training split.
I0215 19:47:23.419020 140586562139968 dataset_info.py:578] Load dataset info from /data/wmt/wmt17_translate/de-en/1.0.0
I0215 19:47:23.421904 140586562139968 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0215 19:47:23.460415 140586562139968 logging_logger.py:49] Constructing tf.data.Dataset wmt17_translate for split train, from /data/wmt/wmt17_translate/de-en/1.0.0
I0215 19:47:31.436794 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 19:52:21.521705 140586562139968 spec.py:333] Evaluating on the validation split.
I0215 19:52:21.527119 140586562139968 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0215 19:52:21.530936 140586562139968 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0215 19:52:21.570037 140586562139968 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split validation, from /data/wmt/wmt14_translate/de-en/1.0.0
I0215 19:52:28.531785 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 19:57:09.181616 140586562139968 spec.py:349] Evaluating on the test split.
I0215 19:57:09.184551 140586562139968 dataset_info.py:578] Load dataset info from /data/wmt/wmt14_translate/de-en/1.0.0
I0215 19:57:09.187598 140586562139968 dataset_info.py:669] Fields info.[splits, supervised_keys] from disk and from code do not match. Keeping the one from code.
I0215 19:57:09.222438 140586562139968 logging_logger.py:49] Constructing tf.data.Dataset wmt14_translate for split test, from /data/wmt/wmt14_translate/de-en/1.0.0
I0215 19:57:12.013105 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 20:01:51.920147 140586562139968 submission_runner.py:408] Time since start: 907.61s, 	Step: 1, 	{'train/accuracy': 0.0006170864216983318, 'train/loss': 10.976629257202148, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 10.968364715576172, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 10.973079681396484, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 39.107139110565186, 'total_duration': 907.612909078598, 'accumulated_submission_time': 39.107139110565186, 'accumulated_eval_time': 868.5057301521301, 'accumulated_logging_time': 0}
I0215 20:01:51.938253 140416866297600 logging_writer.py:48] [1] accumulated_eval_time=868.505730, accumulated_logging_time=0, accumulated_submission_time=39.107139, global_step=1, preemption_count=0, score=39.107139, test/accuracy=0.000709, test/bleu=0.000000, test/loss=10.973080, test/num_examples=3003, total_duration=907.612909, train/accuracy=0.000617, train/bleu=0.000000, train/loss=10.976629, validation/accuracy=0.000484, validation/bleu=0.000000, validation/loss=10.968365, validation/num_examples=3000
I0215 20:01:52.305543 140416857904896 logging_writer.py:48] [1] global_step=1, grad_norm=3.801342010498047, loss=10.929906845092773
I0215 20:01:52.668705 140416866297600 logging_writer.py:48] [2] global_step=2, grad_norm=3.837770700454712, loss=10.926735877990723
I0215 20:01:53.030818 140416857904896 logging_writer.py:48] [3] global_step=3, grad_norm=3.7538273334503174, loss=10.923554420471191
I0215 20:01:53.391736 140416866297600 logging_writer.py:48] [4] global_step=4, grad_norm=3.8148937225341797, loss=10.911555290222168
I0215 20:01:53.754298 140416857904896 logging_writer.py:48] [5] global_step=5, grad_norm=3.8114044666290283, loss=10.915534973144531
I0215 20:01:54.118687 140416866297600 logging_writer.py:48] [6] global_step=6, grad_norm=3.857673406600952, loss=10.900371551513672
I0215 20:01:54.482541 140416857904896 logging_writer.py:48] [7] global_step=7, grad_norm=3.8081579208374023, loss=10.914094924926758
I0215 20:01:54.847455 140416866297600 logging_writer.py:48] [8] global_step=8, grad_norm=3.800947427749634, loss=10.905824661254883
I0215 20:01:55.212926 140416857904896 logging_writer.py:48] [9] global_step=9, grad_norm=3.8446991443634033, loss=10.905168533325195
I0215 20:01:55.577085 140416866297600 logging_writer.py:48] [10] global_step=10, grad_norm=3.788193941116333, loss=10.899495124816895
I0215 20:01:55.942871 140416857904896 logging_writer.py:48] [11] global_step=11, grad_norm=3.7739012241363525, loss=10.885316848754883
I0215 20:01:56.307713 140416866297600 logging_writer.py:48] [12] global_step=12, grad_norm=3.796748399734497, loss=10.886786460876465
I0215 20:01:56.671070 140416857904896 logging_writer.py:48] [13] global_step=13, grad_norm=3.7724320888519287, loss=10.875532150268555
I0215 20:01:57.033828 140416866297600 logging_writer.py:48] [14] global_step=14, grad_norm=3.7799620628356934, loss=10.873101234436035
I0215 20:01:57.397497 140416857904896 logging_writer.py:48] [15] global_step=15, grad_norm=3.7727274894714355, loss=10.850924491882324
I0215 20:01:57.763479 140416866297600 logging_writer.py:48] [16] global_step=16, grad_norm=3.7744131088256836, loss=10.846500396728516
I0215 20:01:58.124019 140416857904896 logging_writer.py:48] [17] global_step=17, grad_norm=3.7741081714630127, loss=10.846524238586426
I0215 20:01:58.486838 140416866297600 logging_writer.py:48] [18] global_step=18, grad_norm=3.7012619972229004, loss=10.8340482711792
I0215 20:01:58.850596 140416857904896 logging_writer.py:48] [19] global_step=19, grad_norm=3.6897342205047607, loss=10.827582359313965
I0215 20:01:59.215205 140416866297600 logging_writer.py:48] [20] global_step=20, grad_norm=3.7525651454925537, loss=10.806329727172852
I0215 20:01:59.578631 140416857904896 logging_writer.py:48] [21] global_step=21, grad_norm=3.7155563831329346, loss=10.79940128326416
I0215 20:01:59.943879 140416866297600 logging_writer.py:48] [22] global_step=22, grad_norm=3.6999099254608154, loss=10.797364234924316
I0215 20:02:00.307044 140416857904896 logging_writer.py:48] [23] global_step=23, grad_norm=3.6362874507904053, loss=10.761146545410156
I0215 20:02:00.669221 140416866297600 logging_writer.py:48] [24] global_step=24, grad_norm=3.636746644973755, loss=10.761299133300781
I0215 20:02:01.034076 140416857904896 logging_writer.py:48] [25] global_step=25, grad_norm=3.7085413932800293, loss=10.748169898986816
I0215 20:02:01.399803 140416866297600 logging_writer.py:48] [26] global_step=26, grad_norm=3.6111223697662354, loss=10.731884956359863
I0215 20:02:01.765326 140416857904896 logging_writer.py:48] [27] global_step=27, grad_norm=3.5803701877593994, loss=10.72701644897461
I0215 20:02:02.129449 140416866297600 logging_writer.py:48] [28] global_step=28, grad_norm=3.6059482097625732, loss=10.702655792236328
I0215 20:02:02.493376 140416857904896 logging_writer.py:48] [29] global_step=29, grad_norm=3.581235885620117, loss=10.686858177185059
I0215 20:02:02.856286 140416866297600 logging_writer.py:48] [30] global_step=30, grad_norm=3.5171525478363037, loss=10.683464050292969
I0215 20:02:03.220689 140416857904896 logging_writer.py:48] [31] global_step=31, grad_norm=3.5713555812835693, loss=10.648771286010742
I0215 20:02:03.585034 140416866297600 logging_writer.py:48] [32] global_step=32, grad_norm=3.535142421722412, loss=10.639009475708008
I0215 20:02:03.950571 140416857904896 logging_writer.py:48] [33] global_step=33, grad_norm=3.503025770187378, loss=10.63758659362793
I0215 20:02:04.315383 140416866297600 logging_writer.py:48] [34] global_step=34, grad_norm=3.483046531677246, loss=10.609949111938477
I0215 20:02:04.678990 140416857904896 logging_writer.py:48] [35] global_step=35, grad_norm=3.4242122173309326, loss=10.59601879119873
I0215 20:02:05.042839 140416866297600 logging_writer.py:48] [36] global_step=36, grad_norm=3.357149362564087, loss=10.58724594116211
I0215 20:02:05.403675 140416857904896 logging_writer.py:48] [37] global_step=37, grad_norm=3.29854416847229, loss=10.570711135864258
I0215 20:02:05.770702 140416866297600 logging_writer.py:48] [38] global_step=38, grad_norm=3.3589630126953125, loss=10.54989242553711
I0215 20:02:06.136301 140416857904896 logging_writer.py:48] [39] global_step=39, grad_norm=3.242246627807617, loss=10.541969299316406
I0215 20:02:06.500856 140416866297600 logging_writer.py:48] [40] global_step=40, grad_norm=3.276644229888916, loss=10.509073257446289
I0215 20:02:06.865745 140416857904896 logging_writer.py:48] [41] global_step=41, grad_norm=3.2384090423583984, loss=10.486104011535645
I0215 20:02:07.229530 140416866297600 logging_writer.py:48] [42] global_step=42, grad_norm=3.191793918609619, loss=10.478462219238281
I0215 20:02:07.596925 140416857904896 logging_writer.py:48] [43] global_step=43, grad_norm=3.185868978500366, loss=10.44819450378418
I0215 20:02:07.962771 140416866297600 logging_writer.py:48] [44] global_step=44, grad_norm=3.1379334926605225, loss=10.428847312927246
I0215 20:02:08.329989 140416857904896 logging_writer.py:48] [45] global_step=45, grad_norm=3.1292896270751953, loss=10.399100303649902
I0215 20:02:08.694847 140416866297600 logging_writer.py:48] [46] global_step=46, grad_norm=3.0877022743225098, loss=10.404276847839355
I0215 20:02:09.058588 140416857904896 logging_writer.py:48] [47] global_step=47, grad_norm=3.028012275695801, loss=10.365524291992188
I0215 20:02:09.423987 140416866297600 logging_writer.py:48] [48] global_step=48, grad_norm=2.9726617336273193, loss=10.35322380065918
I0215 20:02:09.786167 140416857904896 logging_writer.py:48] [49] global_step=49, grad_norm=2.952423572540283, loss=10.344792366027832
I0215 20:02:10.151448 140416866297600 logging_writer.py:48] [50] global_step=50, grad_norm=2.971163511276245, loss=10.295612335205078
I0215 20:02:10.515045 140416857904896 logging_writer.py:48] [51] global_step=51, grad_norm=2.8895020484924316, loss=10.289530754089355
I0215 20:02:10.877682 140416866297600 logging_writer.py:48] [52] global_step=52, grad_norm=2.9102678298950195, loss=10.266451835632324
I0215 20:02:11.239555 140416857904896 logging_writer.py:48] [53] global_step=53, grad_norm=2.794691324234009, loss=10.270760536193848
I0215 20:02:11.600379 140416866297600 logging_writer.py:48] [54] global_step=54, grad_norm=2.7937705516815186, loss=10.23681926727295
I0215 20:02:11.959893 140416857904896 logging_writer.py:48] [55] global_step=55, grad_norm=2.752030372619629, loss=10.22407341003418
I0215 20:02:12.322320 140416866297600 logging_writer.py:48] [56] global_step=56, grad_norm=2.6915061473846436, loss=10.203092575073242
I0215 20:02:12.684211 140416857904896 logging_writer.py:48] [57] global_step=57, grad_norm=2.682164192199707, loss=10.168044090270996
I0215 20:02:13.044838 140416866297600 logging_writer.py:48] [58] global_step=58, grad_norm=2.654466390609741, loss=10.152356147766113
I0215 20:02:13.406242 140416857904896 logging_writer.py:48] [59] global_step=59, grad_norm=2.5775303840637207, loss=10.148234367370605
I0215 20:02:13.766506 140416866297600 logging_writer.py:48] [60] global_step=60, grad_norm=2.581069231033325, loss=10.106036186218262
I0215 20:02:14.128759 140416857904896 logging_writer.py:48] [61] global_step=61, grad_norm=2.54775071144104, loss=10.102106094360352
I0215 20:02:14.492440 140416866297600 logging_writer.py:48] [62] global_step=62, grad_norm=2.5148086547851562, loss=10.073683738708496
I0215 20:02:14.854422 140416857904896 logging_writer.py:48] [63] global_step=63, grad_norm=2.411459445953369, loss=10.085186958312988
I0215 20:02:15.220997 140416866297600 logging_writer.py:48] [64] global_step=64, grad_norm=2.4014549255371094, loss=10.039725303649902
I0215 20:02:15.585630 140416857904896 logging_writer.py:48] [65] global_step=65, grad_norm=2.3894236087799072, loss=10.0239839553833
I0215 20:02:15.951112 140416866297600 logging_writer.py:48] [66] global_step=66, grad_norm=2.3695993423461914, loss=9.985462188720703
I0215 20:02:16.314055 140416857904896 logging_writer.py:48] [67] global_step=67, grad_norm=2.3156747817993164, loss=9.968770980834961
I0215 20:02:16.678632 140416866297600 logging_writer.py:48] [68] global_step=68, grad_norm=2.2839932441711426, loss=9.958069801330566
I0215 20:02:17.040710 140416857904896 logging_writer.py:48] [69] global_step=69, grad_norm=2.24833607673645, loss=9.922799110412598
I0215 20:02:17.404580 140416866297600 logging_writer.py:48] [70] global_step=70, grad_norm=2.22050404548645, loss=9.909907341003418
I0215 20:02:17.768155 140416857904896 logging_writer.py:48] [71] global_step=71, grad_norm=2.172231674194336, loss=9.927807807922363
I0215 20:02:18.131608 140416866297600 logging_writer.py:48] [72] global_step=72, grad_norm=2.1380081176757812, loss=9.877346992492676
I0215 20:02:18.494668 140416857904896 logging_writer.py:48] [73] global_step=73, grad_norm=2.1013426780700684, loss=9.860601425170898
I0215 20:02:18.855266 140416866297600 logging_writer.py:48] [74] global_step=74, grad_norm=2.0778448581695557, loss=9.845342636108398
I0215 20:02:19.218765 140416857904896 logging_writer.py:48] [75] global_step=75, grad_norm=2.04010272026062, loss=9.824312210083008
I0215 20:02:19.583738 140416866297600 logging_writer.py:48] [76] global_step=76, grad_norm=2.028883934020996, loss=9.816808700561523
I0215 20:02:19.945558 140416857904896 logging_writer.py:48] [77] global_step=77, grad_norm=2.006837844848633, loss=9.800357818603516
I0215 20:02:20.308707 140416866297600 logging_writer.py:48] [78] global_step=78, grad_norm=1.9575531482696533, loss=9.77678108215332
I0215 20:02:20.670673 140416857904896 logging_writer.py:48] [79] global_step=79, grad_norm=1.9230750799179077, loss=9.774944305419922
I0215 20:02:21.033158 140416866297600 logging_writer.py:48] [80] global_step=80, grad_norm=1.9098165035247803, loss=9.741290092468262
I0215 20:02:21.397740 140416857904896 logging_writer.py:48] [81] global_step=81, grad_norm=1.8908727169036865, loss=9.70908260345459
I0215 20:02:21.763637 140416866297600 logging_writer.py:48] [82] global_step=82, grad_norm=1.8577494621276855, loss=9.710564613342285
I0215 20:02:22.128762 140416857904896 logging_writer.py:48] [83] global_step=83, grad_norm=1.8447339534759521, loss=9.680319786071777
I0215 20:02:22.490810 140416866297600 logging_writer.py:48] [84] global_step=84, grad_norm=1.7991392612457275, loss=9.680480003356934
I0215 20:02:22.853200 140416857904896 logging_writer.py:48] [85] global_step=85, grad_norm=1.7854483127593994, loss=9.65501594543457
I0215 20:02:23.214817 140416866297600 logging_writer.py:48] [86] global_step=86, grad_norm=1.7802163362503052, loss=9.6337890625
I0215 20:02:23.578374 140416857904896 logging_writer.py:48] [87] global_step=87, grad_norm=1.7256300449371338, loss=9.639731407165527
I0215 20:02:23.941079 140416866297600 logging_writer.py:48] [88] global_step=88, grad_norm=1.719315528869629, loss=9.571976661682129
I0215 20:02:24.303115 140416857904896 logging_writer.py:48] [89] global_step=89, grad_norm=1.7156697511672974, loss=9.578374862670898
I0215 20:02:24.664307 140416866297600 logging_writer.py:48] [90] global_step=90, grad_norm=1.6757402420043945, loss=9.583019256591797
I0215 20:02:25.025652 140416857904896 logging_writer.py:48] [91] global_step=91, grad_norm=1.653391718864441, loss=9.576131820678711
I0215 20:02:25.389117 140416866297600 logging_writer.py:48] [92] global_step=92, grad_norm=1.6462777853012085, loss=9.542118072509766
I0215 20:02:25.750762 140416857904896 logging_writer.py:48] [93] global_step=93, grad_norm=1.6298283338546753, loss=9.547289848327637
I0215 20:02:26.111969 140416866297600 logging_writer.py:48] [94] global_step=94, grad_norm=1.5981364250183105, loss=9.523178100585938
I0215 20:02:26.475757 140416857904896 logging_writer.py:48] [95] global_step=95, grad_norm=1.580067753791809, loss=9.511092185974121
I0215 20:02:26.839290 140416866297600 logging_writer.py:48] [96] global_step=96, grad_norm=1.5619007349014282, loss=9.476371765136719
I0215 20:02:27.202121 140416857904896 logging_writer.py:48] [97] global_step=97, grad_norm=1.5200508832931519, loss=9.487408638000488
I0215 20:02:27.564001 140416866297600 logging_writer.py:48] [98] global_step=98, grad_norm=1.5020397901535034, loss=9.459095001220703
I0215 20:02:27.925864 140416857904896 logging_writer.py:48] [99] global_step=99, grad_norm=1.4685198068618774, loss=9.459417343139648
I0215 20:02:28.290469 140416866297600 logging_writer.py:48] [100] global_step=100, grad_norm=1.4657680988311768, loss=9.434697151184082
I0215 20:04:48.668694 140416857904896 logging_writer.py:48] [500] global_step=500, grad_norm=0.11562429368495941, loss=8.345795631408691
I0215 20:07:44.309767 140416866297600 logging_writer.py:48] [1000] global_step=1000, grad_norm=0.15244989097118378, loss=7.680063724517822
I0215 20:10:40.019554 140416857904896 logging_writer.py:48] [1500] global_step=1500, grad_norm=0.19292870163917542, loss=6.91239070892334
I0215 20:13:35.979223 140416866297600 logging_writer.py:48] [2000] global_step=2000, grad_norm=0.27837270498275757, loss=6.218329429626465
I0215 20:15:52.061959 140586562139968 spec.py:321] Evaluating on the training split.
I0215 20:15:55.017957 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 20:20:33.885387 140586562139968 spec.py:333] Evaluating on the validation split.
I0215 20:20:36.543147 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 20:25:15.467312 140586562139968 spec.py:349] Evaluating on the test split.
I0215 20:25:18.140422 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 20:29:57.006857 140586562139968 submission_runner.py:408] Time since start: 2592.70s, 	Step: 2389, 	{'train/accuracy': 0.2458128184080124, 'train/loss': 5.745951175689697, 'train/bleu': 2.3478506353623434, 'validation/accuracy': 0.22865183651447296, 'validation/loss': 5.938811302185059, 'validation/bleu': 0.6698880617744746, 'validation/num_examples': 3000, 'test/accuracy': 0.21127186715602875, 'test/loss': 6.220549583435059, 'test/bleu': 0.6373000744320632, 'test/num_examples': 3003, 'score': 879.1433081626892, 'total_duration': 2592.6995985507965, 'accumulated_submission_time': 879.1433081626892, 'accumulated_eval_time': 1713.4505932331085, 'accumulated_logging_time': 0.028407812118530273}
I0215 20:29:57.022917 140416857904896 logging_writer.py:48] [2389] accumulated_eval_time=1713.450593, accumulated_logging_time=0.028408, accumulated_submission_time=879.143308, global_step=2389, preemption_count=0, score=879.143308, test/accuracy=0.211272, test/bleu=0.637300, test/loss=6.220550, test/num_examples=3003, total_duration=2592.699599, train/accuracy=0.245813, train/bleu=2.347851, train/loss=5.745951, validation/accuracy=0.228652, validation/bleu=0.669888, validation/loss=5.938811, validation/num_examples=3000
I0215 20:30:36.283020 140416866297600 logging_writer.py:48] [2500] global_step=2500, grad_norm=0.31648585200309753, loss=5.763950824737549
I0215 20:33:32.060547 140416857904896 logging_writer.py:48] [3000] global_step=3000, grad_norm=0.3611472547054291, loss=5.412065029144287
I0215 20:36:27.854705 140416866297600 logging_writer.py:48] [3500] global_step=3500, grad_norm=0.43724125623703003, loss=5.007054328918457
I0215 20:39:23.616873 140416857904896 logging_writer.py:48] [4000] global_step=4000, grad_norm=0.4794255793094635, loss=4.807987213134766
I0215 20:42:19.359572 140416866297600 logging_writer.py:48] [4500] global_step=4500, grad_norm=0.520744264125824, loss=4.4662251472473145
I0215 20:43:57.117227 140586562139968 spec.py:321] Evaluating on the training split.
I0215 20:44:00.076139 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 20:48:39.347575 140586562139968 spec.py:333] Evaluating on the validation split.
I0215 20:48:42.012769 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 20:53:20.074408 140586562139968 spec.py:349] Evaluating on the test split.
I0215 20:53:22.735909 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 20:58:01.293252 140586562139968 submission_runner.py:408] Time since start: 4276.99s, 	Step: 4780, 	{'train/accuracy': 0.3771827220916748, 'train/loss': 4.170366287231445, 'train/bleu': 7.110246717048642, 'validation/accuracy': 0.3570321500301361, 'validation/loss': 4.344737529754639, 'validation/bleu': 3.82068771895165, 'validation/num_examples': 3000, 'test/accuracy': 0.33905062079429626, 'test/loss': 4.592438220977783, 'test/bleu': 2.4781251846158154, 'test/num_examples': 3003, 'score': 1719.1542096138, 'total_duration': 4276.986010313034, 'accumulated_submission_time': 1719.1542096138, 'accumulated_eval_time': 2557.6265757083893, 'accumulated_logging_time': 0.05470848083496094}
I0215 20:58:01.308220 140416857904896 logging_writer.py:48] [4780] accumulated_eval_time=2557.626576, accumulated_logging_time=0.054708, accumulated_submission_time=1719.154210, global_step=4780, preemption_count=0, score=1719.154210, test/accuracy=0.339051, test/bleu=2.478125, test/loss=4.592438, test/num_examples=3003, total_duration=4276.986010, train/accuracy=0.377183, train/bleu=7.110247, train/loss=4.170366, validation/accuracy=0.357032, validation/bleu=3.820688, validation/loss=4.344738, validation/num_examples=3000
I0215 20:59:18.832795 140416866297600 logging_writer.py:48] [5000] global_step=5000, grad_norm=0.6039610505104065, loss=4.167059421539307
I0215 21:02:14.593908 140416857904896 logging_writer.py:48] [5500] global_step=5500, grad_norm=0.6070044636726379, loss=3.8488991260528564
I0215 21:05:10.465983 140416866297600 logging_writer.py:48] [6000] global_step=6000, grad_norm=0.6612117290496826, loss=3.702310085296631
I0215 21:08:06.182579 140416857904896 logging_writer.py:48] [6500] global_step=6500, grad_norm=0.5837352275848389, loss=3.446981906890869
I0215 21:11:01.956774 140416866297600 logging_writer.py:48] [7000] global_step=7000, grad_norm=0.5847938656806946, loss=3.252411365509033
I0215 21:12:01.427638 140586562139968 spec.py:321] Evaluating on the training split.
I0215 21:12:04.371624 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 21:16:44.657297 140586562139968 spec.py:333] Evaluating on the validation split.
I0215 21:16:47.331380 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 21:21:26.299768 140586562139968 spec.py:349] Evaluating on the test split.
I0215 21:21:28.959974 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 21:26:07.728077 140586562139968 submission_runner.py:408] Time since start: 5963.42s, 	Step: 7171, 	{'train/accuracy': 0.5011522769927979, 'train/loss': 3.0411250591278076, 'train/bleu': 7.239529476871624, 'validation/accuracy': 0.49081847071647644, 'validation/loss': 3.1386725902557373, 'validation/bleu': 3.8575186767326137, 'validation/num_examples': 3000, 'test/accuracy': 0.48685145378112793, 'test/loss': 3.223426342010498, 'test/bleu': 2.8498597498391947, 'test/num_examples': 3003, 'score': 2559.1901428699493, 'total_duration': 5963.420837640762, 'accumulated_submission_time': 2559.1901428699493, 'accumulated_eval_time': 3403.926973104477, 'accumulated_logging_time': 0.08030128479003906}
I0215 21:26:07.744429 140416857904896 logging_writer.py:48] [7171] accumulated_eval_time=3403.926973, accumulated_logging_time=0.080301, accumulated_submission_time=2559.190143, global_step=7171, preemption_count=0, score=2559.190143, test/accuracy=0.486851, test/bleu=2.849860, test/loss=3.223426, test/num_examples=3003, total_duration=5963.420838, train/accuracy=0.501152, train/bleu=7.239529, train/loss=3.041125, validation/accuracy=0.490818, validation/bleu=3.857519, validation/loss=3.138673, validation/num_examples=3000
I0215 21:28:03.501621 140416866297600 logging_writer.py:48] [7500] global_step=7500, grad_norm=0.5983092188835144, loss=3.0609986782073975
I0215 21:30:59.253992 140416857904896 logging_writer.py:48] [8000] global_step=8000, grad_norm=0.5682796835899353, loss=3.075010061264038
I0215 21:33:55.070607 140416866297600 logging_writer.py:48] [8500] global_step=8500, grad_norm=0.5549460053443909, loss=2.903238534927368
I0215 21:36:50.946302 140416857904896 logging_writer.py:48] [9000] global_step=9000, grad_norm=0.5042887330055237, loss=2.8101868629455566
I0215 21:39:46.785751 140416866297600 logging_writer.py:48] [9500] global_step=9500, grad_norm=0.505037248134613, loss=2.6173911094665527
I0215 21:40:07.962734 140586562139968 spec.py:321] Evaluating on the training split.
I0215 21:40:10.906260 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 21:44:50.622394 140586562139968 spec.py:333] Evaluating on the validation split.
I0215 21:44:53.271076 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 21:49:33.786134 140586562139968 spec.py:349] Evaluating on the test split.
I0215 21:49:36.445986 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 21:54:16.921200 140586562139968 submission_runner.py:408] Time since start: 7652.61s, 	Step: 9562, 	{'train/accuracy': 0.5441297888755798, 'train/loss': 2.622191905975342, 'train/bleu': 4.742225935986713, 'validation/accuracy': 0.5483502745628357, 'validation/loss': 2.5766243934631348, 'validation/bleu': 2.2991891435125753, 'validation/num_examples': 3000, 'test/accuracy': 0.5496717095375061, 'test/loss': 2.597043991088867, 'test/bleu': 1.3319131884672153, 'test/num_examples': 3003, 'score': 3399.323217153549, 'total_duration': 7652.61395573616, 'accumulated_submission_time': 3399.323217153549, 'accumulated_eval_time': 4252.885382413864, 'accumulated_logging_time': 0.10583043098449707}
I0215 21:54:16.936963 140416857904896 logging_writer.py:48] [9562] accumulated_eval_time=4252.885382, accumulated_logging_time=0.105830, accumulated_submission_time=3399.323217, global_step=9562, preemption_count=0, score=3399.323217, test/accuracy=0.549672, test/bleu=1.331913, test/loss=2.597044, test/num_examples=3003, total_duration=7652.613956, train/accuracy=0.544130, train/bleu=4.742226, train/loss=2.622192, validation/accuracy=0.548350, validation/bleu=2.299189, validation/loss=2.576624, validation/num_examples=3000
I0215 21:56:51.083309 140416866297600 logging_writer.py:48] [10000] global_step=10000, grad_norm=0.4830152094364166, loss=2.578803300857544
I0215 21:59:46.696945 140416857904896 logging_writer.py:48] [10500] global_step=10500, grad_norm=0.4648938775062561, loss=2.6675033569335938
I0215 22:02:42.327465 140416866297600 logging_writer.py:48] [11000] global_step=11000, grad_norm=0.43232235312461853, loss=2.61609148979187
I0215 22:05:38.047809 140416857904896 logging_writer.py:48] [11500] global_step=11500, grad_norm=0.4538624882698059, loss=2.5438430309295654
I0215 22:08:17.261163 140586562139968 spec.py:321] Evaluating on the training split.
I0215 22:08:20.207421 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 22:12:59.858842 140586562139968 spec.py:333] Evaluating on the validation split.
I0215 22:13:02.518026 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 22:17:41.654381 140586562139968 spec.py:349] Evaluating on the test split.
I0215 22:17:44.301117 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 22:22:23.418312 140586562139968 submission_runner.py:408] Time since start: 9339.11s, 	Step: 11955, 	{'train/accuracy': 0.5713308453559875, 'train/loss': 2.3595962524414062, 'train/bleu': 3.0603431391387312, 'validation/accuracy': 0.5816543102264404, 'validation/loss': 2.272934675216675, 'validation/bleu': 1.5253956424916264, 'validation/num_examples': 3000, 'test/accuracy': 0.5853117108345032, 'test/loss': 2.2633817195892334, 'test/bleu': 1.0750510952926602, 'test/num_examples': 3003, 'score': 4239.564382314682, 'total_duration': 9339.111065149307, 'accumulated_submission_time': 4239.564382314682, 'accumulated_eval_time': 5099.042484998703, 'accumulated_logging_time': 0.13095474243164062}
I0215 22:22:23.434332 140416866297600 logging_writer.py:48] [11955] accumulated_eval_time=5099.042485, accumulated_logging_time=0.130955, accumulated_submission_time=4239.564382, global_step=11955, preemption_count=0, score=4239.564382, test/accuracy=0.585312, test/bleu=1.075051, test/loss=2.263382, test/num_examples=3003, total_duration=9339.111065, train/accuracy=0.571331, train/bleu=3.060343, train/loss=2.359596, validation/accuracy=0.581654, validation/bleu=1.525396, validation/loss=2.272935, validation/num_examples=3000
I0215 22:22:39.595283 140416857904896 logging_writer.py:48] [12000] global_step=12000, grad_norm=0.42368459701538086, loss=2.5290215015411377
I0215 22:25:35.319708 140416866297600 logging_writer.py:48] [12500] global_step=12500, grad_norm=0.4139147698879242, loss=2.4778900146484375
I0215 22:28:30.909466 140416857904896 logging_writer.py:48] [13000] global_step=13000, grad_norm=0.4100326895713806, loss=2.3835370540618896
I0215 22:31:26.615207 140416866297600 logging_writer.py:48] [13500] global_step=13500, grad_norm=0.3661050796508789, loss=2.345647096633911
I0215 22:34:22.241993 140416857904896 logging_writer.py:48] [14000] global_step=14000, grad_norm=0.3786434233188629, loss=2.3616130352020264
I0215 22:36:23.576092 140586562139968 spec.py:321] Evaluating on the training split.
I0215 22:36:26.508370 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 22:41:05.503464 140586562139968 spec.py:333] Evaluating on the validation split.
I0215 22:41:08.159726 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 22:45:46.737876 140586562139968 spec.py:349] Evaluating on the test split.
I0215 22:45:49.389270 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 22:50:28.254988 140586562139968 submission_runner.py:408] Time since start: 11023.95s, 	Step: 14347, 	{'train/accuracy': 0.592117965221405, 'train/loss': 2.192519187927246, 'train/bleu': 2.572998072339457, 'validation/accuracy': 0.6020879745483398, 'validation/loss': 2.1017370223999023, 'validation/bleu': 1.3006765295304064, 'validation/num_examples': 3000, 'test/accuracy': 0.6065539717674255, 'test/loss': 2.06955623626709, 'test/bleu': 0.7895075148902687, 'test/num_examples': 3003, 'score': 5079.616499185562, 'total_duration': 11023.947231054306, 'accumulated_submission_time': 5079.616499185562, 'accumulated_eval_time': 5943.72082233429, 'accumulated_logging_time': 0.1576526165008545}
I0215 22:50:28.352143 140416866297600 logging_writer.py:48] [14347] accumulated_eval_time=5943.720822, accumulated_logging_time=0.157653, accumulated_submission_time=5079.616499, global_step=14347, preemption_count=0, score=5079.616499, test/accuracy=0.606554, test/bleu=0.789508, test/loss=2.069556, test/num_examples=3003, total_duration=11023.947231, train/accuracy=0.592118, train/bleu=2.572998, train/loss=2.192519, validation/accuracy=0.602088, validation/bleu=1.300677, validation/loss=2.101737, validation/num_examples=3000
I0215 22:51:22.381941 140416857904896 logging_writer.py:48] [14500] global_step=14500, grad_norm=0.34899547696113586, loss=2.366156578063965
I0215 22:54:17.928612 140416866297600 logging_writer.py:48] [15000] global_step=15000, grad_norm=0.36499375104904175, loss=2.206822633743286
I0215 22:57:13.509047 140416857904896 logging_writer.py:48] [15500] global_step=15500, grad_norm=0.3441821038722992, loss=2.288071393966675
I0215 23:00:09.134428 140416866297600 logging_writer.py:48] [16000] global_step=16000, grad_norm=0.3331279158592224, loss=2.26568341255188
I0215 23:03:04.707585 140416857904896 logging_writer.py:48] [16500] global_step=16500, grad_norm=0.3302876651287079, loss=2.239633798599243
I0215 23:04:28.343232 140586562139968 spec.py:321] Evaluating on the training split.
I0215 23:04:31.286973 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 23:09:10.784669 140586562139968 spec.py:333] Evaluating on the validation split.
I0215 23:09:13.431656 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 23:13:52.404087 140586562139968 spec.py:349] Evaluating on the test split.
I0215 23:13:55.057037 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 23:18:33.995556 140586562139968 submission_runner.py:408] Time since start: 12709.69s, 	Step: 16740, 	{'train/accuracy': 0.6027103662490845, 'train/loss': 2.089191198348999, 'train/bleu': 1.909251600344247, 'validation/accuracy': 0.6182564496994019, 'validation/loss': 1.9873697757720947, 'validation/bleu': 0.8648089967580966, 'validation/num_examples': 3000, 'test/accuracy': 0.6199872493743896, 'test/loss': 1.9505449533462524, 'test/bleu': 0.551373965315805, 'test/num_examples': 3003, 'score': 5919.531648874283, 'total_duration': 12709.68831896782, 'accumulated_submission_time': 5919.531648874283, 'accumulated_eval_time': 6789.37309551239, 'accumulated_logging_time': 0.26473236083984375}
I0215 23:18:34.011393 140416866297600 logging_writer.py:48] [16740] accumulated_eval_time=6789.373096, accumulated_logging_time=0.264732, accumulated_submission_time=5919.531649, global_step=16740, preemption_count=0, score=5919.531649, test/accuracy=0.619987, test/bleu=0.551374, test/loss=1.950545, test/num_examples=3003, total_duration=12709.688319, train/accuracy=0.602710, train/bleu=1.909252, train/loss=2.089191, validation/accuracy=0.618256, validation/bleu=0.864809, validation/loss=1.987370, validation/num_examples=3000
I0215 23:20:05.566188 140416857904896 logging_writer.py:48] [17000] global_step=17000, grad_norm=0.3176268935203552, loss=2.3436779975891113
I0215 23:23:01.155817 140416866297600 logging_writer.py:48] [17500] global_step=17500, grad_norm=0.3185454308986664, loss=2.243440628051758
I0215 23:25:56.733142 140416857904896 logging_writer.py:48] [18000] global_step=18000, grad_norm=0.298694908618927, loss=2.127631664276123
I0215 23:28:52.289579 140416866297600 logging_writer.py:48] [18500] global_step=18500, grad_norm=0.3080468475818634, loss=2.282796621322632
I0215 23:31:47.882473 140416857904896 logging_writer.py:48] [19000] global_step=19000, grad_norm=0.2965356707572937, loss=2.0426247119903564
I0215 23:32:34.309203 140586562139968 spec.py:321] Evaluating on the training split.
I0215 23:32:37.255765 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 23:37:16.928231 140586562139968 spec.py:333] Evaluating on the validation split.
I0215 23:37:19.585075 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 23:41:58.582222 140586562139968 spec.py:349] Evaluating on the test split.
I0215 23:42:01.240472 140586562139968 workload.py:181] Translating evaluation dataset.
I0215 23:46:40.705800 140586562139968 submission_runner.py:408] Time since start: 14396.40s, 	Step: 19134, 	{'train/accuracy': 0.6255302429199219, 'train/loss': 1.9116209745407104, 'train/bleu': 1.7755983770221007, 'validation/accuracy': 0.6259934902191162, 'validation/loss': 1.9125136137008667, 'validation/bleu': 0.6522328774810654, 'validation/num_examples': 3000, 'test/accuracy': 0.6307477951049805, 'test/loss': 1.8631607294082642, 'test/bleu': 0.4865417445374533, 'test/num_examples': 3003, 'score': 6759.755045175552, 'total_duration': 14396.398558139801, 'accumulated_submission_time': 6759.755045175552, 'accumulated_eval_time': 7635.769635438919, 'accumulated_logging_time': 0.28987789154052734}
I0215 23:46:40.721806 140416866297600 logging_writer.py:48] [19134] accumulated_eval_time=7635.769635, accumulated_logging_time=0.289878, accumulated_submission_time=6759.755045, global_step=19134, preemption_count=0, score=6759.755045, test/accuracy=0.630748, test/bleu=0.486542, test/loss=1.863161, test/num_examples=3003, total_duration=14396.398558, train/accuracy=0.625530, train/bleu=1.775598, train/loss=1.911621, validation/accuracy=0.625993, validation/bleu=0.652233, validation/loss=1.912514, validation/num_examples=3000
I0215 23:48:49.581415 140416857904896 logging_writer.py:48] [19500] global_step=19500, grad_norm=0.28421032428741455, loss=2.0950980186462402
I0215 23:51:45.156279 140416866297600 logging_writer.py:48] [20000] global_step=20000, grad_norm=0.2838251292705536, loss=2.100717306137085
I0215 23:54:40.666532 140416857904896 logging_writer.py:48] [20500] global_step=20500, grad_norm=0.2965228855609894, loss=2.0733234882354736
I0215 23:57:36.263573 140416866297600 logging_writer.py:48] [21000] global_step=21000, grad_norm=0.28629791736602783, loss=2.0694398880004883
I0216 00:00:31.835945 140416857904896 logging_writer.py:48] [21500] global_step=21500, grad_norm=0.26728588342666626, loss=1.965911865234375
I0216 00:00:41.039994 140586562139968 spec.py:321] Evaluating on the training split.
I0216 00:00:43.987578 140586562139968 workload.py:181] Translating evaluation dataset.
I0216 00:05:23.533301 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 00:05:26.177865 140586562139968 workload.py:181] Translating evaluation dataset.
I0216 00:10:05.165255 140586562139968 spec.py:349] Evaluating on the test split.
I0216 00:10:07.816969 140586562139968 workload.py:181] Translating evaluation dataset.
I0216 00:14:46.789535 140586562139968 submission_runner.py:408] Time since start: 16082.48s, 	Step: 21528, 	{'train/accuracy': 0.6189650893211365, 'train/loss': 1.9530251026153564, 'train/bleu': 1.5233909675692299, 'validation/accuracy': 0.6333337426185608, 'validation/loss': 1.8469154834747314, 'validation/bleu': 0.6503184151196942, 'validation/num_examples': 3000, 'test/accuracy': 0.6401371359825134, 'test/loss': 1.798127293586731, 'test/bleu': 0.43692168087954864, 'test/num_examples': 3003, 'score': 7599.916054487228, 'total_duration': 16082.482307434082, 'accumulated_submission_time': 7599.916054487228, 'accumulated_eval_time': 8481.519130945206, 'accumulated_logging_time': 0.39734649658203125}
I0216 00:14:46.805665 140416866297600 logging_writer.py:48] [21528] accumulated_eval_time=8481.519131, accumulated_logging_time=0.397346, accumulated_submission_time=7599.916054, global_step=21528, preemption_count=0, score=7599.916054, test/accuracy=0.640137, test/bleu=0.436922, test/loss=1.798127, test/num_examples=3003, total_duration=16082.482307, train/accuracy=0.618965, train/bleu=1.523391, train/loss=1.953025, validation/accuracy=0.633334, validation/bleu=0.650318, validation/loss=1.846915, validation/num_examples=3000
I0216 00:17:32.781037 140416857904896 logging_writer.py:48] [22000] global_step=22000, grad_norm=0.2678459584712982, loss=2.079277992248535
I0216 00:20:28.371260 140416866297600 logging_writer.py:48] [22500] global_step=22500, grad_norm=0.2667364180088043, loss=1.9943740367889404
I0216 00:23:23.948926 140416857904896 logging_writer.py:48] [23000] global_step=23000, grad_norm=0.26274406909942627, loss=2.076751470565796
I0216 00:26:19.542057 140416866297600 logging_writer.py:48] [23500] global_step=23500, grad_norm=0.2636207044124603, loss=1.9859846830368042
I0216 00:28:47.089407 140586562139968 spec.py:321] Evaluating on the training split.
I0216 00:28:50.028660 140586562139968 workload.py:181] Translating evaluation dataset.
I0216 00:33:30.697613 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 00:33:33.350601 140586562139968 workload.py:181] Translating evaluation dataset.
I0216 00:38:13.005412 140586562139968 spec.py:349] Evaluating on the test split.
I0216 00:38:15.651861 140586562139968 workload.py:181] Translating evaluation dataset.
I0216 00:42:54.846504 140586562139968 submission_runner.py:408] Time since start: 17770.54s, 	Step: 23922, 	{'train/accuracy': 0.6264045834541321, 'train/loss': 1.9005932807922363, 'train/bleu': 1.8108257407715773, 'validation/accuracy': 0.639446496963501, 'validation/loss': 1.8049089908599854, 'validation/bleu': 0.6060991441563214, 'validation/num_examples': 3000, 'test/accuracy': 0.6456801295280457, 'test/loss': 1.7494745254516602, 'test/bleu': 0.5081726353399283, 'test/num_examples': 3003, 'score': 8440.124709129333, 'total_duration': 17770.539270162582, 'accumulated_submission_time': 8440.124709129333, 'accumulated_eval_time': 9329.276204586029, 'accumulated_logging_time': 0.4225015640258789}
I0216 00:42:54.864600 140416857904896 logging_writer.py:48] [23922] accumulated_eval_time=9329.276205, accumulated_logging_time=0.422502, accumulated_submission_time=8440.124709, global_step=23922, preemption_count=0, score=8440.124709, test/accuracy=0.645680, test/bleu=0.508173, test/loss=1.749475, test/num_examples=3003, total_duration=17770.539270, train/accuracy=0.626405, train/bleu=1.810826, train/loss=1.900593, validation/accuracy=0.639446, validation/bleu=0.606099, validation/loss=1.804909, validation/num_examples=3000
I0216 00:43:22.567158 140416866297600 logging_writer.py:48] [24000] global_step=24000, grad_norm=0.25848689675331116, loss=2.046602249145508
I0216 00:46:18.053157 140416857904896 logging_writer.py:48] [24500] global_step=24500, grad_norm=0.24852970242500305, loss=1.9384090900421143
I0216 00:49:13.645555 140416866297600 logging_writer.py:48] [25000] global_step=25000, grad_norm=0.2638184726238251, loss=2.066319465637207
I0216 00:52:09.294339 140416857904896 logging_writer.py:48] [25500] global_step=25500, grad_norm=0.25171202421188354, loss=2.0091183185577393
I0216 00:55:04.926618 140416866297600 logging_writer.py:48] [26000] global_step=26000, grad_norm=0.25186845660209656, loss=1.9113941192626953
I0216 00:56:54.984630 140586562139968 spec.py:321] Evaluating on the training split.
I0216 00:56:57.921462 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 01:01:36.910974 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 01:01:36.911183 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 01:01:36.911234 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 01:01:37.477886 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 01:01:40.138212 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 01:06:18.236955 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 01:06:18.237165 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 01:06:18.237220 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 01:06:18.887327 140586562139968 spec.py:349] Evaluating on the test split.
I0216 01:06:21.555408 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 01:10:59.985722 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 01:10:59.985982 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 01:10:59.986055 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 01:11:00.774829 140586562139968 submission_runner.py:408] Time since start: 19456.47s, 	Step: 26315, 	{'train/accuracy': 0.6323251724243164, 'train/loss': 1.8344998359680176, 'train/bleu': 1.595450098951098, 'validation/accuracy': 0.6446292996406555, 'validation/loss': 1.7704890966415405, 'validation/bleu': 0.6017523069566243, 'validation/num_examples': 3000, 'test/accuracy': 0.651548445224762, 'test/loss': 1.7074114084243774, 'test/bleu': 0.3858701982181147, 'test/num_examples': 3003, 'score': 9280.168601751328, 'total_duration': 19456.467591762543, 'accumulated_submission_time': 9280.168601751328, 'accumulated_eval_time': 10175.066358089447, 'accumulated_logging_time': 0.44989681243896484}
I0216 01:11:00.790996 140416857904896 logging_writer.py:48] [26315] accumulated_eval_time=10175.066358, accumulated_logging_time=0.449897, accumulated_submission_time=9280.168602, global_step=26315, preemption_count=0, score=9280.168602, test/accuracy=0.651548, test/bleu=0.385870, test/loss=1.707411, test/num_examples=3003, total_duration=19456.467592, train/accuracy=0.632325, train/bleu=1.595450, train/loss=1.834500, validation/accuracy=0.644629, validation/bleu=0.601752, validation/loss=1.770489, validation/num_examples=3000
I0216 01:12:05.996922 140416866297600 logging_writer.py:48] [26500] global_step=26500, grad_norm=0.24164746701717377, loss=1.9870634078979492
I0216 01:15:01.544691 140416857904896 logging_writer.py:48] [27000] global_step=27000, grad_norm=0.2419177144765854, loss=1.9567525386810303
I0216 01:17:57.108103 140416866297600 logging_writer.py:48] [27500] global_step=27500, grad_norm=0.24461603164672852, loss=1.930365800857544
I0216 01:20:52.683310 140416857904896 logging_writer.py:48] [28000] global_step=28000, grad_norm=0.24565957486629486, loss=1.9260164499282837
I0216 01:23:48.280280 140416866297600 logging_writer.py:48] [28500] global_step=28500, grad_norm=0.23576050996780396, loss=1.8711122274398804
I0216 01:25:01.059154 140586562139968 spec.py:321] Evaluating on the training split.
I0216 01:25:04.007185 140586562139968 workload.py:181] Translating evaluation dataset.
I0216 01:29:43.437701 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 01:29:46.093085 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 01:34:24.871619 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 01:34:24.871837 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 01:34:24.871890 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 01:34:25.002332 140586562139968 spec.py:349] Evaluating on the test split.
I0216 01:34:27.663361 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 01:39:06.166579 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 01:39:06.166772 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 01:39:06.166821 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 01:39:06.522259 140586562139968 submission_runner.py:408] Time since start: 21142.22s, 	Step: 28709, 	{'train/accuracy': 0.6365097761154175, 'train/loss': 1.825494647026062, 'train/bleu': 1.4926065213202608, 'validation/accuracy': 0.6482250690460205, 'validation/loss': 1.7388099431991577, 'validation/bleu': 0.6741754080099109, 'validation/num_examples': 3000, 'test/accuracy': 0.6547789573669434, 'test/loss': 1.675727367401123, 'test/bleu': 0.4481941289102528, 'test/num_examples': 3003, 'score': 10120.359531641006, 'total_duration': 21142.21505522728, 'accumulated_submission_time': 10120.359531641006, 'accumulated_eval_time': 11020.529445886612, 'accumulated_logging_time': 0.4766552448272705}
I0216 01:39:06.538284 140416857904896 logging_writer.py:48] [28709] accumulated_eval_time=11020.529446, accumulated_logging_time=0.476655, accumulated_submission_time=10120.359532, global_step=28709, preemption_count=0, score=10120.359532, test/accuracy=0.654779, test/bleu=0.448194, test/loss=1.675727, test/num_examples=3003, total_duration=21142.215055, train/accuracy=0.636510, train/bleu=1.492607, train/loss=1.825495, validation/accuracy=0.648225, validation/bleu=0.674175, validation/loss=1.738810, validation/num_examples=3000
I0216 01:40:48.920431 140416866297600 logging_writer.py:48] [29000] global_step=29000, grad_norm=0.24726352095603943, loss=1.941298007965088
I0216 01:43:44.475144 140416857904896 logging_writer.py:48] [29500] global_step=29500, grad_norm=0.23952417075634003, loss=1.9838453531265259
I0216 01:46:40.038985 140416866297600 logging_writer.py:48] [30000] global_step=30000, grad_norm=0.23221448063850403, loss=1.9451261758804321
I0216 01:49:35.589505 140416857904896 logging_writer.py:48] [30500] global_step=30500, grad_norm=0.23496684432029724, loss=1.8978229761123657
I0216 01:52:31.162214 140416866297600 logging_writer.py:48] [31000] global_step=31000, grad_norm=0.22934739291667938, loss=1.8657219409942627
I0216 01:53:06.721665 140586562139968 spec.py:321] Evaluating on the training split.
I0216 01:53:09.654150 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 01:57:48.904052 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 01:57:48.904254 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 01:57:48.904303 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 01:57:49.482603 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 01:57:52.132197 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 02:02:30.287613 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 02:02:30.287811 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 02:02:30.287881 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 02:02:30.956843 140586562139968 spec.py:349] Evaluating on the test split.
I0216 02:02:33.611250 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 02:07:11.849478 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 02:07:11.849680 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 02:07:11.849731 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 02:07:12.569637 140586562139968 submission_runner.py:408] Time since start: 22828.26s, 	Step: 31103, 	{'train/accuracy': 0.6336763501167297, 'train/loss': 1.8281629085540771, 'train/bleu': 1.4566051252849477, 'validation/accuracy': 0.6489565968513489, 'validation/loss': 1.7132289409637451, 'validation/bleu': 0.5816486971273545, 'validation/num_examples': 3000, 'test/accuracy': 0.6585439443588257, 'test/loss': 1.6435586214065552, 'test/bleu': 0.3551857944276261, 'test/num_examples': 3003, 'score': 10960.46658539772, 'total_duration': 22828.262434005737, 'accumulated_submission_time': 10960.46658539772, 'accumulated_eval_time': 11866.377401351929, 'accumulated_logging_time': 0.5028150081634521}
I0216 02:07:12.586611 140416857904896 logging_writer.py:48] [31103] accumulated_eval_time=11866.377401, accumulated_logging_time=0.502815, accumulated_submission_time=10960.466585, global_step=31103, preemption_count=0, score=10960.466585, test/accuracy=0.658544, test/bleu=0.355186, test/loss=1.643559, test/num_examples=3003, total_duration=22828.262434, train/accuracy=0.633676, train/bleu=1.456605, train/loss=1.828163, validation/accuracy=0.648957, validation/bleu=0.581649, validation/loss=1.713229, validation/num_examples=3000
I0216 02:09:32.184758 140416866297600 logging_writer.py:48] [31500] global_step=31500, grad_norm=0.2238949090242386, loss=1.934680461883545
I0216 02:12:27.778139 140416857904896 logging_writer.py:48] [32000] global_step=32000, grad_norm=0.22234022617340088, loss=1.8120832443237305
I0216 02:15:23.309036 140416866297600 logging_writer.py:48] [32500] global_step=32500, grad_norm=0.2369665950536728, loss=1.9529863595962524
I0216 02:18:18.898164 140416857904896 logging_writer.py:48] [33000] global_step=33000, grad_norm=0.2270088791847229, loss=1.9272469282150269
I0216 02:21:12.777292 140586562139968 spec.py:321] Evaluating on the training split.
I0216 02:21:15.730402 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 02:25:54.459682 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 02:25:54.459900 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 02:25:54.459954 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 02:25:55.145422 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 02:25:57.800061 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 02:30:35.861361 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 02:30:35.861560 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 02:30:35.861614 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 02:30:36.294375 140586562139968 spec.py:349] Evaluating on the test split.
I0216 02:30:38.959350 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 02:35:16.953720 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 02:35:16.953926 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 02:35:16.953978 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 02:35:17.565865 140586562139968 submission_runner.py:408] Time since start: 24513.26s, 	Step: 33497, 	{'train/accuracy': 0.6402148604393005, 'train/loss': 1.7716810703277588, 'train/bleu': 1.6260398873301878, 'validation/accuracy': 0.6542014479637146, 'validation/loss': 1.6838712692260742, 'validation/bleu': 0.6654986692691285, 'validation/num_examples': 3000, 'test/accuracy': 0.6640055775642395, 'test/loss': 1.6144667863845825, 'test/bleu': 0.45739866682040153, 'test/num_examples': 3003, 'score': 11800.581804990768, 'total_duration': 24513.25865149498, 'accumulated_submission_time': 11800.581804990768, 'accumulated_eval_time': 12711.165947198868, 'accumulated_logging_time': 0.5292415618896484}
I0216 02:35:17.583443 140416866297600 logging_writer.py:48] [33497] accumulated_eval_time=12711.165947, accumulated_logging_time=0.529242, accumulated_submission_time=11800.581805, global_step=33497, preemption_count=0, score=11800.581805, test/accuracy=0.664006, test/bleu=0.457399, test/loss=1.614467, test/num_examples=3003, total_duration=24513.258651, train/accuracy=0.640215, train/bleu=1.626040, train/loss=1.771681, validation/accuracy=0.654201, validation/bleu=0.665499, validation/loss=1.683871, validation/num_examples=3000
I0216 02:35:19.010612 140416857904896 logging_writer.py:48] [33500] global_step=33500, grad_norm=0.22640813887119293, loss=1.9106913805007935
I0216 02:38:14.363765 140416866297600 logging_writer.py:48] [34000] global_step=34000, grad_norm=0.2351280152797699, loss=1.9415136575698853
I0216 02:41:09.862858 140416857904896 logging_writer.py:48] [34500] global_step=34500, grad_norm=0.22041811048984528, loss=1.777107834815979
I0216 02:44:05.375383 140416866297600 logging_writer.py:48] [35000] global_step=35000, grad_norm=0.22168204188346863, loss=1.839193344116211
I0216 02:47:00.912114 140416857904896 logging_writer.py:48] [35500] global_step=35500, grad_norm=0.24338746070861816, loss=1.8813365697860718
I0216 02:49:17.896219 140586562139968 spec.py:321] Evaluating on the training split.
I0216 02:49:20.831647 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 02:53:59.397562 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 02:53:59.397771 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 02:53:59.397825 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 02:54:00.518635 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 02:54:03.172706 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 02:58:41.399857 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 02:58:41.400071 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 02:58:41.400125 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 02:58:42.446000 140586562139968 spec.py:349] Evaluating on the test split.
I0216 02:58:45.102984 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 03:03:22.622295 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 03:03:22.622501 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 03:03:22.622557 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 03:03:23.719001 140586562139968 submission_runner.py:408] Time since start: 26199.41s, 	Step: 35892, 	{'train/accuracy': 0.6402845978736877, 'train/loss': 1.7787836790084839, 'train/bleu': 1.2516411376263012, 'validation/accuracy': 0.6563217043876648, 'validation/loss': 1.670474886894226, 'validation/bleu': 0.6268425038122815, 'validation/num_examples': 3000, 'test/accuracy': 0.6651095151901245, 'test/loss': 1.5953525304794312, 'test/bleu': 0.44507029163516837, 'test/num_examples': 3003, 'score': 12640.81961107254, 'total_duration': 26199.411789417267, 'accumulated_submission_time': 12640.81961107254, 'accumulated_eval_time': 13556.988714933395, 'accumulated_logging_time': 0.5560722351074219}
I0216 03:03:23.735191 140416866297600 logging_writer.py:48] [35892] accumulated_eval_time=13556.988715, accumulated_logging_time=0.556072, accumulated_submission_time=12640.819611, global_step=35892, preemption_count=0, score=12640.819611, test/accuracy=0.665110, test/bleu=0.445070, test/loss=1.595353, test/num_examples=3003, total_duration=26199.411789, train/accuracy=0.640285, train/bleu=1.251641, train/loss=1.778784, validation/accuracy=0.656322, validation/bleu=0.626843, validation/loss=1.670475, validation/num_examples=3000
I0216 03:04:01.927400 140416857904896 logging_writer.py:48] [36000] global_step=36000, grad_norm=0.2223343551158905, loss=1.8774076700210571
I0216 03:06:57.432541 140416866297600 logging_writer.py:48] [36500] global_step=36500, grad_norm=0.2311968207359314, loss=1.8593852519989014
I0216 03:09:52.959791 140416857904896 logging_writer.py:48] [37000] global_step=37000, grad_norm=0.2140725702047348, loss=1.8034913539886475
I0216 03:12:48.540354 140416866297600 logging_writer.py:48] [37500] global_step=37500, grad_norm=0.22174637019634247, loss=1.858551025390625
I0216 03:15:44.131431 140416857904896 logging_writer.py:48] [38000] global_step=38000, grad_norm=0.21852199733257294, loss=1.9008580446243286
I0216 03:17:23.938355 140586562139968 spec.py:321] Evaluating on the training split.
I0216 03:17:26.877852 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 03:22:06.094742 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 03:22:06.094964 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 03:22:06.095019 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 03:22:06.491490 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 03:22:09.138938 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 03:26:47.408667 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 03:26:47.408873 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 03:26:47.408926 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 03:26:47.729799 140586562139968 spec.py:349] Evaluating on the test split.
I0216 03:26:50.391227 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 03:31:29.576190 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 03:31:29.576385 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 03:31:29.576453 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 03:31:29.700992 140586562139968 submission_runner.py:408] Time since start: 27885.39s, 	Step: 38286, 	{'train/accuracy': 0.6507250070571899, 'train/loss': 1.7041484117507935, 'train/bleu': 1.417769145384822, 'validation/accuracy': 0.6598182320594788, 'validation/loss': 1.6482818126678467, 'validation/bleu': 0.591868313438796, 'validation/num_examples': 3000, 'test/accuracy': 0.6699901223182678, 'test/loss': 1.5757300853729248, 'test/bleu': 0.54473681838998, 'test/num_examples': 3003, 'score': 13480.947542667389, 'total_duration': 27885.39377474785, 'accumulated_submission_time': 13480.947542667389, 'accumulated_eval_time': 14402.75132727623, 'accumulated_logging_time': 0.5813107490539551}
I0216 03:31:29.717502 140416866297600 logging_writer.py:48] [38286] accumulated_eval_time=14402.751327, accumulated_logging_time=0.581311, accumulated_submission_time=13480.947543, global_step=38286, preemption_count=0, score=13480.947543, test/accuracy=0.669990, test/bleu=0.544737, test/loss=1.575730, test/num_examples=3003, total_duration=27885.393775, train/accuracy=0.650725, train/bleu=1.417769, train/loss=1.704148, validation/accuracy=0.659818, validation/bleu=0.591868, validation/loss=1.648282, validation/num_examples=3000
I0216 03:32:45.100138 140416857904896 logging_writer.py:48] [38500] global_step=38500, grad_norm=0.22059842944145203, loss=1.7850377559661865
I0216 03:35:40.593170 140416866297600 logging_writer.py:48] [39000] global_step=39000, grad_norm=0.21353569626808167, loss=1.8768092393875122
I0216 03:38:36.139425 140416857904896 logging_writer.py:48] [39500] global_step=39500, grad_norm=0.21784983575344086, loss=1.8009846210479736
I0216 03:41:31.666410 140416866297600 logging_writer.py:48] [40000] global_step=40000, grad_norm=0.22225195169448853, loss=1.7726792097091675
I0216 03:44:27.174192 140416857904896 logging_writer.py:48] [40500] global_step=40500, grad_norm=0.2189539074897766, loss=1.8496915102005005
I0216 03:45:29.729250 140586562139968 spec.py:321] Evaluating on the training split.
I0216 03:45:32.672104 140586562139968 workload.py:181] Translating evaluation dataset.
I0216 03:50:12.081052 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 03:50:14.733893 140586562139968 workload.py:181] Translating evaluation dataset.
I0216 03:54:54.722153 140586562139968 spec.py:349] Evaluating on the test split.
I0216 03:54:57.377387 140586562139968 workload.py:181] Translating evaluation dataset.
I0216 03:59:37.163818 140586562139968 submission_runner.py:408] Time since start: 29572.86s, 	Step: 40680, 	{'train/accuracy': 0.6515622138977051, 'train/loss': 1.694055438041687, 'train/bleu': 1.5096183992532668, 'validation/accuracy': 0.6602646112442017, 'validation/loss': 1.635361909866333, 'validation/bleu': 0.77385307958933, 'validation/num_examples': 3000, 'test/accuracy': 0.6713381409645081, 'test/loss': 1.5545804500579834, 'test/bleu': 0.603416237450704, 'test/num_examples': 3003, 'score': 14320.882312297821, 'total_duration': 29572.85657787323, 'accumulated_submission_time': 14320.882312297821, 'accumulated_eval_time': 15250.18584060669, 'accumulated_logging_time': 0.6085410118103027}
I0216 03:59:37.181956 140416866297600 logging_writer.py:48] [40680] accumulated_eval_time=15250.185841, accumulated_logging_time=0.608541, accumulated_submission_time=14320.882312, global_step=40680, preemption_count=0, score=14320.882312, test/accuracy=0.671338, test/bleu=0.603416, test/loss=1.554580, test/num_examples=3003, total_duration=29572.856578, train/accuracy=0.651562, train/bleu=1.509618, train/loss=1.694055, validation/accuracy=0.660265, validation/bleu=0.773853, validation/loss=1.635362, validation/num_examples=3000
I0216 04:01:29.715938 140416857904896 logging_writer.py:48] [41000] global_step=41000, grad_norm=0.21722520887851715, loss=1.7818130254745483
I0216 04:04:25.319869 140416866297600 logging_writer.py:48] [41500] global_step=41500, grad_norm=0.21671338379383087, loss=1.7339823246002197
I0216 04:07:20.883289 140416857904896 logging_writer.py:48] [42000] global_step=42000, grad_norm=0.2277085781097412, loss=1.7697196006774902
I0216 04:10:16.412613 140416866297600 logging_writer.py:48] [42500] global_step=42500, grad_norm=0.2219969928264618, loss=1.8408780097961426
I0216 04:13:11.924645 140416857904896 logging_writer.py:48] [43000] global_step=43000, grad_norm=0.22056390345096588, loss=1.8429163694381714
I0216 04:13:37.267958 140586562139968 spec.py:321] Evaluating on the training split.
I0216 04:13:40.211347 140586562139968 workload.py:181] Translating evaluation dataset.
I0216 04:18:20.723798 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 04:18:23.389846 140586562139968 workload.py:181] Translating evaluation dataset.
I0216 04:23:02.537150 140586562139968 spec.py:349] Evaluating on the test split.
I0216 04:23:05.208294 140586562139968 workload.py:181] Translating evaluation dataset.
I0216 04:27:44.415737 140586562139968 submission_runner.py:408] Time since start: 31260.11s, 	Step: 43074, 	{'train/accuracy': 0.6454535722732544, 'train/loss': 1.737009048461914, 'train/bleu': 1.26618241468236, 'validation/accuracy': 0.6624964475631714, 'validation/loss': 1.6190532445907593, 'validation/bleu': 0.5638200000200458, 'validation/num_examples': 3000, 'test/accuracy': 0.672930121421814, 'test/loss': 1.540385365486145, 'test/bleu': 0.5401706488289604, 'test/num_examples': 3003, 'score': 15160.891214609146, 'total_duration': 31260.10849881172, 'accumulated_submission_time': 15160.891214609146, 'accumulated_eval_time': 16097.333564043045, 'accumulated_logging_time': 0.6381494998931885}
I0216 04:27:44.432916 140416866297600 logging_writer.py:48] [43074] accumulated_eval_time=16097.333564, accumulated_logging_time=0.638149, accumulated_submission_time=15160.891215, global_step=43074, preemption_count=0, score=15160.891215, test/accuracy=0.672930, test/bleu=0.540171, test/loss=1.540385, test/num_examples=3003, total_duration=31260.108499, train/accuracy=0.645454, train/bleu=1.266182, train/loss=1.737009, validation/accuracy=0.662496, validation/bleu=0.563820, validation/loss=1.619053, validation/num_examples=3000
I0216 04:30:14.170504 140416857904896 logging_writer.py:48] [43500] global_step=43500, grad_norm=0.21666763722896576, loss=1.8205599784851074
I0216 04:33:09.678755 140416866297600 logging_writer.py:48] [44000] global_step=44000, grad_norm=0.2133830487728119, loss=1.7502615451812744
I0216 04:36:05.128732 140416857904896 logging_writer.py:48] [44500] global_step=44500, grad_norm=0.22073985636234283, loss=1.7959794998168945
I0216 04:39:00.663629 140416866297600 logging_writer.py:48] [45000] global_step=45000, grad_norm=0.2138584852218628, loss=1.747826099395752
I0216 04:41:44.668392 140586562139968 spec.py:321] Evaluating on the training split.
I0216 04:41:47.604907 140586562139968 workload.py:181] Translating evaluation dataset.
I0216 04:46:27.688051 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 04:46:30.342259 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 04:51:10.376193 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 04:51:10.376437 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 04:51:10.376500 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 04:51:10.559207 140586562139968 spec.py:349] Evaluating on the test split.
I0216 04:51:13.217153 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 04:55:52.117593 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 04:55:52.117799 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 04:55:52.117852 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 04:55:52.645276 140586562139968 submission_runner.py:408] Time since start: 32948.34s, 	Step: 45469, 	{'train/accuracy': 0.6534406542778015, 'train/loss': 1.6706839799880981, 'train/bleu': 1.2442829359955467, 'validation/accuracy': 0.6639347076416016, 'validation/loss': 1.6081516742706299, 'validation/bleu': 0.7162678380131551, 'validation/num_examples': 3000, 'test/accuracy': 0.6749172210693359, 'test/loss': 1.5294806957244873, 'test/bleu': 0.5581449950107031, 'test/num_examples': 3003, 'score': 16001.05147242546, 'total_duration': 32948.33805012703, 'accumulated_submission_time': 16001.05147242546, 'accumulated_eval_time': 16945.31041121483, 'accumulated_logging_time': 0.6646959781646729}
I0216 04:55:52.662231 140416857904896 logging_writer.py:48] [45469] accumulated_eval_time=16945.310411, accumulated_logging_time=0.664696, accumulated_submission_time=16001.051472, global_step=45469, preemption_count=0, score=16001.051472, test/accuracy=0.674917, test/bleu=0.558145, test/loss=1.529481, test/num_examples=3003, total_duration=32948.338050, train/accuracy=0.653441, train/bleu=1.244283, train/loss=1.670684, validation/accuracy=0.663935, validation/bleu=0.716268, validation/loss=1.608152, validation/num_examples=3000
I0216 04:56:03.893648 140416866297600 logging_writer.py:48] [45500] global_step=45500, grad_norm=0.2079055905342102, loss=1.7897286415100098
I0216 04:58:59.378987 140416857904896 logging_writer.py:48] [46000] global_step=46000, grad_norm=0.22125840187072754, loss=1.7418488264083862
I0216 05:01:54.908088 140416866297600 logging_writer.py:48] [46500] global_step=46500, grad_norm=0.2281874418258667, loss=1.8032584190368652
I0216 05:04:50.488899 140416857904896 logging_writer.py:48] [47000] global_step=47000, grad_norm=0.21821719408035278, loss=1.7130242586135864
I0216 05:07:45.994467 140416866297600 logging_writer.py:48] [47500] global_step=47500, grad_norm=0.21875862777233124, loss=1.727542757987976
I0216 05:09:52.796905 140586562139968 spec.py:321] Evaluating on the training split.
I0216 05:09:55.735938 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 05:14:34.409106 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 05:14:34.409301 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 05:14:34.409367 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 05:14:35.384964 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 05:14:38.028146 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 05:19:16.387719 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 05:19:16.387954 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 05:19:16.388009 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 05:19:17.477215 140586562139968 spec.py:349] Evaluating on the test split.
I0216 05:19:20.144752 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 05:23:58.540801 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 05:23:58.541004 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 05:23:58.541052 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 05:23:59.709731 140586562139968 submission_runner.py:408] Time since start: 34635.40s, 	Step: 47863, 	{'train/accuracy': 0.6578680276870728, 'train/loss': 1.656836748123169, 'train/bleu': 1.0303258298156999, 'validation/accuracy': 0.6661293506622314, 'validation/loss': 1.5941377878189087, 'validation/bleu': 0.41895513156296066, 'validation/num_examples': 3000, 'test/accuracy': 0.6771367192268372, 'test/loss': 1.5114972591400146, 'test/bleu': 0.407981647858489, 'test/num_examples': 3003, 'score': 16841.11019229889, 'total_duration': 34635.40250277519, 'accumulated_submission_time': 16841.11019229889, 'accumulated_eval_time': 17792.223201036453, 'accumulated_logging_time': 0.6907575130462646}
I0216 05:23:59.726954 140416857904896 logging_writer.py:48] [47863] accumulated_eval_time=17792.223201, accumulated_logging_time=0.690758, accumulated_submission_time=16841.110192, global_step=47863, preemption_count=0, score=16841.110192, test/accuracy=0.677137, test/bleu=0.407982, test/loss=1.511497, test/num_examples=3003, total_duration=34635.402503, train/accuracy=0.657868, train/bleu=1.030326, train/loss=1.656837, validation/accuracy=0.666129, validation/bleu=0.418955, validation/loss=1.594138, validation/num_examples=3000
I0216 05:24:48.051665 140416866297600 logging_writer.py:48] [48000] global_step=48000, grad_norm=0.2188626080751419, loss=1.7365771532058716
I0216 05:27:43.469547 140416857904896 logging_writer.py:48] [48500] global_step=48500, grad_norm=0.21819715201854706, loss=1.7837893962860107
I0216 05:30:38.924978 140416866297600 logging_writer.py:48] [49000] global_step=49000, grad_norm=0.21302814781665802, loss=1.784960389137268
I0216 05:33:34.394443 140416857904896 logging_writer.py:48] [49500] global_step=49500, grad_norm=0.21612584590911865, loss=1.7861747741699219
I0216 05:36:29.913984 140416866297600 logging_writer.py:48] [50000] global_step=50000, grad_norm=0.22828060388565063, loss=1.6846369504928589
I0216 05:37:59.888956 140586562139968 spec.py:321] Evaluating on the training split.
I0216 05:38:02.835044 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 05:42:42.108210 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 05:42:42.108419 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 05:42:42.108474 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 05:42:42.869042 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 05:42:45.519674 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 05:47:23.556064 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 05:47:23.556281 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 05:47:23.556336 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 05:47:24.476113 140586562139968 spec.py:349] Evaluating on the test split.
I0216 05:47:27.138417 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 05:52:05.594020 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 05:52:05.594215 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 05:52:05.594283 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 05:52:06.630575 140586562139968 submission_runner.py:408] Time since start: 36322.32s, 	Step: 50258, 	{'train/accuracy': 0.6672159433364868, 'train/loss': 1.5876699686050415, 'train/bleu': 1.1638583005517393, 'validation/accuracy': 0.6682124137878418, 'validation/loss': 1.5849769115447998, 'validation/bleu': 0.5235669011807379, 'validation/num_examples': 3000, 'test/accuracy': 0.6785892844200134, 'test/loss': 1.5016307830810547, 'test/bleu': 0.4389196144382269, 'test/num_examples': 3003, 'score': 17681.194860458374, 'total_duration': 36322.323357105255, 'accumulated_submission_time': 17681.194860458374, 'accumulated_eval_time': 18638.96478843689, 'accumulated_logging_time': 0.719064474105835}
I0216 05:52:06.647067 140416857904896 logging_writer.py:48] [50258] accumulated_eval_time=18638.964788, accumulated_logging_time=0.719064, accumulated_submission_time=17681.194860, global_step=50258, preemption_count=0, score=17681.194860, test/accuracy=0.678589, test/bleu=0.438920, test/loss=1.501631, test/num_examples=3003, total_duration=36322.323357, train/accuracy=0.667216, train/bleu=1.163858, train/loss=1.587670, validation/accuracy=0.668212, validation/bleu=0.523567, validation/loss=1.584977, validation/num_examples=3000
I0216 05:53:31.817755 140416866297600 logging_writer.py:48] [50500] global_step=50500, grad_norm=0.22047051787376404, loss=1.7140569686889648
I0216 05:56:27.277439 140416857904896 logging_writer.py:48] [51000] global_step=51000, grad_norm=0.22448192536830902, loss=1.7600595951080322
I0216 05:59:22.746250 140416866297600 logging_writer.py:48] [51500] global_step=51500, grad_norm=0.21417738497257233, loss=1.693495512008667
I0216 06:02:18.270042 140416857904896 logging_writer.py:48] [52000] global_step=52000, grad_norm=0.2256840616464615, loss=1.7336516380310059
I0216 06:05:13.781143 140416866297600 logging_writer.py:48] [52500] global_step=52500, grad_norm=0.22004400193691254, loss=1.7207084894180298
I0216 06:06:06.863334 140586562139968 spec.py:321] Evaluating on the training split.
I0216 06:06:09.800744 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 06:10:48.656131 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 06:10:48.656326 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 06:10:48.656374 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 06:10:49.843419 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 06:10:52.500620 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 06:15:31.089797 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 06:15:31.089995 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 06:15:31.090053 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 06:15:32.556057 140586562139968 spec.py:349] Evaluating on the test split.
I0216 06:15:35.219827 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 06:20:13.817850 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 06:20:13.818137 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 06:20:13.818191 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 06:20:15.386487 140586562139968 submission_runner.py:408] Time since start: 38011.08s, 	Step: 52653, 	{'train/accuracy': 0.6567549705505371, 'train/loss': 1.646889567375183, 'train/bleu': 0.9231937481106066, 'validation/accuracy': 0.6681876182556152, 'validation/loss': 1.5754570960998535, 'validation/bleu': 0.4099691875837352, 'validation/num_examples': 3000, 'test/accuracy': 0.6799721121788025, 'test/loss': 1.490198016166687, 'test/bleu': 0.3212871340753205, 'test/num_examples': 3003, 'score': 18521.335946798325, 'total_duration': 38011.07923364639, 'accumulated_submission_time': 18521.335946798325, 'accumulated_eval_time': 19487.487874031067, 'accumulated_logging_time': 0.7451581954956055}
I0216 06:20:15.404649 140416857904896 logging_writer.py:48] [52653] accumulated_eval_time=19487.487874, accumulated_logging_time=0.745158, accumulated_submission_time=18521.335947, global_step=52653, preemption_count=0, score=18521.335947, test/accuracy=0.679972, test/bleu=0.321287, test/loss=1.490198, test/num_examples=3003, total_duration=38011.079234, train/accuracy=0.656755, train/bleu=0.923194, train/loss=1.646890, validation/accuracy=0.668188, validation/bleu=0.409969, validation/loss=1.575457, validation/num_examples=3000
I0216 06:22:17.427506 140416866297600 logging_writer.py:48] [53000] global_step=53000, grad_norm=0.21278893947601318, loss=1.786293387413025
I0216 06:25:12.947628 140416857904896 logging_writer.py:48] [53500] global_step=53500, grad_norm=0.2267400026321411, loss=1.765366554260254
I0216 06:28:08.482788 140416866297600 logging_writer.py:48] [54000] global_step=54000, grad_norm=0.21307334303855896, loss=1.7030268907546997
I0216 06:31:03.975284 140416857904896 logging_writer.py:48] [54500] global_step=54500, grad_norm=0.22173091769218445, loss=1.7727000713348389
I0216 06:33:59.422796 140416866297600 logging_writer.py:48] [55000] global_step=55000, grad_norm=0.2110777497291565, loss=1.7286041975021362
I0216 06:34:15.658651 140586562139968 spec.py:321] Evaluating on the training split.
I0216 06:34:18.615744 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 06:38:57.726811 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 06:38:57.727038 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 06:38:57.727101 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 06:38:58.655802 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 06:39:01.305378 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 06:43:39.603205 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 06:43:39.603406 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 06:43:39.603458 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 06:43:40.752824 140586562139968 spec.py:349] Evaluating on the test split.
I0216 06:43:43.422352 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 06:48:21.787336 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 06:48:21.787539 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 06:48:21.787590 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 06:48:23.011928 140586562139968 submission_runner.py:408] Time since start: 39698.70s, 	Step: 55048, 	{'train/accuracy': 0.6556821465492249, 'train/loss': 1.6571015119552612, 'train/bleu': 0.9509054557339609, 'validation/accuracy': 0.6710518002510071, 'validation/loss': 1.5655437707901, 'validation/bleu': 0.49818585396014065, 'validation/num_examples': 3000, 'test/accuracy': 0.6825053691864014, 'test/loss': 1.4769991636276245, 'test/bleu': 0.37356293285647507, 'test/num_examples': 3003, 'score': 19361.51351070404, 'total_duration': 39698.704710006714, 'accumulated_submission_time': 19361.51351070404, 'accumulated_eval_time': 20334.841116428375, 'accumulated_logging_time': 0.7738635540008545}
I0216 06:48:23.029232 140416857904896 logging_writer.py:48] [55048] accumulated_eval_time=20334.841116, accumulated_logging_time=0.773864, accumulated_submission_time=19361.513511, global_step=55048, preemption_count=0, score=19361.513511, test/accuracy=0.682505, test/bleu=0.373563, test/loss=1.476999, test/num_examples=3003, total_duration=39698.704710, train/accuracy=0.655682, train/bleu=0.950905, train/loss=1.657102, validation/accuracy=0.671052, validation/bleu=0.498186, validation/loss=1.565544, validation/num_examples=3000
I0216 06:51:01.819414 140416866297600 logging_writer.py:48] [55500] global_step=55500, grad_norm=0.21692942082881927, loss=1.6553584337234497
I0216 06:53:57.317277 140416857904896 logging_writer.py:48] [56000] global_step=56000, grad_norm=0.2115747034549713, loss=1.678264856338501
I0216 06:56:52.803265 140416866297600 logging_writer.py:48] [56500] global_step=56500, grad_norm=0.22076790034770966, loss=1.6615588665008545
I0216 06:59:48.325126 140416857904896 logging_writer.py:48] [57000] global_step=57000, grad_norm=0.21553288400173187, loss=1.6649856567382812
I0216 07:02:23.130637 140586562139968 spec.py:321] Evaluating on the training split.
I0216 07:02:26.070935 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 07:07:05.927477 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 07:07:05.927686 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 07:07:05.927739 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 07:07:06.873396 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 07:07:09.529819 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 07:11:47.762649 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 07:11:47.762880 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 07:11:47.762948 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 07:11:48.885932 140586562139968 spec.py:349] Evaluating on the test split.
I0216 07:11:51.549184 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 07:16:30.523432 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 07:16:30.523653 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 07:16:30.523741 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 07:16:31.835301 140586562139968 submission_runner.py:408] Time since start: 41387.53s, 	Step: 57443, 	{'train/accuracy': 0.6662589311599731, 'train/loss': 1.5912539958953857, 'train/bleu': 1.1457023591394118, 'validation/accuracy': 0.6710641980171204, 'validation/loss': 1.5589473247528076, 'validation/bleu': 0.5604829885413425, 'validation/num_examples': 3000, 'test/accuracy': 0.6835047602653503, 'test/loss': 1.4732749462127686, 'test/bleu': 0.445304647093264, 'test/num_examples': 3003, 'score': 20201.538850545883, 'total_duration': 41387.52809906006, 'accumulated_submission_time': 20201.538850545883, 'accumulated_eval_time': 21183.54577088356, 'accumulated_logging_time': 0.8022444248199463}
I0216 07:16:31.852553 140416866297600 logging_writer.py:48] [57443] accumulated_eval_time=21183.545771, accumulated_logging_time=0.802244, accumulated_submission_time=20201.538851, global_step=57443, preemption_count=0, score=20201.538851, test/accuracy=0.683505, test/bleu=0.445305, test/loss=1.473275, test/num_examples=3003, total_duration=41387.528099, train/accuracy=0.666259, train/bleu=1.145702, train/loss=1.591254, validation/accuracy=0.671064, validation/bleu=0.560483, validation/loss=1.558947, validation/num_examples=3000
I0216 07:16:52.187597 140416857904896 logging_writer.py:48] [57500] global_step=57500, grad_norm=0.23243291676044464, loss=1.742661476135254
I0216 07:19:47.554565 140416866297600 logging_writer.py:48] [58000] global_step=58000, grad_norm=0.2234000563621521, loss=1.7048720121383667
I0216 07:22:43.088251 140416857904896 logging_writer.py:48] [58500] global_step=58500, grad_norm=0.21573291718959808, loss=1.6009318828582764
I0216 07:25:38.597514 140416866297600 logging_writer.py:48] [59000] global_step=59000, grad_norm=0.2156335860490799, loss=1.6919543743133545
I0216 07:28:34.095273 140416857904896 logging_writer.py:48] [59500] global_step=59500, grad_norm=0.2141021341085434, loss=1.6900441646575928
I0216 07:30:32.109865 140586562139968 spec.py:321] Evaluating on the training split.
I0216 07:30:35.052563 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 07:35:14.285833 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 07:35:14.286055 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 07:35:14.286112 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 07:35:15.260265 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 07:35:17.921677 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 07:39:56.100775 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 07:39:56.100996 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 07:39:56.101048 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 07:39:57.353538 140586562139968 spec.py:349] Evaluating on the test split.
I0216 07:40:00.025178 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 07:44:38.254463 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 07:44:38.254696 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 07:44:38.254781 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 07:44:39.560391 140586562139968 submission_runner.py:408] Time since start: 43075.25s, 	Step: 59838, 	{'train/accuracy': 0.6639460921287537, 'train/loss': 1.6035712957382202, 'train/bleu': 0.9855532535203474, 'validation/accuracy': 0.672576904296875, 'validation/loss': 1.5494859218597412, 'validation/bleu': 0.4860498560049654, 'validation/num_examples': 3000, 'test/accuracy': 0.684980571269989, 'test/loss': 1.4584156274795532, 'test/bleu': 0.3622107624670763, 'test/num_examples': 3003, 'score': 21041.721590042114, 'total_duration': 43075.25318646431, 'accumulated_submission_time': 21041.721590042114, 'accumulated_eval_time': 22030.996285915375, 'accumulated_logging_time': 0.8286912441253662}
I0216 07:44:39.578552 140416866297600 logging_writer.py:48] [59838] accumulated_eval_time=22030.996286, accumulated_logging_time=0.828691, accumulated_submission_time=21041.721590, global_step=59838, preemption_count=0, score=21041.721590, test/accuracy=0.684981, test/bleu=0.362211, test/loss=1.458416, test/num_examples=3003, total_duration=43075.253186, train/accuracy=0.663946, train/bleu=0.985553, train/loss=1.603571, validation/accuracy=0.672577, validation/bleu=0.486050, validation/loss=1.549486, validation/num_examples=3000
I0216 07:45:36.630025 140416857904896 logging_writer.py:48] [60000] global_step=60000, grad_norm=0.2202053666114807, loss=1.6617547273635864
I0216 07:48:32.084299 140416866297600 logging_writer.py:48] [60500] global_step=60500, grad_norm=0.2110339105129242, loss=1.733044147491455
I0216 07:51:27.570432 140416857904896 logging_writer.py:48] [61000] global_step=61000, grad_norm=0.21387237310409546, loss=1.7011103630065918
I0216 07:54:23.052958 140416866297600 logging_writer.py:48] [61500] global_step=61500, grad_norm=0.22049172222614288, loss=1.7124931812286377
I0216 07:57:18.548233 140416857904896 logging_writer.py:48] [62000] global_step=62000, grad_norm=0.22466212511062622, loss=1.7581257820129395
I0216 07:58:39.659998 140586562139968 spec.py:321] Evaluating on the training split.
I0216 07:58:42.606914 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 08:03:22.544241 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 08:03:22.544450 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 08:03:22.544502 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 08:03:22.674249 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 08:03:25.332386 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 08:08:04.330294 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 08:08:04.330508 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 08:08:04.330562 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 08:08:04.818298 140586562139968 spec.py:349] Evaluating on the test split.
I0216 08:08:07.469975 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 08:12:46.187759 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 08:12:46.187964 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 08:12:46.188016 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 08:12:46.770834 140586562139968 submission_runner.py:408] Time since start: 44762.46s, 	Step: 62233, 	{'train/accuracy': 0.6640580296516418, 'train/loss': 1.5954301357269287, 'train/bleu': 1.1335177448441809, 'validation/accuracy': 0.6729612946510315, 'validation/loss': 1.5441699028015137, 'validation/bleu': 0.6258434561373543, 'validation/num_examples': 3000, 'test/accuracy': 0.6856196522712708, 'test/loss': 1.4512062072753906, 'test/bleu': 0.461853140693794, 'test/num_examples': 3003, 'score': 21881.728234052658, 'total_duration': 44762.46363568306, 'accumulated_submission_time': 21881.728234052658, 'accumulated_eval_time': 22878.107117652893, 'accumulated_logging_time': 0.8560175895690918}
I0216 08:12:46.789367 140416866297600 logging_writer.py:48] [62233] accumulated_eval_time=22878.107118, accumulated_logging_time=0.856018, accumulated_submission_time=21881.728234, global_step=62233, preemption_count=0, score=21881.728234, test/accuracy=0.685620, test/bleu=0.461853, test/loss=1.451206, test/num_examples=3003, total_duration=44762.463636, train/accuracy=0.664058, train/bleu=1.133518, train/loss=1.595430, validation/accuracy=0.672961, validation/bleu=0.625843, validation/loss=1.544170, validation/num_examples=3000
I0216 08:14:20.696775 140416857904896 logging_writer.py:48] [62500] global_step=62500, grad_norm=0.21697010099887848, loss=1.6676487922668457
I0216 08:17:16.178912 140416866297600 logging_writer.py:48] [63000] global_step=63000, grad_norm=0.21505795419216156, loss=1.6316823959350586
I0216 08:20:11.688409 140416857904896 logging_writer.py:48] [63500] global_step=63500, grad_norm=0.2150917500257492, loss=1.6596534252166748
I0216 08:23:07.123369 140416866297600 logging_writer.py:48] [64000] global_step=64000, grad_norm=0.2214089334011078, loss=1.6334919929504395
I0216 08:26:02.621585 140416857904896 logging_writer.py:48] [64500] global_step=64500, grad_norm=0.2191772311925888, loss=1.6970576047897339
I0216 08:26:46.939274 140586562139968 spec.py:321] Evaluating on the training split.
I0216 08:26:49.878565 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 08:31:29.715970 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 08:31:29.716181 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 08:31:29.716237 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 08:31:30.986615 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 08:31:33.649992 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 08:36:12.197367 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 08:36:12.197613 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 08:36:12.197699 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 08:36:13.497298 140586562139968 spec.py:349] Evaluating on the test split.
I0216 08:36:16.160290 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 08:40:55.284361 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 08:40:55.284566 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 08:40:55.284619 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 08:40:56.773595 140586562139968 submission_runner.py:408] Time since start: 46452.47s, 	Step: 64628, 	{'train/accuracy': 0.6661494374275208, 'train/loss': 1.583825945854187, 'train/bleu': 0.9069875523650686, 'validation/accuracy': 0.6742135882377625, 'validation/loss': 1.5388855934143066, 'validation/bleu': 0.4506918771675515, 'validation/num_examples': 3000, 'test/accuracy': 0.6861310005187988, 'test/loss': 1.4480352401733398, 'test/bleu': 0.33708735080485613, 'test/num_examples': 3003, 'score': 22721.80181837082, 'total_duration': 46452.4663734436, 'accumulated_submission_time': 22721.80181837082, 'accumulated_eval_time': 23727.94140267372, 'accumulated_logging_time': 0.8857722282409668}
I0216 08:40:56.791304 140416866297600 logging_writer.py:48] [64628] accumulated_eval_time=23727.941403, accumulated_logging_time=0.885772, accumulated_submission_time=22721.801818, global_step=64628, preemption_count=0, score=22721.801818, test/accuracy=0.686131, test/bleu=0.337087, test/loss=1.448035, test/num_examples=3003, total_duration=46452.466373, train/accuracy=0.666149, train/bleu=0.906988, train/loss=1.583826, validation/accuracy=0.674214, validation/bleu=0.450692, validation/loss=1.538886, validation/num_examples=3000
I0216 08:43:07.614227 140416857904896 logging_writer.py:48] [65000] global_step=65000, grad_norm=0.21804338693618774, loss=1.8009836673736572
I0216 08:46:03.113733 140416866297600 logging_writer.py:48] [65500] global_step=65500, grad_norm=0.22044648230075836, loss=1.6674076318740845
I0216 08:48:58.617572 140416857904896 logging_writer.py:48] [66000] global_step=66000, grad_norm=0.23365150392055511, loss=1.6982059478759766
I0216 08:51:54.114845 140416866297600 logging_writer.py:48] [66500] global_step=66500, grad_norm=0.2167094200849533, loss=1.6682332754135132
I0216 08:54:49.608483 140416857904896 logging_writer.py:48] [67000] global_step=67000, grad_norm=0.2176041603088379, loss=1.6982332468032837
I0216 08:54:57.063228 140586562139968 spec.py:321] Evaluating on the training split.
I0216 08:55:00.005620 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 08:59:38.790421 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 08:59:38.790629 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 08:59:38.790682 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 08:59:39.996139 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 08:59:42.651771 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 09:04:20.862748 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 09:04:20.862969 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 09:04:20.863021 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 09:04:22.175090 140586562139968 spec.py:349] Evaluating on the test split.
I0216 09:04:24.840523 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 09:09:03.348371 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 09:09:03.348596 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 09:09:03.348653 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 09:09:04.713568 140586562139968 submission_runner.py:408] Time since start: 48140.41s, 	Step: 67023, 	{'train/accuracy': 0.6694967150688171, 'train/loss': 1.5648820400238037, 'train/bleu': 0.8859116524858676, 'validation/accuracy': 0.6753419041633606, 'validation/loss': 1.5318435430526733, 'validation/bleu': 0.4282227714884274, 'validation/num_examples': 3000, 'test/accuracy': 0.6872349381446838, 'test/loss': 1.440364956855774, 'test/bleu': 0.3590375786218571, 'test/num_examples': 3003, 'score': 23561.999462604523, 'total_duration': 48140.406368494034, 'accumulated_submission_time': 23561.999462604523, 'accumulated_eval_time': 24575.591722249985, 'accumulated_logging_time': 0.9125311374664307}
I0216 09:09:04.731169 140416866297600 logging_writer.py:48] [67023] accumulated_eval_time=24575.591722, accumulated_logging_time=0.912531, accumulated_submission_time=23561.999463, global_step=67023, preemption_count=0, score=23561.999463, test/accuracy=0.687235, test/bleu=0.359038, test/loss=1.440365, test/num_examples=3003, total_duration=48140.406368, train/accuracy=0.669497, train/bleu=0.885912, train/loss=1.564882, validation/accuracy=0.675342, validation/bleu=0.428223, validation/loss=1.531844, validation/num_examples=3000
I0216 09:11:52.310769 140416857904896 logging_writer.py:48] [67500] global_step=67500, grad_norm=0.24275535345077515, loss=1.7443208694458008
I0216 09:14:47.863968 140416866297600 logging_writer.py:48] [68000] global_step=68000, grad_norm=0.2313721477985382, loss=1.682674765586853
I0216 09:17:43.300954 140416857904896 logging_writer.py:48] [68500] global_step=68500, grad_norm=0.22590507566928864, loss=1.7070716619491577
I0216 09:20:38.758122 140416866297600 logging_writer.py:48] [69000] global_step=69000, grad_norm=0.2195543348789215, loss=1.6702804565429688
I0216 09:23:04.844717 140586562139968 spec.py:321] Evaluating on the training split.
I0216 09:23:07.779239 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 09:27:47.820776 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 09:27:47.821002 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 09:27:47.821059 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 09:27:49.056850 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 09:27:51.724341 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 09:32:30.436384 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 09:32:30.436579 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 09:32:30.436632 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 09:32:31.675490 140586562139968 spec.py:349] Evaluating on the test split.
I0216 09:32:34.339752 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 09:37:12.629003 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 09:37:12.629213 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 09:37:12.629266 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 09:37:14.215561 140586562139968 submission_runner.py:408] Time since start: 49829.91s, 	Step: 69418, 	{'train/accuracy': 0.6704220175743103, 'train/loss': 1.5547288656234741, 'train/bleu': 0.8951874215421436, 'validation/accuracy': 0.6762966513633728, 'validation/loss': 1.526335597038269, 'validation/bleu': 0.505989789839016, 'validation/num_examples': 3000, 'test/accuracy': 0.6887223720550537, 'test/loss': 1.4339213371276855, 'test/bleu': 0.38671856162936147, 'test/num_examples': 3003, 'score': 24402.038233280182, 'total_duration': 49829.90833616257, 'accumulated_submission_time': 24402.038233280182, 'accumulated_eval_time': 25424.9625351429, 'accumulated_logging_time': 0.9403717517852783}
I0216 09:37:14.234614 140416857904896 logging_writer.py:48] [69418] accumulated_eval_time=25424.962535, accumulated_logging_time=0.940372, accumulated_submission_time=24402.038233, global_step=69418, preemption_count=0, score=24402.038233, test/accuracy=0.688722, test/bleu=0.386719, test/loss=1.433921, test/num_examples=3003, total_duration=49829.908336, train/accuracy=0.670422, train/bleu=0.895187, train/loss=1.554729, validation/accuracy=0.676297, validation/bleu=0.505990, validation/loss=1.526336, validation/num_examples=3000
I0216 09:37:43.325584 140416866297600 logging_writer.py:48] [69500] global_step=69500, grad_norm=0.21513789892196655, loss=1.646920084953308
I0216 09:40:38.725013 140416857904896 logging_writer.py:48] [70000] global_step=70000, grad_norm=0.22508195042610168, loss=1.6886661052703857
I0216 09:43:34.237239 140416866297600 logging_writer.py:48] [70500] global_step=70500, grad_norm=0.2214115858078003, loss=1.679547667503357
I0216 09:46:29.713214 140416857904896 logging_writer.py:48] [71000] global_step=71000, grad_norm=0.22182497382164001, loss=1.6301777362823486
I0216 09:49:25.268337 140416866297600 logging_writer.py:48] [71500] global_step=71500, grad_norm=0.22335729002952576, loss=1.6074641942977905
I0216 09:51:14.500335 140586562139968 spec.py:321] Evaluating on the training split.
I0216 09:51:17.444350 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 09:55:56.699323 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 09:55:56.699552 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 09:55:56.699618 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 09:55:58.079854 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 09:56:00.738031 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 10:00:39.359354 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 10:00:39.359550 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 10:00:39.359599 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 10:00:40.764578 140586562139968 spec.py:349] Evaluating on the test split.
I0216 10:00:43.438265 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 10:05:22.057014 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 10:05:22.057226 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 10:05:22.057275 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 10:05:23.597661 140586562139968 submission_runner.py:408] Time since start: 51519.29s, 	Step: 71813, 	{'train/accuracy': 0.6714939475059509, 'train/loss': 1.5575363636016846, 'train/bleu': 0.8775270270332404, 'validation/accuracy': 0.6763338446617126, 'validation/loss': 1.5243443250656128, 'validation/bleu': 0.49728411492731145, 'validation/num_examples': 3000, 'test/accuracy': 0.6880367398262024, 'test/loss': 1.430014729499817, 'test/bleu': 0.33933316738732233, 'test/num_examples': 3003, 'score': 25242.228439569473, 'total_duration': 51519.2904586792, 'accumulated_submission_time': 25242.228439569473, 'accumulated_eval_time': 26274.05986571312, 'accumulated_logging_time': 0.969036340713501}
I0216 10:05:23.615931 140416857904896 logging_writer.py:48] [71813] accumulated_eval_time=26274.059866, accumulated_logging_time=0.969036, accumulated_submission_time=25242.228440, global_step=71813, preemption_count=0, score=25242.228440, test/accuracy=0.688037, test/bleu=0.339333, test/loss=1.430015, test/num_examples=3003, total_duration=51519.290459, train/accuracy=0.671494, train/bleu=0.877527, train/loss=1.557536, validation/accuracy=0.676334, validation/bleu=0.497284, validation/loss=1.524344, validation/num_examples=3000
I0216 10:06:29.544029 140416866297600 logging_writer.py:48] [72000] global_step=72000, grad_norm=0.23115034401416779, loss=1.6758533716201782
I0216 10:09:24.996060 140416857904896 logging_writer.py:48] [72500] global_step=72500, grad_norm=0.2208273559808731, loss=1.6643450260162354
I0216 10:12:20.460102 140416866297600 logging_writer.py:48] [73000] global_step=73000, grad_norm=0.2293684333562851, loss=1.6451387405395508
I0216 10:15:15.985964 140416857904896 logging_writer.py:48] [73500] global_step=73500, grad_norm=0.21707187592983246, loss=1.6594491004943848
I0216 10:18:11.474139 140416866297600 logging_writer.py:48] [74000] global_step=74000, grad_norm=0.22866526246070862, loss=1.702158808708191
I0216 10:19:23.844907 140586562139968 spec.py:321] Evaluating on the training split.
I0216 10:19:26.779552 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 10:24:06.101565 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 10:24:06.101785 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 10:24:06.101836 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 10:24:06.586594 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 10:24:09.242167 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 10:28:48.044560 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 10:28:48.044767 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 10:28:48.044817 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 10:28:48.499372 140586562139968 spec.py:349] Evaluating on the test split.
I0216 10:28:51.153878 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 10:33:29.548325 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 10:33:29.548533 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 10:33:29.548583 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 10:33:30.300529 140586562139968 submission_runner.py:408] Time since start: 53205.99s, 	Step: 74208, 	{'train/accuracy': 0.6704035401344299, 'train/loss': 1.5615100860595703, 'train/bleu': 1.1562162523198845, 'validation/accuracy': 0.6765817999839783, 'validation/loss': 1.5207356214523315, 'validation/bleu': 0.6622418176941481, 'validation/num_examples': 3000, 'test/accuracy': 0.689233660697937, 'test/loss': 1.4266027212142944, 'test/bleu': 0.5315353316533425, 'test/num_examples': 3003, 'score': 26082.38233447075, 'total_duration': 53205.99332904816, 'accumulated_submission_time': 26082.38233447075, 'accumulated_eval_time': 27120.515478372574, 'accumulated_logging_time': 0.9975152015686035}
I0216 10:33:30.320241 140416857904896 logging_writer.py:48] [74208] accumulated_eval_time=27120.515478, accumulated_logging_time=0.997515, accumulated_submission_time=26082.382334, global_step=74208, preemption_count=0, score=26082.382334, test/accuracy=0.689234, test/bleu=0.531535, test/loss=1.426603, test/num_examples=3003, total_duration=53205.993329, train/accuracy=0.670404, train/bleu=1.156216, train/loss=1.561510, validation/accuracy=0.676582, validation/bleu=0.662242, validation/loss=1.520736, validation/num_examples=3000
I0216 10:35:12.984759 140416866297600 logging_writer.py:48] [74500] global_step=74500, grad_norm=0.22463712096214294, loss=1.6150654554367065
I0216 10:38:08.390843 140416857904896 logging_writer.py:48] [75000] global_step=75000, grad_norm=0.22728022933006287, loss=1.6343601942062378
I0216 10:41:03.895787 140416866297600 logging_writer.py:48] [75500] global_step=75500, grad_norm=0.22583609819412231, loss=1.6455706357955933
I0216 10:43:59.462340 140416857904896 logging_writer.py:48] [76000] global_step=76000, grad_norm=0.2305513322353363, loss=1.6669329404830933
I0216 10:46:54.998597 140416866297600 logging_writer.py:48] [76500] global_step=76500, grad_norm=0.24140919744968414, loss=1.6029032468795776
I0216 10:47:30.507740 140586562139968 spec.py:321] Evaluating on the training split.
I0216 10:47:33.456594 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 10:52:12.596895 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 10:52:12.597121 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 10:52:12.597174 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 10:52:13.797859 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 10:52:16.455759 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 10:56:54.905342 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 10:56:54.905567 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 10:56:54.905627 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 10:56:56.114475 140586562139968 spec.py:349] Evaluating on the test split.
I0216 10:56:58.772417 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 11:01:37.546556 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 11:01:37.546773 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 11:01:37.546824 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 11:01:39.006753 140586562139968 submission_runner.py:408] Time since start: 54894.70s, 	Step: 76603, 	{'train/accuracy': 0.6772430539131165, 'train/loss': 1.516606092453003, 'train/bleu': 0.8452351623278295, 'validation/accuracy': 0.6767677664756775, 'validation/loss': 1.5164719820022583, 'validation/bleu': 0.4772785874902205, 'validation/num_examples': 3000, 'test/accuracy': 0.6904653906822205, 'test/loss': 1.42171049118042, 'test/bleu': 0.3224086687519668, 'test/num_examples': 3003, 'score': 26922.495762586594, 'total_duration': 54894.699543237686, 'accumulated_submission_time': 26922.495762586594, 'accumulated_eval_time': 27969.01446557045, 'accumulated_logging_time': 1.0263376235961914}
I0216 11:01:39.025352 140416857904896 logging_writer.py:48] [76603] accumulated_eval_time=27969.014466, accumulated_logging_time=1.026338, accumulated_submission_time=26922.495763, global_step=76603, preemption_count=0, score=26922.495763, test/accuracy=0.690465, test/bleu=0.322409, test/loss=1.421710, test/num_examples=3003, total_duration=54894.699543, train/accuracy=0.677243, train/bleu=0.845235, train/loss=1.516606, validation/accuracy=0.676768, validation/bleu=0.477279, validation/loss=1.516472, validation/num_examples=3000
I0216 11:03:58.531574 140416866297600 logging_writer.py:48] [77000] global_step=77000, grad_norm=0.23211021721363068, loss=1.686472773551941
I0216 11:06:53.998791 140416857904896 logging_writer.py:48] [77500] global_step=77500, grad_norm=0.22869884967803955, loss=1.5647735595703125
I0216 11:09:49.477332 140416866297600 logging_writer.py:48] [78000] global_step=78000, grad_norm=0.22525827586650848, loss=1.6880247592926025
I0216 11:12:44.977358 140416857904896 logging_writer.py:48] [78500] global_step=78500, grad_norm=0.23653163015842438, loss=1.6547735929489136
I0216 11:15:39.124649 140586562139968 spec.py:321] Evaluating on the training split.
I0216 11:15:42.068075 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 11:20:22.704218 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 11:20:22.704417 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 11:20:22.704466 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 11:20:23.298857 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 11:20:25.957262 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 11:25:04.724528 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 11:25:04.724736 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 11:25:04.724820 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 11:25:05.529830 140586562139968 spec.py:349] Evaluating on the test split.
I0216 11:25:08.181883 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 11:29:46.566566 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 11:29:46.566779 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 11:29:46.566831 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 11:29:47.607749 140586562139968 submission_runner.py:408] Time since start: 56583.30s, 	Step: 78998, 	{'train/accuracy': 0.6727237105369568, 'train/loss': 1.5408540964126587, 'train/bleu': 0.9136755580149148, 'validation/accuracy': 0.6779333353042603, 'validation/loss': 1.5134196281433105, 'validation/bleu': 0.533290576541226, 'validation/num_examples': 3000, 'test/accuracy': 0.691104531288147, 'test/loss': 1.4155763387680054, 'test/bleu': 0.41150791933791214, 'test/num_examples': 3003, 'score': 27762.519449949265, 'total_duration': 56583.30054783821, 'accumulated_submission_time': 27762.519449949265, 'accumulated_eval_time': 28817.49755859375, 'accumulated_logging_time': 1.0559632778167725}
I0216 11:29:47.626007 140416866297600 logging_writer.py:48] [78998] accumulated_eval_time=28817.497559, accumulated_logging_time=1.055963, accumulated_submission_time=27762.519450, global_step=78998, preemption_count=0, score=27762.519450, test/accuracy=0.691105, test/bleu=0.411508, test/loss=1.415576, test/num_examples=3003, total_duration=56583.300548, train/accuracy=0.672724, train/bleu=0.913676, train/loss=1.540854, validation/accuracy=0.677933, validation/bleu=0.533291, validation/loss=1.513420, validation/num_examples=3000
I0216 11:29:48.700058 140416857904896 logging_writer.py:48] [79000] global_step=79000, grad_norm=0.22444385290145874, loss=1.6049060821533203
I0216 11:32:43.994987 140416866297600 logging_writer.py:48] [79500] global_step=79500, grad_norm=0.22070345282554626, loss=1.5532033443450928
I0216 11:35:39.553401 140416857904896 logging_writer.py:48] [80000] global_step=80000, grad_norm=0.22757159173488617, loss=1.7008312940597534
I0216 11:38:35.132595 140416866297600 logging_writer.py:48] [80500] global_step=80500, grad_norm=0.23578836023807526, loss=1.6910545825958252
I0216 11:41:30.611491 140416857904896 logging_writer.py:48] [81000] global_step=81000, grad_norm=0.23041249811649323, loss=1.658739447593689
I0216 11:43:47.934035 140586562139968 spec.py:321] Evaluating on the training split.
I0216 11:43:50.870522 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 11:48:30.223035 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 11:48:30.223240 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 11:48:30.223296 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 11:48:31.174700 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 11:48:33.849743 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 11:53:13.106427 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 11:53:13.106658 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 11:53:13.106712 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 11:53:14.121563 140586562139968 spec.py:349] Evaluating on the test split.
I0216 11:53:16.785112 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 11:57:55.984344 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 11:57:55.984559 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 11:57:55.984627 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 11:57:57.133785 140586562139968 submission_runner.py:408] Time since start: 58272.83s, 	Step: 81393, 	{'train/accuracy': 0.6805446743965149, 'train/loss': 1.5007292032241821, 'train/bleu': 0.9495280757496707, 'validation/accuracy': 0.678230881690979, 'validation/loss': 1.5101262331008911, 'validation/bleu': 0.5609860201541438, 'validation/num_examples': 3000, 'test/accuracy': 0.6907559037208557, 'test/loss': 1.414063572883606, 'test/bleu': 0.4774863126295314, 'test/num_examples': 3003, 'score': 28602.75276017189, 'total_duration': 58272.826583623886, 'accumulated_submission_time': 28602.75276017189, 'accumulated_eval_time': 29666.697316408157, 'accumulated_logging_time': 1.0847837924957275}
I0216 11:57:57.152460 140416866297600 logging_writer.py:48] [81393] accumulated_eval_time=29666.697316, accumulated_logging_time=1.084784, accumulated_submission_time=28602.752760, global_step=81393, preemption_count=0, score=28602.752760, test/accuracy=0.690756, test/bleu=0.477486, test/loss=1.414064, test/num_examples=3003, total_duration=58272.826584, train/accuracy=0.680545, train/bleu=0.949528, train/loss=1.500729, validation/accuracy=0.678231, validation/bleu=0.560986, validation/loss=1.510126, validation/num_examples=3000
I0216 11:58:34.999226 140416857904896 logging_writer.py:48] [81500] global_step=81500, grad_norm=0.22833271324634552, loss=1.5934300422668457
I0216 12:01:30.475824 140416866297600 logging_writer.py:48] [82000] global_step=82000, grad_norm=0.22441361844539642, loss=1.6302555799484253
I0216 12:04:26.003352 140416857904896 logging_writer.py:48] [82500] global_step=82500, grad_norm=0.22390443086624146, loss=1.5547394752502441
I0216 12:07:21.550181 140416866297600 logging_writer.py:48] [83000] global_step=83000, grad_norm=0.22937214374542236, loss=1.6535533666610718
I0216 12:10:17.073213 140416857904896 logging_writer.py:48] [83500] global_step=83500, grad_norm=0.23317041993141174, loss=1.5704288482666016
I0216 12:11:57.219535 140586562139968 spec.py:321] Evaluating on the training split.
I0216 12:12:00.167616 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 12:16:39.486312 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 12:16:39.486528 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 12:16:39.486587 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 12:16:40.562978 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 12:16:43.234788 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 12:21:22.105380 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 12:21:22.105602 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 12:21:22.105655 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 12:21:23.128262 140586562139968 spec.py:349] Evaluating on the test split.
I0216 12:21:25.801957 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 12:26:04.529536 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 12:26:04.529742 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 12:26:04.529838 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 12:26:05.736624 140586562139968 submission_runner.py:408] Time since start: 59961.43s, 	Step: 83787, 	{'train/accuracy': 0.6764903664588928, 'train/loss': 1.5295292139053345, 'train/bleu': 0.859073984275965, 'validation/accuracy': 0.678900420665741, 'validation/loss': 1.506435751914978, 'validation/bleu': 0.546376316060547, 'validation/num_examples': 3000, 'test/accuracy': 0.6920109391212463, 'test/loss': 1.4107131958007812, 'test/bleu': 0.41058208983964545, 'test/num_examples': 3003, 'score': 29442.746134757996, 'total_duration': 59961.42941093445, 'accumulated_submission_time': 29442.746134757996, 'accumulated_eval_time': 30515.214382648468, 'accumulated_logging_time': 1.1125695705413818}
I0216 12:26:05.755624 140416866297600 logging_writer.py:48] [83787] accumulated_eval_time=30515.214383, accumulated_logging_time=1.112570, accumulated_submission_time=29442.746135, global_step=83787, preemption_count=0, score=29442.746135, test/accuracy=0.692011, test/bleu=0.410582, test/loss=1.410713, test/num_examples=3003, total_duration=59961.429411, train/accuracy=0.676490, train/bleu=0.859074, train/loss=1.529529, validation/accuracy=0.678900, validation/bleu=0.546376, validation/loss=1.506436, validation/num_examples=3000
I0216 12:27:20.783308 140416857904896 logging_writer.py:48] [84000] global_step=84000, grad_norm=0.22941899299621582, loss=1.6571826934814453
I0216 12:30:16.273651 140416866297600 logging_writer.py:48] [84500] global_step=84500, grad_norm=0.22787201404571533, loss=1.5787098407745361
I0216 12:33:11.812514 140416857904896 logging_writer.py:48] [85000] global_step=85000, grad_norm=0.2318795621395111, loss=1.5865987539291382
I0216 12:36:07.362103 140416866297600 logging_writer.py:48] [85500] global_step=85500, grad_norm=0.24177046120166779, loss=1.6385575532913208
I0216 12:39:02.882168 140416857904896 logging_writer.py:48] [86000] global_step=86000, grad_norm=0.23076944053173065, loss=1.6084898710250854
I0216 12:40:05.790714 140586562139968 spec.py:321] Evaluating on the training split.
I0216 12:40:08.737766 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 12:44:48.226108 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 12:44:48.226328 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 12:44:48.226388 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 12:44:49.127012 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 12:44:51.792961 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 12:49:31.717724 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 12:49:31.717942 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 12:49:31.717992 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 12:49:32.778505 140586562139968 spec.py:349] Evaluating on the test split.
I0216 12:49:35.461748 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 12:54:14.054791 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 12:54:14.055009 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 12:54:14.055061 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 12:54:15.414078 140586562139968 submission_runner.py:408] Time since start: 61651.11s, 	Step: 86181, 	{'train/accuracy': 0.6746113300323486, 'train/loss': 1.5305556058883667, 'train/bleu': 0.9416664847486931, 'validation/accuracy': 0.6792352199554443, 'validation/loss': 1.5046359300613403, 'validation/bleu': 0.52443764656063, 'validation/num_examples': 3000, 'test/accuracy': 0.6923711895942688, 'test/loss': 1.4073827266693115, 'test/bleu': 0.3845887708964356, 'test/num_examples': 3003, 'score': 30282.707558631897, 'total_duration': 61651.106850624084, 'accumulated_submission_time': 30282.707558631897, 'accumulated_eval_time': 31364.837710142136, 'accumulated_logging_time': 1.1406257152557373}
I0216 12:54:15.433119 140416866297600 logging_writer.py:48] [86181] accumulated_eval_time=31364.837710, accumulated_logging_time=1.140626, accumulated_submission_time=30282.707559, global_step=86181, preemption_count=0, score=30282.707559, test/accuracy=0.692371, test/bleu=0.384589, test/loss=1.407383, test/num_examples=3003, total_duration=61651.106851, train/accuracy=0.674611, train/bleu=0.941666, train/loss=1.530556, validation/accuracy=0.679235, validation/bleu=0.524438, validation/loss=1.504636, validation/num_examples=3000
I0216 12:56:07.646393 140416857904896 logging_writer.py:48] [86500] global_step=86500, grad_norm=0.22743795812129974, loss=1.5767492055892944
I0216 12:59:03.198220 140416866297600 logging_writer.py:48] [87000] global_step=87000, grad_norm=0.2261318564414978, loss=1.6014671325683594
I0216 13:01:58.762157 140416857904896 logging_writer.py:48] [87500] global_step=87500, grad_norm=0.22293230891227722, loss=1.5202358961105347
I0216 13:04:54.318094 140416866297600 logging_writer.py:48] [88000] global_step=88000, grad_norm=0.22969117760658264, loss=1.5751672983169556
I0216 13:07:49.913103 140416857904896 logging_writer.py:48] [88500] global_step=88500, grad_norm=0.23123404383659363, loss=1.5562758445739746
I0216 13:08:15.617959 140586562139968 spec.py:321] Evaluating on the training split.
I0216 13:08:18.562193 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 13:12:57.919518 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 13:12:57.919714 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 13:12:57.919762 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 13:12:58.846118 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 13:13:01.504778 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 13:17:40.228058 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 13:17:40.228273 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 13:17:40.228324 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 13:17:41.126881 140586562139968 spec.py:349] Evaluating on the test split.
I0216 13:17:43.790692 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 13:22:22.353009 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 13:22:22.353228 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 13:22:22.353287 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 13:22:23.528146 140586562139968 submission_runner.py:408] Time since start: 63339.22s, 	Step: 88575, 	{'train/accuracy': 0.6760222911834717, 'train/loss': 1.521141767501831, 'train/bleu': 1.0150668833560517, 'validation/accuracy': 0.6793220043182373, 'validation/loss': 1.5058515071868896, 'validation/bleu': 0.6247547808560047, 'validation/num_examples': 3000, 'test/accuracy': 0.6922317147254944, 'test/loss': 1.407009243965149, 'test/bleu': 0.4774770852085636, 'test/num_examples': 3003, 'score': 31122.81944990158, 'total_duration': 63339.22094464302, 'accumulated_submission_time': 31122.81944990158, 'accumulated_eval_time': 32212.74788045883, 'accumulated_logging_time': 1.1686944961547852}
I0216 13:22:23.547258 140416866297600 logging_writer.py:48] [88575] accumulated_eval_time=32212.747880, accumulated_logging_time=1.168694, accumulated_submission_time=31122.819450, global_step=88575, preemption_count=0, score=31122.819450, test/accuracy=0.692232, test/bleu=0.477477, test/loss=1.407009, test/num_examples=3003, total_duration=63339.220945, train/accuracy=0.676022, train/bleu=1.015067, train/loss=1.521142, validation/accuracy=0.679322, validation/bleu=0.624755, validation/loss=1.505852, validation/num_examples=3000
I0216 13:24:52.983124 140416857904896 logging_writer.py:48] [89000] global_step=89000, grad_norm=0.22732844948768616, loss=1.5940580368041992
I0216 13:27:48.444891 140416866297600 logging_writer.py:48] [89500] global_step=89500, grad_norm=0.2363971471786499, loss=1.676961898803711
I0216 13:30:43.991064 140416857904896 logging_writer.py:48] [90000] global_step=90000, grad_norm=0.23453235626220703, loss=1.629969596862793
I0216 13:33:39.506169 140416866297600 logging_writer.py:48] [90500] global_step=90500, grad_norm=0.2318088710308075, loss=1.6194264888763428
I0216 13:36:23.854454 140586562139968 spec.py:321] Evaluating on the training split.
I0216 13:36:26.795226 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 13:41:06.890509 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 13:41:06.890714 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 13:41:06.890764 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 13:41:08.038308 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 13:41:10.713751 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 13:45:49.158902 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 13:45:49.159122 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 13:45:49.159174 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 13:45:50.444930 140586562139968 spec.py:349] Evaluating on the test split.
I0216 13:45:53.121449 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 13:50:31.823446 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 13:50:31.823646 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 13:50:31.823697 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 13:50:33.257400 140586562139968 submission_runner.py:408] Time since start: 65028.95s, 	Step: 90970, 	{'train/accuracy': 0.67889803647995, 'train/loss': 1.5041018724441528, 'train/bleu': 0.9045160978581913, 'validation/accuracy': 0.678900420665741, 'validation/loss': 1.5031237602233887, 'validation/bleu': 0.515080399985915, 'validation/num_examples': 3000, 'test/accuracy': 0.6921852231025696, 'test/loss': 1.404951810836792, 'test/bleu': 0.38004671592859546, 'test/num_examples': 3003, 'score': 31963.053261756897, 'total_duration': 65028.950191259384, 'accumulated_submission_time': 31963.053261756897, 'accumulated_eval_time': 33062.15080952644, 'accumulated_logging_time': 1.1970951557159424}
I0216 13:50:33.276530 140416857904896 logging_writer.py:48] [90970] accumulated_eval_time=33062.150810, accumulated_logging_time=1.197095, accumulated_submission_time=31963.053262, global_step=90970, preemption_count=0, score=31963.053262, test/accuracy=0.692185, test/bleu=0.380047, test/loss=1.404952, test/num_examples=3003, total_duration=65028.950191, train/accuracy=0.678898, train/bleu=0.904516, train/loss=1.504102, validation/accuracy=0.678900, validation/bleu=0.515080, validation/loss=1.503124, validation/num_examples=3000
I0216 13:50:44.182035 140416866297600 logging_writer.py:48] [91000] global_step=91000, grad_norm=0.2251429259777069, loss=1.5803518295288086
I0216 13:53:39.551377 140416857904896 logging_writer.py:48] [91500] global_step=91500, grad_norm=0.24104130268096924, loss=1.6433837413787842
I0216 13:56:35.017364 140416866297600 logging_writer.py:48] [92000] global_step=92000, grad_norm=0.22933679819107056, loss=1.6979035139083862
I0216 13:59:30.613021 140416857904896 logging_writer.py:48] [92500] global_step=92500, grad_norm=0.23011474311351776, loss=1.6373627185821533
I0216 14:02:26.187300 140416866297600 logging_writer.py:48] [93000] global_step=93000, grad_norm=0.22637662291526794, loss=1.5997533798217773
I0216 14:04:33.368871 140586562139968 spec.py:321] Evaluating on the training split.
I0216 14:04:36.308428 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 14:09:15.735756 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 14:09:15.735962 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 14:09:15.736014 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 14:09:16.760903 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 14:09:19.430468 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 14:13:57.852037 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 14:13:57.852262 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 14:13:57.852313 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 14:13:59.063238 140586562139968 spec.py:349] Evaluating on the test split.
I0216 14:14:01.736327 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 14:18:40.748381 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 14:18:40.748622 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 14:18:40.748682 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 14:18:42.060637 140586562139968 submission_runner.py:408] Time since start: 66717.75s, 	Step: 93364, 	{'train/accuracy': 0.6780661940574646, 'train/loss': 1.5103566646575928, 'train/bleu': 0.8871624495665741, 'validation/accuracy': 0.6795700192451477, 'validation/loss': 1.5039381980895996, 'validation/bleu': 0.5127885878338722, 'validation/num_examples': 3000, 'test/accuracy': 0.6927778720855713, 'test/loss': 1.4051051139831543, 'test/bleu': 0.3804507336463232, 'test/num_examples': 3003, 'score': 32803.07182216644, 'total_duration': 66717.75342822075, 'accumulated_submission_time': 32803.07182216644, 'accumulated_eval_time': 33910.84256863594, 'accumulated_logging_time': 1.2257564067840576}
I0216 14:18:42.080190 140416857904896 logging_writer.py:48] [93364] accumulated_eval_time=33910.842569, accumulated_logging_time=1.225756, accumulated_submission_time=32803.071822, global_step=93364, preemption_count=0, score=32803.071822, test/accuracy=0.692778, test/bleu=0.380451, test/loss=1.405105, test/num_examples=3003, total_duration=66717.753428, train/accuracy=0.678066, train/bleu=0.887162, train/loss=1.510357, validation/accuracy=0.679570, validation/bleu=0.512789, validation/loss=1.503938, validation/num_examples=3000
I0216 14:19:30.120429 140416866297600 logging_writer.py:48] [93500] global_step=93500, grad_norm=0.22538545727729797, loss=1.5896461009979248
I0216 14:22:25.589556 140416857904896 logging_writer.py:48] [94000] global_step=94000, grad_norm=0.2295224368572235, loss=1.5484310388565063
I0216 14:25:21.124808 140416866297600 logging_writer.py:48] [94500] global_step=94500, grad_norm=0.22806429862976074, loss=1.5939404964447021
I0216 14:28:16.657977 140416857904896 logging_writer.py:48] [95000] global_step=95000, grad_norm=0.23738811910152435, loss=1.6175775527954102
I0216 14:31:12.236255 140416866297600 logging_writer.py:48] [95500] global_step=95500, grad_norm=0.2272031456232071, loss=1.6164418458938599
I0216 14:32:42.151540 140586562139968 spec.py:321] Evaluating on the training split.
I0216 14:32:45.092793 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 14:37:24.577617 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 14:37:24.577840 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 14:37:24.577891 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 14:37:25.752408 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 14:37:28.418560 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 14:42:07.104022 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 14:42:07.104223 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 14:42:07.104274 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 14:42:08.298674 140586562139968 spec.py:349] Evaluating on the test split.
I0216 14:42:10.965890 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 14:46:50.228174 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 14:46:50.228376 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 14:46:50.228424 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 14:46:51.630022 140586562139968 submission_runner.py:408] Time since start: 68407.32s, 	Step: 95758, 	{'train/accuracy': 0.6793121695518494, 'train/loss': 1.505177617073059, 'train/bleu': 0.8673642913620275, 'validation/accuracy': 0.6795700192451477, 'validation/loss': 1.5029948949813843, 'validation/bleu': 0.523359226292987, 'validation/num_examples': 3000, 'test/accuracy': 0.6928127408027649, 'test/loss': 1.4044814109802246, 'test/bleu': 0.386587649015786, 'test/num_examples': 3003, 'score': 33643.0678896904, 'total_duration': 68407.3227880001, 'accumulated_submission_time': 33643.0678896904, 'accumulated_eval_time': 34760.3210067749, 'accumulated_logging_time': 1.2555572986602783}
I0216 14:46:51.649548 140416857904896 logging_writer.py:48] [95758] accumulated_eval_time=34760.321007, accumulated_logging_time=1.255557, accumulated_submission_time=33643.067890, global_step=95758, preemption_count=0, score=33643.067890, test/accuracy=0.692813, test/bleu=0.386588, test/loss=1.404481, test/num_examples=3003, total_duration=68407.322788, train/accuracy=0.679312, train/bleu=0.867364, train/loss=1.505178, validation/accuracy=0.679570, validation/bleu=0.523359, validation/loss=1.502995, validation/num_examples=3000
I0216 14:48:16.815250 140416866297600 logging_writer.py:48] [96000] global_step=96000, grad_norm=0.21636724472045898, loss=1.4793552160263062
I0216 14:51:12.354916 140416857904896 logging_writer.py:48] [96500] global_step=96500, grad_norm=0.22910644114017487, loss=1.6047589778900146
I0216 14:54:07.894613 140416866297600 logging_writer.py:48] [97000] global_step=97000, grad_norm=0.23360353708267212, loss=1.5678565502166748
I0216 14:57:03.512143 140416857904896 logging_writer.py:48] [97500] global_step=97500, grad_norm=0.23504751920700073, loss=1.5983026027679443
I0216 14:59:59.048327 140416866297600 logging_writer.py:48] [98000] global_step=98000, grad_norm=0.22992102801799774, loss=1.5312150716781616
I0216 15:00:51.796297 140586562139968 spec.py:321] Evaluating on the training split.
I0216 15:00:54.736841 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 15:05:34.242738 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 15:05:34.242954 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 15:05:34.243007 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 15:05:35.223633 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 15:05:37.895376 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 15:10:17.549628 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 15:10:17.549858 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 15:10:17.549911 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 15:10:18.828366 140586562139968 spec.py:349] Evaluating on the test split.
I0216 15:10:21.497629 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 15:15:00.526469 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 15:15:00.526690 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 15:15:00.526743 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 15:15:02.026653 140586562139968 submission_runner.py:408] Time since start: 70097.72s, 	Step: 98152, 	{'train/accuracy': 0.6763709783554077, 'train/loss': 1.516389012336731, 'train/bleu': 0.8976345558363864, 'validation/accuracy': 0.6796568036079407, 'validation/loss': 1.5025255680084229, 'validation/bleu': 0.49151668342239696, 'validation/num_examples': 3000, 'test/accuracy': 0.6928359866142273, 'test/loss': 1.4041082859039307, 'test/bleu': 0.3693655299069083, 'test/num_examples': 3003, 'score': 34483.13967490196, 'total_duration': 70097.71943283081, 'accumulated_submission_time': 34483.13967490196, 'accumulated_eval_time': 35610.55132985115, 'accumulated_logging_time': 1.285590410232544}
I0216 15:15:02.046301 140416857904896 logging_writer.py:48] [98152] accumulated_eval_time=35610.551330, accumulated_logging_time=1.285590, accumulated_submission_time=34483.139675, global_step=98152, preemption_count=0, score=34483.139675, test/accuracy=0.692836, test/bleu=0.369366, test/loss=1.404108, test/num_examples=3003, total_duration=70097.719433, train/accuracy=0.676371, train/bleu=0.897635, train/loss=1.516389, validation/accuracy=0.679657, validation/bleu=0.491517, validation/loss=1.502526, validation/num_examples=3000
I0216 15:17:04.478429 140416866297600 logging_writer.py:48] [98500] global_step=98500, grad_norm=0.24377450346946716, loss=1.688995122909546
I0216 15:20:00.132818 140416857904896 logging_writer.py:48] [99000] global_step=99000, grad_norm=0.23233062028884888, loss=1.563533067703247
I0216 15:22:55.726427 140416866297600 logging_writer.py:48] [99500] global_step=99500, grad_norm=0.22533541917800903, loss=1.5541735887527466
I0216 15:25:51.373890 140416857904896 logging_writer.py:48] [100000] global_step=100000, grad_norm=0.23063361644744873, loss=1.617869257926941
I0216 15:28:46.974575 140416866297600 logging_writer.py:48] [100500] global_step=100500, grad_norm=0.23543132841587067, loss=1.607032299041748
I0216 15:29:02.155575 140586562139968 spec.py:321] Evaluating on the training split.
I0216 15:29:05.103305 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 15:33:44.583171 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 15:33:44.583375 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 15:33:44.583427 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 15:33:45.629481 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 15:33:48.302957 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 15:38:27.000457 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 15:38:27.000661 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 15:38:27.000711 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 15:38:28.278316 140586562139968 spec.py:349] Evaluating on the test split.
I0216 15:38:30.936324 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 15:43:09.166115 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 15:43:09.166310 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 15:43:09.166376 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 15:43:10.604605 140586562139968 submission_runner.py:408] Time since start: 71786.30s, 	Step: 100545, 	{'train/accuracy': 0.6781322360038757, 'train/loss': 1.5094672441482544, 'train/bleu': 0.8622555939881164, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 35323.17393708229, 'total_duration': 71786.29736566544, 'accumulated_submission_time': 35323.17393708229, 'accumulated_eval_time': 36459.000322818756, 'accumulated_logging_time': 1.3155901432037354}
I0216 15:43:10.624637 140416857904896 logging_writer.py:48] [100545] accumulated_eval_time=36459.000323, accumulated_logging_time=1.315590, accumulated_submission_time=35323.173937, global_step=100545, preemption_count=0, score=35323.173937, test/accuracy=0.692801, test/bleu=0.364778, test/loss=1.404211, test/num_examples=3003, total_duration=71786.297366, train/accuracy=0.678132, train/bleu=0.862256, train/loss=1.509467, validation/accuracy=0.679682, validation/bleu=0.495580, validation/loss=1.502655, validation/num_examples=3000
I0216 15:45:50.589705 140416866297600 logging_writer.py:48] [101000] global_step=101000, grad_norm=0.23139336705207825, loss=1.628406286239624
I0216 15:48:46.166648 140416857904896 logging_writer.py:48] [101500] global_step=101500, grad_norm=0.2277677357196808, loss=1.6013424396514893
I0216 15:51:41.779531 140416866297600 logging_writer.py:48] [102000] global_step=102000, grad_norm=0.22719913721084595, loss=1.6302571296691895
I0216 15:54:37.280227 140416857904896 logging_writer.py:48] [102500] global_step=102500, grad_norm=0.22978825867176056, loss=1.610938310623169
I0216 15:57:10.776643 140586562139968 spec.py:321] Evaluating on the training split.
I0216 15:57:13.725299 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 16:01:53.107030 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 16:01:53.107250 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 16:01:53.107303 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 16:01:54.200695 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 16:01:56.867865 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 16:06:35.572438 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 16:06:35.572639 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 16:06:35.572688 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 16:06:36.837667 140586562139968 spec.py:349] Evaluating on the test split.
I0216 16:06:39.505841 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 16:11:18.121391 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 16:11:18.121585 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 16:11:18.121635 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 16:11:19.540312 140586562139968 submission_runner.py:408] Time since start: 73475.23s, 	Step: 102939, 	{'train/accuracy': 0.6791573166847229, 'train/loss': 1.5019841194152832, 'train/bleu': 0.8796615660519261, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 36163.25124049187, 'total_duration': 73475.23304414749, 'accumulated_submission_time': 36163.25124049187, 'accumulated_eval_time': 37307.76391506195, 'accumulated_logging_time': 1.3453726768493652}
I0216 16:11:19.560729 140416866297600 logging_writer.py:48] [102939] accumulated_eval_time=37307.763915, accumulated_logging_time=1.345373, accumulated_submission_time=36163.251240, global_step=102939, preemption_count=0, score=36163.251240, test/accuracy=0.692801, test/bleu=0.364778, test/loss=1.404211, test/num_examples=3003, total_duration=73475.233044, train/accuracy=0.679157, train/bleu=0.879662, train/loss=1.501984, validation/accuracy=0.679682, validation/bleu=0.495580, validation/loss=1.502655, validation/num_examples=3000
I0216 16:11:41.297880 140416857904896 logging_writer.py:48] [103000] global_step=103000, grad_norm=0.2251804620027542, loss=1.5952718257904053
I0216 16:14:36.673336 140416866297600 logging_writer.py:48] [103500] global_step=103500, grad_norm=0.231646329164505, loss=1.5877368450164795
I0216 16:17:32.216730 140416857904896 logging_writer.py:48] [104000] global_step=104000, grad_norm=0.24883460998535156, loss=1.625439167022705
I0216 16:20:27.768614 140416866297600 logging_writer.py:48] [104500] global_step=104500, grad_norm=0.23015932738780975, loss=1.5955743789672852
I0216 16:23:23.325630 140416857904896 logging_writer.py:48] [105000] global_step=105000, grad_norm=0.22917819023132324, loss=1.6265803575515747
I0216 16:25:19.628371 140586562139968 spec.py:321] Evaluating on the training split.
I0216 16:25:22.563462 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 16:30:02.759644 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 16:30:02.759868 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 16:30:02.759920 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 16:30:03.929329 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 16:30:06.597325 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 16:34:45.482649 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 16:34:45.482862 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 16:34:45.482916 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 16:34:46.803173 140586562139968 spec.py:349] Evaluating on the test split.
I0216 16:34:49.469114 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 16:39:28.420858 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 16:39:28.421070 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 16:39:28.421123 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 16:39:29.883192 140586562139968 submission_runner.py:408] Time since start: 75165.58s, 	Step: 105333, 	{'train/accuracy': 0.6792973875999451, 'train/loss': 1.5029296875, 'train/bleu': 0.8759814466324114, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 37003.243255615234, 'total_duration': 75165.5759370327, 'accumulated_submission_time': 37003.243255615234, 'accumulated_eval_time': 38158.0186650753, 'accumulated_logging_time': 1.3764944076538086}
I0216 16:39:29.902993 140416866297600 logging_writer.py:48] [105333] accumulated_eval_time=38158.018665, accumulated_logging_time=1.376494, accumulated_submission_time=37003.243256, global_step=105333, preemption_count=0, score=37003.243256, test/accuracy=0.692801, test/bleu=0.364778, test/loss=1.404211, test/num_examples=3003, total_duration=75165.575937, train/accuracy=0.679297, train/bleu=0.875981, train/loss=1.502930, validation/accuracy=0.679682, validation/bleu=0.495580, validation/loss=1.502655, validation/num_examples=3000
I0216 16:40:28.789177 140416857904896 logging_writer.py:48] [105500] global_step=105500, grad_norm=0.23050197958946228, loss=1.5592433214187622
I0216 16:43:24.300193 140416866297600 logging_writer.py:48] [106000] global_step=106000, grad_norm=0.23099689185619354, loss=1.6619009971618652
I0216 16:46:19.847338 140416857904896 logging_writer.py:48] [106500] global_step=106500, grad_norm=0.2263106107711792, loss=1.6102150678634644
I0216 16:49:15.404050 140416866297600 logging_writer.py:48] [107000] global_step=107000, grad_norm=0.22998201847076416, loss=1.5057581663131714
I0216 16:52:10.977663 140416857904896 logging_writer.py:48] [107500] global_step=107500, grad_norm=0.2260725349187851, loss=1.6145122051239014
I0216 16:53:30.029700 140586562139968 spec.py:321] Evaluating on the training split.
I0216 16:53:32.986932 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 16:58:12.770637 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 16:58:12.770847 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 16:58:12.770901 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 16:58:13.958794 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 16:58:16.615127 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 17:02:55.231532 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 17:02:55.231740 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 17:02:55.231796 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 17:02:56.496375 140586562139968 spec.py:349] Evaluating on the test split.
I0216 17:02:59.157086 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 17:07:37.449176 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 17:07:37.449380 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 17:07:37.449433 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 17:07:38.862060 140586562139968 submission_runner.py:408] Time since start: 76854.55s, 	Step: 107727, 	{'train/accuracy': 0.6778503656387329, 'train/loss': 1.5166131258010864, 'train/bleu': 0.8659995256777958, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 37843.29526305199, 'total_duration': 76854.55484104156, 'accumulated_submission_time': 37843.29526305199, 'accumulated_eval_time': 39006.85099196434, 'accumulated_logging_time': 1.4052481651306152}
I0216 17:07:38.881552 140416866297600 logging_writer.py:48] [107727] accumulated_eval_time=39006.850992, accumulated_logging_time=1.405248, accumulated_submission_time=37843.295263, global_step=107727, preemption_count=0, score=37843.295263, test/accuracy=0.692801, test/bleu=0.364778, test/loss=1.404211, test/num_examples=3003, total_duration=76854.554841, train/accuracy=0.677850, train/bleu=0.866000, train/loss=1.516613, validation/accuracy=0.679682, validation/bleu=0.495580, validation/loss=1.502655, validation/num_examples=3000
I0216 17:09:14.935290 140416857904896 logging_writer.py:48] [108000] global_step=108000, grad_norm=0.22642776370048523, loss=1.6448653936386108
I0216 17:12:10.527632 140416866297600 logging_writer.py:48] [108500] global_step=108500, grad_norm=0.22181124985218048, loss=1.5397554636001587
I0216 17:15:06.094615 140416857904896 logging_writer.py:48] [109000] global_step=109000, grad_norm=0.2352754771709442, loss=1.6945308446884155
I0216 17:18:01.696029 140416866297600 logging_writer.py:48] [109500] global_step=109500, grad_norm=0.2348964512348175, loss=1.6503983736038208
I0216 17:20:57.307612 140416857904896 logging_writer.py:48] [110000] global_step=110000, grad_norm=0.23650267720222473, loss=1.6066750288009644
I0216 17:21:39.156404 140586562139968 spec.py:321] Evaluating on the training split.
I0216 17:21:42.104887 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 17:26:22.029630 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 17:26:22.029826 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 17:26:22.029875 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 17:26:23.135693 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 17:26:25.801524 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 17:31:04.441489 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 17:31:04.441711 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 17:31:04.441767 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 17:31:05.772395 140586562139968 spec.py:349] Evaluating on the test split.
I0216 17:31:08.435318 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 17:35:46.685485 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 17:35:46.685688 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 17:35:46.685740 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 17:35:48.109788 140586562139968 submission_runner.py:408] Time since start: 78543.80s, 	Step: 110121, 	{'train/accuracy': 0.6810750365257263, 'train/loss': 1.4913475513458252, 'train/bleu': 0.8142720261515743, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 38683.49532032013, 'total_duration': 78543.80255484581, 'accumulated_submission_time': 38683.49532032013, 'accumulated_eval_time': 39855.80432534218, 'accumulated_logging_time': 1.4346356391906738}
I0216 17:35:48.130105 140416866297600 logging_writer.py:48] [110121] accumulated_eval_time=39855.804325, accumulated_logging_time=1.434636, accumulated_submission_time=38683.495320, global_step=110121, preemption_count=0, score=38683.495320, test/accuracy=0.692801, test/bleu=0.364778, test/loss=1.404211, test/num_examples=3003, total_duration=78543.802555, train/accuracy=0.681075, train/bleu=0.814272, train/loss=1.491348, validation/accuracy=0.679682, validation/bleu=0.495580, validation/loss=1.502655, validation/num_examples=3000
I0216 17:38:01.403928 140416857904896 logging_writer.py:48] [110500] global_step=110500, grad_norm=0.23539088666439056, loss=1.633961796760559
I0216 17:40:56.908326 140416866297600 logging_writer.py:48] [111000] global_step=111000, grad_norm=0.23408973217010498, loss=1.6937203407287598
I0216 17:43:52.438202 140416857904896 logging_writer.py:48] [111500] global_step=111500, grad_norm=0.2312936782836914, loss=1.659284234046936
I0216 17:46:47.999785 140416866297600 logging_writer.py:48] [112000] global_step=112000, grad_norm=0.2328573316335678, loss=1.6249984502792358
I0216 17:49:43.545323 140416857904896 logging_writer.py:48] [112500] global_step=112500, grad_norm=0.23417356610298157, loss=1.646620512008667
I0216 17:49:48.191015 140586562139968 spec.py:321] Evaluating on the training split.
I0216 17:49:51.130043 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 17:54:30.894136 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 17:54:30.894339 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 17:54:30.894389 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 17:54:32.063780 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 17:54:34.728991 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 17:59:13.517075 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 17:59:13.517298 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 17:59:13.517352 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 17:59:14.802318 140586562139968 spec.py:349] Evaluating on the test split.
I0216 17:59:17.460029 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 18:03:55.827105 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 18:03:55.827312 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 18:03:55.827362 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 18:03:57.276445 140586562139968 submission_runner.py:408] Time since start: 80232.97s, 	Step: 112515, 	{'train/accuracy': 0.6797104477882385, 'train/loss': 1.5017367601394653, 'train/bleu': 0.8988757813813029, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 39523.48179721832, 'total_duration': 80232.96921467781, 'accumulated_submission_time': 39523.48179721832, 'accumulated_eval_time': 40704.88970685005, 'accumulated_logging_time': 1.4642624855041504}
I0216 18:03:57.296680 140416866297600 logging_writer.py:48] [112515] accumulated_eval_time=40704.889707, accumulated_logging_time=1.464262, accumulated_submission_time=39523.481797, global_step=112515, preemption_count=0, score=39523.481797, test/accuracy=0.692801, test/bleu=0.364778, test/loss=1.404211, test/num_examples=3003, total_duration=80232.969215, train/accuracy=0.679710, train/bleu=0.898876, train/loss=1.501737, validation/accuracy=0.679682, validation/bleu=0.495580, validation/loss=1.502655, validation/num_examples=3000
I0216 18:06:47.767901 140416857904896 logging_writer.py:48] [113000] global_step=113000, grad_norm=0.22388634085655212, loss=1.5566860437393188
I0216 18:09:43.334903 140416866297600 logging_writer.py:48] [113500] global_step=113500, grad_norm=0.2241709977388382, loss=1.5814064741134644
I0216 18:12:38.886788 140416857904896 logging_writer.py:48] [114000] global_step=114000, grad_norm=0.23078761994838715, loss=1.6353617906570435
I0216 18:15:34.469849 140416866297600 logging_writer.py:48] [114500] global_step=114500, grad_norm=0.23411160707473755, loss=1.5988794565200806
I0216 18:17:57.459927 140586562139968 spec.py:321] Evaluating on the training split.
I0216 18:18:00.407872 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 18:22:39.856987 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 18:22:39.857206 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 18:22:39.857258 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 18:22:41.016531 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 18:22:43.678821 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 18:27:21.949244 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 18:27:21.949445 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 18:27:21.949511 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 18:27:23.215080 140586562139968 spec.py:349] Evaluating on the test split.
I0216 18:27:25.874780 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 18:32:03.931150 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 18:32:03.931352 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 18:32:03.931403 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 18:32:05.336455 140586562139968 submission_runner.py:408] Time since start: 81921.03s, 	Step: 114909, 	{'train/accuracy': 0.67814040184021, 'train/loss': 1.5123997926712036, 'train/bleu': 0.8834111774587976, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 40363.57060956955, 'total_duration': 81921.0292289257, 'accumulated_submission_time': 40363.57060956955, 'accumulated_eval_time': 41552.766201496124, 'accumulated_logging_time': 1.4935240745544434}
I0216 18:32:05.356430 140416857904896 logging_writer.py:48] [114909] accumulated_eval_time=41552.766201, accumulated_logging_time=1.493524, accumulated_submission_time=40363.570610, global_step=114909, preemption_count=0, score=40363.570610, test/accuracy=0.692801, test/bleu=0.364778, test/loss=1.404211, test/num_examples=3003, total_duration=81921.029229, train/accuracy=0.678140, train/bleu=0.883411, train/loss=1.512400, validation/accuracy=0.679682, validation/bleu=0.495580, validation/loss=1.502655, validation/num_examples=3000
I0216 18:32:37.595949 140416866297600 logging_writer.py:48] [115000] global_step=115000, grad_norm=0.2289925515651703, loss=1.606045126914978
I0216 18:35:33.010264 140416857904896 logging_writer.py:48] [115500] global_step=115500, grad_norm=0.24302254617214203, loss=1.6814671754837036
I0216 18:38:28.540054 140416866297600 logging_writer.py:48] [116000] global_step=116000, grad_norm=0.23067250847816467, loss=1.6426728963851929
I0216 18:41:24.050960 140416857904896 logging_writer.py:48] [116500] global_step=116500, grad_norm=0.22948560118675232, loss=1.5714246034622192
I0216 18:44:19.594574 140416866297600 logging_writer.py:48] [117000] global_step=117000, grad_norm=0.22878964245319366, loss=1.6153028011322021
I0216 18:46:05.671162 140586562139968 spec.py:321] Evaluating on the training split.
I0216 18:46:08.607920 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 18:50:47.654710 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 18:50:47.654921 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 18:50:47.654977 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 18:50:48.756247 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 18:50:51.417087 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 18:55:29.822107 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 18:55:29.822300 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 18:55:29.822349 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 18:55:31.090403 140586562139968 spec.py:349] Evaluating on the test split.
I0216 18:55:33.761381 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 19:00:12.290124 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 19:00:12.290338 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 19:00:12.290393 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 19:00:13.694637 140586562139968 submission_runner.py:408] Time since start: 83609.39s, 	Step: 117304, 	{'train/accuracy': 0.6779496669769287, 'train/loss': 1.5125502347946167, 'train/bleu': 0.7821541450059453, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 41203.812079668045, 'total_duration': 83609.38738274574, 'accumulated_submission_time': 41203.812079668045, 'accumulated_eval_time': 42400.789622306824, 'accumulated_logging_time': 1.522200584411621}
I0216 19:00:13.714240 140416857904896 logging_writer.py:48] [117304] accumulated_eval_time=42400.789622, accumulated_logging_time=1.522201, accumulated_submission_time=41203.812080, global_step=117304, preemption_count=0, score=41203.812080, test/accuracy=0.692801, test/bleu=0.364778, test/loss=1.404211, test/num_examples=3003, total_duration=83609.387383, train/accuracy=0.677950, train/bleu=0.782154, train/loss=1.512550, validation/accuracy=0.679682, validation/bleu=0.495580, validation/loss=1.502655, validation/num_examples=3000
I0216 19:01:22.690151 140416866297600 logging_writer.py:48] [117500] global_step=117500, grad_norm=0.23403286933898926, loss=1.6166287660598755
I0216 19:04:18.195817 140416857904896 logging_writer.py:48] [118000] global_step=118000, grad_norm=0.2363714873790741, loss=1.6808512210845947
I0216 19:07:13.780104 140416866297600 logging_writer.py:48] [118500] global_step=118500, grad_norm=0.2261439859867096, loss=1.5788447856903076
I0216 19:10:09.307980 140416857904896 logging_writer.py:48] [119000] global_step=119000, grad_norm=0.22796185314655304, loss=1.5706791877746582
I0216 19:13:04.843158 140416866297600 logging_writer.py:48] [119500] global_step=119500, grad_norm=0.23193511366844177, loss=1.598103642463684
I0216 19:14:13.766864 140586562139968 spec.py:321] Evaluating on the training split.
I0216 19:14:16.710914 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 19:18:55.915828 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 19:18:55.916032 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 19:18:55.916082 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 19:18:57.060547 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 19:18:59.722827 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 19:23:38.014129 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 19:23:38.014366 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 19:23:38.014424 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 19:23:39.279288 140586562139968 spec.py:349] Evaluating on the test split.
I0216 19:23:41.955416 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 19:28:20.087104 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 19:28:20.087317 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 19:28:20.087377 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 19:28:21.535385 140586562139968 submission_runner.py:408] Time since start: 85297.23s, 	Step: 119698, 	{'train/accuracy': 0.6783627867698669, 'train/loss': 1.5092692375183105, 'train/bleu': 0.8736388961001806, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 42043.790246486664, 'total_duration': 85297.22816991806, 'accumulated_submission_time': 42043.790246486664, 'accumulated_eval_time': 43248.55811715126, 'accumulated_logging_time': 1.551501989364624}
I0216 19:28:21.555701 140416857904896 logging_writer.py:48] [119698] accumulated_eval_time=43248.558117, accumulated_logging_time=1.551502, accumulated_submission_time=42043.790246, global_step=119698, preemption_count=0, score=42043.790246, test/accuracy=0.692801, test/bleu=0.364778, test/loss=1.404211, test/num_examples=3003, total_duration=85297.228170, train/accuracy=0.678363, train/bleu=0.873639, train/loss=1.509269, validation/accuracy=0.679682, validation/bleu=0.495580, validation/loss=1.502655, validation/num_examples=3000
I0216 19:30:07.801814 140416866297600 logging_writer.py:48] [120000] global_step=120000, grad_norm=0.224982351064682, loss=1.605460524559021
I0216 19:33:03.323307 140416857904896 logging_writer.py:48] [120500] global_step=120500, grad_norm=0.22684147953987122, loss=1.5255078077316284
I0216 19:35:58.866296 140416866297600 logging_writer.py:48] [121000] global_step=121000, grad_norm=0.23030850291252136, loss=1.59853196144104
I0216 19:38:54.415681 140416857904896 logging_writer.py:48] [121500] global_step=121500, grad_norm=0.232788547873497, loss=1.657301664352417
I0216 19:41:49.970498 140416866297600 logging_writer.py:48] [122000] global_step=122000, grad_norm=0.22566737234592438, loss=1.570650577545166
I0216 19:42:21.646511 140586562139968 spec.py:321] Evaluating on the training split.
I0216 19:42:24.582748 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 19:47:04.805988 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 19:47:04.806207 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 19:47:04.806283 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 19:47:05.958304 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 19:47:08.626885 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 19:51:47.682427 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 19:51:47.682619 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 19:51:47.682668 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 19:51:48.939679 140586562139968 spec.py:349] Evaluating on the test split.
I0216 19:51:51.601183 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 19:56:31.004872 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 19:56:31.005149 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 19:56:31.005204 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 19:56:32.423229 140586562139968 submission_runner.py:408] Time since start: 86988.12s, 	Step: 122092, 	{'train/accuracy': 0.6781395077705383, 'train/loss': 1.5147134065628052, 'train/bleu': 0.8441222750957387, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 42883.80718421936, 'total_duration': 86988.11598706245, 'accumulated_submission_time': 42883.80718421936, 'accumulated_eval_time': 44099.33477449417, 'accumulated_logging_time': 1.5809645652770996}
I0216 19:56:32.444733 140416857904896 logging_writer.py:48] [122092] accumulated_eval_time=44099.334774, accumulated_logging_time=1.580965, accumulated_submission_time=42883.807184, global_step=122092, preemption_count=0, score=42883.807184, test/accuracy=0.692801, test/bleu=0.364778, test/loss=1.404211, test/num_examples=3003, total_duration=86988.115987, train/accuracy=0.678140, train/bleu=0.844122, train/loss=1.514713, validation/accuracy=0.679682, validation/bleu=0.495580, validation/loss=1.502655, validation/num_examples=3000
I0216 19:58:55.870451 140416866297600 logging_writer.py:48] [122500] global_step=122500, grad_norm=0.22984547913074493, loss=1.6250109672546387
I0216 20:01:51.434201 140416857904896 logging_writer.py:48] [123000] global_step=123000, grad_norm=0.230139821767807, loss=1.643859624862671
I0216 20:04:47.029255 140416866297600 logging_writer.py:48] [123500] global_step=123500, grad_norm=0.22724710404872894, loss=1.5993523597717285
I0216 20:07:42.588269 140416857904896 logging_writer.py:48] [124000] global_step=124000, grad_norm=0.22864919900894165, loss=1.5967367887496948
I0216 20:10:32.577668 140586562139968 spec.py:321] Evaluating on the training split.
I0216 20:10:35.525341 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 20:15:14.595358 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 20:15:14.595575 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 20:15:14.595626 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 20:15:15.808614 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 20:15:18.481168 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 20:19:56.620174 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 20:19:56.620367 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 20:19:56.620413 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 20:19:57.887024 140586562139968 spec.py:349] Evaluating on the test split.
I0216 20:20:00.547498 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 20:24:38.987247 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 20:24:38.987447 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 20:24:38.987494 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 20:24:40.478507 140586562139968 submission_runner.py:408] Time since start: 88676.17s, 	Step: 124486, 	{'train/accuracy': 0.6764692664146423, 'train/loss': 1.5242559909820557, 'train/bleu': 0.8952322732039091, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 43723.86380791664, 'total_duration': 88676.17128062248, 'accumulated_submission_time': 43723.86380791664, 'accumulated_eval_time': 44947.23557949066, 'accumulated_logging_time': 1.6131224632263184}
I0216 20:24:40.500129 140416866297600 logging_writer.py:48] [124486] accumulated_eval_time=44947.235579, accumulated_logging_time=1.613122, accumulated_submission_time=43723.863808, global_step=124486, preemption_count=0, score=43723.863808, test/accuracy=0.692801, test/bleu=0.364778, test/loss=1.404211, test/num_examples=3003, total_duration=88676.171281, train/accuracy=0.676469, train/bleu=0.895232, train/loss=1.524256, validation/accuracy=0.679682, validation/bleu=0.495580, validation/loss=1.502655, validation/num_examples=3000
I0216 20:24:45.767106 140416857904896 logging_writer.py:48] [124500] global_step=124500, grad_norm=0.22678862512111664, loss=1.5450814962387085
I0216 20:27:41.090616 140416866297600 logging_writer.py:48] [125000] global_step=125000, grad_norm=0.23002561926841736, loss=1.515315294265747
I0216 20:30:36.664992 140416857904896 logging_writer.py:48] [125500] global_step=125500, grad_norm=0.22943338751792908, loss=1.5745635032653809
I0216 20:33:32.144598 140416866297600 logging_writer.py:48] [126000] global_step=126000, grad_norm=0.22797273099422455, loss=1.62262761592865
I0216 20:36:27.710969 140416857904896 logging_writer.py:48] [126500] global_step=126500, grad_norm=0.2227957546710968, loss=1.5283411741256714
I0216 20:38:40.529170 140586562139968 spec.py:321] Evaluating on the training split.
I0216 20:38:43.477498 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 20:43:23.103637 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 20:43:23.103852 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 20:43:23.103912 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 20:43:24.118073 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 20:43:26.776364 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 20:48:05.575829 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 20:48:05.576044 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 20:48:05.576094 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 20:48:06.840942 140586562139968 spec.py:349] Evaluating on the test split.
I0216 20:48:09.505506 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 20:52:48.542056 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 20:52:48.542256 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 20:52:48.542305 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 20:52:49.956372 140586562139968 submission_runner.py:408] Time since start: 90365.65s, 	Step: 126880, 	{'train/accuracy': 0.6770322322845459, 'train/loss': 1.5215171575546265, 'train/bleu': 0.8859733050844208, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 44563.81946134567, 'total_duration': 90365.64916229248, 'accumulated_submission_time': 44563.81946134567, 'accumulated_eval_time': 45796.6627702713, 'accumulated_logging_time': 1.6438257694244385}
I0216 20:52:49.977177 140416866297600 logging_writer.py:48] [126880] accumulated_eval_time=45796.662770, accumulated_logging_time=1.643826, accumulated_submission_time=44563.819461, global_step=126880, preemption_count=0, score=44563.819461, test/accuracy=0.692801, test/bleu=0.364778, test/loss=1.404211, test/num_examples=3003, total_duration=90365.649162, train/accuracy=0.677032, train/bleu=0.885973, train/loss=1.521517, validation/accuracy=0.679682, validation/bleu=0.495580, validation/loss=1.502655, validation/num_examples=3000
I0216 20:53:32.374127 140416857904896 logging_writer.py:48] [127000] global_step=127000, grad_norm=0.23522865772247314, loss=1.5575900077819824
I0216 20:56:27.843946 140416866297600 logging_writer.py:48] [127500] global_step=127500, grad_norm=0.22654543817043304, loss=1.6001908779144287
I0216 20:59:23.395090 140416857904896 logging_writer.py:48] [128000] global_step=128000, grad_norm=0.23776637017726898, loss=1.6816389560699463
I0216 21:02:18.920371 140416866297600 logging_writer.py:48] [128500] global_step=128500, grad_norm=0.22795666754245758, loss=1.624346375465393
I0216 21:05:14.504858 140416857904896 logging_writer.py:48] [129000] global_step=129000, grad_norm=0.22854848206043243, loss=1.6265136003494263
I0216 21:06:50.070653 140586562139968 spec.py:321] Evaluating on the training split.
I0216 21:06:53.014876 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 21:11:32.209511 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 21:11:32.209724 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 21:11:32.209774 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 21:11:33.273533 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 21:11:35.937276 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 21:16:14.385824 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 21:16:14.386079 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 21:16:14.386129 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 21:16:15.646417 140586562139968 spec.py:349] Evaluating on the test split.
I0216 21:16:18.308899 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 21:20:56.832261 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 21:20:56.832458 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 21:20:56.832507 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 21:20:58.250364 140586562139968 submission_runner.py:408] Time since start: 92053.94s, 	Step: 129274, 	{'train/accuracy': 0.6785484552383423, 'train/loss': 1.5130327939987183, 'train/bleu': 0.8773451833303113, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 45403.8377289772, 'total_duration': 92053.94316267967, 'accumulated_submission_time': 45403.8377289772, 'accumulated_eval_time': 46644.84246993065, 'accumulated_logging_time': 1.6754138469696045}
I0216 21:20:58.271485 140416866297600 logging_writer.py:48] [129274] accumulated_eval_time=46644.842470, accumulated_logging_time=1.675414, accumulated_submission_time=45403.837729, global_step=129274, preemption_count=0, score=45403.837729, test/accuracy=0.692801, test/bleu=0.364778, test/loss=1.404211, test/num_examples=3003, total_duration=92053.943163, train/accuracy=0.678548, train/bleu=0.877345, train/loss=1.513033, validation/accuracy=0.679682, validation/bleu=0.495580, validation/loss=1.502655, validation/num_examples=3000
I0216 21:22:17.840539 140416857904896 logging_writer.py:48] [129500] global_step=129500, grad_norm=0.22919300198554993, loss=1.6020327806472778
I0216 21:25:13.330528 140416866297600 logging_writer.py:48] [130000] global_step=130000, grad_norm=0.2329823523759842, loss=1.59740149974823
I0216 21:28:08.887731 140416857904896 logging_writer.py:48] [130500] global_step=130500, grad_norm=0.23190917074680328, loss=1.6356028318405151
I0216 21:31:04.415469 140416866297600 logging_writer.py:48] [131000] global_step=131000, grad_norm=0.2214840054512024, loss=1.5867732763290405
I0216 21:33:59.963891 140416857904896 logging_writer.py:48] [131500] global_step=131500, grad_norm=0.24638326466083527, loss=1.6696183681488037
I0216 21:34:58.311772 140586562139968 spec.py:321] Evaluating on the training split.
I0216 21:35:01.252874 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 21:39:41.214537 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 21:39:41.214789 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 21:39:41.214843 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 21:39:42.344520 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 21:39:45.011674 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 21:44:24.395336 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 21:44:24.395544 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 21:44:24.395593 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 21:44:25.682118 140586562139968 spec.py:349] Evaluating on the test split.
I0216 21:44:28.349198 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 21:49:06.944914 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 21:49:06.945142 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 21:49:06.945195 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 21:49:08.380991 140586562139968 submission_runner.py:408] Time since start: 93744.07s, 	Step: 131668, 	{'train/accuracy': 0.6794716119766235, 'train/loss': 1.4994914531707764, 'train/bleu': 0.8765173463913584, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 46243.803193092346, 'total_duration': 93744.07378149033, 'accumulated_submission_time': 46243.803193092346, 'accumulated_eval_time': 47494.91166520119, 'accumulated_logging_time': 1.7066540718078613}
I0216 21:49:08.401833 140416866297600 logging_writer.py:48] [131668] accumulated_eval_time=47494.911665, accumulated_logging_time=1.706654, accumulated_submission_time=46243.803193, global_step=131668, preemption_count=0, score=46243.803193, test/accuracy=0.692801, test/bleu=0.364778, test/loss=1.404211, test/num_examples=3003, total_duration=93744.073781, train/accuracy=0.679472, train/bleu=0.876517, train/loss=1.499491, validation/accuracy=0.679682, validation/bleu=0.495580, validation/loss=1.502655, validation/num_examples=3000
I0216 21:51:05.212158 140416857904896 logging_writer.py:48] [132000] global_step=132000, grad_norm=0.227087140083313, loss=1.6484171152114868
I0216 21:54:00.747009 140416866297600 logging_writer.py:48] [132500] global_step=132500, grad_norm=0.22817300260066986, loss=1.6586307287216187
I0216 21:56:56.246028 140416857904896 logging_writer.py:48] [133000] global_step=133000, grad_norm=0.2225990742444992, loss=1.6534138917922974
I0216 21:58:52.558311 140586562139968 spec.py:321] Evaluating on the training split.
I0216 21:58:55.511920 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 22:03:34.356257 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 22:03:34.356477 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 22:03:34.356528 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 22:03:35.547851 140586562139968 spec.py:333] Evaluating on the validation split.
I0216 22:03:38.216542 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 22:08:16.992922 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 22:08:16.993153 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 22:08:16.993206 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 22:08:18.273957 140586562139968 spec.py:349] Evaluating on the test split.
I0216 22:08:20.941879 140586562139968 workload.py:181] Translating evaluation dataset.
W0216 22:12:59.394165 140586562139968 bleu.py:65] That's 100 lines that end in a tokenized period ('.')
W0216 22:12:59.394366 140586562139968 bleu.py:67] It looks like you forgot to detokenize your test data, which may hurt your score.
W0216 22:12:59.394415 140586562139968 bleu.py:69] If you insist your data is detokenized, or don't care, you can suppress this message with '--force'.
I0216 22:13:00.805742 140586562139968 submission_runner.py:408] Time since start: 95176.50s, 	Step: 133333, 	{'train/accuracy': 0.6741835474967957, 'train/loss': 1.5339800119400024, 'train/bleu': 0.8956700961044436, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 46827.90598034859, 'total_duration': 95176.49853992462, 'accumulated_submission_time': 46827.90598034859, 'accumulated_eval_time': 48343.15908789635, 'accumulated_logging_time': 1.736173391342163}
I0216 22:13:00.826584 140416866297600 logging_writer.py:48] [133333] accumulated_eval_time=48343.159088, accumulated_logging_time=1.736173, accumulated_submission_time=46827.905980, global_step=133333, preemption_count=0, score=46827.905980, test/accuracy=0.692801, test/bleu=0.364778, test/loss=1.404211, test/num_examples=3003, total_duration=95176.498540, train/accuracy=0.674184, train/bleu=0.895670, train/loss=1.533980, validation/accuracy=0.679682, validation/bleu=0.495580, validation/loss=1.502655, validation/num_examples=3000
I0216 22:13:00.847070 140416857904896 logging_writer.py:48] [133333] global_step=133333, preemption_count=0, score=46827.905980
I0216 22:13:01.958580 140586562139968 checkpoints.py:490] Saving checkpoint at step: 133333
I0216 22:13:05.754997 140586562139968 checkpoints.py:422] Saved checkpoint at /experiment_runs/variants_target_setting/study_0/wmt_attention_temp_jax/trial_1/checkpoint_133333
I0216 22:13:05.760009 140586562139968 checkpoint_utils.py:240] Saved checkpoint to /experiment_runs/variants_target_setting/study_0/wmt_attention_temp_jax/trial_1/checkpoint_133333.
I0216 22:13:05.851299 140586562139968 submission_runner.py:583] Tuning trial 1/1
I0216 22:13:05.851464 140586562139968 submission_runner.py:584] Hyperparameters: Hyperparameters(learning_rate=0.0003477912008450351, beta1=0.9936632117510711, beta2=0.9967873550453692, warmup_steps=9999, weight_decay=0.04120183162940475, label_smoothing=0.0)
I0216 22:13:05.856418 140586562139968 submission_runner.py:585] Metrics: {'eval_results': [(1, {'train/accuracy': 0.0006170864216983318, 'train/loss': 10.976629257202148, 'train/bleu': 0.0, 'validation/accuracy': 0.0004835649742744863, 'validation/loss': 10.968364715576172, 'validation/bleu': 0.0, 'validation/num_examples': 3000, 'test/accuracy': 0.0007088489946909249, 'test/loss': 10.973079681396484, 'test/bleu': 0.0, 'test/num_examples': 3003, 'score': 39.107139110565186, 'total_duration': 907.612909078598, 'accumulated_submission_time': 39.107139110565186, 'accumulated_eval_time': 868.5057301521301, 'accumulated_logging_time': 0, 'global_step': 1, 'preemption_count': 0}), (2389, {'train/accuracy': 0.2458128184080124, 'train/loss': 5.745951175689697, 'train/bleu': 2.3478506353623434, 'validation/accuracy': 0.22865183651447296, 'validation/loss': 5.938811302185059, 'validation/bleu': 0.6698880617744746, 'validation/num_examples': 3000, 'test/accuracy': 0.21127186715602875, 'test/loss': 6.220549583435059, 'test/bleu': 0.6373000744320632, 'test/num_examples': 3003, 'score': 879.1433081626892, 'total_duration': 2592.6995985507965, 'accumulated_submission_time': 879.1433081626892, 'accumulated_eval_time': 1713.4505932331085, 'accumulated_logging_time': 0.028407812118530273, 'global_step': 2389, 'preemption_count': 0}), (4780, {'train/accuracy': 0.3771827220916748, 'train/loss': 4.170366287231445, 'train/bleu': 7.110246717048642, 'validation/accuracy': 0.3570321500301361, 'validation/loss': 4.344737529754639, 'validation/bleu': 3.82068771895165, 'validation/num_examples': 3000, 'test/accuracy': 0.33905062079429626, 'test/loss': 4.592438220977783, 'test/bleu': 2.4781251846158154, 'test/num_examples': 3003, 'score': 1719.1542096138, 'total_duration': 4276.986010313034, 'accumulated_submission_time': 1719.1542096138, 'accumulated_eval_time': 2557.6265757083893, 'accumulated_logging_time': 0.05470848083496094, 'global_step': 4780, 'preemption_count': 0}), (7171, {'train/accuracy': 0.5011522769927979, 'train/loss': 3.0411250591278076, 'train/bleu': 7.239529476871624, 'validation/accuracy': 0.49081847071647644, 'validation/loss': 3.1386725902557373, 'validation/bleu': 3.8575186767326137, 'validation/num_examples': 3000, 'test/accuracy': 0.48685145378112793, 'test/loss': 3.223426342010498, 'test/bleu': 2.8498597498391947, 'test/num_examples': 3003, 'score': 2559.1901428699493, 'total_duration': 5963.420837640762, 'accumulated_submission_time': 2559.1901428699493, 'accumulated_eval_time': 3403.926973104477, 'accumulated_logging_time': 0.08030128479003906, 'global_step': 7171, 'preemption_count': 0}), (9562, {'train/accuracy': 0.5441297888755798, 'train/loss': 2.622191905975342, 'train/bleu': 4.742225935986713, 'validation/accuracy': 0.5483502745628357, 'validation/loss': 2.5766243934631348, 'validation/bleu': 2.2991891435125753, 'validation/num_examples': 3000, 'test/accuracy': 0.5496717095375061, 'test/loss': 2.597043991088867, 'test/bleu': 1.3319131884672153, 'test/num_examples': 3003, 'score': 3399.323217153549, 'total_duration': 7652.61395573616, 'accumulated_submission_time': 3399.323217153549, 'accumulated_eval_time': 4252.885382413864, 'accumulated_logging_time': 0.10583043098449707, 'global_step': 9562, 'preemption_count': 0}), (11955, {'train/accuracy': 0.5713308453559875, 'train/loss': 2.3595962524414062, 'train/bleu': 3.0603431391387312, 'validation/accuracy': 0.5816543102264404, 'validation/loss': 2.272934675216675, 'validation/bleu': 1.5253956424916264, 'validation/num_examples': 3000, 'test/accuracy': 0.5853117108345032, 'test/loss': 2.2633817195892334, 'test/bleu': 1.0750510952926602, 'test/num_examples': 3003, 'score': 4239.564382314682, 'total_duration': 9339.111065149307, 'accumulated_submission_time': 4239.564382314682, 'accumulated_eval_time': 5099.042484998703, 'accumulated_logging_time': 0.13095474243164062, 'global_step': 11955, 'preemption_count': 0}), (14347, {'train/accuracy': 0.592117965221405, 'train/loss': 2.192519187927246, 'train/bleu': 2.572998072339457, 'validation/accuracy': 0.6020879745483398, 'validation/loss': 2.1017370223999023, 'validation/bleu': 1.3006765295304064, 'validation/num_examples': 3000, 'test/accuracy': 0.6065539717674255, 'test/loss': 2.06955623626709, 'test/bleu': 0.7895075148902687, 'test/num_examples': 3003, 'score': 5079.616499185562, 'total_duration': 11023.947231054306, 'accumulated_submission_time': 5079.616499185562, 'accumulated_eval_time': 5943.72082233429, 'accumulated_logging_time': 0.1576526165008545, 'global_step': 14347, 'preemption_count': 0}), (16740, {'train/accuracy': 0.6027103662490845, 'train/loss': 2.089191198348999, 'train/bleu': 1.909251600344247, 'validation/accuracy': 0.6182564496994019, 'validation/loss': 1.9873697757720947, 'validation/bleu': 0.8648089967580966, 'validation/num_examples': 3000, 'test/accuracy': 0.6199872493743896, 'test/loss': 1.9505449533462524, 'test/bleu': 0.551373965315805, 'test/num_examples': 3003, 'score': 5919.531648874283, 'total_duration': 12709.68831896782, 'accumulated_submission_time': 5919.531648874283, 'accumulated_eval_time': 6789.37309551239, 'accumulated_logging_time': 0.26473236083984375, 'global_step': 16740, 'preemption_count': 0}), (19134, {'train/accuracy': 0.6255302429199219, 'train/loss': 1.9116209745407104, 'train/bleu': 1.7755983770221007, 'validation/accuracy': 0.6259934902191162, 'validation/loss': 1.9125136137008667, 'validation/bleu': 0.6522328774810654, 'validation/num_examples': 3000, 'test/accuracy': 0.6307477951049805, 'test/loss': 1.8631607294082642, 'test/bleu': 0.4865417445374533, 'test/num_examples': 3003, 'score': 6759.755045175552, 'total_duration': 14396.398558139801, 'accumulated_submission_time': 6759.755045175552, 'accumulated_eval_time': 7635.769635438919, 'accumulated_logging_time': 0.28987789154052734, 'global_step': 19134, 'preemption_count': 0}), (21528, {'train/accuracy': 0.6189650893211365, 'train/loss': 1.9530251026153564, 'train/bleu': 1.5233909675692299, 'validation/accuracy': 0.6333337426185608, 'validation/loss': 1.8469154834747314, 'validation/bleu': 0.6503184151196942, 'validation/num_examples': 3000, 'test/accuracy': 0.6401371359825134, 'test/loss': 1.798127293586731, 'test/bleu': 0.43692168087954864, 'test/num_examples': 3003, 'score': 7599.916054487228, 'total_duration': 16082.482307434082, 'accumulated_submission_time': 7599.916054487228, 'accumulated_eval_time': 8481.519130945206, 'accumulated_logging_time': 0.39734649658203125, 'global_step': 21528, 'preemption_count': 0}), (23922, {'train/accuracy': 0.6264045834541321, 'train/loss': 1.9005932807922363, 'train/bleu': 1.8108257407715773, 'validation/accuracy': 0.639446496963501, 'validation/loss': 1.8049089908599854, 'validation/bleu': 0.6060991441563214, 'validation/num_examples': 3000, 'test/accuracy': 0.6456801295280457, 'test/loss': 1.7494745254516602, 'test/bleu': 0.5081726353399283, 'test/num_examples': 3003, 'score': 8440.124709129333, 'total_duration': 17770.539270162582, 'accumulated_submission_time': 8440.124709129333, 'accumulated_eval_time': 9329.276204586029, 'accumulated_logging_time': 0.4225015640258789, 'global_step': 23922, 'preemption_count': 0}), (26315, {'train/accuracy': 0.6323251724243164, 'train/loss': 1.8344998359680176, 'train/bleu': 1.595450098951098, 'validation/accuracy': 0.6446292996406555, 'validation/loss': 1.7704890966415405, 'validation/bleu': 0.6017523069566243, 'validation/num_examples': 3000, 'test/accuracy': 0.651548445224762, 'test/loss': 1.7074114084243774, 'test/bleu': 0.3858701982181147, 'test/num_examples': 3003, 'score': 9280.168601751328, 'total_duration': 19456.467591762543, 'accumulated_submission_time': 9280.168601751328, 'accumulated_eval_time': 10175.066358089447, 'accumulated_logging_time': 0.44989681243896484, 'global_step': 26315, 'preemption_count': 0}), (28709, {'train/accuracy': 0.6365097761154175, 'train/loss': 1.825494647026062, 'train/bleu': 1.4926065213202608, 'validation/accuracy': 0.6482250690460205, 'validation/loss': 1.7388099431991577, 'validation/bleu': 0.6741754080099109, 'validation/num_examples': 3000, 'test/accuracy': 0.6547789573669434, 'test/loss': 1.675727367401123, 'test/bleu': 0.4481941289102528, 'test/num_examples': 3003, 'score': 10120.359531641006, 'total_duration': 21142.21505522728, 'accumulated_submission_time': 10120.359531641006, 'accumulated_eval_time': 11020.529445886612, 'accumulated_logging_time': 0.4766552448272705, 'global_step': 28709, 'preemption_count': 0}), (31103, {'train/accuracy': 0.6336763501167297, 'train/loss': 1.8281629085540771, 'train/bleu': 1.4566051252849477, 'validation/accuracy': 0.6489565968513489, 'validation/loss': 1.7132289409637451, 'validation/bleu': 0.5816486971273545, 'validation/num_examples': 3000, 'test/accuracy': 0.6585439443588257, 'test/loss': 1.6435586214065552, 'test/bleu': 0.3551857944276261, 'test/num_examples': 3003, 'score': 10960.46658539772, 'total_duration': 22828.262434005737, 'accumulated_submission_time': 10960.46658539772, 'accumulated_eval_time': 11866.377401351929, 'accumulated_logging_time': 0.5028150081634521, 'global_step': 31103, 'preemption_count': 0}), (33497, {'train/accuracy': 0.6402148604393005, 'train/loss': 1.7716810703277588, 'train/bleu': 1.6260398873301878, 'validation/accuracy': 0.6542014479637146, 'validation/loss': 1.6838712692260742, 'validation/bleu': 0.6654986692691285, 'validation/num_examples': 3000, 'test/accuracy': 0.6640055775642395, 'test/loss': 1.6144667863845825, 'test/bleu': 0.45739866682040153, 'test/num_examples': 3003, 'score': 11800.581804990768, 'total_duration': 24513.25865149498, 'accumulated_submission_time': 11800.581804990768, 'accumulated_eval_time': 12711.165947198868, 'accumulated_logging_time': 0.5292415618896484, 'global_step': 33497, 'preemption_count': 0}), (35892, {'train/accuracy': 0.6402845978736877, 'train/loss': 1.7787836790084839, 'train/bleu': 1.2516411376263012, 'validation/accuracy': 0.6563217043876648, 'validation/loss': 1.670474886894226, 'validation/bleu': 0.6268425038122815, 'validation/num_examples': 3000, 'test/accuracy': 0.6651095151901245, 'test/loss': 1.5953525304794312, 'test/bleu': 0.44507029163516837, 'test/num_examples': 3003, 'score': 12640.81961107254, 'total_duration': 26199.411789417267, 'accumulated_submission_time': 12640.81961107254, 'accumulated_eval_time': 13556.988714933395, 'accumulated_logging_time': 0.5560722351074219, 'global_step': 35892, 'preemption_count': 0}), (38286, {'train/accuracy': 0.6507250070571899, 'train/loss': 1.7041484117507935, 'train/bleu': 1.417769145384822, 'validation/accuracy': 0.6598182320594788, 'validation/loss': 1.6482818126678467, 'validation/bleu': 0.591868313438796, 'validation/num_examples': 3000, 'test/accuracy': 0.6699901223182678, 'test/loss': 1.5757300853729248, 'test/bleu': 0.54473681838998, 'test/num_examples': 3003, 'score': 13480.947542667389, 'total_duration': 27885.39377474785, 'accumulated_submission_time': 13480.947542667389, 'accumulated_eval_time': 14402.75132727623, 'accumulated_logging_time': 0.5813107490539551, 'global_step': 38286, 'preemption_count': 0}), (40680, {'train/accuracy': 0.6515622138977051, 'train/loss': 1.694055438041687, 'train/bleu': 1.5096183992532668, 'validation/accuracy': 0.6602646112442017, 'validation/loss': 1.635361909866333, 'validation/bleu': 0.77385307958933, 'validation/num_examples': 3000, 'test/accuracy': 0.6713381409645081, 'test/loss': 1.5545804500579834, 'test/bleu': 0.603416237450704, 'test/num_examples': 3003, 'score': 14320.882312297821, 'total_duration': 29572.85657787323, 'accumulated_submission_time': 14320.882312297821, 'accumulated_eval_time': 15250.18584060669, 'accumulated_logging_time': 0.6085410118103027, 'global_step': 40680, 'preemption_count': 0}), (43074, {'train/accuracy': 0.6454535722732544, 'train/loss': 1.737009048461914, 'train/bleu': 1.26618241468236, 'validation/accuracy': 0.6624964475631714, 'validation/loss': 1.6190532445907593, 'validation/bleu': 0.5638200000200458, 'validation/num_examples': 3000, 'test/accuracy': 0.672930121421814, 'test/loss': 1.540385365486145, 'test/bleu': 0.5401706488289604, 'test/num_examples': 3003, 'score': 15160.891214609146, 'total_duration': 31260.10849881172, 'accumulated_submission_time': 15160.891214609146, 'accumulated_eval_time': 16097.333564043045, 'accumulated_logging_time': 0.6381494998931885, 'global_step': 43074, 'preemption_count': 0}), (45469, {'train/accuracy': 0.6534406542778015, 'train/loss': 1.6706839799880981, 'train/bleu': 1.2442829359955467, 'validation/accuracy': 0.6639347076416016, 'validation/loss': 1.6081516742706299, 'validation/bleu': 0.7162678380131551, 'validation/num_examples': 3000, 'test/accuracy': 0.6749172210693359, 'test/loss': 1.5294806957244873, 'test/bleu': 0.5581449950107031, 'test/num_examples': 3003, 'score': 16001.05147242546, 'total_duration': 32948.33805012703, 'accumulated_submission_time': 16001.05147242546, 'accumulated_eval_time': 16945.31041121483, 'accumulated_logging_time': 0.6646959781646729, 'global_step': 45469, 'preemption_count': 0}), (47863, {'train/accuracy': 0.6578680276870728, 'train/loss': 1.656836748123169, 'train/bleu': 1.0303258298156999, 'validation/accuracy': 0.6661293506622314, 'validation/loss': 1.5941377878189087, 'validation/bleu': 0.41895513156296066, 'validation/num_examples': 3000, 'test/accuracy': 0.6771367192268372, 'test/loss': 1.5114972591400146, 'test/bleu': 0.407981647858489, 'test/num_examples': 3003, 'score': 16841.11019229889, 'total_duration': 34635.40250277519, 'accumulated_submission_time': 16841.11019229889, 'accumulated_eval_time': 17792.223201036453, 'accumulated_logging_time': 0.6907575130462646, 'global_step': 47863, 'preemption_count': 0}), (50258, {'train/accuracy': 0.6672159433364868, 'train/loss': 1.5876699686050415, 'train/bleu': 1.1638583005517393, 'validation/accuracy': 0.6682124137878418, 'validation/loss': 1.5849769115447998, 'validation/bleu': 0.5235669011807379, 'validation/num_examples': 3000, 'test/accuracy': 0.6785892844200134, 'test/loss': 1.5016307830810547, 'test/bleu': 0.4389196144382269, 'test/num_examples': 3003, 'score': 17681.194860458374, 'total_duration': 36322.323357105255, 'accumulated_submission_time': 17681.194860458374, 'accumulated_eval_time': 18638.96478843689, 'accumulated_logging_time': 0.719064474105835, 'global_step': 50258, 'preemption_count': 0}), (52653, {'train/accuracy': 0.6567549705505371, 'train/loss': 1.646889567375183, 'train/bleu': 0.9231937481106066, 'validation/accuracy': 0.6681876182556152, 'validation/loss': 1.5754570960998535, 'validation/bleu': 0.4099691875837352, 'validation/num_examples': 3000, 'test/accuracy': 0.6799721121788025, 'test/loss': 1.490198016166687, 'test/bleu': 0.3212871340753205, 'test/num_examples': 3003, 'score': 18521.335946798325, 'total_duration': 38011.07923364639, 'accumulated_submission_time': 18521.335946798325, 'accumulated_eval_time': 19487.487874031067, 'accumulated_logging_time': 0.7451581954956055, 'global_step': 52653, 'preemption_count': 0}), (55048, {'train/accuracy': 0.6556821465492249, 'train/loss': 1.6571015119552612, 'train/bleu': 0.9509054557339609, 'validation/accuracy': 0.6710518002510071, 'validation/loss': 1.5655437707901, 'validation/bleu': 0.49818585396014065, 'validation/num_examples': 3000, 'test/accuracy': 0.6825053691864014, 'test/loss': 1.4769991636276245, 'test/bleu': 0.37356293285647507, 'test/num_examples': 3003, 'score': 19361.51351070404, 'total_duration': 39698.704710006714, 'accumulated_submission_time': 19361.51351070404, 'accumulated_eval_time': 20334.841116428375, 'accumulated_logging_time': 0.7738635540008545, 'global_step': 55048, 'preemption_count': 0}), (57443, {'train/accuracy': 0.6662589311599731, 'train/loss': 1.5912539958953857, 'train/bleu': 1.1457023591394118, 'validation/accuracy': 0.6710641980171204, 'validation/loss': 1.5589473247528076, 'validation/bleu': 0.5604829885413425, 'validation/num_examples': 3000, 'test/accuracy': 0.6835047602653503, 'test/loss': 1.4732749462127686, 'test/bleu': 0.445304647093264, 'test/num_examples': 3003, 'score': 20201.538850545883, 'total_duration': 41387.52809906006, 'accumulated_submission_time': 20201.538850545883, 'accumulated_eval_time': 21183.54577088356, 'accumulated_logging_time': 0.8022444248199463, 'global_step': 57443, 'preemption_count': 0}), (59838, {'train/accuracy': 0.6639460921287537, 'train/loss': 1.6035712957382202, 'train/bleu': 0.9855532535203474, 'validation/accuracy': 0.672576904296875, 'validation/loss': 1.5494859218597412, 'validation/bleu': 0.4860498560049654, 'validation/num_examples': 3000, 'test/accuracy': 0.684980571269989, 'test/loss': 1.4584156274795532, 'test/bleu': 0.3622107624670763, 'test/num_examples': 3003, 'score': 21041.721590042114, 'total_duration': 43075.25318646431, 'accumulated_submission_time': 21041.721590042114, 'accumulated_eval_time': 22030.996285915375, 'accumulated_logging_time': 0.8286912441253662, 'global_step': 59838, 'preemption_count': 0}), (62233, {'train/accuracy': 0.6640580296516418, 'train/loss': 1.5954301357269287, 'train/bleu': 1.1335177448441809, 'validation/accuracy': 0.6729612946510315, 'validation/loss': 1.5441699028015137, 'validation/bleu': 0.6258434561373543, 'validation/num_examples': 3000, 'test/accuracy': 0.6856196522712708, 'test/loss': 1.4512062072753906, 'test/bleu': 0.461853140693794, 'test/num_examples': 3003, 'score': 21881.728234052658, 'total_duration': 44762.46363568306, 'accumulated_submission_time': 21881.728234052658, 'accumulated_eval_time': 22878.107117652893, 'accumulated_logging_time': 0.8560175895690918, 'global_step': 62233, 'preemption_count': 0}), (64628, {'train/accuracy': 0.6661494374275208, 'train/loss': 1.583825945854187, 'train/bleu': 0.9069875523650686, 'validation/accuracy': 0.6742135882377625, 'validation/loss': 1.5388855934143066, 'validation/bleu': 0.4506918771675515, 'validation/num_examples': 3000, 'test/accuracy': 0.6861310005187988, 'test/loss': 1.4480352401733398, 'test/bleu': 0.33708735080485613, 'test/num_examples': 3003, 'score': 22721.80181837082, 'total_duration': 46452.4663734436, 'accumulated_submission_time': 22721.80181837082, 'accumulated_eval_time': 23727.94140267372, 'accumulated_logging_time': 0.8857722282409668, 'global_step': 64628, 'preemption_count': 0}), (67023, {'train/accuracy': 0.6694967150688171, 'train/loss': 1.5648820400238037, 'train/bleu': 0.8859116524858676, 'validation/accuracy': 0.6753419041633606, 'validation/loss': 1.5318435430526733, 'validation/bleu': 0.4282227714884274, 'validation/num_examples': 3000, 'test/accuracy': 0.6872349381446838, 'test/loss': 1.440364956855774, 'test/bleu': 0.3590375786218571, 'test/num_examples': 3003, 'score': 23561.999462604523, 'total_duration': 48140.406368494034, 'accumulated_submission_time': 23561.999462604523, 'accumulated_eval_time': 24575.591722249985, 'accumulated_logging_time': 0.9125311374664307, 'global_step': 67023, 'preemption_count': 0}), (69418, {'train/accuracy': 0.6704220175743103, 'train/loss': 1.5547288656234741, 'train/bleu': 0.8951874215421436, 'validation/accuracy': 0.6762966513633728, 'validation/loss': 1.526335597038269, 'validation/bleu': 0.505989789839016, 'validation/num_examples': 3000, 'test/accuracy': 0.6887223720550537, 'test/loss': 1.4339213371276855, 'test/bleu': 0.38671856162936147, 'test/num_examples': 3003, 'score': 24402.038233280182, 'total_duration': 49829.90833616257, 'accumulated_submission_time': 24402.038233280182, 'accumulated_eval_time': 25424.9625351429, 'accumulated_logging_time': 0.9403717517852783, 'global_step': 69418, 'preemption_count': 0}), (71813, {'train/accuracy': 0.6714939475059509, 'train/loss': 1.5575363636016846, 'train/bleu': 0.8775270270332404, 'validation/accuracy': 0.6763338446617126, 'validation/loss': 1.5243443250656128, 'validation/bleu': 0.49728411492731145, 'validation/num_examples': 3000, 'test/accuracy': 0.6880367398262024, 'test/loss': 1.430014729499817, 'test/bleu': 0.33933316738732233, 'test/num_examples': 3003, 'score': 25242.228439569473, 'total_duration': 51519.2904586792, 'accumulated_submission_time': 25242.228439569473, 'accumulated_eval_time': 26274.05986571312, 'accumulated_logging_time': 0.969036340713501, 'global_step': 71813, 'preemption_count': 0}), (74208, {'train/accuracy': 0.6704035401344299, 'train/loss': 1.5615100860595703, 'train/bleu': 1.1562162523198845, 'validation/accuracy': 0.6765817999839783, 'validation/loss': 1.5207356214523315, 'validation/bleu': 0.6622418176941481, 'validation/num_examples': 3000, 'test/accuracy': 0.689233660697937, 'test/loss': 1.4266027212142944, 'test/bleu': 0.5315353316533425, 'test/num_examples': 3003, 'score': 26082.38233447075, 'total_duration': 53205.99332904816, 'accumulated_submission_time': 26082.38233447075, 'accumulated_eval_time': 27120.515478372574, 'accumulated_logging_time': 0.9975152015686035, 'global_step': 74208, 'preemption_count': 0}), (76603, {'train/accuracy': 0.6772430539131165, 'train/loss': 1.516606092453003, 'train/bleu': 0.8452351623278295, 'validation/accuracy': 0.6767677664756775, 'validation/loss': 1.5164719820022583, 'validation/bleu': 0.4772785874902205, 'validation/num_examples': 3000, 'test/accuracy': 0.6904653906822205, 'test/loss': 1.42171049118042, 'test/bleu': 0.3224086687519668, 'test/num_examples': 3003, 'score': 26922.495762586594, 'total_duration': 54894.699543237686, 'accumulated_submission_time': 26922.495762586594, 'accumulated_eval_time': 27969.01446557045, 'accumulated_logging_time': 1.0263376235961914, 'global_step': 76603, 'preemption_count': 0}), (78998, {'train/accuracy': 0.6727237105369568, 'train/loss': 1.5408540964126587, 'train/bleu': 0.9136755580149148, 'validation/accuracy': 0.6779333353042603, 'validation/loss': 1.5134196281433105, 'validation/bleu': 0.533290576541226, 'validation/num_examples': 3000, 'test/accuracy': 0.691104531288147, 'test/loss': 1.4155763387680054, 'test/bleu': 0.41150791933791214, 'test/num_examples': 3003, 'score': 27762.519449949265, 'total_duration': 56583.30054783821, 'accumulated_submission_time': 27762.519449949265, 'accumulated_eval_time': 28817.49755859375, 'accumulated_logging_time': 1.0559632778167725, 'global_step': 78998, 'preemption_count': 0}), (81393, {'train/accuracy': 0.6805446743965149, 'train/loss': 1.5007292032241821, 'train/bleu': 0.9495280757496707, 'validation/accuracy': 0.678230881690979, 'validation/loss': 1.5101262331008911, 'validation/bleu': 0.5609860201541438, 'validation/num_examples': 3000, 'test/accuracy': 0.6907559037208557, 'test/loss': 1.414063572883606, 'test/bleu': 0.4774863126295314, 'test/num_examples': 3003, 'score': 28602.75276017189, 'total_duration': 58272.826583623886, 'accumulated_submission_time': 28602.75276017189, 'accumulated_eval_time': 29666.697316408157, 'accumulated_logging_time': 1.0847837924957275, 'global_step': 81393, 'preemption_count': 0}), (83787, {'train/accuracy': 0.6764903664588928, 'train/loss': 1.5295292139053345, 'train/bleu': 0.859073984275965, 'validation/accuracy': 0.678900420665741, 'validation/loss': 1.506435751914978, 'validation/bleu': 0.546376316060547, 'validation/num_examples': 3000, 'test/accuracy': 0.6920109391212463, 'test/loss': 1.4107131958007812, 'test/bleu': 0.41058208983964545, 'test/num_examples': 3003, 'score': 29442.746134757996, 'total_duration': 59961.42941093445, 'accumulated_submission_time': 29442.746134757996, 'accumulated_eval_time': 30515.214382648468, 'accumulated_logging_time': 1.1125695705413818, 'global_step': 83787, 'preemption_count': 0}), (86181, {'train/accuracy': 0.6746113300323486, 'train/loss': 1.5305556058883667, 'train/bleu': 0.9416664847486931, 'validation/accuracy': 0.6792352199554443, 'validation/loss': 1.5046359300613403, 'validation/bleu': 0.52443764656063, 'validation/num_examples': 3000, 'test/accuracy': 0.6923711895942688, 'test/loss': 1.4073827266693115, 'test/bleu': 0.3845887708964356, 'test/num_examples': 3003, 'score': 30282.707558631897, 'total_duration': 61651.106850624084, 'accumulated_submission_time': 30282.707558631897, 'accumulated_eval_time': 31364.837710142136, 'accumulated_logging_time': 1.1406257152557373, 'global_step': 86181, 'preemption_count': 0}), (88575, {'train/accuracy': 0.6760222911834717, 'train/loss': 1.521141767501831, 'train/bleu': 1.0150668833560517, 'validation/accuracy': 0.6793220043182373, 'validation/loss': 1.5058515071868896, 'validation/bleu': 0.6247547808560047, 'validation/num_examples': 3000, 'test/accuracy': 0.6922317147254944, 'test/loss': 1.407009243965149, 'test/bleu': 0.4774770852085636, 'test/num_examples': 3003, 'score': 31122.81944990158, 'total_duration': 63339.22094464302, 'accumulated_submission_time': 31122.81944990158, 'accumulated_eval_time': 32212.74788045883, 'accumulated_logging_time': 1.1686944961547852, 'global_step': 88575, 'preemption_count': 0}), (90970, {'train/accuracy': 0.67889803647995, 'train/loss': 1.5041018724441528, 'train/bleu': 0.9045160978581913, 'validation/accuracy': 0.678900420665741, 'validation/loss': 1.5031237602233887, 'validation/bleu': 0.515080399985915, 'validation/num_examples': 3000, 'test/accuracy': 0.6921852231025696, 'test/loss': 1.404951810836792, 'test/bleu': 0.38004671592859546, 'test/num_examples': 3003, 'score': 31963.053261756897, 'total_duration': 65028.950191259384, 'accumulated_submission_time': 31963.053261756897, 'accumulated_eval_time': 33062.15080952644, 'accumulated_logging_time': 1.1970951557159424, 'global_step': 90970, 'preemption_count': 0}), (93364, {'train/accuracy': 0.6780661940574646, 'train/loss': 1.5103566646575928, 'train/bleu': 0.8871624495665741, 'validation/accuracy': 0.6795700192451477, 'validation/loss': 1.5039381980895996, 'validation/bleu': 0.5127885878338722, 'validation/num_examples': 3000, 'test/accuracy': 0.6927778720855713, 'test/loss': 1.4051051139831543, 'test/bleu': 0.3804507336463232, 'test/num_examples': 3003, 'score': 32803.07182216644, 'total_duration': 66717.75342822075, 'accumulated_submission_time': 32803.07182216644, 'accumulated_eval_time': 33910.84256863594, 'accumulated_logging_time': 1.2257564067840576, 'global_step': 93364, 'preemption_count': 0}), (95758, {'train/accuracy': 0.6793121695518494, 'train/loss': 1.505177617073059, 'train/bleu': 0.8673642913620275, 'validation/accuracy': 0.6795700192451477, 'validation/loss': 1.5029948949813843, 'validation/bleu': 0.523359226292987, 'validation/num_examples': 3000, 'test/accuracy': 0.6928127408027649, 'test/loss': 1.4044814109802246, 'test/bleu': 0.386587649015786, 'test/num_examples': 3003, 'score': 33643.0678896904, 'total_duration': 68407.3227880001, 'accumulated_submission_time': 33643.0678896904, 'accumulated_eval_time': 34760.3210067749, 'accumulated_logging_time': 1.2555572986602783, 'global_step': 95758, 'preemption_count': 0}), (98152, {'train/accuracy': 0.6763709783554077, 'train/loss': 1.516389012336731, 'train/bleu': 0.8976345558363864, 'validation/accuracy': 0.6796568036079407, 'validation/loss': 1.5025255680084229, 'validation/bleu': 0.49151668342239696, 'validation/num_examples': 3000, 'test/accuracy': 0.6928359866142273, 'test/loss': 1.4041082859039307, 'test/bleu': 0.3693655299069083, 'test/num_examples': 3003, 'score': 34483.13967490196, 'total_duration': 70097.71943283081, 'accumulated_submission_time': 34483.13967490196, 'accumulated_eval_time': 35610.55132985115, 'accumulated_logging_time': 1.285590410232544, 'global_step': 98152, 'preemption_count': 0}), (100545, {'train/accuracy': 0.6781322360038757, 'train/loss': 1.5094672441482544, 'train/bleu': 0.8622555939881164, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 35323.17393708229, 'total_duration': 71786.29736566544, 'accumulated_submission_time': 35323.17393708229, 'accumulated_eval_time': 36459.000322818756, 'accumulated_logging_time': 1.3155901432037354, 'global_step': 100545, 'preemption_count': 0}), (102939, {'train/accuracy': 0.6791573166847229, 'train/loss': 1.5019841194152832, 'train/bleu': 0.8796615660519261, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 36163.25124049187, 'total_duration': 73475.23304414749, 'accumulated_submission_time': 36163.25124049187, 'accumulated_eval_time': 37307.76391506195, 'accumulated_logging_time': 1.3453726768493652, 'global_step': 102939, 'preemption_count': 0}), (105333, {'train/accuracy': 0.6792973875999451, 'train/loss': 1.5029296875, 'train/bleu': 0.8759814466324114, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 37003.243255615234, 'total_duration': 75165.5759370327, 'accumulated_submission_time': 37003.243255615234, 'accumulated_eval_time': 38158.0186650753, 'accumulated_logging_time': 1.3764944076538086, 'global_step': 105333, 'preemption_count': 0}), (107727, {'train/accuracy': 0.6778503656387329, 'train/loss': 1.5166131258010864, 'train/bleu': 0.8659995256777958, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 37843.29526305199, 'total_duration': 76854.55484104156, 'accumulated_submission_time': 37843.29526305199, 'accumulated_eval_time': 39006.85099196434, 'accumulated_logging_time': 1.4052481651306152, 'global_step': 107727, 'preemption_count': 0}), (110121, {'train/accuracy': 0.6810750365257263, 'train/loss': 1.4913475513458252, 'train/bleu': 0.8142720261515743, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 38683.49532032013, 'total_duration': 78543.80255484581, 'accumulated_submission_time': 38683.49532032013, 'accumulated_eval_time': 39855.80432534218, 'accumulated_logging_time': 1.4346356391906738, 'global_step': 110121, 'preemption_count': 0}), (112515, {'train/accuracy': 0.6797104477882385, 'train/loss': 1.5017367601394653, 'train/bleu': 0.8988757813813029, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 39523.48179721832, 'total_duration': 80232.96921467781, 'accumulated_submission_time': 39523.48179721832, 'accumulated_eval_time': 40704.88970685005, 'accumulated_logging_time': 1.4642624855041504, 'global_step': 112515, 'preemption_count': 0}), (114909, {'train/accuracy': 0.67814040184021, 'train/loss': 1.5123997926712036, 'train/bleu': 0.8834111774587976, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 40363.57060956955, 'total_duration': 81921.0292289257, 'accumulated_submission_time': 40363.57060956955, 'accumulated_eval_time': 41552.766201496124, 'accumulated_logging_time': 1.4935240745544434, 'global_step': 114909, 'preemption_count': 0}), (117304, {'train/accuracy': 0.6779496669769287, 'train/loss': 1.5125502347946167, 'train/bleu': 0.7821541450059453, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 41203.812079668045, 'total_duration': 83609.38738274574, 'accumulated_submission_time': 41203.812079668045, 'accumulated_eval_time': 42400.789622306824, 'accumulated_logging_time': 1.522200584411621, 'global_step': 117304, 'preemption_count': 0}), (119698, {'train/accuracy': 0.6783627867698669, 'train/loss': 1.5092692375183105, 'train/bleu': 0.8736388961001806, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 42043.790246486664, 'total_duration': 85297.22816991806, 'accumulated_submission_time': 42043.790246486664, 'accumulated_eval_time': 43248.55811715126, 'accumulated_logging_time': 1.551501989364624, 'global_step': 119698, 'preemption_count': 0}), (122092, {'train/accuracy': 0.6781395077705383, 'train/loss': 1.5147134065628052, 'train/bleu': 0.8441222750957387, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 42883.80718421936, 'total_duration': 86988.11598706245, 'accumulated_submission_time': 42883.80718421936, 'accumulated_eval_time': 44099.33477449417, 'accumulated_logging_time': 1.5809645652770996, 'global_step': 122092, 'preemption_count': 0}), (124486, {'train/accuracy': 0.6764692664146423, 'train/loss': 1.5242559909820557, 'train/bleu': 0.8952322732039091, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 43723.86380791664, 'total_duration': 88676.17128062248, 'accumulated_submission_time': 43723.86380791664, 'accumulated_eval_time': 44947.23557949066, 'accumulated_logging_time': 1.6131224632263184, 'global_step': 124486, 'preemption_count': 0}), (126880, {'train/accuracy': 0.6770322322845459, 'train/loss': 1.5215171575546265, 'train/bleu': 0.8859733050844208, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 44563.81946134567, 'total_duration': 90365.64916229248, 'accumulated_submission_time': 44563.81946134567, 'accumulated_eval_time': 45796.6627702713, 'accumulated_logging_time': 1.6438257694244385, 'global_step': 126880, 'preemption_count': 0}), (129274, {'train/accuracy': 0.6785484552383423, 'train/loss': 1.5130327939987183, 'train/bleu': 0.8773451833303113, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 45403.8377289772, 'total_duration': 92053.94316267967, 'accumulated_submission_time': 45403.8377289772, 'accumulated_eval_time': 46644.84246993065, 'accumulated_logging_time': 1.6754138469696045, 'global_step': 129274, 'preemption_count': 0}), (131668, {'train/accuracy': 0.6794716119766235, 'train/loss': 1.4994914531707764, 'train/bleu': 0.8765173463913584, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 46243.803193092346, 'total_duration': 93744.07378149033, 'accumulated_submission_time': 46243.803193092346, 'accumulated_eval_time': 47494.91166520119, 'accumulated_logging_time': 1.7066540718078613, 'global_step': 131668, 'preemption_count': 0}), (133333, {'train/accuracy': 0.6741835474967957, 'train/loss': 1.5339800119400024, 'train/bleu': 0.8956700961044436, 'validation/accuracy': 0.6796815991401672, 'validation/loss': 1.502655267715454, 'validation/bleu': 0.49558015436111147, 'validation/num_examples': 3000, 'test/accuracy': 0.6928011178970337, 'test/loss': 1.4042105674743652, 'test/bleu': 0.364778410349466, 'test/num_examples': 3003, 'score': 46827.90598034859, 'total_duration': 95176.49853992462, 'accumulated_submission_time': 46827.90598034859, 'accumulated_eval_time': 48343.15908789635, 'accumulated_logging_time': 1.736173391342163, 'global_step': 133333, 'preemption_count': 0})], 'global_step': 133333}
I0216 22:13:05.856579 140586562139968 submission_runner.py:586] Timing: 46827.90598034859
I0216 22:13:05.856630 140586562139968 submission_runner.py:588] Total number of evals: 57
I0216 22:13:05.856673 140586562139968 submission_runner.py:589] ====================
I0216 22:13:05.856828 140586562139968 submission_runner.py:673] Final wmt_attention_temp score: 46827.90598034859
